<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Go Singleflight导致死锁问题分析]]></title>
      <url>http://team.jiunile.com/blog/2020/09/go-singleflight-deadlock.html</url>
      <content type="html"><![CDATA[<h2 id="思路排查"><a href="#思路排查" class="headerlink" title="思路排查"></a>思路排查</h2><h3 id="Dump-堆栈很重要"><a href="#Dump-堆栈很重要" class="headerlink" title="Dump 堆栈很重要"></a>Dump 堆栈很重要</h3><p>线上某个环境发现 S3 上传请求卡住，请求不返回，卡了30分钟，长时间没有发现有效日志。一般来讲，死锁问题还是好排查的，因为现场一般都在。类似于 c 程序，遇到死锁问题都会用 pstack 看一把。golang 死锁排查思路也类似（golang 不适合使用 pstack，因为 golang 调度的是协程，pstack 只能看到线程栈），我们其实是需要知道 S3 程序里 goroutine 的栈状态。golang 遇到这个问题我们有两个办法：</p>
<ol>
<li>方法一：条件允许的话，gcore 出一个堆栈，这个是最有效的方法，因为是把整个 golang 程序的内存镜像 dump 出来，然后用 dlv 分析</li>
<li>方法二：如果你提前开启 net/pprof 库的引用，开启了 debug 接口，那么就可以调用 curl 接口，通过 http 接口获取进程的状态信息</li>
</ol>
<p>需要注意到，golang 程序和 c 程序还是有点区别，goroutine 非常多，成百上千个 goroutine 是常态，甚至上万个也不稀奇。所以我们一般无法在终端上直接看完所有的栈，一般都是把所有的 goroutine 栈 dump 到文件，然用 vi 打开慢慢分析。</p>
<ul>
<li>调试这个 core 文件，意图从堆栈里找到些东西，由于堆栈太多了，所以就使用 <code>gorouties -t -u</code> 这个命令，并且把输出 dump 到文件；</li>
<li><code>curl xxx/debug/pprof/goroutine</code></li>
</ul>
<a id="more"></a>
<h3 id="关键思路"><a href="#关键思路" class="headerlink" title="关键思路"></a>关键思路</h3><p>成千上万个 goroutine ，直接显示到终端是不合适的，我们 dump 到文件 test.txt，然后分析 test.txt 这个文件。<strong>去查找发现了一些可疑堆栈，那么什么是可疑堆栈？重点关注加锁等待的堆栈，关键字是 <code>runtime_notifyListWait</code> 、<code>semaphore</code> 、<code>sync.(*Cond).Wait</code> 、<code>Acquire</code>  这些阻塞场景才会用到的，如果业务堆栈上出现这个加锁调用，就非常可疑</strong>。</p>
<p><strong>划重点：</strong></p>
<ol>
<li>留意阻塞关键字 <code>runtime_notifyListWait</code> 、<code>semaphore</code> 、<code>sync.(*Cond).Wait</code> 、<code>Acquire</code></li>
<li>业务堆栈（非 runtime 的一些内部堆栈）</li>
</ol>
<p><img src="/images/go/singleflight01.png" alt="singleflight-heap"></p>
<p>统计分析发现，有 11 个这个堆栈都在这同一个地方，都是在等同一把锁 <code>blockingKeyCountLimit.lock</code>，所以基本确认了阻塞的位置，就是这个地方阻塞到了所有的请求，但是这把锁我们使用 defer 释放的，使用姿势如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// do someting</span></span><br><span class="line">lock.Acquire(key)</span><br><span class="line"><span class="keyword">defer</span> lock.Release(key)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下为锁内操作；</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>blockingKeyCountLimit 是我们封装针对 key 操作流控的组件。举个例子，如果 limit == 1，key为 “test” 在 g1 上 Acquire 成功，g2 acquire(“test”) 就会等待，这个可以算是我们优化的一个逻辑。如果 limit == 2，那么就允许两个人加锁到，后面的人都等待。</p>
</blockquote>
<p>从代码来看，函数退出一定会释放的，但是偏偏现在锁就卡在这个地方，所以就非常奇怪。我们先找哪个 goroutine 占着这把锁不释放，看看能不能搞清楚怎样导致这里抢不到锁的原因。</p>
<p>通过审查业务代码分析，发现可能的源头函数（这个函数是向后端请求的函数）：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">api.(*Client).getBytesNolc</span><br></pre></td></tr></table></figure></p>
<p>确认是 <code>getBytesNolc</code> 这个函数执行的操作，那么大概率就是卡在这个地方了。用这个 <code>getBytesNolc</code> 字符串搜索堆栈，找下是哪个堆栈 ？搜索到这个堆栈 <code>goroutine 19458</code><br><img src="/images/go/singleflight02.png" alt="singleflight-heap"></p>
<p>大概率就是第 1 个堆栈了，也就是其他的 11 个 goroutine 都在等这 <code>goroutine 19458</code>  来放锁，仔细看这个堆栈。那么为啥这个堆栈不放锁呢？这里有个细节要注意下，这里是卡到 <code>gihub.com/golang/groupcache/singleflight/singleflight.go:48</code> 这一行：<br><img src="/images/go/singleflight03.png" alt="singleflight"></p>
<p>这是一个开源库，singleflight 实现了缓存防击穿的功能。</p>
<blockquote>
<p>简单介绍下 <code>singleflight</code> 的功能，这是一个非常有效的工具。在缓存大量失效的场景，如果针对同一个 key ，其实只需要有一个人穿透到后端请求数据，其他人等待他完成，然后取缓存结果即可。这个就是 <code>singleflight</code> 实现的功能。具体实现就是：来了请求之后，把 key 插入到 map 里，后面的请求如果发现同名 key 在 map 里面，那么就等待它完成就好；</p>
</blockquote>
<p>截屏显示卡到 <code>c.wg.Wait()</code> 这一行，那么说明 map 里面肯定有已经存在的 key，说明 <code>goroutine 19458</code> 不是第一个人？但是外面还有一个 <code>blockingKeyCountLimit</code> 的互斥呢，按道理其他的人也进不来（因为 limit == 1），这里这么讲来肯定要是源头才对？</p>
<h3 id="思路整理"><a href="#思路整理" class="headerlink" title="思路整理"></a>思路整理</h3><p>伪代码显示如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">xxx</span> <span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 大部分协程都卡在这里（11个）</span></span><br><span class="line">    <span class="comment">// 这个锁的效果主要是流控，limit 值初始化赋值，可以是 1，也可以是其他；</span></span><br><span class="line">    <span class="comment">// locker 为 blockingKeyCountLimit 类型</span></span><br><span class="line">    limitLocker.Acquire( key )</span><br><span class="line">    <span class="keyword">defer</span> limitLocker.Release( key )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取数据</span></span><br><span class="line">    getBytesNolc( key , ...)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getBytesNolc</span> <span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 下面就是 singleflight.Group 的用法，防穿透</span></span><br><span class="line">    <span class="comment">// 同一时间只允许一个人去后端更新</span></span><br><span class="line">    ret, err = x.Group.Do(id, <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="params">(<span class="keyword">interface</span>&#123;&#125;, error)</span></span> &#123;</span><br><span class="line">        <span class="comment">// 去服务后台获取，更新数据；</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>图示显示当前的现状：<br><img src="/images/go/singleflight04.png" alt="singleflight"></p>
<p>现状小结：</p>
<ol>
<li>大量的协程都在等 <code>blockingKeyCountLimit</code> 这把锁释放；</li>
<li>协程 <code>goroutine 19458</code> 持有 <code>blockingKeyCountLimit</code> 这把锁；</li>
<li>协程 <code>goroutine 19458</code>  却在等一个相同 key 名字的任务的完成（ <code>singleflight</code> 一个防击穿的库，同一时间相同 key 只允许放到一个后端去执行），却永远没等到，协程因此呈现死锁；</li>
</ol>
<p>当前的疑问就是第一个 key 的任务为啥永远完不成，堆栈也找不到了，去哪里了？</p>
<h3 id="发现蛛丝马迹"><a href="#发现蛛丝马迹" class="headerlink" title="发现蛛丝马迹"></a>发现蛛丝马迹</h3><p>我们再仔细审一下 <code>singleflight</code> 的代码：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *Group)</span> <span class="title">Do</span><span class="params">(key <span class="keyword">string</span>, fn <span class="keyword">func</span>()</span> <span class="params">(<span class="keyword">interface</span>&#123;&#125;, error)</span>) <span class="params">(<span class="keyword">interface</span>&#123;&#125;, error)</span></span> &#123;</span><br><span class="line">    g.mu.Lock()</span><br><span class="line">    <span class="keyword">if</span> g.m == <span class="literal">nil</span> &#123;</span><br><span class="line">        g.m = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]*call)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果找到同名 key 已经存在；</span></span><br><span class="line">    <span class="keyword">if</span> c, ok := g.m[key]; ok &#123;</span><br><span class="line">        g.mu.Unlock()</span><br><span class="line">        <span class="comment">// 等待者走到这个分支：等待第一个人执行完成，最后直接返回它的结果就行了；</span></span><br><span class="line">        c.wg.Wait()</span><br><span class="line">        <span class="keyword">return</span> c.val, c.err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果同名 key 不存在（第一个人走到这个分支）</span></span><br><span class="line">    c := <span class="built_in">new</span>(call)</span><br><span class="line">    c.wg.Add(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// map 里放置 key</span></span><br><span class="line">    g.m[key] = c</span><br><span class="line">    g.mu.Unlock()</span><br><span class="line">    <span class="comment">// 执行任务</span></span><br><span class="line">    c.val, c.err = fn()</span><br><span class="line">    <span class="comment">// 唤醒所有的等待者</span></span><br><span class="line">    c.wg.Done()</span><br><span class="line"></span><br><span class="line">    g.mu.Lock()</span><br><span class="line">    <span class="comment">// 删除 map 里的 key</span></span><br><span class="line">    <span class="built_in">delete</span>(g.m, key)</span><br><span class="line">    g.mu.Unlock()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c.val, c.err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>发现有个线索，我们的 S3 服务程序一个 http 请求对应一个协程处理，为了提高服务端进程的可用性，在框架里会捕捉 <code>panic</code>，这样确保单个协程处理不会影响到其他的请求。基于这个前提，我们假设：如果 <code>fn()</code> 执行异常，<code>panic</code> 掉了，那么就不会走 <code>delete(g.m, key)</code> 的代码，那么 key 就永远都残留在 map 里面，而进程却又还活着。恍然大悟。</p>
<h3 id="完整的推理流程"><a href="#完整的推理流程" class="headerlink" title="完整的推理流程"></a>完整的推理流程</h3><ol>
<li>第一个协程 g1 来了，加了 <code>blockingKeyCountLimit</code> 锁，然后准备穿透到后端，调用函数 <code>getBytesNolc</code> 获取数据，并走进了 <code>singlelight</code> ，添加了一个 key：x， 准备干活；<ol>
<li>干活发生了一些不可预期的异常（后面发现是配置的异常），nil 指针引用之类的， <code>panic</code> 堆栈了，<code>panic</code> 导致后面 <code>delete key</code> 操作没有执行</li>
<li>虽然 g1 现在 <code>panic</code> 了，但是由于在函数 <code>func xxx</code> 里面 <code>blockingKeyCountLimit</code> 是 defer 执行的，所以这把锁还是，但是 <code>singlelight</code> 的 key 还存在，于是残留在 map 里面</li>
<li>但是由于我们服务程序为了高可用是 <code>recover</code> 了 <code>panic</code> 的，单个请求的失败不会导致整个进程挂掉，所以进程还是好好的</li>
</ol>
</li>
<li>第二个 <code>goroutine 19458</code> 协程来了，<code>blockingKeyCountLimit</code> 加锁，然后走到 <code>singlelight</code> 的时候，发现有 <code>key: x</code> 了，于是就等待<ol>
<li>并且等待的是一个永远得不到的锁，因为 g1 早就没了；</li>
</ol>
</li>
<li>后续的 11 个 协程来了，于是被 <code>blockingKeyCountLimit</code> 阻塞住，并且永远不能释放</li>
</ol>
<p>实锤：后续基于这个猜想，再去搜索一遍日志，发现确实是有一条 panic 相关的日志。这个时间点后面的请求全部被卡住。</p>
<h2 id="思考总结"><a href="#思考总结" class="headerlink" title="思考总结"></a>思考总结</h2><p>一般来讲 c 语言写程序容易出现死锁问题，因为各种异常逻辑可能会导致忘记放锁，从而导致抢一个永远都不可能得到的锁。<strong>golang 为了解决这个问题，一般是用 defer 机制来实现，使用姿势如下</strong>：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span> <span class="params">()</span></span> &#123;</span><br><span class="line">    mtx.Lock()</span><br><span class="line">    <span class="keyword">defer</span> mtx.Unlock()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 临界区 */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>golang 的 defer 机制是一个经过经验沉淀下来的有效功能</strong>。我们必须要合理使用。defer 实现原理是和所在函数绑定，保证函数 return 的时候一定能调用到（ panic 退出也能），所以 golang 加锁放锁的有效实践是写在相邻的两行。</p>
<p>其实思考下，<strong><code>singleflight</code> 作为一个通用开源库，其实可以把 <code>delete map key</code> 放到 defer 里，这样就能保证 map 里面的 key 一定是可以被清理的</strong>。</p>
<p>还有一点，<strong>其实 golang 是不提倡异常-捕捉这样的方式编程</strong>，<code>panic</code> 一般不让随便用，如果真是严重的问题，挂掉就挂掉，这个估计还好一些。当然这是要看场景的，还是有一些特殊场景的，毕竟 golang 都已经提供了 <code>panic-recover</code> 这样的一个手段，就说明还是有需求。这个就跟 unsafe 库一样，你只有明确知道自己的行为影响，才去使用这个工具，否则别用。</p>
<p>来源：奇伢云存储</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Istio 实战系列(1) - 应用容器对 Envoy Sidecar 的启动依赖问题]]></title>
      <url>http://team.jiunile.com/blog/2020/09/istio-depend-sidecar.html</url>
      <content type="html"><![CDATA[<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>典型案例：某运维同学反馈：昨天晚上 <code>Istio</code> 环境中应用的心跳检测报 <code>connect reset</code>，然后服务重启了。怀疑是 <code>Istio</code> 环境中网络不稳定导致了服务重启。</p>
<p>该问题的表现是安装了 <code>sidecar proxy</code> 的应用，在启动后的一小段时间内无法通过网络访问 pod 外部的其他服务，例如外部的 HTTP，MySQL，Redis等服务。如果应用没有对依赖服务的异常进行容错处理，该问题还常常会导致应用启动失败。</p>
<p>下面我们以该问题导致的一个典型故障的分析过程为例，对该问题的原因进行说明。<br><a id="more"></a></p>
<h2 id="故障分析"><a href="#故障分析" class="headerlink" title="故障分析"></a>故障分析</h2><p>根据运维同学的反馈，该 pod 曾多次重启。因此我们先用 <code>kubectl logs --previous</code> 命令查询 awesome-app 容器最后一次重启前的日志，以从日志中查找其重启的原因。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs --previous awesome-app-cd1234567-gzgwg -c awesome-app</span><br></pre></td></tr></table></figure></p>
<p>从日志中查询到了其重启前最后的错误信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Logging system failed to initialize using configuration from <span class="string">'http://log-config-server:12345/******/logback-spring.xml'</span></span><br><span class="line">java.net.ConnectException: Connection refused (Connection refused)</span><br><span class="line">        at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)</span><br></pre></td></tr></table></figure></p>
<p>从错误信息可以得知，应用进程在启动时试图通过 HTTP 协议从配置中心拉取 logback 的配置信息，但该操作由于网络异常失败了，导致应用进程启动失败，最终导致容器重启。</p>
<p><strong>是什么导致了网络异常呢？</strong>我们再用 <code>Kubectl get pod</code> 命令查询 Pod 的运行状态，尝试找到更多的线索：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod awesome-app-cd1234567-gzgwg  -o yaml</span><br></pre></td></tr></table></figure></p>
<p>命令输出的 pod 详细内容如下，该 yaml 片段省略了其他无关的细节，只显示了 lastState 和 state 部分的容器状态信息。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">containerStatuses:</span></span><br><span class="line"><span class="attr">  - containerID:</span> </span><br><span class="line"><span class="attr">    lastState:</span></span><br><span class="line"><span class="attr">      terminated:</span></span><br><span class="line"><span class="attr">        containerID:</span> </span><br><span class="line"><span class="attr">        exitCode:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">        finishedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">23</span>Z</span><br><span class="line"><span class="attr">        reason:</span> Error</span><br><span class="line"><span class="attr">        startedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">22</span>Z</span><br><span class="line"><span class="attr">    name:</span> awesome-app</span><br><span class="line"><span class="attr">    ready:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    restartCount:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    state:</span></span><br><span class="line"><span class="attr">      running:</span></span><br><span class="line"><span class="attr">        startedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">36</span>Z</span><br><span class="line"><span class="attr">  - containerID:</span> </span><br><span class="line"><span class="attr">    lastState:</span> &#123;&#125;</span><br><span class="line"><span class="attr">    name:</span> istio-proxy</span><br><span class="line"><span class="attr">    ready:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    restartCount:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">    state:</span></span><br><span class="line"><span class="attr">      running:</span></span><br><span class="line"><span class="attr">        startedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">20</span>Z</span><br><span class="line"><span class="attr">  hostIP:</span> <span class="number">10.0</span><span class="number">.6</span><span class="number">.161</span></span><br></pre></td></tr></table></figure></p>
<p>从该输出可以看到 pod 中的应用容器 awesome-app 重启了两次。整理该 pod 中 awesome-app 应用容器和 istio-proxy sidecar 容器的启动和终止的时间顺序，可以得到下面的时间线：</p>
<ol>
<li>2020-09-01T13:16:20Z istio-proxy 启动</li>
<li>2020-09-01T13:16:22Z awesome-app 上一次启动时间</li>
<li>2020-09-01T13:16:23Z awesome-app 上一次异常退出时间</li>
<li>2020-09-01T13:16:36Z awesome-app 最后一次启动，以后就一直正常运行</li>
</ol>
<p>可以看到在 <code>istio-proxy</code> 启动2秒后，awesome-app 启动，并于1秒后异常退出。结合前面的日志信息，我们知道这次启动失败的直接原因是应用访问配置中心失败导致。在 <code>istio-proxy</code> 启动16秒后，awesome-app 再次启动，这次启动成功，之后一直正常运行。</p>
<p><code>istio-proxy</code> 启动和 awesome-app 上一次异常退出的时间间隔很短，只有2秒钟，因此我们基本可以判断此时 <code>istio-proxy</code> 尚未启动初始化完成，导致 awesome-app 不能通过<code>istio-proxy</code> 连接到外部服务，导致其启动失败。待 awesome-app 于 2020-09-01T13:16:36Z 再次启动时，由于 <code>istio-proxy</code> 已经启动了较长时间，完成了从 pilot 获取动态配置的过程，因此 awesome-app 向 pod 外部的网络访问就正常了。</p>
<p>如下图所示，Envoy 启动后会通过 xDS 协议向 pilot 请求服务和路由配置信息，Pilot 收到请求后会根据 Envoy 所在的节点（pod或者VM）组装配置信息，包括 <code>Listener</code>、<code>Route</code>、<code>Cluster</code> 等，然后再通过 xDS 协议下发给 Envoy。根据 Mesh 的规模和网络情况，该配置下发过程需要数秒到数十秒的时间。由于初始化容器已经在 pod 中创建了 Iptables rule 规则，因此这段时间内应用向外发送的网络流量会被重定向到 Envoy ，而此时 Envoy 中尚没有对这些网络请求进行处理的监听器和路由规则，无法对此进行处理，导致网络请求失败。（关于 <code>Envoy sidecar</code> 初始化过程和 <code>Istio</code> 流量管理原理的更多内容，可以参考 <a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/" target="_blank" rel="external">Istio流量管理实现机制深度解析-基于1.4.0更新</a>）</p>
<p><img src="/images/istio/istio-1.png" alt="Envory sidecar初始化"></p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="在应用启动命令中判断-Envoy-初始化状态"><a href="#在应用启动命令中判断-Envoy-初始化状态" class="headerlink" title="在应用启动命令中判断 Envoy 初始化状态"></a>在应用启动命令中判断 Envoy 初始化状态</h3><p>从前面的分析可以得知，该问题的根本原因是由于应用进程对 <code>Envoy sidecar</code> 配置初始化的依赖导致的。因此最直接的解决思路就是：在应用进程启动时判断 <code>Envoy sidecar</code> 的初始化状态，待其初始化完成后再启动应用进程。</p>
<p>Envoy 的健康检查接口 <code>localhost:15020/healthz/ready</code> 会在 xDS 配置初始化完成后才返回 200，否则将返回 503，因此可以根据该接口判断 Envoy 的配置初始化状态，待其完成后再启动应用容器。我们可以在应用容器的启动命令中加入调用 Envoy 健康检查的脚本，如下面的配置片段所示。在其他应用中使用时，将 <code>start-awesome-app-cmd</code> 改为容器中的应用启动命令即可。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> awesome-app-deployment</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> awesome-app</span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> awesome-app</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> awesome-app</span><br><span class="line"><span class="attr">        image:</span> awesome-app</span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        command:</span> [<span class="string">"/bin/bash"</span>, <span class="string">"-c"</span>]</span><br><span class="line"><span class="attr">        args:</span> [<span class="string">"while [[ \"$(curl -s -o /dev/null -w ''<span class="template-variable">%&#123;http_code&#125;</span>'' localhost:15020/healthz/ready)\" != '200' ]]; do echo Waiting for Sidecar;sleep 1; done; echo Sidecar available; start-awesome-app-cmd"</span>]</span><br></pre></td></tr></table></figure></p>
<p>该流程的执行顺序如下：</p>
<ol>
<li><code>Kubernetes</code> 启动 应用容器。</li>
<li>应用容器启动脚本中通过 <code>curl get localhost:15020/healthz/ready</code> 查询 <code>Envoy sidcar</code> 状态，由于此时 <code>Envoy sidecar</code> 尚未就绪，因此该脚本会不断重试。</li>
<li><code>Kubernetes</code> 启动 <code>Envoy sidecar</code>。</li>
<li><code>Envoy sidecar</code> 通过 xDS 连接 Pilot，进行配置初始化。</li>
<li>应用容器启动脚本通过 <code>Envoy sidecar</code> 的健康检查接口判断其初始化已经完成，启动应用进程。</li>
</ol>
<p>该方案虽然可以规避依赖顺序的问题，但需要对应用容器的启动脚本进行修改，对 Envoy 的健康状态进行判断。更理想的方案应该是应用对 <code>Envoy sidecar</code> 不感知。</p>
<h3 id="通过-pod-容器启动顺序进行控制"><a href="#通过-pod-容器启动顺序进行控制" class="headerlink" title="通过 pod 容器启动顺序进行控制"></a>通过 pod 容器启动顺序进行控制</h3><p>通过阅读 <a href="https://github.com/kubernetes/kubernetes/blob/537a602195efdc04cdf2cb0368792afad082d9fd/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L827-L830" target="_blank" rel="external">Kubernetes源码</a> ，我们可以发现当 pod 中有多个容器时，<code>Kubernetes</code> 会在一个线程中依次启动这些容器，如下面的代码片段所示：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Step 7: start containers in podContainerChanges.ContainersToStart.</span></span><br><span class="line"><span class="keyword">for</span> _, idx := <span class="keyword">range</span> podContainerChanges.ContainersToStart &#123;</span><br><span class="line">  start(<span class="string">"container"</span>, containerStartSpec(&amp;pod.Spec.Containers[idx]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>因此我们可以在向 pod 中注入 <code>Envoy sidecar</code> 时将 <code>Envoy sidecar</code> 放到应用容器之前，这样 <code>Kubernetes</code> 会先启动 <code>Envoy sidecar</code>，再启动应用容器。但是还有一个问题，Envoy 启动后我们并不能立即启动应用容器，还需要等待 xDS 配置初始化完成。这时我们就可以采用容器的 <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/" target="_blank" rel="external">postStart lifecycle hook</a> 来达成该目的。<code>Kubernetes</code> 会在启动容器后调用该容器的 <code>postStart hook</code>，<code>postStart hook</code> 会阻塞 pod 中的下一个容器的启动，直到 <code>postStart hook</code> 执行完成。因此如果在 <code>Envoy sidecar</code> 的 <code>postStart hook</code> 中对 Envoy 的配置初始化状态进行判断，待完成初始化后再返回，就可以保证 <code>Kubernetes</code> 在 <code>Envoy sidecar</code> 配置初始化完成后再启动应用容器。该流程的执行顺序如下：</p>
<ol>
<li><code>Kubernetes</code> 启动 <code>Envoy sidecar</code>。</li>
<li><code>Kubernetes</code> 执行 <code>postStart hook</code>。</li>
<li><code>postStart hook</code> 通过 Envoy 健康检查接口判断其配置初始化状态，直到 Envoy 启动完成 。</li>
<li><code>Kubernetes</code> 启动应用容器。</li>
</ol>
<p><code>Istio</code> 已经在 1.7 中合入了该修复方案，参见 <a href="https://github.com/istio/istio/pull/24737" target="_blank" rel="external">Allow users to delay application start until proxy is ready</a>。</p>
<p>插入 <code>sidecar</code> 后的 pod spec 如下面的 yaml 片段所示。<code>postStart hook</code> 配置的 <code>pilot-agent wait</code> 命令会持续调用 <code>Envoy</code> 的健康检查接口 ‘/healthz/ready’ 检查其状态，直到 Envoy 完成配置初始化。这篇文章 <a href="https://medium.com/@marko.luksa/delaying-application-start-until-sidecar-is-ready-2ec2d21a7b74" target="_blank" rel="external">Delaying application start until sidecar is ready</a> 中介绍了更多关于该方案的细节。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sidecar-starts-first</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> istio-proxy</span><br><span class="line"><span class="attr">    image:</span> </span><br><span class="line"><span class="attr">    lifecycle:</span></span><br><span class="line"><span class="attr">      postStart:</span></span><br><span class="line"><span class="attr">        exec:</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">          -</span> pilot-agent</span><br><span class="line"><span class="bullet">          -</span> wait</span><br><span class="line"><span class="attr">  - name:</span> application</span><br><span class="line"><span class="attr">    image:</span> my-application</span><br></pre></td></tr></table></figure></p>
<p>该方案在不对应用进行修改的情况下比较完美地解决了应用容器和 <code>Envoy sidecar</code> 初始化的依赖问题。但是该解决方案对 <code>Kubernetes</code> 有两个隐式依赖条件：<code>Kubernetes</code> 在一个线程中按定义顺序依次启动 pod 中的多个容器，以及前一个容器的 <code>postStart hook</code> 执行完毕后再启动下一个容器。这两个前提条件在目前的 <code>Kuberenetes</code> 代码实现中是满足的，但由于这并不是 <code>Kubernetes</code> 的 API 规范，因此该前提在将来 <code>Kubernetes</code> 升级后很可能被打破，导致该问题再次出现。</p>
<h3 id="Kubernetes-支持定义-pod-中容器之间的依赖关系"><a href="#Kubernetes-支持定义-pod-中容器之间的依赖关系" class="headerlink" title="Kubernetes 支持定义 pod 中容器之间的依赖关系"></a>Kubernetes 支持定义 pod 中容器之间的依赖关系</h3><p>为了彻底解决该问题，避免 <code>Kubernetes</code> 代码变动后该问题再次出现，更合理的方式应该是由 <code>Kubernetes</code> 支持显式定义 pod 中一个容器的启动依赖于另一个容器的健康状态。目前 <code>Kubernetes</code> 中已经有一个 <a href="https://github.com/kubernetes/kubernetes/issues/65502" target="_blank" rel="external">issue Support startup dependencies between containers on the same Pod</a> 对该问题进行跟踪处理。如果 <code>Kubernetes</code> 支持了该特性，则该流程的执行顺序如下：</p>
<ol>
<li><code>Kubernetes</code> 启动 <code>Envoy sidecar</code> 容器。</li>
<li><code>Kubernetes</code> 通过 <code>Envoy sidecar</code> 容器的 readiness probe 检查其状态，直到 readiness probe 反馈 <code>Envoy sidecar</code> 已经 ready，即已经初始化完毕。</li>
<li><code>Kubernetes</code> 启动应用容器。</li>
</ol>
<h3 id="解耦应用服务之间的启动依赖关系"><a href="#解耦应用服务之间的启动依赖关系" class="headerlink" title="解耦应用服务之间的启动依赖关系"></a>解耦应用服务之间的启动依赖关系</h3><p>以上几个解决方案的思路都是控制 pod 中容器的启动顺序，在 <code>Envoy sidecar</code> 初始化完成后再启动应用容器，以确保应用容器启动时能够通过网络正常访问其他服务。但这些方案只是『头痛医头，脚痛医脚』,是治标不治本的方法。因为即使 pod 中对外的网络访问没有问题，应用容器依赖的其他服务也可能由于尚未启动，或者某些问题而不能在此时正常提供服务。要彻底解决该问题，我们需要解耦应用服务之间的启动依赖关系，使应用容器的启动不再强依赖其他服务。</p>
<p>在一个微服务系统中，原单体应用中的各个业务模块被拆分为多个独立进程（服务）。这些服务的启动顺序是随机的，并且服务之间通过不可靠的网络进行通信。微服务多进程部署、跨进程网络通信的特定决定了服务之间的调用出现异常是一个常见的情况。为了应对微服务的该特点，微服务的一个基本的设计原则是 <strong>“design for failure”</strong>，即需要以优雅的方式应对可能出现的各种异常情况。当在微服务进程中不能访问一个依赖的外部服务时，需要通过重试、降级、超时、断路等策略对异常进行容错处理，以尽可能保证系统的正常运行。</p>
<p><code>Envoy sidecar</code> 初始化期间网络暂时不能访问的情况只是放大了微服务系统未能正确处理服务依赖的问题，即使解决了 <code>Envoy sidecar</code> 的依赖顺序，该问题依然存在。例如在本案例中，配置中心也是一个独立的微服务，当一个依赖配置中心的微服务启动时，配置中心有可能尚未启动，或者尚未初始化完成。在这种情况下，如果在代码中没有对该异常情况进行处理，也会导致依赖配置中心的微服务启动失败。在一个更为复杂的系统中，多个微服务进程之间可能存在网状依赖关系，如果没有按照 <strong>“design for failure”</strong> 的原则对微服务进行容错处理，那么只是将整个系统启动起来就将是一个巨大的挑战。对于本例而言，可以采用一个类似这样的简单容错策略：先用一个缺省的 logback 配置启动应用进程，并在启动后对配置中心进行重试，待连接上配置中心后，再使用配置中心下发的配置对 logback 进行设置。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>应用容器对 <code>Envoy Sidecar</code> 启动依赖问题的典型表现是应用容器在刚启动的一小段时间内调用外部服务失败。原因是此时 <code>Envoy sidecar</code> 尚未完成 xDS 配置的初始化，因此不能为应用容器转发网络请求。该调用失败可能导致应用容器不能正常启动。此问题的根本原因是微服务应用中对依赖服务的调用失败没有进行合理的容错处理。</p>
<p>对于遗留系统，为了尽量避免对应用的影响，我们可以通过在应用启动命令中判断 <code>Envoy</code> 初始化状态的方案，或者升级到 <code>Istio 1.7</code> 来缓解该问题。但为了彻底解决服务依赖导致的错误，建议参考 <strong>“design for failure”</strong> 的设计原则，解耦微服务之间的强依赖关系，在出现暂时不能访问一个依赖的外部服务的情况时，通过重试、降级、超时、断路等策略进行处理，以尽可能保证系统的正常运行。</p>
<p>来源：mp.weixin.qq.com/s/iXU2LH90_ZA3VeN7xORmBw</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes securityContext]]></title>
      <url>http://team.jiunile.com/blog/2020/09/k8s-securitycontext.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>kubernetes 中的 <code>securityContext</code> 是什么？在什么场景下来使用？第一感觉反正是和安全相关的东西，来自官方定义如下：</p>
<p>安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。 安全上下文包括但不限于：</p>
<ul>
<li>自主访问控制（Discretionary Access Control）：基于 用户 ID（UID）和组 ID（GID）. 来判定对对象（例如文件）的访问权限</li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%85%A8%E5%A2%9E%E5%BC%BA%E5%BC%8FLinux" target="_blank" rel="external">安全性增强的 Linux（SELinux）</a>： 为对象赋予安全性标签。</li>
<li>以特权模式或者非特权模式运行。</li>
<li><a href="https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/" target="_blank" rel="external">Linux 权能</a>: 为进程赋予 root 用户的部分特权而非全部特权。</li>
<li><a href="https://kubernetes.io/zh/docs/tutorials/clusters/apparmor/" target="_blank" rel="external">AppArmor</a>：使用程序文件来限制单个程序的权限。</li>
<li><a href="https://en.wikipedia.org/wiki/Seccomp" target="_blank" rel="external">Seccomp</a>：限制一个进程访问文件描述符的权限。</li>
<li>AllowPrivilegeEscalation：控制进程是否可以获得超出其父进程的特权。 此布尔值直接控制是否为容器进程设置 <a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt" target="_blank" rel="external">no_new_privs</a> 标志。 当容器以特权模式运行或者具有 <code>CAP_SYS_ADMIN</code> 权能时，AllowPrivilegeEscalation 总是为 <strong><code>true</code></strong>。</li>
<li>readOnlyRootFilesystem：以只读方式加载容器的根文件系统。</li>
</ul>
<p>以上条目不是安全上下文设置的完整列表 – 请参阅 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#securitycontext-v1-core" target="_blank" rel="external">SecurityContext</a> 了解其完整列表。</p>
<p>关于在 Linux 系统中的安全机制的更多信息，可参阅 <a href="https://www.linux.com/learn/overview-linux-kernel-security-features" target="_blank" rel="external">Linux 内核安全性能力概述</a>。</p>
<p>上面的定义有些似懂非懂，能不能更加直白的描述下呢？好吧，下面来给大家来一些实际的应用场景来讲解下。</p>
<a id="more"></a>
<h2 id="Security-Context-应用场景"><a href="#Security-Context-应用场景" class="headerlink" title="Security Context 应用场景"></a>Security Context 应用场景</h2><h3 id="场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户"><a href="#场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户" class="headerlink" title="场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户"></a>场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户</h3><p>有了上面的场景，那我们如何来进行设定呢？假设运行用户对应的 uid为999 gid为999<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> StatefulSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sc-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      runAsUser:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">      runAsGroup:</span> <span class="number">999</span></span><br><span class="line"><span class="attr">      fsGroup:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">    containers:</span></span><br><span class="line"><span class="attr">    - name:</span> sc-demo</span><br><span class="line"><span class="attr">      image:</span> xxxxxx</span><br><span class="line"><span class="attr">      command:</span> [ <span class="string">"sh"</span>, <span class="string">"-c"</span>, <span class="string">"sleep 1h"</span> ]</span><br><span class="line"><span class="attr">      volumeMounts:</span></span><br><span class="line"><span class="attr">      - name:</span> sc-vol</span><br><span class="line"><span class="attr">        mountPath:</span> /data/demo</span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        runAsUser:</span> <span class="number">999</span></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">    - metadata:</span></span><br><span class="line"><span class="attr">        name:</span> sc-vol</span><br><span class="line"><span class="attr">      spec:</span></span><br><span class="line"><span class="attr">        accessModes:</span></span><br><span class="line"><span class="bullet">          -</span> ReadWriteOnce</span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            storage:</span> <span class="number">30</span>Gi</span><br><span class="line"><span class="attr">        storageClassName:</span> gp2</span><br></pre></td></tr></table></figure></p>
<p>这里有两个 <code>securityContext</code> 设定，一个是针对 pod 级别的，另外一个则是针对 container 级别的，那两者的优先级如何定义呢？规则就是：<strong>container中的会覆写pod中的定义</strong>。了解了优先级后，来讲解下 runAsUser、runAsGroup、fsGroup 这三个参数的意义。</p>
<ul>
<li><code>runAsUser</code> 字段指定 Pod 中的所有容器内的进程都使用 用户ID 1000 来运行。<code>但这里 sc-demo 容器进行了覆写，如果 sc-demo 容器内的进行使用用户 ID 为999来运行</code>。</li>
<li><code>runAsGroup</code> 字段指定所有容器中的进程都以主 组ID 999 来运行。 如果忽略此字段，则容器的 主组ID 将是 root（0）。 当 <code>runAsGroup</code> 被设置时，所有创建的文件也会划归为 用户1000 和 组999。</li>
<li><code>fsGroup</code> 由于 <code>fsGroup</code> 被设置，容器中所有进程也会是附 组ID 0 (root) 的一部分。 卷 <code>/data/demo</code> 及在该卷中创建的任何文件的属主都会是 组ID 0 (root)。</li>
</ul>
<p>进入容器中查看<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ id</span><br><span class="line">uid=999(xx) gid=999(xx) groups=999(xx),0(root)</span><br><span class="line"></span><br><span class="line">$ ls <span class="_">-l</span> /data/demo</span><br><span class="line">drwxrwsr-x 3 xx root  4096 Sep  8 17:51 <span class="built_in">test</span></span><br></pre></td></tr></table></figure></p>
<p><strong>为 Pod 配置卷访问权限和属主变更策略</strong></p>
<blockquote>
<p>FEATURE STATE: Kubernetes v1.18 [alpha]</p>
</blockquote>
<p>默认情况下，Kubernetes 在挂载一个卷时，会递归地更改每个卷中的内容的属主和访问权限，使之与 Pod 的 <code>securityContext</code> 中指定的 <code>fsGroup</code> 匹配。 对于较大的数据卷，检查和变更属主与访问权限可能会花费很长时间，降低 Pod 启动速度。 你可以在 <code>securityContext</code> 中使用 <code>fsGroupChangePolicy</code> 字段来控制 Kubernetes 检查和管理卷属主和访问权限的方式。</p>
<p><strong>fsGroupChangePolicy</strong> - <code>fsGroupChangePolicy</code> 定义在卷被暴露给 Pod 内部之前对其 内容的属主和访问许可进行变更的行为。此字段仅适用于那些支持使用 <code>fsGroup</code> 来 控制属主与访问权限的卷类型。此字段的取值可以是：</p>
<ul>
<li><code>OnRootMismatch</code>：只有根目录的属主与访问权限与卷所期望的权限不一致时，才改变其中内容的属主和访问权限。这一设置有助于缩短更改卷的属主与访问权限所需要的时间。</li>
<li><code>Always</code>：在挂载卷时总是更改卷中内容的属主和访问权限。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">  runAsUser:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">  runAsGroup:</span> <span class="number">3000</span></span><br><span class="line"><span class="attr">  fsGroup:</span> <span class="number">2000</span></span><br><span class="line"><span class="attr">  fsGroupChangePolicy:</span> <span class="string">"OnRootMismatch"</span></span><br></pre></td></tr></table></figure>
<p>这是一个 Alpha 阶段的功能特性。要使用此特性，需要在 kube-apiserver、kube-controller-manager 和 kubelet 上启用 ConfigurableFSGroupPolicy <a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/" target="_blank" rel="external">特性门控</a>。</p>
<h3 id="场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。"><a href="#场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。" class="headerlink" title="场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。"></a>场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> StatefulSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sc-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    containers:</span></span><br><span class="line"><span class="attr">    - name:</span> sc-demo</span><br><span class="line"><span class="attr">      image:</span> xxxxxx</span><br><span class="line"><span class="attr">      command:</span> [ <span class="string">"sh"</span>, <span class="string">"-c"</span>, <span class="string">"sleep 1h"</span> ]</span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        privileged:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>给予 <code>privileged: true</code> 是一个比较粗的权限，一般不建议如此，可以为权限定义更细粒度的权限，类似需要在容器中使用 <code>perf</code> 命令，则可以进行如下定义：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">    capabilities:</span></span><br><span class="line"><span class="attr">        add:</span> [<span class="string">"SYS_ADMIN"</span>]</span><br></pre></td></tr></table></figure></p>
<p>具体 <code>capabilities</code> 的使用规则可参考：<a href="http://team.jiunile.com/blog/2019/12/capabilities.html">在 Kubernetes 中配置 Container Capabilities</a></p>
<h3 id="场景三：我需要对启动的容器赋予SELinux标签"><a href="#场景三：我需要对启动的容器赋予SELinux标签" class="headerlink" title="场景三：我需要对启动的容器赋予SELinux标签"></a>场景三：我需要对启动的容器赋予SELinux标签</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">  seLinuxOptions:</span></span><br><span class="line"><span class="attr">    level:</span> <span class="string">"s0:c123,c456"</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>要指定 SELinux，需要在宿主操作系统中装载 SELinux 安全性模块。<code>seLinuxOptions</code> 字段的取值是一个 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#selinuxoptions-v1-core" target="_blank" rel="external">SELinuxOptions</a> 对象</p>
</blockquote>
<p>参考</p>
<ul>
<li><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/security-context/" target="_blank" rel="external">https://kubernetes.io/zh/docs/tasks/configure-pod-container/security-context/</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes in Kuberntes]]></title>
      <url>http://team.jiunile.com/blog/2020/06/k8s-k8s-in-k8s.html</url>
      <content type="html"><![CDATA[<p><code>KinD</code> 是一个非常轻量级的 Kubernetes 安装工具，他将 Docker 容器当成 Kubernetes 的节点，使用非常方便。既然在 Docker 容器中可以运行 Kubernetes 集群，那么我们自然就会想到是否可以在 Pod 中来运行呢？在 Pod 中运行会遇到哪些问题呢？</p>
<h2 id="在-Pod-中安装-Docker-Daemon"><a href="#在-Pod-中安装-Docker-Daemon" class="headerlink" title="在 Pod 中安装 Docker Daemon"></a>在 Pod 中安装 Docker Daemon</h2><p><code>KIND</code> 当前依赖于 Docker（尽管他们计划很快支持其他容器运行时，例如<code>podman</code>）。因此，第一步是创建一个容器映像，该映像允许您在 Pod 内运行 Docker守护程序，以便使诸如<code>docker run</code>之类的命令在Pod内运行（又名 Docker-in-Docker 或 DIND ）。</p>
<p>Docker-in-Docker 是一个众所周知的问题，并且已经解决了相当一段时间。尽管如此，当尝试在生产 Kubernetes 集群中正确设置 Docker-in-Docker 时，我们仍然遇到很多问题。<br><a id="more"></a></p>
<h3 id="MTU问题"><a href="#MTU问题" class="headerlink" title="MTU问题"></a>MTU问题</h3><p>MTU 问题的性质实际上取决于生产 Kubernetes 集群的网络提供商。我们用于 CI 的Kubernetes 发行版是 <a href="https://d2iq.com/solutions/ksphere/konvoy" target="_blank" rel="external">Konvoy</a>。Konvoy 使用<code>Calico</code>作为其默认网络提供商，并且默认情况下使用<code>IPIP</code>封装。<code>IPIP</code>封装产生20字节的开销。换句话说，如果群集中主机网络的主网络接口的<code>MTU</code>为1500，则Pod中网络接口的<code>MTU</code>将为1480。如果您的生产群集在某些云提供商（例如GCE）上运行，则<code>MTU Pod</code>的最大值甚至更低（1460-20 = 1440）。</p>
<p>重要的是，我们在 Pod 内配置默认 Docker 网络的<code>MTU</code>（<code>dockerd</code> 的 <code>--mtu</code>标志），使其等于或小于 Pod 的网络接口的<code>MTU</code>。否则，您将无法与外界建立连接（例如，从互联网上获取容器图像时）。</p>
<h3 id="PID-1-的问题"><a href="#PID-1-的问题" class="headerlink" title="PID 1 的问题"></a>PID 1 的问题</h3><p>比如我们需要在一个容器中去运行 <code>Docker Daemon</code> 以及一些 Kubernetes 的集群测试，而这些测试依赖于 <code>KinD</code> 和 <code>Docker Damon</code>，在一个容器中运行多个服务我们可能会去使用 <code>systemd</code>，但是使用 <code>systemd</code> 也会有一些问题。</p>
<p>比如我们需要保留测试的退出状态，Kubernetes 中使用的容器运行时可以 watch 到容器中的第一个进程（PID 1）的退出状态。如果我们使用 <code>systemd</code> 的话，那么我们测试的进程退出状态不会被转发到 Kubernetes。</p>
<p>此外获取测试的日志也是非常重要的，在 Kubernetes 中会自动获取写入到 stdout 和 stderr 的容器日志，但是如果使用 <code>systemd</code> 的话，要想获取应用的日志就比较麻烦的。</p>
<p>为了解决上面的问题，我们可以在容器镜像中使用如下所示的启动脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dockerd &amp;</span><br><span class="line"><span class="comment"># Wait until dockerd is ready.</span></span><br><span class="line">until docker ps &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Waiting for dockerd..."</span></span><br><span class="line">  sleep 1</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$@</span>"</span></span><br></pre></td></tr></table></figure></p>
<p>但是需要注意的是我们不能将上面的脚本作为容器的 entrypoint，在镜像中定义的 entrypoint 会在容器中以 PID 1 的形式运行在一个单独的 <code>pid namespace</code> 中。PID 1 是一个内核中的一个特殊进程，它的行为和其他进程不同。</p>
<p>本质上，接收信号的进程是 PID 1：它会被内核做特殊处理；如果它没有为信号注册一个处理器，内核就不会回到默认行为（即杀死进程）。由于当收到 SIGTERM 信号时，内核会默认杀死这个进程，所以一些进程也许不会为 SIGTERM 信号注册信号处理程序。如果出现了这种情况，当 Kubernetes 尝试终止 Pod 时，SIGTERM 将被吞噬，你会注意到 Pod 会被卡在 Terminating 的状态下。</p>
<p>这其实不是一个什么新鲜的问题，但是了解这个问题的人却并不多，而且还一直在构建有这样问题的容器。我们可以使用 tini 这个应用来解决这个问题，将其作为镜像的入口点，如在 <code>Dockerfile</code> 中所示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENTRYPOINT [<span class="string">"/usr/bin/tini"</span>, <span class="string">"--"</span>, <span class="string">"/entrypoint.sh"</span>]</span><br></pre></td></tr></table></figure></p>
<p>这个程序会正确注册信号处理程序和转发信号。它还会执行一些其他 PID 1 的事情，比如回收容器中的僵尸进程。</p>
<h3 id="挂载-cgroups"><a href="#挂载-cgroups" class="headerlink" title="挂载 cgroups"></a>挂载 cgroups</h3><p>由于 <code>Docker Daemon</code> 需要控制 cgroups，所以需要将 cgroup 文件系统挂载到容器中去。但是由于 cgroups 和宿主机是共享的，所以我们需要确保 <code>Docker Daemon</code> 控制的 cgroups 不会影响到其他容器或者宿主机进程使用的其他 cgroups，还需要确保 <code>Docker Daemon</code> 在容器中创建的 cgroups 在容器退出后不会被泄露。</p>
<p><code>Docker Daemon</code> 中有一个 <code>--cgroup—parent</code> 参数来告诉 Daemon 将所有容器的 cgroups 嵌套在指定的 cgroup 下面。当容器运行在 Kubernetes 集群下面时，我们在容器中设置 <code>Docker Daemon</code> 的 <code>--cgroup—parent</code> 参数，这样它的所有 cgroups 就会被嵌套在 Kubernetes 为容器创建的 cgroup 下面了。</p>
<p>在以前为了让 cgroup 文件系统在容器中可用，一些用户会将宿主机中的 <code>/sys/fs/cgroup</code> 挂载到容器中的这个位置，如果这样使用的话，我们就需要在容器启动脚本中把 <code>--cgroup—parent</code> 设置为下面的内容，这样 <code>Docker Daemon</code> 创建的 cgroups 就可以正确被嵌套了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CGROUP_PARENT=<span class="string">"<span class="variable">$(grep systemd /proc/self/cgroup | cut -d: -f3)</span>/docker"</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意：<code>/proc/self/cgroup</code> 显示的是调用进程的 cgroup 路径。</p>
</blockquote>
<p>但是我们要知道，挂载宿主机的 <code>/sys/fs/cgroup</code> 文件是非常危险的事情，因为他把整个宿主机的 cgroup 层次结构都暴露给了容器。以前为了解决这个问题，Docker 用了一个小技巧把不相关的 cgroups 隐藏起来，不让容器看到。Docker 从容器的 cgroups 对每个 cgroup 系统的 cgroup 层次结构的根部进行绑定挂载。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm debian findmnt -lo <span class="built_in">source</span>,target -t cgroup</span><br><span class="line">SOURCE                                                                               TARGET</span><br><span class="line">cpuset[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/cpuset</span><br><span class="line">cpu[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]        /sys/fs/cgroup/cpu</span><br><span class="line">cpuacct[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]    /sys/fs/cgroup/cpuacct</span><br><span class="line">blkio[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/blkio</span><br><span class="line">memory[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/memory</span><br><span class="line"> </span><br><span class="line">cgroup[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/systemd</span><br></pre></td></tr></table></figure></p>
<p>从上面我们可以看出 cgroups 通过将宿主机 cgroup 文件系统上的 <code>/sys/fs/cgroup/memory/memory.limit_in_bytes</code> 文件映射到 <code>/sys/fs/cgroup/memory/docker/&lt;CONTAINER_ID&gt;/memory.limit_in_bytes</code> 来控制容器内 cgroup 层次结构根部的文件，这种方式可以防止容器进程意外地修改宿主机的 cgroup。</p>
<p>但是这种方式有时候会让 cadvisor 和 kubelet 这样的应用感动困惑，因为绑定挂载并不会改变 <code>/proc/&lt;PID&gt;/cgroup</code> 里面的内容。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm debian cat /proc/1/cgroup</span><br><span class="line">14:name=systemd:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line"> </span><br><span class="line">5:memory:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">4:blkio:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">3:cpuacct:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">2:cpu:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">1:cpuset:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">0::/</span><br></pre></td></tr></table></figure></p>
<p>cadvisor 会通过查看 <code>/proc/&lt;PID&gt;/cgroup</code> 来获取给定进程的 cgroup，并尝试从对应的 cgroup 中获取 CPU 或内存统计数据。但是由于 <code>Docker Daemon</code> 进程做了绑定挂载，cadvisor 就无法找到容器进程对应的 cgroup。为了解决这个问题，我们在容器内部又做了一次挂载，从 <code>/sys/fs/cgroup/memory</code> 挂载到 <code>/sys/fs/cgroup/memory/docker/&lt;CONTAINER_ID&gt;/</code>（针对所有的 cgroup 子系统），这个方法可以很好的解决这个问题。</p>
<p>现在新的解决方法是使用 <code>cgroup namespace</code>，如果你运行在一个内核版本 4.6+ 的 Linux 系统下面，runc 和 docker 都加入了 cgroup 命名空间的支持。但是目前 Kubernetes 暂时还不支持 cgroup 命名空间，但是很快会作为 <code>cgroups v2</code> 支持的一部分。</p>
<h3 id="IPtables"><a href="#IPtables" class="headerlink" title="IPtables"></a>IPtables</h3><p>在使用的时候我们发现在线上的 Kubernetes 集群运行时，有时候容器内的 <code>Docker Daemon</code> 启动的嵌套容器无法访问外网，但是在本地开发电脑上却可以很正常的工作，大部分开发者应该都会经常遇到这种情况。</p>
<p>最后发现当出现这个问题的时候，来自嵌套的 Docker 容器的数据包并没有打到 iptables 的 POSTROUTING 链，所以没有做 masqueraded。</p>
<p>这个问题是因为包含 <code>Docker Daemon</code> 的镜像是基于 Debian buster 的，而默认情况下，Debian buster 使用的是 nftables 作为 iptables 的默认后端，然而 Docker 本身还不支持 nftables。要解决这个问题只需要在容器镜像中切换到 iptables 命令即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN update-alternatives --set iptables  /usr/sbin/iptables-legacy || <span class="literal">true</span> &amp;&amp; \</span><br><span class="line">    update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy || <span class="literal">true</span> &amp;&amp; \</span><br><span class="line">    update-alternatives --set arptables /usr/sbin/arptables-legacy || <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>完整的 Dockerfile 文件和启动脚本可以在 <a href="https://github.com/jieyu/docker-images/tree/master/dind" target="_blank" rel="external">GitHub</a> 上面获取，也可以直接使用 <code>jieyu/dind-buster:v0.1.8</code> 这个镜像来测试。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm --privileged jieyu/dind-buster:v0.1.8 docker run alpine wget baidu.com</span><br></pre></td></tr></table></figure></p>
<p>在 Kubernetes 集群下使用如下所示的 Pod 资源清单部署即可：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> dind</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> jieyu/dind-buster:v0<span class="number">.1</span><span class="number">.8</span></span><br><span class="line"><span class="attr">    name:</span> dind</span><br><span class="line"><span class="attr">    stdin:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    tty:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> /bin/bash</span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /var/lib/docker</span><br><span class="line"><span class="attr">      name:</span> varlibdocker</span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> varlibdocker</span><br><span class="line"><span class="attr">    emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="在-Pod-中运行-KinD"><a href="#在-Pod-中运行-KinD" class="headerlink" title="在 Pod 中运行 KinD"></a>在 Pod 中运行 KinD</h2><p>上面我们成功配置了 Docker-in-Docker(DinD)，接下来我们就来在该容器中使用 KinD 启动 Kubernetes 集群。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -ti --rm --privileged jieyu/dind-buster:v0.1.8 /bin/bash</span><br><span class="line">Waiting <span class="keyword">for</span> dockerd...</span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># curl -Lso ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64</span></span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># chmod +x ./kind</span></span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># mv ./kind /usr/bin/</span></span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># kind create cluster</span></span><br><span class="line">Creating cluster <span class="string">"kind"</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.18.2) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line">Set kubectl context to <span class="string">"kind-kind"</span></span><br><span class="line">You can now use your cluster with:</span><br><span class="line">kubectl cluster-info --context kind-kind</span><br><span class="line">Have a nice day! 👋</span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                 STATUS   ROLES    AGE   VERSION</span><br><span class="line">kind-control-plane   Ready    master   11m   v1.18.2</span><br></pre></td></tr></table></figure></p>
<p>由于某些原因可能你用上面的命令下载不了 kind，我们可以想办法提前下载到宿主机上面，然后直接挂载到容器中去也可以，我这里将 kind 和 kubectl 命令都挂载到容器中去，使用下面的命令启动容器即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it --rm --privileged -v /usr/<span class="built_in">local</span>/bin/kind:/usr/bin/kind -v /usr/<span class="built_in">local</span>/bin/kubectl:/usr/bin/kubectl jieyu/dind-buster:v0.1.8 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/k8s_in_k8s_1.png" alt="k8s-in-k8s"><br>可以看到在容器中可以很好的使用 KinD 来创建 Kubernetes 集群。接下来我们直接在 Kubernetes 中来测试一次：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply <span class="_">-f</span> dind.yaml</span><br><span class="line">$ kubectl <span class="built_in">exec</span> -ti dind /bin/bash</span><br><span class="line">root@dind:/<span class="comment"># curl -Lso ./kind https://kind.sigs.k8s.io/dl/v0.7.0/kind-$(uname)-amd64</span></span><br><span class="line">root@dind:/<span class="comment"># chmod +x ./kind</span></span><br><span class="line">root@dind:/<span class="comment"># mv ./kind /usr/bin/</span></span><br><span class="line">root@dind:/<span class="comment"># kind create cluster</span></span><br><span class="line">Creating cluster <span class="string">"kind"</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.17.0) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✗ Starting control-plane 🕹️</span><br><span class="line">ERROR: failed to create cluster: failed to init node with kubeadm: <span class="built_in">command</span> <span class="string">"docker exec --privileged kind-control-plane kubeadm init --ignore-preflight-errors=all --config=/kind/kubeadm.conf --skip-token-print --v=6"</span> failed with error: <span class="built_in">exit</span> status 137</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到在 Pod 中使用 KinD 来创建集群失败了，这是因为在 KinD 节点嵌套容器内运行的 kubelet 会随机杀死顶层容器内的进程，这其实还是和上面讨论的 cgroups 的挂载有关。</p>
<p>但其实我自己在使用 v0.8.1 版本的 KinD 的时候，在上面的 Pod 中是可以正常创建集群的，不知道是否是 KinD 搭建的集群有什么特殊处理，这里需要再深入研究：<br><img src="/images/k8s/k8s_in_k8s_2.png" alt="k8s-in-k8s"></p>
<p>如果你在使用的过程中也遇到了上述的问题，则可以继续往下看解决方案。</p>
<p>当顶层容器（DIND）在 Kubernetes  Pod 中运行的时候，对于每个 cgroup 子系统（比如内存），从宿主机的角度来看，它的 cgroup 路径是 <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;</code>。</p>
<p>当 KinD 在 DIND 容器内的嵌套节点容器内启动 kubelet 的时候，kubelet 将在 <code>/kubepods/burstable/</code> 下相对于嵌套 KIND 节点容器的根 cgroup 为其 Pods 来操作 cgroup。从宿主机的角度来看，cgroup 路径就是 <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;/docker/&lt;KIND_CID&gt;/kubepods/burstable/</code>。</p>
<p>这些都是正确的，但是在嵌套的 KinD 节点容器中，有另一个 cgroup 存在于 <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;/docker/&lt;DIND_CID&gt;</code> 下面，相对于嵌套的 KinD 节点容器的根 cgroup，在 kubelet 启动之前就存在了，这是上面我们讨论过的 cgroups 挂载造成的，通过 KinD entrypoint 脚本设置。而如果你在 KinD 节点容器里面做一个 <code>cat /kubepods/burstable/&lt;POD_ID&gt;/docker/&lt;DIND_CID&gt;/tasks</code>，你会看到 DinD 容器的进程。<br><img src="/images/k8s/k8s_in_k8s_3.png" alt="k8s-in-k8s"></p>
<p>这就是最根本的原因，KinD 节点容器里面的 kubelet 看到了这个 cgroup，以为应该由它来管理，但是却找不到和这个 cgroup 相关联的 Pod，所以就会尝试来杀死属于这个 cgroup 的进程来删除这个 cgroup。这个操作的结果就是随机进程被杀死。解决这个问题的方法可以通过设置 kubelet 的 <code>--cgroup-root</code> 参数，通过该标志来指示 KinD 节点容器内的 kubelet 为其 Pods 使用不同的 cgroup 根路径（比如 /kubelet）。这样就可以在 Kubernetes 集群中来启动 KinD 集群了，我们可以通过下面的 YAML 资源清单文件来修复这个问题。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kind-cluster</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> jieyu/kind-cluster-buster:v0<span class="number">.1</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    name:</span> kind-cluster</span><br><span class="line"><span class="attr">    stdin:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    tty:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> /bin/bash</span><br><span class="line"><span class="attr">    env:</span></span><br><span class="line"><span class="attr">    - name:</span> API_SERVER_ADDRESS</span><br><span class="line"><span class="attr">      valueFrom:</span></span><br><span class="line"><span class="attr">        fieldRef:</span></span><br><span class="line"><span class="attr">          fieldPath:</span> status.podIP</span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /var/lib/docker</span><br><span class="line"><span class="attr">      name:</span> varlibdocker</span><br><span class="line"><span class="attr">    - mountPath:</span> /lib/modules</span><br><span class="line"><span class="attr">      name:</span> libmodules</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">    - containerPort:</span> <span class="number">30001</span></span><br><span class="line"><span class="attr">      name:</span> api-server-port</span><br><span class="line"><span class="attr">      protocol:</span> TCP</span><br><span class="line"><span class="attr">    readinessProbe:</span></span><br><span class="line"><span class="attr">      failureThreshold:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">      httpGet:</span></span><br><span class="line"><span class="attr">        path:</span> /healthz</span><br><span class="line"><span class="attr">        port:</span> api-server-port</span><br><span class="line"><span class="attr">        scheme:</span> HTTPS</span><br><span class="line"><span class="attr">      initialDelaySeconds:</span> <span class="number">120</span></span><br><span class="line"><span class="attr">      periodSeconds:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">      successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> varlibdocker</span><br><span class="line"><span class="attr">    emptyDir:</span> &#123;&#125;</span><br><span class="line"><span class="attr">  - name:</span> libmodules</span><br><span class="line"><span class="attr">    hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /lib/modules</span><br></pre></td></tr></table></figure></p>
<p>使用上面的资源清单文件创建完成后，稍等一会儿我们就可以进入 Pod 中来验证。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> -ti kind-cluster /bin/bash</span><br><span class="line">root@kind-cluster:/<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                 STATUS   ROLES    AGE   VERSION</span><br><span class="line">kind-control-plane   Ready    master   72s   v1.17.0</span><br></pre></td></tr></table></figure></p>
<p>同样也可以直接使用 Docker CLI 来进行测试：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -ti --rm --privileged jieyu/kind-cluster-buster:v0.1.0 /bin/bash</span><br><span class="line">Waiting <span class="keyword">for</span> dockerd...</span><br><span class="line">Setting up KIND cluster</span><br><span class="line">Creating cluster <span class="string">"kind"</span> ...</span><br><span class="line"> ✓ Ensuring node image (jieyu/kind-node:v1.17.0) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line"> ✓ Waiting ≤ 15m0s <span class="keyword">for</span> control-plane = Ready ⏳</span><br><span class="line"> • Ready after 31s 💚</span><br><span class="line">Set kubectl context to <span class="string">"kind-kind"</span></span><br><span class="line">You can now use your cluster with:</span><br><span class="line">kubectl cluster-info --context kind-kind</span><br><span class="line">Have a nice day! 👋</span><br><span class="line">root@d95fa1302557:/<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                 STATUS   ROLES    AGE   VERSION</span><br><span class="line">kind-control-plane   Ready    master   71s   v1.17.0</span><br><span class="line">root@d95fa1302557:/<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>上面镜像对应的 Dockerfile 和启动脚本地址：<a href="https://github.com/jieyu/docker-images/tree/master/kind-cluster" target="_blank" rel="external">https://github.com/jieyu/docker-images/tree/master/kind-cluster</a></p>
</blockquote>
<p>下图是我在 KinD 搭建的 Kubernetes 集群中，创建的一个 Pod，然后在 Pod 中创建的一个独立的 Kubernetes 集群最终效果：<br><img src="/images/k8s/k8s_in_k8s_4.png" alt="k8s-in-k8s"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在实现上面功能的时候，过程中还是遇到了不少的障碍，其中大部分都是因为 Docker 容器没有提供和宿主机完全隔离的功能造成的，某些内核资源比如 cgroups 是在内核中共享的，如果很多容器同时操作它们，也可能会造成潜在的冲突。但是一旦解决了这些问题，我们就可以非常方便的在 Kubernetes 集群 Pod 中轻松地运行一个独立的 Kubernetes 集群了，这应该算真正的 Kubernetes IN Kubernetes 了吧~</p>
<p>参考：<a href="https://d2iq.com/blog/running-kind-inside-a-kubernetes-cluster-for-continuous-integration" target="_blank" rel="external">https://d2iq.com/blog/running-kind-inside-a-kubernetes-cluster-for-continuous-integration</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[多集群kubernetes dashboard 通过ldap统一登录与授权]]></title>
      <url>http://team.jiunile.com/blog/2020/06/k8s-mutiboard-ldap.html</url>
      <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/icyxp/kubernetes-dashboard-ldap/master/assets/images/logo.png" alt="logo.png"></p>
<h1 id="工具由来"><a href="#工具由来" class="headerlink" title="工具由来"></a>工具由来</h1><p>为什么要写这样的一个工具呢？这是因为我司有多个 <code>kubernetes</code> 集群(8+)，且都是云托管服务无法接触到Apiserver配置，这就给我们带来一个痛点，<strong>开发、sre需要登录k8s dashbaord且不同部门和角色间需要不同的授权</strong>，原先都是通过 <code>sa token</code> 进行登录dashboard，但随着k8s集群的增长，每增加一个集群，就需要告知使用方对应dashboard访问地址以及对应的token，这不管是提供方还是使用方都让人感觉非常的痛苦。那是否有一款工具能<strong>提供统一地址统一登录多集群dashboard的方案</strong>呢？经过一番搜索后，发现并没有，市面上大多数是单集群集成 <code>LDAP</code> 的方案，主要是以 <code>DEX</code> 为主，但光单集群的统一登录授权方案就让人感觉非常的困难。难道就没有简单方便的工具供我们使用吗？好吧，那我就来打造这样一款工具吧。</p>
<p>Dashboard LDAP集成方案：</p>
<ul>
<li><a href="https://k2r2bai.com/2019/09/29/ironman2020/day14/" target="_blank" rel="external">https://k2r2bai.com/2019/09/29/ironman2020/day14/</a></li>
<li><a href="https://blog.inkubate.io/access-your-kubernetes-cluster-with-your-active-directory-credentials" target="_blank" rel="external">https://blog.inkubate.io/access-your-kubernetes-cluster-with-your-active-directory-credentials</a></li>
</ul>
<p>以上两篇文档是成LDAP的方案，个人感觉还不错，供有需要的人参考！<br><a id="more"></a></p>
<h1 id="如何打造"><a href="#如何打造" class="headerlink" title="如何打造"></a>如何打造</h1><p>好吧既然没有，那就自动动手打造一个！</p>
<blockquote>
<p>目标： <strong>简单使用</strong>！！！通过访问同一地址，使用LDAP登录且可切换不同集群的dashboard，同时对应不同的集群权限可单独配置！</p>
</blockquote>
<p>有了上面的目标，那如何来实现呢？</p>
<p>实现方式其实很简单，首先写一个登录界面与公司的AD进行打通获取用户与组，然后将用户或者组与k8s集群中的 <code>service account</code> 进行关联就实现了对应的rbac与登录token，最后在登录后实现一个反向代理服务即可完成。</p>
<p>是不是非常的简单！！！</p>
<p>实现技术栈：golang(gin、client-go、viper、ldap) + Kubernetes Dashboard</p>
<h1 id="如何部署"><a href="#如何部署" class="headerlink" title="如何部署"></a>如何部署</h1><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>在使用此工具前，需要有以下一些条件约束：</p>
<ol>
<li>已在各k8s集群部署 <code>dashboard</code> 且能被此工具访问到</li>
<li>已有 <code>ldap</code> 且有管理权限能进行访问操作</li>
<li>各集群中有对应的 <code>service account</code> 可进行映射，如需对不同用户和组需要有不同的操作权限，则对sa进行rbac授权即可，下面会详细说明。</li>
<li>此工具需要操作各集群的api，故需要获取每个集群的 <code>apiserver地址</code>、<code>ca.crt</code> 以及 <code>token</code> 进行配置，至于每个集群的 <code>ca.crt</code> 和 <code>token</code> 如果获取，后面会进行说明</li>
</ol>
<h2 id="如何获取-ca-crt-及-token"><a href="#如何获取-ca-crt-及-token" class="headerlink" title="如何获取 ca.crt 及 token"></a>如何获取 ca.crt 及 token</h2><p>此工具需要操作每个集群的api来获取对应的 sa 以及 token，故需要有对各集群操作的权限。那如何在各集群生成对应的 ca证书 及 token 呢？答案就是创建一个 sa 并给予一定的权限。</p>
<p>在每个k8s集群中执行如下yaml文件:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap-crb</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap-view</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap-view</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> serviceaccounts</span><br><span class="line"><span class="bullet">  -</span> secrets</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> get</span><br><span class="line"><span class="bullet">  -</span> list</span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> namespaces</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> get</span><br><span class="line"><span class="bullet">  -</span> list</span><br><span class="line"><span class="bullet">  -</span> watch</span><br></pre></td></tr></table></figure></p>
<p>此yaml文件的含义：创建一个名为<code>mutiboard-ldap</code>的 sa，并且给予<code>serviceaccounts</code>和<code>secrets</code>的get和list的权限。</p>
<p>获取 <code>mutiboard-ldap</code> 的 ca.crt：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(⎈|aws-local:default)❯ <span class="built_in">echo</span> $(kubectl get secret $(kubectl get secret | grep mutiboard-ldap | awk <span class="string">'&#123;print $1&#125;'</span>) -o go-template=<span class="string">'&#123;&#123;index .data "ca.crt"&#125;&#125;'</span>) | base64 <span class="_">-d</span></span><br><span class="line"></span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl</span><br><span class="line">cm5ldGVzMB4XDTE5MDEyNTEwMTgzNFoXDTI5MDEyMjEwMTgzNFowFTETMBEGA1UE</span><br><span class="line">AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMmc</span><br><span class="line">TW0stLLP+M6Pc9wpRgZufg6eQ7puBfbYgik20QlO4LFtocgNUDa0y+aSXjxheA2C</span><br><span class="line">A+o9wW0IC3GHQHKgeFY8KXIJu6wM0TO+JNQy5XZAWfbsLeXU/sLhKuWET/KJzVWT</span><br><span class="line">0uBE+GCADAAQIec1oQXMbQ551hU5gBFcr67NXHpa2qwEGA1mGtZ7ztmW4+IFUD74</span><br><span class="line">G166z4AOgmR4YWxBs/+8NhfWudFD32xevBfSKuHRxRGG5dtffY8QnRbnrmy70HE5</span><br><span class="line">yzLtBvAGfCwtHLTP2ngCAnn2Fb6IeMdIYGpI1544ZjRbzT1YIWsG1v3dlu6tvK1q</span><br><span class="line">X5Pj+UTDmJuf2SW52A0CAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB</span><br><span class="line">/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKE2hV0DIG8fSf4/eOi5R2sPRfBW</span><br><span class="line">qTwgZDDT9dxZNhbxEInALdruwRUbKRpwaUBOGVpIlaK3/rZkAfjUwoDJ+J4fmmCX</span><br><span class="line">w3ySrYFjx6tqVFqCPjDkBHh4xpMwUlvsvryRuCEQUQgjqBvj6sWm9GERF2n3VYBF</span><br><span class="line">S8bjsQQAZJoE4W+OKchlEoSFlKhxAoeZx9CD3Rxnhj2og6<span class="keyword">do</span>VoGCUqAMh4WZWX+w</span><br><span class="line">pENnui6M96SysH3SkrA02RXWTGeKzK4E6Av3IG+2a2hauHorbqVfaM6HeL3hkU/B</span><br><span class="line">JCWpOgN3T4Fw7E359CBQxnSHPasmZ5VBoyIk/HUU6ZlMK6Xo6JlbS7ZvVl4=</span><br><span class="line">-----END CERTIFICATE-----</span><br></pre></td></tr></table></figure></p>
<p>获取 <code>mutiboard-ldap</code> 的 token：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(⎈|aws-local:default)❯ <span class="built_in">echo</span> $(kubectl get secret $(kubectl get secret | grep mutiboard-ldap | awk <span class="string">'&#123;print $1&#125;'</span>) -o go-template=<span class="string">'&#123;&#123;.data.token&#125;&#125;'</span>) | base64 <span class="_">-d</span></span><br><span class="line"></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im11dGlib2FyZC1sZGFwLXRva2VuLWJ3NWdmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im11dGlib2FyZC1sZGFwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMjVmZjI4MGQtYWJhMi0xMWVhLWFlOGEtMDIzOTBjMzcyNzhlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6bXV0aWJvYXJkLWxkYXA<span class="keyword">if</span>Q.q14hqEu2p70_YczDviR6c8McDM5vfnKPzjO9usCsC-uQUxciBbuJU_PK9j3uawppUNlrs3rAPrZIGUS7Jv14rifEXpGxIIfGR6n8-le0b-9YvMZCgs9-jhf-1r01EAnZFh6gcXfxESFguFQI0vYOsX4P2LQvZ9XTMzsqXbW3KGYao5elAjCE4e8Rg4--9e_zU8NGTEycsvUMxP-9p0SaAzn9Iak3saZtAnzJq5hkSf1t7l2_CgEsYN-3b7uGpHupK_zdgAeOflj9ze4Cz2YScv5eixwVXJ-RcI4lgSFCgt5yzSbnIuHgxRZyN3NcYLrSBYKftezZysWm3jELgLPogQ</span><br></pre></td></tr></table></figure></p>
<p>至此，各集群的 <code>ca.crt</code> 和 <code>token</code> 都已获取，下面会告知如何进行配置使用这些 <code>ca.crt</code> 和 <code>token</code></p>
<h2 id="SA-RBAC列子"><a href="#SA-RBAC列子" class="headerlink" title="SA RBAC列子"></a>SA RBAC列子</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-role</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">  resources:</span> [<span class="string">"namespaces"</span>]</span><br><span class="line"><span class="attr">  verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-listnamespace</span><br><span class="line"><span class="attr">roleRef:</span> <span class="comment">#引用的角色</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> ops-role</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">subjects:</span> <span class="comment">#主体</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> RoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-ci-admin</span><br><span class="line"><span class="attr">  namespace:</span> ops-ci</span><br><span class="line"><span class="attr">roleRef:</span> <span class="comment">#引用的角色</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> admin</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">subjects:</span> <span class="comment">#主体</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> RoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-qa-admin</span><br><span class="line"><span class="attr">  namespace:</span> ops-qa</span><br><span class="line"><span class="attr">roleRef:</span> <span class="comment">#引用的角色</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> admin</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">subjects:</span> <span class="comment">#主体</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br></pre></td></tr></table></figure>
<p>这段YAML的意思为：创建了一个ops-admin的sa，并为这个sa赋予了两个命名空间(ops-ci、ops-qa) admin 的权限。</p>
<p>具体想了解更多rbac相关的说明，可参考：<a href="https://www.cnblogs.com/wlbl/p/10694364.html" target="_blank" rel="external">https://www.cnblogs.com/wlbl/p/10694364.html</a></p>
<h2 id="ldap说明"><a href="#ldap说明" class="headerlink" title="ldap说明"></a>ldap说明</h2><p>我司<code>ldap</code>目录规则如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|--域</span><br><span class="line">|--|---公司</span><br><span class="line">|--|----|----分公司</span><br><span class="line">|--|----|-----|----部门</span><br><span class="line">|--|----|-----|-----|-----用户</span><br></pre></td></tr></table></figure></p>
<p>对应的<code>Distinguished Name</code>显示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CN=Peng Xu,OU=部门,OU=分公司,OU=公司,DC=corp,DC=xxx,DC=com</span><br></pre></td></tr></table></figure></p>
<p>这里我会获取第一个<code>OU</code>作为<code>group</code>，如果你的需求和我不一样，可以给我提 issue 进行适配</p>
<p>ldap 详细说明请参考：<a href="https://blog.poychang.net/ldap-introduction" target="_blank" rel="external">https://blog.poychang.net/ldap-introduction</a></p>
<h2 id="configmap-yaml-配置说明"><a href="#configmap-yaml-配置说明" class="headerlink" title="configmap.yaml 配置说明"></a>configmap.yaml 配置说明</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ldap:</span></span><br><span class="line"><span class="attr">  addr:</span> ldap://<span class="number">192.168</span><span class="number">.3</span><span class="number">.81</span>:<span class="number">389</span></span><br><span class="line"><span class="attr">  adminUser:</span> xxxxx</span><br><span class="line"><span class="attr">  adminPwd:</span> xxxxxx</span><br><span class="line"><span class="attr">  baseDN:</span> dc=corp,dc=patsnap,dc=com</span><br><span class="line"><span class="attr">  filter:</span> (&amp;(objectClass=person)(sAMAccountName=%s))</span><br><span class="line"><span class="attr">  attributes:</span> user_dn</span><br><span class="line"><span class="attr">  orgUnitName:</span> OU=</span><br><span class="line"><span class="comment">#全局用户/用户组与SA的映射</span></span><br><span class="line"><span class="attr">rbac:</span></span><br><span class="line">  DevOps team:</span><br><span class="line"><span class="attr">    sa:</span> ops-admin</span><br><span class="line"><span class="attr">    ns:</span> kube-system</span><br><span class="line"><span class="attr">  xupeng:</span></span><br><span class="line"><span class="attr">    sa:</span> inno-admin</span><br><span class="line"><span class="attr">    ns:</span> default</span><br><span class="line"><span class="attr">clusters:</span></span><br><span class="line">  <span class="comment">#集群别名，在登录下拉框中显示的key，这个别名需要和secret.sh中的ca.crt和token的键名一一对应</span></span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line">    <span class="comment">#apiserver地址，能够被当前工具访问到</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-dev.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">6443</span></span><br><span class="line">    <span class="comment">#kubernetes dashboard地址，能够被当前工具访问到</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-dev.jiunile.com</span><br><span class="line">    <span class="comment">#集群说明，在登录下拉框中显示的名称</span></span><br><span class="line"><span class="attr">    desc:</span> Dev Cluster</span><br><span class="line">    <span class="comment">#针对单独集群细分</span></span><br><span class="line">    <span class="comment">#rbac:</span></span><br><span class="line">    <span class="comment">#  DevOps team:</span></span><br><span class="line">    <span class="comment">#    sa: admin</span></span><br><span class="line">    <span class="comment">#    ns: kube-system</span></span><br><span class="line">    <span class="comment">#  xupeng:</span></span><br><span class="line">    <span class="comment">#    sa: ops-admin</span></span><br><span class="line">    <span class="comment">#    ns: default</span></span><br><span class="line"><span class="attr">  cnrelease:</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-cn-release.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-cn-release.jiunile.com</span><br><span class="line"><span class="attr">    desc:</span> CN Release Cluster</span><br><span class="line"><span class="attr">  usrelease:</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-us-release.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-us-release.jiunile.com</span><br><span class="line"><span class="attr">    desc:</span> US Release Cluster</span><br><span class="line"><span class="attr">  euprod:</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-eu-prod.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-eu-prod.jiunile.com</span><br><span class="line"><span class="attr">    desc:</span> EU Prod Cluster</span><br></pre></td></tr></table></figure>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ol>
<li>修改并部署<code>deploy/configmap.yaml</code></li>
<li>将各集群获取的 <code>ca.crt</code> 和 <code>token</code> 写入到对应的deploy/token下</li>
<li><p>执行 deploy 下的 secret.sh 脚本 <code>sh deploy/secret.sh</code></p>
<blockquote>
<p>注意: secret.sh 中的<code>xx_token/xxx_ca.crt</code>中的 <code>xx</code> 对应于<code>configmap.yaml</code> 中的<strong>集群别名，必须要一一对应</strong></p>
</blockquote>
</li>
<li><p>部署<code>deploy/deployment.yaml</code></p>
</li>
</ol>
<h2 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h2><p><a href="http://{nodeip}:31000" target="_blank" rel="external">http://{nodeip}:31000</a></p>
<p>视频地址：<a href="http://www.youtube.com/watch?v=ILiviSLbSq8" target="_blank" rel="external">http://www.youtube.com/watch?v=ILiviSLbSq8</a></p>
<p>git地址：<a href="https://github.com/icyxp/kubernetes-dashboard-ldap" target="_blank" rel="external">https://github.com/icyxp/kubernetes-dashboard-ldap</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[自定义 Kubernetes 调度器]]></title>
      <url>http://team.jiunile.com/blog/2020/06/k8s-custom-scheduler.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><code>kube-scheduler</code> 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。</p>
<h2 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h2><p>默认情况下，<code>kube-scheduler</code> 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。</p>
<p><code>kube-scheduler</code> 的主要作用就是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上去，是一个独立的二进制程序，启动之后会一直监听 API Server，获取到 <code>PodSpec.NodeName</code> 为空的 Pod，对每个 Pod 都会创建一个 binding。<br><img src="/images/k8s/kube-scheduler-overview.png" alt="kube-scheduler-overview"><br><a id="more"></a><br>这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：</p>
<ul>
<li>如何保证全部的节点调度的公平性？要知道并不是所有节点资源配置一定都是一样的</li>
<li>如何保证每个节点都能被分配资源？</li>
<li>集群资源如何能够被高效利用？</li>
<li>集群资源如何才能被最大化使用？</li>
<li>如何保证 Pod 调度的性能和效率？</li>
<li>用户是否可以根据自己的实际需求定制自己的调度策略？</li>
</ul>
<p>考虑到实际环境中的各种复杂情况，kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</p>
<p>kubernetes 调度器的源码位于 <code>kubernetes/pkg/scheduler</code> 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/scheduler</span><br><span class="line">-- scheduler.go         //调度相关的具体实现</span><br><span class="line">|-- algorithm</span><br><span class="line">|   |-- predicates      //节点筛选策略</span><br><span class="line">|   |-- priorities      //节点打分策略</span><br><span class="line">|-- algorithmprovider</span><br><span class="line">|   |-- defaults         //定义默认的调度器</span><br></pre></td></tr></table></figure></p>
<p>其中 Scheduler 创建和运行的核心程序，对应的代码在 <code>pkg/scheduler/scheduler.go</code>，如果要查看 <code>kube-scheduler</code> 的入口程序，对应的代码在 <code>cmd/kube-scheduler/scheduler.go</code>。</p>
<h2 id="自定义调度器"><a href="#自定义调度器" class="headerlink" title="自定义调度器"></a>自定义调度器</h2><p>一般来说，我们有4种扩展 Kubernetes 调度器的方法。</p>
<ul>
<li>一种方法就是直接 clone 官方的 kube-scheduler 源代码，在合适的位置直接修改代码，然后重新编译运行修改后的程序，当然这种方法是最不建议使用的，也不实用，因为需要花费大量额外的精力来和上游的调度程序更改保持一致。</li>
<li>第二种方法就是和默认的调度程序一起运行独立的调度程序，默认的调度器和我们自定义的调度器可以通过 Pod 的 <code>spec.schedulerName</code> 来覆盖各自的 Pod，默认是使用 default 默认的调度器，但是多个调度程序共存的情况下也比较麻烦，比如当多个调度器将 Pod 调度到同一个节点的时候，可能会遇到一些问题，因为很有可能两个调度器都同时将两个 Pod 调度到同一个节点上去，但是很有可能其中一个 Pod 运行后其实资源就消耗完了，并且维护一个高质量的自定义调度程序也不是很容易的，因为我们需要全面了解默认的调度程序，整体 Kubernetes 的架构知识以及各种 Kubernetes API 对象的各种关系或限制。</li>
<li>第三种方法是<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md" target="_blank" rel="external">调度器扩展程序</a>，这个方案目前是一个可行的方案，可以和上游调度程序兼容，所谓的调度器扩展程序其实就是一个可配置的 Webhook 而已，里面包含 <code>过滤器</code> 和 <code>优先级</code> 两个端点，分别对应调度周期中的两个主要阶段（过滤和打分）。</li>
<li>第四种方法是通过调度框架（Scheduling Framework），Kubernetes v1.15 版本中引入了可插拔架构的调度框架，使得定制调度器这个任务变得更加的容易。调库框架向现有的调度器中添加了一组插件化的 API，该 API 在保持调度程序“核心”简单且易于维护的同时，使得大部分的调度功能以插件的形式存在，而且在我们现在的 v1.16 版本中上面的 <code>调度器扩展程序</code> 也已经被废弃了，所以以后调度框架才是自定义调度器的核心方式。</li>
</ul>
<p>这里我们可以简单介绍下后面两种方式的实现。</p>
<h3 id="调度器扩展程序"><a href="#调度器扩展程序" class="headerlink" title="调度器扩展程序"></a>调度器扩展程序</h3><p>在进入调度器扩展程序之前，我们再来了解下 Kubernetes 调度程序是如何工作的:</p>
<ol>
<li>默认调度器根据指定的参数启动（我们使用 kubeadm 搭建的集群，启动配置文件位于 <code>/etc/kubernetes/manifests/kube-schdueler.yaml</code>）</li>
<li>watch apiserver，将 <code>spec.nodeName</code> 为空的 Pod 放入调度器内部的调度队列中</li>
<li>从调度队列中 Pop 出一个 Pod，开始一个标准的调度周期</li>
<li>从 Pod 属性中检索“硬性要求”（比如 CPU/内存请求值，nodeSelector/nodeAffinity），然后过滤阶段发生，在该阶段计算出满足要求的节点候选列表</li>
<li>从 Pod 属性中检索“软需求”，并应用一些默认的“软策略”（比如 Pod 倾向于在节点上更加聚拢或分散），最后，它为每个候选节点给出一个分数，并挑选出得分最高的最终获胜者</li>
<li>和 apiserver 通信（发送绑定调用），然后设置 Pod 的 <code>spec.nodeName</code> 属性以表示将该 Pod 调度到的节点。</li>
</ol>
<p>我们可以通过查看<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" rel="external">官方文档</a>，可以通过 <code>--config</code> 参数指定调度器将使用哪些参数，该配置文件应该包含一个 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration" target="_blank" rel="external">KubeSchedulerConfiguration</a> 对象，如下所示格式：（/etc/kubernetes/scheduler-extender.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 通过"--config" 传递文件内容</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeSchedulerConfiguration</span><br><span class="line"><span class="attr">clientConnection:</span></span><br><span class="line"><span class="attr">  kubeconfig:</span> <span class="string">"/etc/kubernetes/scheduler.conf"</span></span><br><span class="line"><span class="attr">algorithmSource:</span></span><br><span class="line"><span class="attr">  policy:</span></span><br><span class="line"><span class="attr">    file:</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">"/etc/kubernetes/scheduler-extender-policy.yaml"</span>  <span class="comment"># 指定自定义调度策略文件</span></span><br></pre></td></tr></table></figure></p>
<p>我们在这里应该输入的关键参数是 <code>algorithmSource.policy</code>，这个策略文件可以是本地文件也可以是 ConfigMap 资源对象，这取决于调度程序的部署方式，比如我们这里默认的调度器是静态 Pod 方式启动的，所以我们可以用本地文件的形式来配置。</p>
<p>该策略文件 <code>/etc/kubernetes/scheduler-extender-policy.yaml</code> 应该遵循 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#Policy" target="_blank" rel="external">kubernetes/pkg/scheduler/apis/config/legacy_types.go#L28</a> 的要求，在我们这里的 v1.16.2 版本中已经支持 JSON 和 YAML 两种格式的策略文件，下面是我们定义的一个简单的示例，可以查看 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#Extender" target="_blank" rel="external">Extender</a> 描述了解策略文件的定义规范：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Policy</span><br><span class="line"><span class="attr">extenders:</span></span><br><span class="line"><span class="attr">- urlPrefix:</span> <span class="string">"http://127.0.0.1:8888/"</span></span><br><span class="line"><span class="attr">  filterVerb:</span> <span class="string">"filter"</span></span><br><span class="line"><span class="attr">  prioritizeVerb:</span> <span class="string">"prioritize"</span></span><br><span class="line"><span class="attr">  weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  enableHttps:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>我们这里的 Policy 策略文件是通过定义 <code>extenders</code> 来扩展调度器的，有时候我们不需要去编写代码，可以直接在该配置文件中通过指定 <code>predicates</code> 和 <code>priorities</code> 来进行自定义，如果没有指定则会使用默认的 <code>DefaultProvier</code>：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="attr">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">    <span class="attr">"predicates"</span>: [&#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"MatchNodeSelector"</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"PodFitsResources"</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"PodFitsHostPorts"</span></span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"HostName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"priorities"</span>: [&#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"EqualPriority"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">2</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"ImageLocalityPriority"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">4</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"LeastRequestedPriority"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">2</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"BalancedResourceAllocation"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"extenders"</span>: [&#123;</span><br><span class="line">        <span class="attr">"urlPrefix"</span>: <span class="string">"/prefix"</span>,</span><br><span class="line">        <span class="attr">"filterVerb"</span>: <span class="string">"filter"</span>,</span><br><span class="line">        <span class="attr">"prioritizeVerb"</span>: <span class="string">"prioritize"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">"bindVerb"</span>: <span class="string">"bind"</span>,</span><br><span class="line">        <span class="attr">"enableHttps"</span>: <span class="literal">false</span></span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>改策略文件定义了一个 HTTP 的扩展程序服务，该服务运行在 <code>127.0.0.1:8888</code> 下面，并且已经将该策略注册到了默认的调度器中，这样在过滤和打分阶段结束后，可以将结果分别传递给该扩展程序的端点 <code>&lt;urlPrefix&gt;/&lt;filterVerb&gt;</code> 和 <code>&lt;urlPrefix&gt;/&lt;prioritizeVerb&gt;</code>，在扩展程序中，我们可以进一步过滤并确定优先级，以适应我们的特定业务需求。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>我们直接用 golang 来实现一个简单的调度器扩展程序，当然你可以使用其他任何编程语言，如下所示：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    router := httprouter.New()</span><br><span class="line">    router.GET(<span class="string">"/"</span>, Index)</span><br><span class="line">    router.POST(<span class="string">"/filter"</span>, Filter)</span><br><span class="line">    router.POST(<span class="string">"/prioritize"</span>, Prioritize)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(<span class="string">":8888"</span>, router))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后接下来我们需要实现 <code>/filter</code> 和 <code>/prioritize</code> 两个端点的处理程序。</p>
<p>其中 <code>Filter</code> 这个扩展函数接收一个输入类型为 <code>schedulerapi.ExtenderArgs</code> 的参数，然后返回一个类型为 <code>*schedulerapi.ExtenderFilterResult</code> 的值。在函数中，我们可以进一步过滤输入的节点：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// filter 根据扩展程序定义的预选规则来过滤节点</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">filter</span><span class="params">(args schedulerapi.ExtenderArgs)</span> *<span class="title">schedulerapi</span>.<span class="title">ExtenderFilterResult</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> filteredNodes []v1.Node</span><br><span class="line">	failedNodes := <span class="built_in">make</span>(schedulerapi.FailedNodesMap)</span><br><span class="line">	pod := args.Pod</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _, node := <span class="keyword">range</span> args.Nodes.Items &#123;</span><br><span class="line">		fits, failReasons, _ := podFitsOnNode(pod, node)</span><br><span class="line">		<span class="keyword">if</span> fits &#123;</span><br><span class="line">			filteredNodes = <span class="built_in">append</span>(filteredNodes, node)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			failedNodes[node.Name] = strings.Join(failReasons, <span class="string">","</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	result := schedulerapi.ExtenderFilterResult&#123;</span><br><span class="line">		Nodes: &amp;v1.NodeList&#123;</span><br><span class="line">			Items: filteredNodes,</span><br><span class="line">		&#125;,</span><br><span class="line">		FailedNodes: failedNodes,</span><br><span class="line">		Error:       <span class="string">""</span>,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在过滤函数中，我们循环每个节点然后用我们自己实现的业务逻辑来判断是否应该批准该节点，这里我们实现比较简单，在 <code>podFitsOnNode()</code> 函数中我们只是简单的检查随机数是否为偶数来判断即可，如果是的话我们就认为这是一个幸运的节点，否则拒绝批准该节点。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> predicatesSorted = []<span class="keyword">string</span>&#123;LuckyPred&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> predicatesFuncs = <span class="keyword">map</span>[<span class="keyword">string</span>]FitPredicate&#123;</span><br><span class="line">    LuckyPred: LuckyPredicate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> FitPredicate <span class="function"><span class="keyword">func</span><span class="params">(pod *v1.Pod, node v1.Node)</span> <span class="params">(<span class="keyword">bool</span>, []<span class="keyword">string</span>, error)</span></span><br><span class="line"></span><br><span class="line"><span class="title">func</span> <span class="title">podFitsOnNode</span><span class="params">(pod *v1.Pod, node v1.Node)</span> <span class="params">(<span class="keyword">bool</span>, []<span class="keyword">string</span>, error)</span></span> &#123;</span><br><span class="line">    fits := <span class="literal">true</span></span><br><span class="line">    <span class="keyword">var</span> failReasons []<span class="keyword">string</span></span><br><span class="line">    <span class="keyword">for</span> _, predicateKey := <span class="keyword">range</span> predicatesSorted &#123;</span><br><span class="line">        fit, failures, err := predicatesFuncs[predicateKey](pod, node)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>, <span class="literal">nil</span>, err</span><br><span class="line">        &#125;</span><br><span class="line">        fits = fits &amp;&amp; fit</span><br><span class="line">        failReasons = <span class="built_in">append</span>(failReasons, failures...)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fits, failReasons, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LuckyPredicate</span><span class="params">(pod *v1.Pod, node v1.Node)</span> <span class="params">(<span class="keyword">bool</span>, []<span class="keyword">string</span>, error)</span></span> &#123;</span><br><span class="line">    lucky := rand.Intn(<span class="number">2</span>) == <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> lucky &#123;</span><br><span class="line">        log.Printf(<span class="string">"pod %v/%v is lucky to fit on node %v\n"</span>, pod.Name, pod.Namespace, node.Name)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>, <span class="literal">nil</span>, <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">    log.Printf(<span class="string">"pod %v/%v is unlucky to fit on node %v\n"</span>, pod.Name, pod.Namespace, node.Name)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>, []<span class="keyword">string</span>&#123;LuckyPredFailMsg&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>同样的打分功能用同样的方式来实现，我们在每个节点上随机给出一个分数：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// it's webhooked to pkg/scheduler/core/generic_scheduler.go#PrioritizeNodes()</span></span><br><span class="line"><span class="comment">// 这个函数输出的分数会被添加会默认的调度器</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">prioritize</span><span class="params">(args schedulerapi.ExtenderArgs)</span> *<span class="title">schedulerapi</span>.<span class="title">HostPriorityList</span></span> &#123;</span><br><span class="line">	pod := args.Pod</span><br><span class="line">	nodes := args.Nodes.Items</span><br><span class="line"></span><br><span class="line">	hostPriorityList := <span class="built_in">make</span>(schedulerapi.HostPriorityList, <span class="built_in">len</span>(nodes))</span><br><span class="line">	<span class="keyword">for</span> i, node := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">		score := rand.Intn(schedulerapi.MaxPriority + <span class="number">1</span>)  <span class="comment">// 在最大优先级内随机取一个值</span></span><br><span class="line">		log.Printf(luckyPrioMsg, pod.Name, pod.Namespace, score)</span><br><span class="line">		hostPriorityList[i] = schedulerapi.HostPriority&#123;</span><br><span class="line">			Host:  node.Name,</span><br><span class="line">			Score: score,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;hostPriorityList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以使用下面的命令来编译打包我们的应用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GOOS=linux GOARCH=amd64 go build -o app</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>本节调度器扩展程序完整的代码获取地址：<a href="https://github.com/icyxp/sample-scheduler-extender" target="_blank" rel="external">https://github.com/icyxp/sample-scheduler-extender</a>。</p>
</blockquote>
<p>构建完成后，将应用 <code>app</code> 拷贝到 <code>kube-scheduler</code> 所在的节点直接运行即可。现在我们就可以将上面的策略文件配置到 <code>kube-scheduler</code> 组件中去了，我们这里集群是 kubeadm 搭建的，所以直接修改文件 <code>/etc/kubernetes/manifests/kube-schduler.yaml</code> 文件即可，内容如下所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    component:</span> kube-scheduler</span><br><span class="line"><span class="attr">    tier:</span> control-plane</span><br><span class="line"><span class="attr">  name:</span> kube-scheduler</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> kube-scheduler</span><br><span class="line"><span class="bullet">    -</span> --authentication-kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="bullet">    -</span> --authorization-kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="bullet">    -</span> --bind-address=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="bullet">    -</span> --kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="bullet">    -</span> --leader-elect=<span class="literal">true</span></span><br><span class="line"><span class="bullet">    -</span> --config=/etc/kubernetes/scheduler-extender.yaml</span><br><span class="line"><span class="bullet">    -</span> --v=<span class="number">9</span></span><br><span class="line"><span class="attr">    image:</span> gcr.azk8s.cn/google_containers/kube-scheduler:v1<span class="number">.16</span><span class="number">.2</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">    livenessProbe:</span></span><br><span class="line"><span class="attr">      failureThreshold:</span> <span class="number">8</span></span><br><span class="line"><span class="attr">      httpGet:</span></span><br><span class="line"><span class="attr">        host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">        path:</span> /healthz</span><br><span class="line"><span class="attr">        port:</span> <span class="number">10251</span></span><br><span class="line"><span class="attr">        scheme:</span> HTTP</span><br><span class="line"><span class="attr">      initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">      timeoutSeconds:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">    name:</span> kube-scheduler</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="attr">      name:</span> kubeconfig</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /etc/kubernetes/scheduler-extender.yaml</span><br><span class="line"><span class="attr">      name:</span> extender</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /etc/kubernetes/scheduler-extender-policy.yaml</span><br><span class="line"><span class="attr">      name:</span> extender-policy</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  priorityClassName:</span> system-cluster-critical</span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="attr">      type:</span> FileOrCreate</span><br><span class="line"><span class="attr">    name:</span> kubeconfig</span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /etc/kubernetes/scheduler-extender.yaml</span><br><span class="line"><span class="attr">      type:</span> FileOrCreate</span><br><span class="line"><span class="attr">    name:</span> extender</span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /etc/kubernetes/scheduler-extender-policy.yaml</span><br><span class="line"><span class="attr">      type:</span> FileOrCreate</span><br><span class="line"><span class="attr">    name:</span> extender-policy</span><br><span class="line"><span class="attr">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>当然我们这个地方是直接在系统默认的 <code>kube-scheduler</code> 上面配置的，我们也可以复制一个调度器的 YAML 文件然后更改下 schedulerName 来部署，这样就不会影响默认的调度器了，然后在需要使用这个测试的调度器的 Pod 上面指定 <code>spec.schedulerName</code> 即可。对于多调度器的使用可以查看官方文档 <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-multiple-schedulers/" target="_blank" rel="external">配置多个调度器</a>。</p>
</blockquote>
<p><code>kube-scheduler</code> 重新配置后可以查看日志来验证是否重启成功，需要注意的是一定需要将 <code>/etc/kubernetes/scheduler-extender.yaml</code> 和 <code>/etc/kubernetes/scheduler-extender-policy.yaml</code> 两个文件挂载到 Pod 中去：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs <span class="_">-f</span> kube-scheduler-ydzs-master -n kube-system</span><br><span class="line">I0102 15:17:38.824657       1 serving.go:319] Generated self-signed cert <span class="keyword">in</span>-memory</span><br><span class="line">I0102 15:17:39.472276       1 server.go:143] Version: v1.16.2</span><br><span class="line">I0102 15:17:39.472674       1 defaults.go:91] TaintNodesByCondition is enabled, PodToleratesNodeTaints predicate is mandatory</span><br><span class="line">W0102 15:17:39.479704       1 authorization.go:47] Authorization is disabled</span><br><span class="line">W0102 15:17:39.479733       1 authentication.go:79] Authentication is disabled</span><br><span class="line">I0102 15:17:39.479777       1 deprecated_insecure_serving.go:51] Serving healthz insecurely on [::]:10251</span><br><span class="line">I0102 15:17:39.480559       1 secure_serving.go:123] Serving securely on 127.0.0.1:10259</span><br><span class="line">I0102 15:17:39.682180       1 leaderelection.go:241] attempting to acquire leader lease  kube-system/kube-scheduler...</span><br><span class="line">I0102 15:17:56.500505       1 leaderelection.go:251] successfully acquired lease kube-system/kube-scheduler</span><br></pre></td></tr></table></figure></p>
<p>到这里我们就创建并配置了一个非常简单的调度扩展程序，现在我们来运行一个 Deployment 查看其工作原理，我们准备一个包含20个副本的部署 Yaml：(test-scheduler.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> pause</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> pause</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> pause</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> pause</span><br><span class="line"><span class="attr">        image:</span> gcr.azk8s.cn/google_containers/pause:<span class="number">3.1</span></span><br></pre></td></tr></table></figure></p>
<p>直接创建上面的资源对象：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kuectl apply <span class="_">-f</span> <span class="built_in">test</span>-scheduler.yaml</span><br><span class="line">deployment.apps/pause created</span><br></pre></td></tr></table></figure></p>
<p>这个时候我们去查看下我们编写的调度器扩展程序日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./app</span><br><span class="line">......</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is unlucky to fit on node ydzs-node1</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 7</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 9</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node3</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node4</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node1</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node2</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 4</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 8</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 Pod 调度的过程，另外默认调度程序会定期重试失败的 Pod，因此它们将一次又一次地重新传递到我们的调度扩展程序上，我们的逻辑是检查随机数是否为偶数，所以最终所有 Pod 都将处于运行状态。</p>
<p>调度器扩展程序可能是在一些情况下可以满足我们的需求，但是他仍然有一些限制和缺点：</p>
<ul>
<li>通信成本：数据在默认调度程序和调度器扩展程序之间以 <code>http（s）</code> 传输，在执行序列化和反序列化的时候有一定成本</li>
<li>有限的扩展点：扩展程序只能在某些阶段的末尾参与，例如 <code>“Filter”</code> 和 <code>“Prioritize”</code> ，它们不能在任何阶段的开始或中间被调用</li>
<li>减法优于加法：与默认调度程序传递的节点候选列表相比，我们可能有一些需求需要添加新的候选节点列表，但这是比较冒险的操作，因为不能保证新节点可以通过其他要求，所以，调度器扩展程序最好执行 <code>“减法”</code>（进一步过滤），而不是 <code>“加法”</code>（添加节点）</li>
<li>缓存共享：上面只是一个简单的测试示例，但在真实的项目中，我们是需要通过查看整个集群的状态来做出调度决策的，默认调度程序可以很好地调度决策，但是无法共享其缓存，这意味着我们必须构建和维护自己的缓存</li>
</ul>
<p>由于这些局限性，Kubernetes 调度小组就提出了上面第四种方法来进行更好的扩展，也就是<code>调度框架（Scheduler Framework）</code>，它基本上可以解决我们遇到的所有难题，现在也已经成官方推荐的扩展方式，所以这将是以后扩展调度器的最主流的方式。</p>
<h2 id="调度框架"><a href="#调度框架" class="headerlink" title="调度框架"></a>调度框架</h2><p>调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑（我们称之为扩展），并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知。</p>
<p>我们知道每当调度一个 Pod 时，都会按照两个过程来执行：调度过程和绑定过程。</p>
<p>调度过程为 Pod 选择一个合适的节点，绑定过程则将调度过程的决策应用到集群中（也就是在被选定的节点上运行 Pod），将调度过程和绑定过程合在一起，称之为调度上下文（<strong>scheduling context</strong>）。需要注意的是调度过程是<code>同步</code>运行的（同一时间点只为一个 Pod 进行调度），绑定过程可异步运行（同一时间点可并发为多个 Pod 执行绑定）。</p>
<p>调度过程和绑定过程遇到如下情况时会中途退出：</p>
<ul>
<li>调度程序认为当前没有该 Pod 的可选节点</li>
<li>内部错误</li>
</ul>
<p>这个时候，该 Pod 将被放回到 待调度队列，并等待下次重试。</p>
<h3 id="扩展点（Extension-Points）"><a href="#扩展点（Extension-Points）" class="headerlink" title="扩展点（Extension Points）"></a>扩展点（Extension Points）</h3><p>下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。<br><img src="/images/k8s/scheduling-framework-extensions.png" alt="scheduling-framework-extensions"></p>
<ol>
<li><code>QueueSort</code> 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，<code>QueueSort</code> 扩展本质上只需要实现一个方法 <code>Less(Pod1, Pod2)</code> 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 <code>QueueSort</code> 插件生效。</li>
<li><code>Pre-filter</code> 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 <code>pre-filter</code> 返回了 error，则调度过程终止。</li>
<li><code>Filter</code> 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 <code>filter</code> 扩展；如果任何一个 <code>filter</code> 将节点标记为不可选，则余下的 <code>filter</code> 扩展将不会被执行。调度器可以同时对多个节点执行 <code>filter</code> 扩展。</li>
<li><code>Post-filter</code> 是一个通知类型的扩展点，调用该扩展的参数是 <code>filter</code> 阶段结束后被筛选为可选节点的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。</li>
<li><code>Scoring</code> 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 <code>Soring</code> 扩展，评分结果是一个范围内的整数。在 <code>normalize scoring</code> 阶段，调度器将会把每个 <code>scoring</code> 扩展对具体某个节点的评分结果和该扩展的权重合并起来，作为最终评分结果。</li>
<li><code>Normalize scoring</code> 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 <code>scoring</code> 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 <code>normalize scoring</code> 扩展一次。</li>
<li><code>Reserve</code> 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况。（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 <code>reserved</code> 状态以后，要么在绑定失败时触发 <code>Unreserve</code> 扩展，要么在绑定成功时，由 <code>Post-bind</code> 扩展结束绑定过程。</li>
<li><code>Permit</code> 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项：<ul>
<li><code>approve</code>（批准）：当所有的 <code>permit</code> 扩展都 <code>approve</code> 了 Pod 与节点的绑定，调度器将继续执行绑定过程</li>
<li><code>deny</code>（拒绝）：如果任何一个 <code>permit</code> 扩展 <code>deny</code> 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展</li>
<li><code>wait</code>（等待）：如果一个 <code>permit</code> 扩展返回了 <code>wait</code>，则 Pod 将保持在 <code>permit</code> 阶段，直到被其他扩展 <code>approve</code>，如果超时事件发生，<code>wait</code> 状态变成 <code>deny</code>，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展</li>
</ul>
</li>
<li><code>Pre-bind</code> 扩展用于在 Pod 绑定之前执行某些逻辑。例如，<code>pre-bind</code> 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 <code>pre-bind</code> 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展。</li>
<li><code>Bind</code> 扩展用于将 Pod 绑定到节点上：<ul>
<li>只有所有的 <code>pre-bind</code> 扩展都成功执行了，<code>bind</code> 扩展才会执行</li>
<li>调度框架按照 <code>bind</code> 扩展注册的顺序逐个调用 <code>bind</code> 扩展</li>
<li>具体某个 <code>bind</code> 扩展可以选择处理或者不处理该 Pod</li>
<li>如果某个 <code>` 扩展处理了该 Pod 与节点的绑定，余下的</code>bind` 扩展将被忽略</li>
</ul>
</li>
<li><code>Post-bind</code> 是一个通知性质的扩展：<ul>
<li><code>Post-bind</code> 扩展在 Pod 成功绑定到节点上之后被动调用</li>
<li><code>Post-bind</code> 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作</li>
</ul>
</li>
<li><code>Unreserve</code> 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 <code>unreserve</code> 扩展将被调用。<code>Unreserve</code> 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，<code>reserve</code> 扩展和 <code>unreserve</code> 扩展应该成对出现。</li>
</ol>
<p>如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现扩展点接口，对应的扩展点接口我们可以在源码 <code>pkg/scheduler/framework/v1alpha1/interface.go</code> 文件中找到，如下所示：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Plugin is the parent type for all the scheduling framework plugins.</span></span><br><span class="line"><span class="keyword">type</span> Plugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Name() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> QueueSortPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Less(*PodInfo, *PodInfo) <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreFilterPlugin is an interface that must be implemented by "prefilter" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called at the beginning of the scheduling cycle.</span></span><br><span class="line"><span class="keyword">type</span> PreFilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PreFilter(pc *PluginContext, p *v1.Pod) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// FilterPlugin is an interface for Filter plugins. These plugins are called at the</span></span><br><span class="line"><span class="comment">// filter extension point for filtering out hosts that cannot run a pod.</span></span><br><span class="line"><span class="comment">// This concept used to be called 'predicate' in the original scheduler.</span></span><br><span class="line"><span class="comment">// These plugins should return "Success", "Unschedulable" or "Error" in Status.code.</span></span><br><span class="line"><span class="comment">// However, the scheduler accepts other valid codes as well.</span></span><br><span class="line"><span class="comment">// Anything other than "Success" will lead to exclusion of the given host from</span></span><br><span class="line"><span class="comment">// running the pod.</span></span><br><span class="line"><span class="keyword">type</span> FilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Filter(pc *PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PostFilterPlugin is an interface for Post-filter plugin. Post-filter is an</span></span><br><span class="line"><span class="comment">// informational extension point. Plugins will be called with a list of nodes</span></span><br><span class="line"><span class="comment">// that passed the filtering phase. A plugin may use this data to update internal</span></span><br><span class="line"><span class="comment">// state or to generate logs/metrics.</span></span><br><span class="line"><span class="keyword">type</span> PostFilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PostFilter(pc *PluginContext, pod *v1.Pod, nodes []*v1.Node, filteredNodesStatuses NodeToStatusMap) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlugin is an interface that must be implemented by "score" plugins to rank</span></span><br><span class="line"><span class="comment">// nodes that passed the filtering phase.</span></span><br><span class="line"><span class="keyword">type</span> ScorePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Score(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) (<span class="keyword">int</span>, *Status)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScoreWithNormalizePlugin is an interface that must be implemented by "score"</span></span><br><span class="line"><span class="comment">// plugins that also need to normalize the node scoring results produced by the same</span></span><br><span class="line"><span class="comment">// plugin's "Score" method.</span></span><br><span class="line"><span class="keyword">type</span> ScoreWithNormalizePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	ScorePlugin</span><br><span class="line">	NormalizeScore(pc *PluginContext, p *v1.Pod, scores NodeScoreList) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ReservePlugin is an interface for Reserve plugins. These plugins are called</span></span><br><span class="line"><span class="comment">// at the reservation point. These are meant to update the state of the plugin.</span></span><br><span class="line"><span class="comment">// This concept used to be called 'assume' in the original scheduler.</span></span><br><span class="line"><span class="comment">// These plugins should return only Success or Error in Status.code. However,</span></span><br><span class="line"><span class="comment">// the scheduler accepts other valid codes as well. Anything other than Success</span></span><br><span class="line"><span class="comment">// will lead to rejection of the pod.</span></span><br><span class="line"><span class="keyword">type</span> ReservePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Reserve(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreBindPlugin is an interface that must be implemented by "prebind" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called before a pod being scheduled.</span></span><br><span class="line"><span class="keyword">type</span> PreBindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PreBind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PostBindPlugin is an interface that must be implemented by "postbind" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called after a pod is successfully bound to a node.</span></span><br><span class="line"><span class="keyword">type</span> PostBindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PostBind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// UnreservePlugin is an interface for Unreserve plugins. This is an informational</span></span><br><span class="line"><span class="comment">// extension point. If a pod was reserved and then rejected in a later phase, then</span></span><br><span class="line"><span class="comment">// un-reserve plugins will be notified. Un-reserve plugins should clean up state</span></span><br><span class="line"><span class="comment">// associated with the reserved Pod.</span></span><br><span class="line"><span class="keyword">type</span> UnreservePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Unreserve(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PermitPlugin is an interface that must be implemented by "permit" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called before a pod is bound to a node.</span></span><br><span class="line"><span class="keyword">type</span> PermitPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Permit(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) (*Status, time.Duration)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// BindPlugin is an interface that must be implemented by "bind" plugins. Bind</span></span><br><span class="line"><span class="comment">// plugins are used to bind a pod to a Node.</span></span><br><span class="line"><span class="keyword">type</span> BindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Bind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对于调度框架插件的启用或者禁用，我们同样可以使用上面的 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration" target="_blank" rel="external">KubeSchedulerConfiguration</a> 资源对象来进行配置。下面的例子中的配置启用了一个实现了 <code>reserve</code> 和 <code>preBind</code> 扩展点的插件，并且禁用了另外一个插件，同时为插件 foo 提供了一些配置信息：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeSchedulerConfiguration</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line"><span class="attr">  reserve:</span></span><br><span class="line"><span class="attr">    enabled:</span></span><br><span class="line"><span class="attr">    - name:</span> foo</span><br><span class="line"><span class="attr">    - name:</span> bar</span><br><span class="line"><span class="attr">    disabled:</span></span><br><span class="line"><span class="attr">    - name:</span> baz</span><br><span class="line"><span class="attr">  preBind:</span></span><br><span class="line"><span class="attr">    enabled:</span></span><br><span class="line"><span class="attr">    - name:</span> foo</span><br><span class="line"><span class="attr">    disabled:</span></span><br><span class="line"><span class="attr">    - name:</span> baz</span><br><span class="line"></span><br><span class="line"><span class="attr">pluginConfig:</span></span><br><span class="line"><span class="attr">- name:</span> foo</span><br><span class="line"><span class="attr">  args:</span> <span class="string">&gt;</span><br><span class="line">    foo插件可以解析的任意内容</span></span><br></pre></td></tr></table></figure></p>
<p>扩展的调用顺序如下：</p>
<ul>
<li>如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展</li>
<li>如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展</li>
<li>默认插件的扩展始终被最先调用，然后按照 <code>KubeSchedulerConfiguration</code> 中扩展的激活 <code>enabled</code> 顺序逐个调用扩展点的扩展</li>
<li>可以先禁用默认插件的扩展，然后在 <code>enabled</code> 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序</li>
</ul>
<p>假设默认插件 foo 实现了 <code>reserve</code> 扩展点，此时我们要添加一个插件 bar，想要在 foo 之前被调用，则应该先禁用 foo 再按照 bar foo 的顺序激活。示例配置如下所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeSchedulerConfiguration</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line"><span class="attr">  reserve:</span></span><br><span class="line"><span class="attr">    enabled:</span></span><br><span class="line"><span class="attr">    - name:</span> bar</span><br><span class="line"><span class="attr">    - name:</span> foo</span><br><span class="line"><span class="attr">    disabled:</span></span><br><span class="line"><span class="attr">    - name:</span> foo</span><br></pre></td></tr></table></figure></p>
<p>在源码目录 <code>pkg/scheduler/framework/plugins/examples</code> 中有几个示范插件，我们可以参照其实现方式。</p>
<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><p>其实要实现一个调度框架的插件，并不难，我们只要实现对应的扩展点，然后将插件注册到调度器中即可，下面是默认调度器在初始化的时候注册的插件：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewRegistry</span><span class="params">()</span> <span class="title">Registry</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> Registry&#123;</span><br><span class="line">		<span class="comment">// FactoryMap:</span></span><br><span class="line">		<span class="comment">// New plugins are registered here.</span></span><br><span class="line">		<span class="comment">// example:</span></span><br><span class="line">		<span class="comment">// &#123;</span></span><br><span class="line">		<span class="comment">//  stateful_plugin.Name: stateful.NewStatefulMultipointExample,</span></span><br><span class="line">		<span class="comment">//  fooplugin.Name: fooplugin.New,</span></span><br><span class="line">		<span class="comment">// &#125;</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是可以看到默认并没有注册一些插件，所以要想让调度器能够识别我们的插件代码，就需要自己来实现一个调度器了，当然这个调度器我们完全没必要完全自己实现，直接调用默认的调度器，然后在上面的 <code>NewRegistry()</code> 函数中将我们的插件注册进去即可。在 <code>kube-scheduler</code> 的源码文件 <code>kubernetes/cmd/kube-scheduler/app/server.go</code> 中有一个 <code>NewSchedulerCommand</code> 入口函数，其中的参数是一个类型为 <code>Option</code> 的列表，而这个 <code>Option</code> 恰好就是一个插件配置的定义：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Option configures a framework.Registry.</span></span><br><span class="line"><span class="keyword">type</span> Option <span class="function"><span class="keyword">func</span><span class="params">(framework.Registry)</span> <span class="title">error</span></span><br><span class="line"></span><br><span class="line">// <span class="title">NewSchedulerCommand</span> <span class="title">creates</span> <span class="title">a</span> *<span class="title">cobra</span>.<span class="title">Command</span> <span class="title">object</span> <span class="title">with</span> <span class="title">default</span> <span class="title">parameters</span> <span class="title">and</span> <span class="title">registryOptions</span></span><br><span class="line"><span class="title">func</span> <span class="title">NewSchedulerCommand</span><span class="params">(registryOptions ...Option)</span> *<span class="title">cobra</span>.<span class="title">Command</span></span> &#123;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>所以我们完全就可以直接调用这个函数来作为我们的函数入口，并且传入我们自己实现的插件作为参数即可，而且该文件下面还有一个名为 <code>WithPlugin</code> 的函数可以来创建一个 <code>Option</code> 实例：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WithPlugin creates an Option based on plugin name and factory.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithPlugin</span><span class="params">(name <span class="keyword">string</span>, factory framework.PluginFactory)</span> <span class="title">Option</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="function"><span class="keyword">func</span><span class="params">(registry framework.Registry)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">		<span class="keyword">return</span> registry.Register(name, factory)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>所以最终我们的入口函数如下所示：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	rand.Seed(time.Now().UTC().UnixNano())</span><br><span class="line"></span><br><span class="line">	command := app.NewSchedulerCommand(</span><br><span class="line">		app.WithPlugin(sample.Name, sample.New), </span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	logs.InitLogs()</span><br><span class="line">	<span class="keyword">defer</span> logs.FlushLogs()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err := command.Execute(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		_, _ = fmt.Fprintf(os.Stderr, <span class="string">"%v\n"</span>, err)</span><br><span class="line">		os.Exit(<span class="number">1</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中 <code>app.WithPlugin(sample.Name, sample.New)</code> 就是我们接下来要实现的插件，从 <code>WithPlugin</code> 函数的参数也可以看出我们这里的 <code>sample.New</code> 必须是一个 <code>framework.PluginFactory</code> 类型的值，而 <code>PluginFactory</code> 的定义就是一个函数：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> PluginFactory = <span class="function"><span class="keyword">func</span><span class="params">(configuration *runtime.Unknown, f FrameworkHandle)</span> <span class="params">(Plugin, error)</span></span></span><br></pre></td></tr></table></figure></p>
<p>所以 <code>sample.New</code> 实际上就是上面的这个函数，在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可，插件实现如下所示，我们这里只是简单获取下数据打印日志，如果你有实际需求的可以根据获取的数据就行处理即可，我们这里只是实现了 <code>PreFilter</code>、<code>Filter</code>、<code>PreBind</code> 三个扩展点，其他的可以用同样的方式来扩展即可：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插件名称</span></span><br><span class="line"><span class="keyword">const</span> Name = <span class="string">"sample-plugin"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Args <span class="keyword">struct</span> &#123;</span><br><span class="line">	FavoriteColor  <span class="keyword">string</span> <span class="string">`json:"favorite_color,omitempty"`</span></span><br><span class="line">	FavoriteNumber <span class="keyword">int</span>    <span class="string">`json:"favorite_number,omitempty"`</span></span><br><span class="line">	ThanksTo       <span class="keyword">string</span> <span class="string">`json:"thanks_to,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Sample <span class="keyword">struct</span> &#123;</span><br><span class="line">	args   *Args</span><br><span class="line">	handle framework.FrameworkHandle</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">Name</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> Name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">PreFilter</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">	klog.V(<span class="number">3</span>).Infof(<span class="string">"prefilter pod: %v"</span>, pod.Name)</span><br><span class="line">	<span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">""</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">Filter</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">	klog.V(<span class="number">3</span>).Infof(<span class="string">"filter pod: %v, node: %v"</span>, pod.Name, nodeName)</span><br><span class="line">	<span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">""</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">PreBind</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> nodeInfo, ok := s.handle.NodeInfoSnapshot().NodeInfoMap[nodeName]; !ok &#123;</span><br><span class="line">		<span class="keyword">return</span> framework.NewStatus(framework.Error, fmt.Sprintf(<span class="string">"prebind get node info error: %+v"</span>, nodeName))</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		klog.V(<span class="number">3</span>).Infof(<span class="string">"prebind node info: %+v"</span>, nodeInfo.Node())</span><br><span class="line">		<span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">""</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(configuration *runtime.Unknown, f framework.FrameworkHandle)</span> <span class="params">(framework.Plugin, error)</span></span> &#123;</span><br><span class="line">	args := &amp;Args&#123;&#125;</span><br><span class="line">	<span class="keyword">if</span> err := framework.DecodeInto(configuration, args); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line">	klog.V(<span class="number">3</span>).Infof(<span class="string">"get plugin config args: %+v"</span>, args)</span><br><span class="line">	<span class="keyword">return</span> &amp;Sample&#123;</span><br><span class="line">		args: args,</span><br><span class="line">		handle: f,</span><br><span class="line">	&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>完整代码可以前往仓库 <a href="https://github.com/icyxp/sample-scheduler-framework" target="_blank" rel="external">https://github.com/icyxp/sample-scheduler-framework</a> 获取。</p>
</blockquote>
<p>实现完成后，编译打包成镜像即可，然后我们就可以当成普通的应用用一个 <code>Deployment</code> 控制器来部署即可，由于我们需要去获取集群中的一些资源对象，所以当然需要申请 RBAC 权限，然后同样通过 <code>--config</code> 参数来配置我们的调度器，同样还是使用一个 <code>KubeSchedulerConfiguration</code> 资源对象配置，可以通过 <code>plugins</code> 来启用或者禁用我们实现的插件，也可以通过 <code>pluginConfig</code> 来传递一些参数值给插件：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-clusterrole</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> endpoints</span><br><span class="line"><span class="bullet">      -</span> events</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> nodes</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> pods</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> delete</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> bindings</span><br><span class="line"><span class="bullet">      -</span> pods/binding</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> pods/status</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> patch</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> replicationcontrollers</span><br><span class="line"><span class="bullet">      -</span> services</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> apps</span><br><span class="line"><span class="bullet">      -</span> extensions</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> replicasets</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> apps</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> statefulsets</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> policy</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> poddisruptionbudgets</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> persistentvolumeclaims</span><br><span class="line"><span class="bullet">      -</span> persistentvolumes</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> configmaps</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"storage.k8s.io"</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> storageclasses</span><br><span class="line"><span class="bullet">      -</span> csinodes</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"coordination.k8s.io"</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> leases</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"events.k8s.io"</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> events</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="bullet">      -</span> patch</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-sa</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-clusterrolebinding</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-clusterrole</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-sa</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> scheduler-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  scheduler-config.yaml: <span class="string">|</span><br><span class="line"></span><span class="attr">    apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">    kind:</span> KubeSchedulerConfiguration</span><br><span class="line"><span class="attr">    schedulerName:</span> sample-scheduler</span><br><span class="line"><span class="attr">    leaderElection:</span></span><br><span class="line"><span class="attr">      leaderElect:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      lockObjectName:</span> sample-scheduler</span><br><span class="line"><span class="attr">      lockObjectNamespace:</span> kube-system</span><br><span class="line"><span class="attr">    plugins:</span></span><br><span class="line"><span class="attr">      preFilter:</span></span><br><span class="line"><span class="attr">        enabled:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">      filter:</span></span><br><span class="line"><span class="attr">        enabled:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">      preBind:</span></span><br><span class="line"><span class="attr">        enabled:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">    pluginConfig:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">      args:</span></span><br><span class="line"><span class="attr">        favorite_color:</span> <span class="string">"#326CE5"</span></span><br><span class="line"><span class="attr">        favorite_number:</span> <span class="number">7</span></span><br><span class="line"><span class="attr">        thanks_to:</span> <span class="string">"thockin"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    component:</span> sample-scheduler</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      component:</span> sample-scheduler</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        component:</span> sample-scheduler</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccount:</span> sample-scheduler-sa</span><br><span class="line"><span class="attr">      priorityClassName:</span> system-cluster-critical</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> scheduler-config</span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> scheduler-config</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> scheduler-ctrl</span><br><span class="line"><span class="attr">          image:</span> cnych/sample-scheduler:v0<span class="number">.1</span><span class="number">.6</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> sample-scheduler-framework</span><br><span class="line"><span class="bullet">            -</span> --config=/etc/kubernetes/scheduler-config.yaml</span><br><span class="line"><span class="bullet">            -</span> --v=<span class="number">3</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="string">"50m"</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> scheduler-config</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/kubernetes</span><br></pre></td></tr></table></figure></p>
<p>直接部署上面的资源对象即可，这样我们就部署了一个名为 <code>sample-scheduler</code> 的调度器了，接下来我们可以部署一个应用来使用这个调度器进行调度：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> test-scheduler</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> test-scheduler</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> test-scheduler</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      schedulerName:</span> sample-scheduler</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> nginx</span><br><span class="line"><span class="attr">        imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">        name:</span> nginx</span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure></p>
<p>这里需要注意的是我们现在手动指定了一个 <code>schedulerName</code> 的字段，将其设置成上面我们自定义的调度器名称 <code>sample-scheduler</code>。</p>
<p>我们直接创建这个资源对象，创建完成后查看我们自定义调度器的日志信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system <span class="_">-l</span> component=sample-scheduler</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">sample-scheduler-7c469787f-rwhhd   1/1     Running   0          13m</span><br><span class="line">$ kubectl logs <span class="_">-f</span> sample-scheduler-7c469787f-rwhhd -n kube-system</span><br><span class="line">I0104 08:24:22.087881       1 scheduler.go:530] Attempting to schedule pod: default/<span class="built_in">test</span>-scheduler-6d779d9465-rq2bb</span><br><span class="line">I0104 08:24:22.087992       1 plugins.go:23] prefilter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb</span><br><span class="line">I0104 08:24:22.088657       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node1</span><br><span class="line">I0104 08:24:22.088797       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node2</span><br><span class="line">I0104 08:24:22.088871       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node3</span><br><span class="line">I0104 08:24:22.088946       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node4</span><br><span class="line">I0104 08:24:22.088992       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-master</span><br><span class="line">I0104 08:24:22.090653       1 plugins.go:36] prebind node info: &amp;Node&#123;ObjectMeta:&#123;ydzs-node3   /api/v1/nodes/ydzs-node3 1ff6e228-4d98-4737-b6d3-30a5d55ccdc2 15466372 0 2019-11-10 09:05:09 +0000 UTC &lt;nil&gt; &lt;nil&gt; ......&#125;</span><br><span class="line">I0104 08:24:22.091761       1 factory.go:610] Attempting to <span class="built_in">bind</span> <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb to ydzs-node3</span><br><span class="line">I0104 08:24:22.104994       1 scheduler.go:667] pod default/<span class="built_in">test</span>-scheduler-6d779d9465-rq2bb is bound successfully on node <span class="string">"ydzs-node3"</span>, 5 nodes evaluated, 4 nodes were found feasible. Bound node resource: <span class="string">"Capacity: CPU&lt;4&gt;|Memory&lt;8008820Ki&gt;|Pods&lt;110&gt;|StorageEphemeral&lt;17921Mi&gt;; Allocatable: CPU&lt;4&gt;|Memory&lt;7906420Ki&gt;|Pods&lt;110&gt;|StorageEphemeral&lt;16912377419&gt;."</span>.</span><br></pre></td></tr></table></figure></p>
<p>可以看到当我们创建完 Pod 后，在我们自定义的调度器中就出现了对应的日志，并且在我们定义的扩展点上面都出现了对应的日志，证明我们的示例成功了，也可以通过查看 Pod 的 <code>schedulerName</code> 来验证：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>-scheduler-6d779d9465-rq2bb           1/1     Running   0          22m</span><br><span class="line">$ kubectl get pod <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb -o yaml</span><br><span class="line">......</span><br><span class="line">restartPolicy: Always</span><br><span class="line">schedulerName: sample-scheduler</span><br><span class="line">securityContext: &#123;&#125;</span><br><span class="line">serviceAccount: default</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>在最新的 Kubernetes v1.17 版本中，<code>Scheduler Framework</code> 内置的预选和优选函数已经全部插件化，所以要扩展调度器我们应该掌握并理解调度框架这种方式。</p>
<p>参考：明阳的博客</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[k8s v1.17 新增拓扑感知服务路由]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-service-topology.html</url>
      <content type="html"><![CDATA[<h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><ul>
<li><code>拓扑域</code>: 表示在集群中的某一类 “地方”，比如某节点、某机架、某可用区或某地域等，这些都可以作为某种拓扑域。</li>
<li><code>endpoint</code>: k8s 某个服务的某个 ip+port，通常是 pod 的 ip+port。</li>
<li><code>service</code>: k8s 的 service 资源(服务)，关联一组 endpoint ，访问 service 会被转发到关联的某个 endpoint 上。</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>拓扑感知服务路由，此特性最初由杜军大佬提出并设计。为什么要设计此特性呢？想象一下，k8s 集群节点分布在不同的地方，service 对应的 endpoints 分布在不同节点，传统转发策略会对所有 endpoint 做负载均衡，通常会等概率转发，当访问 service 时，流量就可能被分散打到这些不同的地方。虽然 service 转发做了负载均衡，但如果 endpoint 距离比较远，流量转发过去网络时延就相对比较高，会影响网络性能，在某些情况下甚至还可能会付出额外的流量费用。要是如能实现 service 就近转发 endpoint，是不是就可以实现降低网络时延，提升网络性能了呢？是的！这也正是该特性所提出的目的和意义。<br><a id="more"></a></p>
<h2 id="k8s-亲和性"><a href="#k8s-亲和性" class="headerlink" title="k8s 亲和性"></a>k8s 亲和性</h2><p>service 的就近转发实际就是一种网络的亲和性，倾向于转发到离自己比较近的 endpoint。在此特性之前，已经在调度和存储方面有一些亲和性的设计与实现:</p>
<ul>
<li><code>节点亲和性 (Node Affinity)</code>: 让 Pod 被调度到符合一些期望条件的 Node 上，比如限制调度到某一可用区，或者要求节点支持 GPU，这算是调度亲和，调度结果取决于节点属性。</li>
<li><code>Pod 亲和性与反亲和性 (Pod Affinity/AntiAffinity)</code>: 让一组 Pod 调度到同一拓扑域的节点上，或者打散到不同拓扑域的节点， 这也算是调度亲和，调度结果取决于其它 Pod。</li>
<li><code>数据卷拓扑感知调度 (Volume Topology-aware Scheduling)</code>: 让 Pod 只被调度到符合其绑定的存储所在拓扑域的节点上，这算是调度与存储的亲和，调度结果取决于存储的拓扑域。</li>
<li><code>本地数据卷 (Local Persistent Volume)</code>: 让 Pod 使用本地数据卷，比如高性能 SSD，在某些需要高 IOPS 低时延的场景很有用，它还会保证 Pod 始终被调度到同一节点，数据就不会不丢失，这也算是调度与存储的亲和，调度结果取决于存储所在节点。</li>
<li><code>数据卷拓扑感知动态创建 (Topology-Aware Volume Dynamic Provisioning)</code>: 先调度 Pod，再根据 Pod 所在节点的拓扑域来创建存储，这算是存储与调度的亲和，存储的创建取决于调度的结果。</li>
</ul>
<p>而 k8s 目前在网络方面还没有亲和性能力，拓扑感知服务路由这个新特性恰好可以补齐这个的空缺，此特性使得 service 可以实现就近转发而不是所有 endpoint 等概率转发。</p>
<h2 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h2><p>我们知道，service 转发主要是 node 上的 kube-proxy 进程通过 watch apiserver 获取 service 对应的 endpoint，再写入 iptables 或 ipvs 规则来实现的; 对于 headless service，主要是通过 kube-dns 或 coredns 动态解析到不同 endpoint ip 来实现的。实现 service 就近转发的关键点就在于如何将流量转发到跟当前节点在同一拓扑域的 endpoint 上，也就是会进行一次 endpoint 筛选，选出一部分符合当前节点拓扑域的 endpoint 进行转发。</p>
<p>那么如何判断 endpoint 跟当前节点是否在同一拓扑域里呢？只要能获取到 endpoint 的拓扑信息，用它跟当前节点拓扑对比下就可以知道了。那又如何获取 endpoint 的拓扑信息呢？答案是通过 endpoint 所在节点的 label，我们可以使用 node label 来描述拓扑域。</p>
<p>通常在节点初始化的时候，controller-manager 就会为节点打上许多 label，比如 <code>kubernetes.io/hostname</code> 表示节点的 hostname 来区分节点；另外，在云厂商提供的 k8s 服务，或者使用 cloud-controller-manager 的自建集群，通常还会给节点打上 <code>failure-domain.beta.kubernetes.io/zone</code> 和 <code>failure-domain.beta.kubernetes.io/region</code> 以区分节点所在可用区和所在地域，但自 v1.17 开始将会改名成 <code>topology.kubernetes.io/zone</code> 和 <code>topology.kubernetes.io/region</code>，参见 <a href="https://github.com/kubernetes/kubernetes/pull/81431" target="_blank" rel="external">PR #81431</a>。</p>
<p>如何根据 endpoint 查到它所在节点的这些 label 呢？答案是通过 <code>Endpoint Slice</code>，该特性在 v1.16 发布了 alpha，在 v1.17 将会进入 beta，它相当于 Endpoint API 增强版，通过将 endpoint 做数据分片来解决大规模 endpoint 的性能问题，并且可以携带更多的信息，包括 endpoint 所在节点的拓扑信息，拓扑感知服务路由特性会通过 <code>Endpoint Slice</code> 获取这些拓扑信息实现 endpoint 筛选 (过滤出在同一拓扑域的 endpoint)，然后再转换为 iptables 或 ipvs 规则写入节点以实现拓扑感知的路由转发。</p>
<p>细心的你可能已经发现，之前每个节点上转发 service 的 iptables/ipvs 规则基本是一样的，但启用了拓扑感知服务路由特性之后，每个节点上的转发规则就可能不一样了，因为不同节点的拓扑信息不一样，导致过滤出的 endpoint 就不一样，也正是因为这样，service 转发变得不再等概率，灵活的就近转发才得以实现。</p>
<p>当前还不支持 headless service 的拓扑路由，计划在 beta 阶段支持。由于 <a href="https://zhuanlan.zhihu.com/p/54153164" target="_blank" rel="external">headless service</a> 不是通过 kube-proxy 生成转发规则，而是通过 dns 动态解析实现的，所以需要改 kube-dns/coredns 来支持这个特性。</p>
<h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>启用当前 alpha 实现的拓扑感知服务路由特性需要满足以下前提条件:</p>
<ul>
<li>集群版本在 v1.17 及其以上。</li>
<li>Kube-proxy 以 iptables 或 IPVS 模式运行 (alpha 阶段暂时只实现了这两种模式)。</li>
<li>启用了 <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/" target="_blank" rel="external">Endpoint Slices</a> (此特性虽然在 v1.17 进入 beta，但没有默认开启)。</li>
</ul>
<h2 id="如何启用此特性"><a href="#如何启用此特性" class="headerlink" title="如何启用此特性"></a>如何启用此特性</h2><p>给所有 k8s 组件打开 <code>ServiceTopology</code> 和 <code>EndpointSlice</code> 这两个 feature:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--feature-gates=<span class="string">"ServiceTopology=true,EndpointSlice=true"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>在 Service spec 里加上 <code>topologyKeys</code> 字段，表示该 Service 优先顺序选用的拓扑域列表，对应节点标签的 key；当访问此 Service 时，会找是否有 endpoint 有对应 topology key 的拓扑信息并且 value 跟当前节点也一样，如果是，那就选定此 topology key 作为当前转发的拓扑域，并且筛选出其余所有在这个拓扑域的 endpoint 来进行转发；如果没有找到任何 endpoint 在当前 topology key 对应拓扑域，就会尝试第二个 topology key，依此类推；如果遍历完所有 topology key 也没有匹配到 endpoint 就会拒绝转发，就像此 service 没有后端 endpoint 一样。</p>
<p>有一个特殊的 topology key “<code>*</code>”，它可以匹配所有 endpoint，如果 <code>topologyKeys</code> 包含了 <code>*</code>，它必须在列表末尾，通常是在没有匹配到合适的拓扑域来实现就近转发时，就打消就近转发的念头，可以转发到任意 endpoint 上。</p>
<p>当前 topology key 支持以下可能的值（未来会增加更多）:</p>
<ul>
<li><code>kubernetes.io/hostname</code>: 节点的 hostname，通常将它放列表中第一个，表示如果本机有 endpoint 就直接转发到本机的 endpoint。</li>
<li><code>topology.kubernetes.io/zone</code>: 节点所在的可用区，通常将它放在 <code>kubernetes.io/hostname</code> 后面，表示如果本机没有对应 endpoint，就转发到当前可用区其它节点上的 endpoint（部分云厂商跨可用区通信会收取额外的流量费用）。</li>
<li><code>topology.kubernetes.io/region</code>: 表示节点所在的地域，表示转发到当前地域的 endpoint，这个用的应该会比较少，因为通常集群所有节点都只会在同一个地域，如果节点跨地域了，节点之间通信延时将会很高。</li>
<li><code>*</code>: 忽略拓扑域，匹配所有 endpoint，相当于一个保底策略，避免丢包，只能放在列表末尾。</li>
</ul>
<p>除此之外，还有以下约束:</p>
<ul>
<li><code>topologyKeys</code> 与 <code>externalTrafficPolicy=Local</code> 不兼容，是互斥的，如果 <code>externalTrafficPolicy</code> 为 <code>Local</code>，就不能定义 <code>topologyKeys</code>，反之亦然。</li>
<li>topology key 必须是合法的 label 格式，并且最多定义 16 个 key。</li>
</ul>
<p>这里给出一个简单的 Service 示例:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> ClusterIP</span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> http</span><br><span class="line"><span class="attr">    port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    protocol:</span> TCP</span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">  topologyKeys:</span> [<span class="string">"kubernetes.io/hostname"</span>, <span class="string">"topology.kubernetes.io/zone"</span>, <span class="string">"*"</span>]</span><br></pre></td></tr></table></figure></p>
<p>解释: 当访问 nginx 服务时，首先看本机是否有这个服务的 endpoint，如果有就直接本机路由过去；如果没有，就看是否有 endpoint 位于当前节点所在可用区，如果有，就转发过去，如果还是没有，就转发给任意 endpoint。<br><img src="/images/k8s/service-topology.png" alt="service-topology"></p>
<p>上图就是其中一次转发的例子：Pod 访问 nginx 这个 service 时，发现本机没有 endpoint，就找当前可用区的，找到了就转发过去，也就不会考虑转发给另一可用区的 endpoint。</p>
<p>参考：imroc</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用NodeLocal DNSCache来提升CoreDNS的性能及压力]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-nodelocal-dnscache.html</url>
      <content type="html"><![CDATA[<h2 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h2><p>之前在解决 CoreDNS 的5秒超时问题的时候，除了通过 <code>dnsConfig</code> 去强制使用 tcp 方式解析之外，我们提到过使用 <code>NodeLocal DNSCache</code> 来解决这个问题。<code>NodeLocal DNSCache</code> 通过在集群节点上运行一个 DaemonSet 来提高 clusterDNS 性能和可靠性。处于 <code>ClusterFirst</code> 的 DNS 模式下的 Pod 可以连接到 <code>kube-dns</code> 的 serviceIP 进行 DNS 查询。通过 <code>kube-proxy</code> 组件添加的 <code>iptables</code> 规则将其转换为 <code>CoreDNS</code> 端点。通过在每个集群节点上运行 DNS 缓存，<code>NodeLocal DNSCache</code> 可以缩短 DNS 查找的延迟时间、使 DNS 查找时间更加一致，以及减少发送到 <code>kube-dns</code> 的 DNS 查询次数。</p>
<p>在集群中运行 NodeLocal DNSCache 有如下几个好处：</p>
<ul>
<li>如果本地没有 CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须到另一个节点进行解析，使用 NodeLocal DNSCache 后，拥有本地缓存将有助于改善延迟</li>
<li>跳过 iptables DNAT 和连接跟踪将有助于减少 <code>conntrack</code> 竞争并避免 UDP DNS 条目填满 <code>conntrack</code> 表（常见的5s超时问题就是这个原因造成的）</li>
<li>从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP，TCP conntrack 条目将在连接关闭时被删除，而 UDP 条目必须超时(<a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt" target="_blank" rel="external">默认 nf_conntrack_udp_timeout 是 30 秒</a>)</li>
<li>将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）</li>
</ul>
<a id="more"></a>
<p><img src="/images/k8s/dnscache.png" alt="NodeLocal DNSCache"></p>
<h2 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h2><p>要安装 NodeLocal DNSCache 也非常简单，直接获取官方的资源清单即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml</span><br></pre></td></tr></table></figure></p>
<p>该资源清单文件中包含几个变量，其中：</p>
<ul>
<li><code>__PILLAR__DNS__SERVER__</code> ：表示 <code>kube-dns</code> 这个 Service 的 ClusterIP，可以通过命令 <code>kubectl get svc -n kube-system | grep kube-dns | awk &#39;{ print $3 }&#39;</code> 获取。</li>
<li><code>__PILLAR__LOCAL__DNS__</code>：表示 DNSCache 本地的 IP，默认为 169.254.20.10</li>
<li><code>__PILLAR__DNS__DOMAIN__</code>：表示集群域，默认就是 cluster.local</li>
</ul>
<p>另外还有两个参数 <code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code>，这两个参数会通过镜像 1.15.6 版本以上的去进行配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的 Upstream Server 配置。直接执行如下所示的命令即可安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sed <span class="string">'s/k8s.gcr.io/cnych/g</span><br><span class="line">s/__PILLAR__DNS__SERVER__/10.96.0.10/g</span><br><span class="line">s/__PILLAR__LOCAL__DNS__/169.254.20.10/g</span><br><span class="line">s/__PILLAR__DNS__DOMAIN__/cluster.local/g'</span> nodelocaldns.yaml |</span><br><span class="line">kubectl apply <span class="_">-f</span> -</span><br></pre></td></tr></table></figure></p>
<p>可以通过如下命令来查看对应的 Pod 是否已经启动成功：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system | grep node-local-dns</span><br><span class="line">node-local-dns-8zm2f                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-dd4xg                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-hs8qq                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-pxfxn                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-stjm9                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-wjxvz                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-wn5wc                    1/1     Running     0          7m49s</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>需要注意的是这里使用 DaemonSet 部署 node-local-dns 使用了 <code>hostNetwork=true</code>，会占用宿主机的 8080 端口，所以需要保证该端口未被占用。</p>
</blockquote>
<p>但是到这里还没有完，如果 kube-proxy 组件使用的是 ipvs 模式的话我们还需要修改 kubelet 的 <code>--cluster-dns</code> 参数，将其指向 <code>169.254.20.10</code>，Daemonset 会在每个节点创建一个网卡来绑这个 IP，Pod 向本节点这个 IP 发 DNS 请求，缓存没有命中的时候才会再代理到上游集群 DNS 进行查询。 <code>iptables</code> 模式下 Pod 还是向原来的集群 DNS 请求，节点上有这个 IP 监听，会被本机拦截，再请求集群上游 DNS，所以不需要更改 <code>--cluster-dns</code> 参数。</p>
<h3 id="ipvs使用localDNS修改方式一"><a href="#ipvs使用localDNS修改方式一" class="headerlink" title="ipvs使用localDNS修改方式一"></a>ipvs使用localDNS修改方式一</h3><p>由于我这里使用的是 kubeadm 安装的 1.16 版本的集群，所以我们只需要替换节点上 <code>/var/lib/kubelet/config.yaml</code> 文件中的 <code>clusterDNS</code> 这个参数值，然后重启即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">'s/10.96.0.10/169.254.20.10/g'</span> /var/lib/kubelet/config.yaml</span><br><span class="line">$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure></p>
<h3 id="ipvs使用localDNS修改方式二-（不推荐）"><a href="#ipvs使用localDNS修改方式二-（不推荐）" class="headerlink" title="ipvs使用localDNS修改方式二 （不推荐）"></a>ipvs使用localDNS修改方式二 （不推荐）</h3><p>我们也可以完全在官方的 DaemonSet 资源对象中添加一个 initContainer 来完成这个工作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">initContainers:  <span class="comment"># ipvs模式下需要修改dns配置，重启kubelet</span></span><br><span class="line">  - name: setup</span><br><span class="line">    image: alpine</span><br><span class="line">    tty: <span class="literal">true</span></span><br><span class="line">    stdin: <span class="literal">true</span></span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: <span class="literal">true</span></span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">    - nsenter</span><br><span class="line">    - --target</span><br><span class="line">    - <span class="string">"1"</span></span><br><span class="line">    - --mount</span><br><span class="line">    - --uts</span><br><span class="line">    - --ipc</span><br><span class="line">    - --net</span><br><span class="line">    - --pid</span><br><span class="line">    - --</span><br><span class="line">    - bash</span><br><span class="line">    - -c</span><br><span class="line">    - |</span><br><span class="line">      <span class="comment"># 确保 kubelet --cluster-dns 被设置为 169.254.20.10</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"Configuring kubelet --cluster-dns=169.254.20.10"</span></span><br><span class="line">      sed -i <span class="string">'s/10.96.0.10/169.254.20.10/g'</span> /var/lib/kubelet/config.yaml</span><br><span class="line">      systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure></p>
<p>但是需要注意的是对于线上环境还是不推荐用上面的方式，因为它会优先将 kubelet 的 <code>cluster-dns</code> 参数进行修改，然后再去安装 NodeLocal，这中间毕竟有一段真空期。</p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>待 <code>node-local-dns</code> 安装配置完成后，我们可以部署一个新的 Pod 来验证下：(test-node-local-dns.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> test-node-local-dns</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> local-dns</span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>, <span class="string">"sleep 60m"</span>]</span><br></pre></td></tr></table></figure></p>
<p>直接部署：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply <span class="_">-f</span> <span class="built_in">test</span>-node-local-dns.yaml</span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it <span class="built_in">test</span>-node-local-dns /bin/sh</span><br><span class="line">/ <span class="comment"># cat /etc/resolv.conf</span></span><br><span class="line">nameserver 169.254.20.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 <code>nameserver</code> 已经变成 <code>169.254.20.10</code> 了，当然对于之前的历史 Pod 要想使用 <code>node-local-dns</code> 则需要重建，当然如果要想去跟踪 DNS 的解析过程的话可以去通过抓包来观察。</p>
<h2 id="番外篇"><a href="#番外篇" class="headerlink" title="番外篇"></a>番外篇</h2><p>在使用了<code>NodeLocal DNSCache</code>后，如果在配置自定义域名？</p>
<p>首先我们需要在 CoreDNS 的 ConfigMap 中添加 hosts 插件：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hosts &#123;</span><br><span class="line">  <span class="number">192.168</span><span class="number">.3</span><span class="number">.211</span> git.k8s.local</span><br><span class="line">  fallthrough</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其次我们需要修改 <code>NodeLocal DNSCache</code> 的 ConfigMap，当前配置如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">cluster.local:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache &#123;</span><br><span class="line">            success <span class="number">9984</span> <span class="number">30</span></span><br><span class="line">            denial <span class="number">9984</span> <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . <span class="number">10.96</span><span class="number">.207</span><span class="number">.156</span> &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    health <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span>:<span class="number">8080</span></span><br><span class="line">    &#125;</span><br><span class="line">in-addr.arpa:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . <span class="number">10.96</span><span class="number">.207</span><span class="number">.156</span> &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    &#125;</span><br><span class="line">ip6.arpa:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . <span class="number">10.96</span><span class="number">.207</span><span class="number">.156</span> &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    &#125;</span><br><span class="line">.:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . /etc/resolv.conf &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>分析上面的 LocalDNS 的配置信息，其中 <code>10.96.0.10</code> 为 CoreDNS 的 Service ClusterIP，<code>169.254.20.10</code> 为 LocalDNS 的 IP 地址，<code>10.96.207.156</code> 是 LocalDNS 新建的一个 Service ClusterIP，该 Service 和 CoreDNS 一样都是关联以前的 CoreDNS 的 Endpoints 列表。</p>
<p>仔细观察可以发现 <code>cluster.local</code>、<code>in-addr.arpa</code> 以及 <code>ip6.arpa</code> 都会通过 forward 转发到 <code>10.96.207.156</code>，也就是去 CoreDNS 解析，其他的则是 <strong><code>forward . /etc/resolv.conf</code></strong> 通过 <code>resolv.conf</code> 文件去解析，该文件的内容如下所示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 169.254.20.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure></p>
<p>所以当我们解析域名 <code>git.k8s.local</code> 的时候需要走一遍搜索域，而 <code>k8s.local</code> 不在 <code>cluster.local</code>、<code>in-addr.arpa</code> 以及 <code>ip6.arpa</code> 这些域中，所以就会走到 <code>/etc/resolv.conf</code> 去解析。这样就会导致 <code>git.k8s.local</code> 无法进行解析。这个时候我们需要把 <code>forward . /etc/resolv.conf</code> 更改成 <code>forward . 10.96.207.156</code>，这样就会去 CoreDNS 解析了，在 NodeLocalDNS 的 ConfigMap 中做如下的修改即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit cm node-local-dns -n kube-system</span><br><span class="line">......</span><br><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    <span class="built_in">bind</span> 169.254.20.10 10.96.0.10</span><br><span class="line">    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">&#125;</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>同样修改完成后，需要重建 NodeLocalDNS 的 Pod 才会生效。</p>
<blockquote>
<p><code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code> 这两个参数在镜像 1.15.6 版本以上中会自动进行配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的 Upstream Server 地址。</p>
</blockquote>
<p>参考：明阳的博客</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 性能优化实战]]></title>
      <url>http://team.jiunile.com/blog/2020/05/go-performance.html</url>
      <content type="html"><![CDATA[<h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><p>网关服务作为统一接入服务，是大部分服务的统一入口。为了避免成功瓶颈，需要对其进行尽可能地优化。因此，特别总结一下 golang 后台服务性能优化的方式，并对网关服务进行优化。</p>
<p>技术背景：</p>
<ul>
<li>基于 tarsgo 框架的 http 接入服务，下游服务使用 tarsgo 协议进行交互</li>
</ul>
<h2 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h2><p>网关服务本身没有业务逻辑处理，仅作为统一入口进行请求转发，因此我们主要关注下列指标</p>
<ul>
<li>吞吐量：每秒钟可以处理的请求数</li>
<li>响应时间：从客户端发出请求，到收到回包的总耗时</li>
</ul>
<h2 id="定位瓶颈"><a href="#定位瓶颈" class="headerlink" title="定位瓶颈"></a>定位瓶颈</h2><p>一般后台服务的瓶颈主要为 CPU，内存，IO 操作中的一个或多个。若这三者的负载都不高，但系统吞吐量低，基本就是代码逻辑出问题了。</p>
<p>在代码正常运行的情况下，我们要针对某个方面的高负载进行优化，才能提高系统的性能。golang 可通过 benchmark 加 pprof 来定位具体的性能瓶颈。</p>
<a id="more"></a>
<h3 id="benchmark-简介"><a href="#benchmark-简介" class="headerlink" title="benchmark 简介"></a>benchmark 简介</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -v gate_test.go -run=none -bench=. -benchtime=3s -cpuprofile cpu.prof -memprofile mem.prof</span><br></pre></td></tr></table></figure>
<ul>
<li>-run 知道单次测试，一般用于代码逻辑验证</li>
<li>-bench=. 执行所有 Benchmark，也可以通过用例函数名来指定部分测试用例</li>
<li>-benchtime 指定测试执行时长</li>
<li>-cpuprofile 输出 cpu 的 pprof 信息文件</li>
<li>-memprofile 输出 heap 的 pprof 信息文件。</li>
<li>-blockprofile 阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置</li>
<li>-mutexprofile 互斥锁分析，报告互斥锁的竞争情况</li>
</ul>
<p><strong>benchmark 测试用例常用函数</strong></p>
<ul>
<li>b.ReportAllocs() 输出单次循环使用的内存数量和对象 allocs 信息</li>
<li>b.RunParallel() 使用协程并发测试</li>
<li>b.SetBytes(n int64) 设置单次循环使用的内存数量</li>
</ul>
<h3 id="pprof-简介"><a href="#pprof-简介" class="headerlink" title="pprof 简介"></a>pprof 简介</h3><p><strong>生成方式</strong></p>
<ul>
<li><code>runtime/pprof</code>: 手动调用如<code>runtime.StartCPUProfile</code>或者<code>runtime.StopCPUProfile</code>等 API 来生成和写入采样文件，灵活性高。主要用于本地测试。</li>
<li><code>net/http/pprof</code>: 通过 http 服务获取 Profile 采样文件，简单易用，适用于对应用程序的整体监控。通过 runtime/pprof 实现。主要用于服务器端测试。</li>
<li><code>go test</code>: 通过 <code>go test -bench . -cpuprofile cpuprofile.out</code> 生成采样文件，主要用于本地基准测试。可用于重点测试某些函数。</li>
</ul>
<p><strong>查看方式</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof [options][binary] ...</span><br></pre></td></tr></table></figure></p>
<ul>
<li>–text 纯文本</li>
<li>–web 生成 svg 并用浏览器打开（如果 svg 的默认打开方式是浏览器)</li>
<li>–svg 只生成 svg</li>
<li>–list funcname 筛选出正则匹配 funcname 的函数的信息</li>
<li>-http=”:port” 直接本地浏览器打开 profile 查看（包括 top，graph，火焰图等）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof -base profile1 profile2</span><br></pre></td></tr></table></figure>
<p>对比查看 2 个 profile，一般用于代码修改前后对比，定位差异点。</p>
<p>通过命令行方式查看 profile 时，可以在命令行对话中，使用下列命令，查看相关信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flat flat%   sum%        cum   cum%</span><br><span class="line">5.95s 27.56% 27.56%      5.95s 27.56%  runtime.usleep</span><br><span class="line">4.97s 23.02% 50.58%      5.08s 23.53%  sync.(*RWMutex).RLock</span><br><span class="line">4.46s 20.66% 71.24%      4.46s 20.66%  sync.(*RWMutex).RUnlock</span><br><span class="line">2.69s 12.46% 83.70%      2.69s 12.46%  runtime.pthread_cond_<span class="built_in">wait</span></span><br><span class="line">1.50s  6.95% 90.64%      1.50s  6.95%  runtime.pthread_cond_signal</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>lat</code>: 采样时，该函数正在运行的次数*采样频率(10ms)，即得到估算的函数运行”采样时间”。这里不包括函数等待子函数返回。</li>
<li><code>flat%</code>: flat / 总采样时间值</li>
<li><code>sum%</code>: 前面所有行的 flat% 的累加值，如第三行 sum% = 71.24% = 27.56% + 50.58%</li>
<li><code>cum</code>: 采样时，该函数出现在调用堆栈的采样时间，包括函数等待子函数返回。因此 flat &lt;= cum</li>
<li><code>cum%</code>: cum / 总采样时间值</li>
</ul>
<p><code>topN [-cum]</code> 查看前 N 个数据：</p>
<p><code>list ncname</code> 查看某个函数的详细信息，可以明确具体的资源（cpu，内存等）是由哪一行触发的。</p>
<h3 id="pprof-接入"><a href="#pprof-接入" class="headerlink" title="pprof 接入"></a>pprof 接入</h3><p>服务中 main 方法插入代码<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cfg := tars.GetServerConfig()</span><br><span class="line">profMux := &amp;tars.TarsHttpMux&#123;&#125;</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/"</span>, pprof.Index)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/cmdline"</span>, pprof.Cmdline)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/profile"</span>, pprof.Profile)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/symbol"</span>, pprof.Symbol)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/trace"</span>, pprof.Trace)</span><br><span class="line">tars.AddHttpServant(profMux, cfg.App+<span class="string">"."</span>+cfg.Server+<span class="string">".ProfObj"</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="查看服务的-pprof"><a href="#查看服务的-pprof" class="headerlink" title="查看服务的 pprof"></a>查看服务的 pprof</h3><ul>
<li>保证开发机能直接访问到节点部署的 ip 和 port。</li>
<li>查看 profile(http 地址中的 ip,port 为 ProfObj 的 ip 和 port)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载cpu profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/profile?seconds=120 <span class="comment"># 等待120s，不带此参数时等待30s</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载heap profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/heap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载goroutine profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/goroutine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载block profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/block</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载mutex profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/mutex</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载20秒的trace记录（遇到棘手问题时，查看trace会比较容易定位)</span></span><br><span class="line"> curl http://100.97.1.35:10078/debug/pprof/trace?seconds=20 &gt; trace.out</span><br><span class="line"> go tool trace trace.out 查看</span><br></pre></td></tr></table></figure>
<ul>
<li>直接在终端中通过 pprof 命令查看</li>
<li>sz 上面命令执行时出现的<code>Saved profile in /root/pprof/pprof.binary.alloc_objects.xxxxxxx.xxxx.pb.gz</code>到本地 </li>
<li>在本地环境，执行<code>go tool pprof -http=&quot;:8081&quot; pprof.binary.alloc_objects.xxxxxxx.xxxx.pb.gz</code> 即可直接通过<code>http://localhost:8081</code>页面查看。包括topN，火焰图信息等,会更方便一点。</li>
</ul>
<h3 id="GC-Trace"><a href="#GC-Trace" class="headerlink" title="GC Trace"></a>GC Trace</h3><p>golang 具备 GC 功能，而 GC 是最容易被忽视的性能影响因素。尤其是在本地使用 benchmark 测试时，由于时间较短，占用内存较少。往往不会触发 GC。而一旦线上出现 GC 问题，又不太好定位。目前常用的定位方式有两种：</p>
<h4 id="本地-gctrace"><a href="#本地-gctrace" class="headerlink" title="本地 gctrace"></a>本地 gctrace</h4><p>在执行程序前加 <code>GODEBUG=gctrace=1</code>，每次 gc 时会输出一行如下内容<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gc 1 @0.001s 11%: 0.007+1.5+0.004 ms clock, 0.089+1.5/2.8/0.27+0.054 ms cpu, 4-&gt;4-&gt;3 MB, 5 MB goal, 12 P</span><br><span class="line">scvg: inuse: 4, idle: 57, sys: 62, released: 57, consumed: 4 (MB)</span><br></pre></td></tr></table></figure></p>
<p>也通过日志转为图形化：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GODEBUG=gctrace=1 godoc -index -http=:6060 2&gt; stderr.log</span><br><span class="line">cat stderr.log | gcvis</span><br></pre></td></tr></table></figure></p>
<ul>
<li>inuse：使用了多少 M 内存</li>
<li>idle：剩下要清除的内存</li>
<li>sys：系统映射的内存</li>
<li>released：释放的系统内存</li>
<li>consumed：申请的系统内存</li>
<li>gc 1 表示第 1 次 gc</li>
<li>@0.001s 表示程序执行的总时间</li>
<li>11% 表示垃圾回收时间占用总的运行时间百分比</li>
<li>0.007+1.5+0.004 ms clock 表示工作线程完成 GC 的 stop-the-world,sweeping,marking 和 waiting 的时间</li>
<li>0.089+1.5/2.8/0.27+0.054 ms cpu 垃圾回收占用 cpu 时间</li>
<li>4-&gt;4-&gt;3 MB 表示堆的大小，gc 后堆的大小，存活堆的大小</li>
<li>5 MB goal 整体堆的大小</li>
<li>12 P 使用的处理器数量</li>
<li>scvg: inuse: 4, idle: 57, sys: 62, released: 57, consumed: 4 (MB) 表示系统内存回收信息</li>
<li>采用图形化的方式查看：<a href="https://github.com/davecheney/gcvis" target="_blank" rel="external">https://github.com/davecheney/gcvis</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GODEBUG=gctrace=1 go <span class="built_in">test</span> -v *.go -bench=. -run=none -benchtime 3m |&amp; gcvis</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="线上-trace"><a href="#线上-trace" class="headerlink" title="线上 trace"></a>线上 trace</h4><p>在线上业务中添加<strong>net/http/pprof</strong>后，可通过下列命令采集 20 秒的 trace 信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://ip:port/debug/pprof/trace?seconds=20 &gt; trace.out</span><br></pre></td></tr></table></figure></p>
<p>再通过<code>go tool trace trace.out</code> 即可在本地浏览器中查看 trace 信息。<br><img src="/images/go/performance_1.png" alt="go performance analysis"></p>
<ul>
<li>View trace：查看跟踪</li>
<li>Goroutine analysis：Goroutine 分析</li>
<li>Network blocking profile：网络阻塞概况</li>
<li>Synchronization blocking profile：同步阻塞概况</li>
<li>Syscall blocking profile：系统调用阻塞概况</li>
<li>Scheduler latency profile：调度延迟概况</li>
<li>User defined tasks：用户自定义任务</li>
<li>User defined regions：用户自定义区域</li>
<li>Minimum mutator utilization：最低 Mutator 利用率</li>
</ul>
<p>GC 相关的信息可以在 View trace 中看到<br><img src="/images/go/performance_2.png" alt="go performance analysis"></p>
<p>可通过点击 heap 的色块区域，查看 heap 信息。<br><img src="/images/go/performance_3.png" alt="go performance analysis"></p>
<p>点击 GC 对应行的蓝色色块，查看 GC 耗时及相关回收信息。<br><img src="/images/go/performance_4.png" alt="go performance analysis"></p>
<p>通过这两个信息就可以确认是否存在 GC 问题，以及造成高 GC 的可能原因。</p>
<h4 id="使用问题"><a href="#使用问题" class="headerlink" title="使用问题"></a>使用问题</h4><p>trace 的展示仅支持 chrome 浏览器。但是目前常用的 chrome 浏览器屏蔽了 go tool trace 使用的 HTML import 功能。即打开“view trace”时，会出现一片空白。并可以在 console 中看到警告信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HTML Imports is deprecated and has now been removed as of M80. See https://www.chromestatus.com/features/5144752345317376 and https://developers.google.com/web/updates/2019/07/web-components-time-to-upgrade <span class="keyword">for</span> more details.</span><br></pre></td></tr></table></figure></p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p><strong>申请 token</strong></p>
<ul>
<li><a href="https://developers.chrome.com/origintrials/#/register_trial/2431943798780067841" target="_blank" rel="external">https://developers.chrome.com/origintrials/#/register_trial/2431943798780067841</a> 然后登录</li>
<li>web origin 处填写 <a href="http://localhost:8001" target="_blank" rel="external">http://localhost:8001</a> 端口只能是 8000 - 8003，支持 http 和 https。（也可以填写 127.0.0.1:8001,依赖于你浏览器中显示的地址，否则对不上的话，还要手动改一下)<br><img src="/images/go/performance_5.png" alt="go performance analysis"></li>
<li>点击注册后即可看到 token</li>
</ul>
<p><strong>修改 trace.go</strong><br>编辑<code>${GOROOT}/src/cmd/trace/trace.go</code> 文件，在文件中找到 templTrace 然后在  标签的下一行添加<code>&lt;meta http-equiv=&quot;origin-trial&quot; content=&quot;你复制的token&quot;&gt;</code></p>
<p><strong>重新编译 go</strong></p>
<ul>
<li>${GOROOT}/src 目录，执行 ./all.bash</li>
<li>若提示：<code>ERROR: Cannot find go1.4\bin\go Set GOROOT_BOOTSTRAP to a working Go tree &gt;= Go 1.4</code> 则需要先安装一个 go1.4 的版本，再通过它来编译 go。（下载链接<a href="https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz）" target="_blank" rel="external">https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz）</a> 在 <code>go1.4/src</code> 下执行 <code>./make.bash</code> . 指定 GOROOT_BOOTSTRAP 为 go1.4 的根目录。然后就可以重新编译 go</li>
</ul>
<p><strong>查看 trace</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool trace -http=localhost:8001 trace.out</span><br></pre></td></tr></table></figure></p>
<p>若打开 view trace 还是空白,则检查一下浏览器地址栏中的地址，是否与注册时的一样。即注册用的 localhost 或 127.0.0.1 则地址栏中也要一样。</p>
<h2 id="常见性能瓶颈"><a href="#常见性能瓶颈" class="headerlink" title="常见性能瓶颈"></a>常见性能瓶颈</h2><h3 id="业务逻辑"><a href="#业务逻辑" class="headerlink" title="业务逻辑"></a>业务逻辑</h3><p>出现无效甚至降低性能的逻辑。常见的有：</p>
<ul>
<li>逻辑重复：相同的操作在不同的位置做了多次或循环跳出的条件设置不当。</li>
<li>资源未复用：内存频繁申请和释放，数据库链接频繁建立和销毁等。</li>
<li>无效代码。</li>
</ul>
<h3 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h3><p>未选择恰当的存储方式，常见的有：</p>
<ul>
<li>临时数据存放到数据库中，导致频繁读写数据库。</li>
<li>将复杂的树状结构的数据用 SQL 数据库存储，出现大量冗余列，并且在读写时要进行拆解和拼接。</li>
<li>数据库表设计不当，无法有效利用索引查询，导致查询操作耗时高甚至出现大量慢查询。</li>
<li>热点数据未使用缓存，导致数据库负载过高，响应速度下降。</li>
</ul>
<h3 id="并发处理"><a href="#并发处理" class="headerlink" title="并发处理"></a>并发处理</h3><p>并发操作的问题主要出现在资源竞争上，常见的有：</p>
<ul>
<li>死锁/活锁导致大量阻塞，性能严重下降。</li>
<li>资源竞争激烈：大量的线程或协程抢夺一个锁。</li>
<li>临界区过大：将不必要的操作也放入临界区，导致锁的释放速度过慢，引起其他线程或协程阻塞。</li>
</ul>
<h2 id="golang-部分细节简介"><a href="#golang-部分细节简介" class="headerlink" title="golang 部分细节简介"></a>golang 部分细节简介</h2><p>在优化之前，我们需要对 golang 的实现细节有一个简单的了解，才能明白哪些地方有问题，哪些地方可以优化，以及怎么优化。以下内容的详细讲解建议查阅网上优秀的 blog。对语言的底层实现机制最好有个基本的了解，否则有时候掉到坑里都不知道为啥。</p>
<h3 id="协程调度"><a href="#协程调度" class="headerlink" title="协程调度"></a>协程调度</h3><p>Golang 调度是非抢占式多任务处理，由协程主动交出控制权。遇到如下条件时，才有可能交出控制权</p>
<ul>
<li>I/O,select</li>
<li>channel</li>
<li>等待锁</li>
<li>函数调用（是一个切换的机会，是否会切换由调度器决定）</li>
<li>runtime.Gosched()</li>
</ul>
<p>因此，若存在较长时间的 for 循环处理，并且循环内没有上述逻辑时，会阻塞住其他的协程调度。在实际编码中一定要注意。</p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>Go 为每个逻辑处理器（P）提供了一个称为mcache的本地内存线程缓存。每个 mcache 中持有 67 个级别的 mspan。每个 msapn 又包含两种：scan（包含指针的对象）和 noscan（不包含指针的对象）。<strong>在进行垃圾收集时，GC 无需遍历 noscan 对象</strong>。<br><img src="/images/go/performance_6.png" alt="go performance analysis"></p>
<h3 id="GC-处理"><a href="#GC-处理" class="headerlink" title="GC 处理"></a>GC 处理</h3><p>GC 的工作就是确定哪些内存可以释放，它是通过扫描内存查找内存分配的指针来完成这个工作的。GC 触发时机：</p>
<ul>
<li>到达堆阈值：默认情况下，它将在堆大小加倍时运行，可通过 GOGC 来设定更高阈值（不建议变更此配置）</li>
<li>到达时间阈值：每两分钟会强制启动一次 GC 循环</li>
</ul>
<p>为啥要注意 GC，是因为 GC 时出现 2 次 Stop the world，即停止所有协程，进行扫描操作。若是 GC 耗时高，则会严重影响服务器性能。<br><img src="/images/go/performance_7.png" alt="go performance analysis"></p>
<h3 id="变量逃逸"><a href="#变量逃逸" class="headerlink" title="变量逃逸"></a>变量逃逸</h3><blockquote>
<p>注意，golang 中的栈是跟函数绑定的，函数结束时栈被回收。</p>
</blockquote>
<p><strong>变量内存回收：</strong></p>
<ul>
<li>如果分配在栈中，则函数执行结束可自动将内存回收；</li>
<li>如果分配在堆中，则函数执行结束可交给 GC（垃圾回收）处理；</li>
</ul>
<p>而变量逃逸就意味着增加了堆中的对象个数，影响 GC 耗时。一般要尽量避免逃逸。</p>
<p><strong>逃逸分析不变性：</strong></p>
<ul>
<li>指向栈对象的指针不能存在于堆中；</li>
<li>指向栈对象的指针不能在栈对象回收后存活；</li>
</ul>
<p>在逃逸分析过程中，凡是发现出现违反上述约定的变量，就将其移到堆中。</p>
<p><strong>逃逸常见的情况：</strong></p>
<ul>
<li>指针逃逸：返回局部变量的地址（不变性 2）</li>
<li>栈空间不足</li>
<li>动态类型逃逸：如 fmt.Sprintf,json.Marshel 等接受变量为…interface{}函数的调用，会导致传入的变量逃逸。</li>
<li>闭包引用</li>
</ul>
<h3 id="包含指针类型的底层结构"><a href="#包含指针类型的底层结构" class="headerlink" title="包含指针类型的底层结构"></a>包含指针类型的底层结构</h3><p><strong>string</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> StringHeader <span class="keyword">struct</span> &#123;</span><br><span class="line"> Data <span class="keyword">uintptr</span></span><br><span class="line"> Len  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>slice</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> SliceHeader <span class="keyword">struct</span> &#123;</span><br><span class="line"> Data <span class="keyword">uintptr</span></span><br><span class="line"> Len  <span class="keyword">int</span></span><br><span class="line"> Cap  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>map</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> hmap <span class="keyword">struct</span> &#123;</span><br><span class="line"> count     <span class="keyword">int</span></span><br><span class="line"> flags     <span class="keyword">uint8</span></span><br><span class="line"> B         <span class="keyword">uint8</span></span><br><span class="line"> noverflow <span class="keyword">uint16</span></span><br><span class="line"> hash0     <span class="keyword">uint32</span></span><br><span class="line"> buckets    unsafe.Pointer</span><br><span class="line"> oldbuckets unsafe.Pointer</span><br><span class="line"> nevacuate  <span class="keyword">uintptr</span></span><br><span class="line"> extra *mapextra</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这些是常见会包含指针的对象。尤其是 string，在后台应用中大量出现。并经常会作为 map 的 key 或 value。若数据量较大时，就会引发 GC 耗时上升。同时，我们可以注意到 string 和 slice 非常相似，从某种意义上说它们之间是可以直接互相转换的。这就可以避免 string 和[]byte 之间类型转换时，进行内存拷贝</p>
<p><strong>类型转换优化</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">String</span><span class="params">(b []<span class="keyword">byte</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"> <span class="keyword">return</span> *(*<span class="keyword">string</span>)(unsafe.Pointer(&amp;b))</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Str2Bytes</span><span class="params">(s <span class="keyword">string</span>)</span> []<span class="title">byte</span></span> &#123;</span><br><span class="line"> x := (*[<span class="number">2</span>]<span class="keyword">uintptr</span>)(unsafe.Pointer(&amp;s))</span><br><span class="line"> h := [<span class="number">3</span>]<span class="keyword">uintptr</span>&#123;x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">1</span>]&#125;</span><br><span class="line"> <span class="keyword">return</span> *(*[]<span class="keyword">byte</span>)(unsafe.Pointer(&amp;h))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="性能测试方式"><a href="#性能测试方式" class="headerlink" title="性能测试方式"></a>性能测试方式</h2><h3 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h3><p>将服务处理的核心逻辑，使用 go test 的 benchmark 加 pprof 来测试。建议上线前，就对整个业务逻辑的性能进行测试，提前优化瓶颈。</p>
<h3 id="线上测试"><a href="#线上测试" class="headerlink" title="线上测试"></a>线上测试</h3><p>一般 http 服务可以通过常见的测试工具进行压测，如 wrk，locust 等。taf 服务则需要我们自己编写一些测试脚本。同时，要注意的是，压测的目的是定位出服务的最佳性能，而不是盲目的高并发请求测试。因此，一般需要逐步提升并发请求数量，来定位出服务的最佳性能点。</p>
<blockquote>
<p>注意：由于 taf 平台具备扩容功能，因此为了更准确的测试，我们应该在测试前关闭要测试节点的自动扩容。</p>
</blockquote>
<h2 id="实际项目优化"><a href="#实际项目优化" class="headerlink" title="实际项目优化"></a>实际项目优化</h2><p>为了避免影响后端服务，也为了避免后端服务影响网关自身。因此，我们需要在压测前，将对后端服务的调用屏蔽。</p>
<ul>
<li>测试准备：屏蔽远程调用：下游服务调用，健康度上报，统计上报，远程日志。以便关注网关自身性能。<h3 id="QPS-现状"><a href="#QPS-现状" class="headerlink" title="QPS 现状"></a>QPS 现状</h3>首先看下当前业务的性能指标，使用 wrk 压测网关服务<br><img src="/images/go/performance_8.png" alt="go performance analysis"><br>可以看出，在总链接数为 70 的时候，QPS 最高，为 13245。<h3 id="火焰图"><a href="#火焰图" class="headerlink" title="火焰图"></a>火焰图</h3><img src="/images/go/performance_9.png" alt="go performance analysis"><br>根据火焰图我们定位出 cpu 占比较高的几个方法为：</li>
<li>json.Marshal</li>
<li>json.Unmarshal</li>
<li>rogger.Infof</li>
</ul>
<p>为了方便测试，将代码改为本地运行，并通过 benchmark 的方式来对比修改前后的差异。</p>
<blockquote>
<p>由于正式环境使用的 golang 版本为 1.12，因此本地测试时，也要使用同样的版本。</p>
</blockquote>
<h3 id="benchmark"><a href="#benchmark" class="headerlink" title="benchmark"></a>benchmark</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	50000000	      3669 ns/op	    4601 B/op	      73 allocs/op</span><br></pre></td></tr></table></figure>
<p>查看 cpu 和 memory 的 profile，发现健康度上报的数据结构填充占比较高。这部分逻辑基于 tars 框架实现。暂时忽略，为避免影响其他测试，先注释掉。再看看 benchmark。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	  500000	      3146 ns/op	    2069 B/op	      55 allocs/op</span><br></pre></td></tr></table></figure></p>
<h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><h3 id="JSON-优化"><a href="#JSON-优化" class="headerlink" title="JSON 优化"></a>JSON 优化</h3><p>先查看 json 解析的部分，看看是否有优化空间</p>
<h4 id="请求处理"><a href="#请求处理" class="headerlink" title="请求处理"></a>请求处理</h4><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RootHandle view.ReadReq2Json readJsonReq 中进行json解析</span></span><br><span class="line"><span class="keyword">type</span> GatewayReqBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayReqBodyHeader   <span class="string">`json:"header"`</span></span><br><span class="line"> Payload <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> dataMap := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line"> err := jsoniter.Unmarshal(data, &amp;dataMap)</span><br><span class="line"> ...</span><br><span class="line">  headerMap, ok := header.(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line">  businessName, ok := headerMap[<span class="string">"businessName"</span>]</span><br><span class="line">  qua, ok := headerMap[<span class="string">"qua"</span>]</span><br><span class="line">  sessionId, ok := headerMap[<span class="string">"sessionId"</span>]</span><br><span class="line">  ...</span><br><span class="line">  payload, ok := dataMap[<span class="string">"payload"</span>]</span><br><span class="line">  req.Payload, ok = payload.(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数本质上将 data 解析为 model.GatewayReqBody 类型的结构体。但是这里却存在 2 个问题</p>
<ol>
<li>使用了复杂的解析方式，先将 data 解析为 map，再通过每个字段的名字来取值，并进行类型转换。</li>
<li>Req.Playload 解析为一个 map。但又未使用。我们看看后面这个 payload 是用来做啥。确认是否为无效代码。</li>
</ol>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">invokeTafServant</span><span class="params">(resp http.ResponseWriter, gatewayHttpReq *model.GatewayHttpReq)</span></span> &#123;</span><br><span class="line"> ...</span><br><span class="line">  payloadBytes, err := json.Marshal(gatewayHttpReq.ReqBody.Payload)</span><br><span class="line"> <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">  commonReq.Payload = <span class="keyword">string</span>(payloadBytes)</span><br><span class="line"> &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  responseData(gatewayHttpReq, StatusInternalServerError, <span class="string">"封装json异常"</span>, <span class="string">""</span>, resp)</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line"> &#125;</span><br><span class="line">  ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>后续的使用中，我们可以看到，又将这个 payload 转为 string。因此，我们可以确定，上面的 json 解析是没有意义，同时也会浪费资源（payload 数据量一般不小）。</p>
<h4 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h4><ul>
<li>golang 自带的 json 解析性能较低，这里我们可以替换为github.com/json-iterator来提升性能</li>
<li>在 golang 中，遇到不需要解析的 json 数据，可以将其类型声明为json.RawMessage. 即，可以将上述 2 个方法优化为</li>
</ul>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> GatewayReqBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayReqBodyHeader <span class="string">`json:"header"`</span></span><br><span class="line"> Payload json.RawMessage      <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> err := jsoniter.Unmarshal(data, req)</span><br><span class="line"> <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> jsonParseErr</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">for</span> k, v := <span class="keyword">range</span> req.Header.Qua &#123;</span><br><span class="line">  req.Header.Qua[k] = v</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(req.Header.QuaStr) == <span class="number">0</span> &#123;</span><br><span class="line">   req.Header.QuaStr = k + <span class="string">"="</span> + v</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   req.Header.QuaStr += <span class="string">"&amp;"</span> + k + <span class="string">"="</span> + v</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">invokeTafServant</span><span class="params">(resp http.ResponseWriter, gatewayHttpReq *model.GatewayHttpReq)</span></span> &#123;</span><br><span class="line"> commonReq.Payload = <span class="keyword">string</span>(gatewayHttpReq.ReqBody.Payload)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>这里注意！出现了 string 和[]byte 之间的类型转换.为了避免内存拷贝，这里将 string()改为上面的类型转换优化中所定义的转换函数，即 <code>commonReq.Payload = encode.String(gatewayHttpReq.ReqBody.Payload)</code></li>
</ul>
<h4 id="回包处理"><a href="#回包处理" class="headerlink" title="回包处理"></a>回包处理</h4><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> GatewayRespBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayRespBodyHeader  <span class="string">`json:"header"`</span></span><br><span class="line"> Payload <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">responseData</span><span class="params">(gatewayReq *model.GatewayHttpReq, code <span class="keyword">int32</span>, message <span class="keyword">string</span>, payload <span class="keyword">string</span>, resp http.ResponseWriter)</span></span> &#123;</span><br><span class="line"> jsonPayload := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> <span class="built_in">len</span>(payload) != <span class="number">0</span> &#123;</span><br><span class="line">  err := json.Unmarshal([]<span class="keyword">byte</span>(payload), &amp;jsonPayload)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">   ...</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> body := model.GatewayRespBody&#123;</span><br><span class="line">  Header: model.GatewayRespBodyHeader&#123;</span><br><span class="line">   Code:    code,</span><br><span class="line">   Message: message,</span><br><span class="line">  &#125;,</span><br><span class="line">  Payload: jsonPayload,</span><br><span class="line"> &#125;</span><br><span class="line">  data, err := view.RenderResp(<span class="string">"json"</span>, &amp;body)</span><br><span class="line">  ...</span><br><span class="line">  resp.WriteHeader(http.StatusOK)</span><br><span class="line"> resp.Write(data)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样的，这里的 jsonPayload，也是出现了不必要的 json 解析。我们可以改为<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> GatewayRespBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayRespBodyHeader  <span class="string">`json:"header"`</span></span><br><span class="line"> Payload json.RawMessage <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">body := model.GatewayRespBody&#123;</span><br><span class="line">  Header: model.GatewayRespBodyHeader&#123;</span><br><span class="line">   Code:    code,</span><br><span class="line">   Message: message,</span><br><span class="line">  &#125;,</span><br><span class="line">  Payload: encode.Str2Bytes(payload),</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<p>然后在 view.RenderResp 方法中<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RenderResp</span><span class="params">(format <span class="keyword">string</span>, resp <span class="keyword">interface</span>&#123;&#125;)</span> <span class="params">([]<span class="keyword">byte</span>, error)</span></span> &#123;</span><br><span class="line"> <span class="keyword">if</span> <span class="string">"json"</span> == format &#123;</span><br><span class="line">  <span class="keyword">return</span> jsoniter.Marshal(resp)</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">"format error"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="benchmark-1"><a href="#benchmark-1" class="headerlink" title="benchmark"></a>benchmark</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	  500000	      3326 ns/op	    2842 B/op	      50 allocs/op</span><br></pre></td></tr></table></figure>
<p>虽然对象 alloc 减少了，但单次操作内存使用增加了，且性能下降了。这就有点奇怪了。我们来对比一下 2 个情况下的 pprof。</p>
<h3 id="逃逸分析及处理"><a href="#逃逸分析及处理" class="headerlink" title="逃逸分析及处理"></a>逃逸分析及处理</h3><h4 id="go-tool-pprof-base"><a href="#go-tool-pprof-base" class="headerlink" title="go tool pprof -base"></a>go tool pprof -base</h4><ul>
<li>cpu 差异</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flat  flat%   sum%        cum   cum%</span><br><span class="line"> 0.09s  1.17%  1.17%      0.40s  5.20%  runtime.mallocgc</span><br><span class="line"> 0.01s  0.13%  1.30%      0.35s  4.55%  /vendor/github.com/json-iterator/go.(*Iterator).readObjectStart</span><br><span class="line"> 0      0%     1.30%      0.35s  4.55%  /vendor/github.com/json-iterator/go.(*twoFieldsStructDecoder).Decode</span><br></pre></td></tr></table></figure>
<ul>
<li>mem 差异</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    flat  flat%   sum%        cum   cum%</span><br><span class="line">478.96MB 20.33% 20.33%   279.94MB 11.88%  gateway.RootHandle</span><br><span class="line">       0     0% 20.33%   279.94MB 11.88%  <span class="built_in">command</span>-line-arguments.BenchmarkTestHttp.func1</span><br><span class="line">       0     0% 20.33%   279.94MB 11.88%  testing.(*B).RunParallel.func1</span><br></pre></td></tr></table></figure>
<p>可以看出 RootHandle 多了 478.96M 的内存使用。通过 list RootHandle 对比 2 个情况下的内存使用。发现修改后的 RootHandle 中多出了这一行：<code>475.46MB 475.46MB 158: gatewayHttpReq := model.GatewayHttpReq{}</code> 这一般意味着变量 gatewayHttpReq 出现了逃逸。</p>
<ul>
<li>go build -gcflags “-m -m” gateway/*.go</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gateway/logic.go:270:26: &amp;gatewayHttpReq escapes to heap</span><br></pre></td></tr></table></figure>
<p>可以看到确实出现了逃逸。这个对应的代码为<code>err = view.ReadReq2Json(&amp;gatewayHttpReq)</code>,而造成逃逸的本质是因为上面改动了函数 readJsonReq（动态类型逃逸，即函数参数为 interface 类型，无法在编译时确定具体类型的）<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> err := jsoniter.Unmarshal(data, req)</span><br><span class="line"> ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>因此，这里需要特殊处理一下，改为<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> tmp model.GatewayReqBody</span><br><span class="line">	err := jsoniter.Unmarshal(data, &amp;tmp)</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="benchmark-2"><a href="#benchmark-2" class="headerlink" title="benchmark"></a>benchmark</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	  500000	      2994 ns/op	    1892 B/op	      50 allocs/op</span><br></pre></td></tr></table></figure>
<p>可以看到堆内存使用明显下降。性能也提升了。再看一下 pprof，寻找下个瓶颈。</p>
<h4 id="cpu-profile"><a href="#cpu-profile" class="headerlink" title="cpu profile"></a>cpu profile</h4><p><img src="/images/go/performance_10.png" alt="go performance analysis"><br>抛开 responeseData(他内部主要是日志打印占比高），占比较高的为 util.GenerateSessionId，先来看看这个怎么优化。</p>
<h3 id="随机字符串生成"><a href="#随机字符串生成" class="headerlink" title="随机字符串生成"></a>随机字符串生成</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> letterRunes = []<span class="keyword">rune</span>(<span class="string">"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RandStringRunes</span><span class="params">(n <span class="keyword">int</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"> b := <span class="built_in">make</span>([]<span class="keyword">rune</span>, n)</span><br><span class="line"> <span class="keyword">for</span> i := <span class="keyword">range</span> b &#123;</span><br><span class="line">  b[i] = letterRunes[rand.Intn(<span class="built_in">len</span>(letterRunes))]</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="keyword">string</span>(b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目前的生成方式使用的类型是 rune，但其实用 byte 就够了。另外，letterRunes 是 62 个字符，即最大需要 6 位的 index 就可以遍历完成了。而随机数获取的是 63 位。即每个随机数，其实可以产生 10 个随机字符。而不用每个字符都获取一次随机数。所以我们改为<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line"> letterBytes   = <span class="string">"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></span><br><span class="line"> letterIdxBits = <span class="number">6</span></span><br><span class="line"> letterIdxMask = <span class="number">1</span>&lt;&lt;letterIdxBits - <span class="number">1</span></span><br><span class="line"> letterIdxMax  = <span class="number">63</span> / letterIdxBits</span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RandStringRunes</span><span class="params">(n <span class="keyword">int</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"> b := <span class="built_in">make</span>([]<span class="keyword">byte</span>, n)</span><br><span class="line"> <span class="keyword">for</span> i, cache, remain := n<span class="number">-1</span>, rand.Int63(), letterIdxMax; i &gt;= <span class="number">0</span>; &#123;</span><br><span class="line">  <span class="keyword">if</span> remain == <span class="number">0</span> &#123;</span><br><span class="line">   cache, remain = rand.Int63(), letterIdxMax</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> idx := <span class="keyword">int</span>(cache &amp; letterIdxMask); idx &lt; <span class="built_in">len</span>(letterBytes) &#123;</span><br><span class="line">   b[i] = letterBytes[idx]</span><br><span class="line">   i--</span><br><span class="line">  &#125;</span><br><span class="line">  cache &gt;&gt;= letterIdxBits</span><br><span class="line">  remain--</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="keyword">string</span>(b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="benchmark-3"><a href="#benchmark-3" class="headerlink" title="benchmark"></a>benchmark</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	 1000000	      1487 ns/op	    1843 B/op	      50 allocs/op</span><br></pre></td></tr></table></figure>
<h3 id="类型转换及字符串拼接"><a href="#类型转换及字符串拼接" class="headerlink" title="类型转换及字符串拼接"></a>类型转换及字符串拼接</h3><p>一般情况下，都会说将 string 和[]byte 的转换改为 unsafe；以及在字符串拼接时，用 byte.Buffer 代替 fmt.Sprintf。但是网关这里的情况比较特殊，字符串的操作基本集中在打印日志的操作。而 tars 的日志打印本身就是通过 byte.Buffer 拼接的。所以这可以避免。另外，由于日志打印量大，使用 unsafe 转换[]byte 为 string 带来的收益，往往会因为逃逸从而影响 GC，反正会影响性能。因此，不同的场景下，不能简单的套用一些优化方法。需要通过压测及结果分析来判断具体的优化策略。</p>
<h2 id="优化结果"><a href="#优化结果" class="headerlink" title="优化结果"></a>优化结果</h2><p><img src="/images/go/performance_11.png" alt="go performance analysis"><br>可以看到优化后，最大链接数为 110，最高 QPS 为<strong>21153.35</strong>。对比之前的<strong>13245</strong>，大约提升 60%。</p>
<h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>从 pprof 中可以看到日志打印，远程日志，健康上报等信息占用较多 cpu 资源，且导致多个数据逃逸（尤其是日志打印）。过多的日志基本等于没有日志。后续可考虑裁剪日志，仅保留出错时的上下文信息。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>性能查看工具 pprof,trace 及压测工具 wrk 或其他压测工具的使用要比较了解。</li>
<li>代码逻辑层面的走读非常重要，要尽量避免无效逻辑。</li>
<li>对于 golang 自身库存在缺陷的，可以寻找第三方库或自己改造。</li>
<li>golang 版本尽量更新，这次的测试是在 golang1.12 下进行的。而 go1.13 甚至 go1.14 在很多地方进行了改进。比如 fmt.Sprintf，sync.Pool 等。替换成新版本应该能进一步提升性能。</li>
<li>本地 benchmark 结果不等于线上运行结果。尤其是在使用缓存来提高处理速度时，要考虑 GC 的影响。</li>
<li>传参数或返回值时，尽量按 golang 的设计哲学，少用指针，多用值对象，避免引起过多的变量逃逸，导致 GC 耗时暴涨。struct 的大小一般在 2K 以下的拷贝传值，比使用指针要快（可针对不同的机器压测，判断各自的阈值)。</li>
<li>值类型在满足需要的情况下，越小越好。能用 int8，就不要用 int64。</li>
<li>资源尽量复用,在 golang1.13 以上，可以考虑使用 sync.Pool 缓存会重复申请的内存或对象。或者自己使用并管理大块内存，用来存储小对象，避免 GC 影响（如本地缓存的场景)。</li>
</ul>
<p>推荐阅读: <a href="https://mp.weixin.qq.com/s?__biz=MzAxMTA4Njc0OQ==&amp;mid=2651439020&amp;idx=1&amp;sn=c2094f4dccb53385dc207958e7f42f9e&amp;chksm=80bb615eb7cce8481eb7a8f09d4a13e2974b3785c241dd31245647cd7540dde414d64f2b3719&amp;scene=21#wechat_redirect" target="_blank" rel="external">滴滴实战分享：通过 profiling 定位 golang 性能问题 - 内存篇</a></p>
<p>来源：trumanyan</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[调试golang的bug以及性能问题的实践方法]]></title>
      <url>http://team.jiunile.com/blog/2020/05/go-debug-practice.html</url>
      <content type="html"><![CDATA[<h2 id="场景1：-如何分析程序的运行时间与CPU利用率情况？"><a href="#场景1：-如何分析程序的运行时间与CPU利用率情况？" class="headerlink" title="场景1： 如何分析程序的运行时间与CPU利用率情况？"></a>场景1： 如何分析程序的运行时间与CPU利用率情况？</h2><h3 id="shell内置time指令"><a href="#shell内置time指令" class="headerlink" title="shell内置time指令"></a>shell内置time指令</h3><p>这个方法不算新颖，但是确很实用。 time是Unix/Linux内置多命令，使用时一般不用传过多参数，直接跟上需要调试多程序即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ time go run <span class="built_in">test</span>2.go </span><br><span class="line">&amp;&#123;&#123;0 0&#125; 张三 0&#125;</span><br><span class="line"></span><br><span class="line">real    0m0.843s</span><br><span class="line">user    0m0.216s</span><br><span class="line">sys 0m0.389s</span><br></pre></td></tr></table></figure></p>
<p>上面是使用time对 <code>go run test2.go</code> 对执行程序坐了性能分析，得到3个指标。</p>
<ul>
<li><code>real</code>：从程序开始到结束，实际度过的时间；</li>
<li><code>user</code>：程序在用户态度过的时间；</li>
<li><code>sys</code>：程序在内核态度过的时间</li>
</ul>
<p>一般情况下 <code>real</code> &gt;= <code>user</code> + <code>sys</code>，因为系统还有其它进程(切换其他进程中间对于本进程回有空白期)。</p>
<a id="more"></a>
<h3 id="usr-bin-time指令"><a href="#usr-bin-time指令" class="headerlink" title="/usr/bin/time指令"></a>/usr/bin/time指令</h3><p>这个指令比内置的time更加详细一些，使用的时候需要用绝对路径，而且要加上参数 <code>-v</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/bin/time -v go run <span class="built_in">test</span>2.go  </span><br><span class="line"></span><br><span class="line">    Command being timed: <span class="string">"go run test2.go"</span></span><br><span class="line">    User time (seconds): 0.12</span><br><span class="line">    System time (seconds): 0.06</span><br><span class="line">    Percent of CPU this job got: 115%</span><br><span class="line">    Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.16</span><br><span class="line">    Average shared text size (kbytes): 0</span><br><span class="line">    Average unshared data size (kbytes): 0</span><br><span class="line">    Average stack size (kbytes): 0</span><br><span class="line">    Average total size (kbytes): 0</span><br><span class="line">    Maximum resident <span class="built_in">set</span> size (kbytes): 41172</span><br><span class="line">    Average resident <span class="built_in">set</span> size (kbytes): 0</span><br><span class="line">    Major (requiring I/O) page faults: 1</span><br><span class="line">    Minor (reclaiming a frame) page faults: 15880</span><br><span class="line">    Voluntary context switches: 897</span><br><span class="line">    Involuntary context switches: 183</span><br><span class="line">    Swaps: 0</span><br><span class="line">    File system inputs: 256</span><br><span class="line">    File system outputs: 2664</span><br><span class="line">    Socket messages sent: 0</span><br><span class="line">    Socket messages received: 0</span><br><span class="line">    Signals delivered: 0</span><br><span class="line">    Page size (bytes): 4096</span><br><span class="line">    Exit status: 0</span><br></pre></td></tr></table></figure></p>
<p>可以看到这里的功能要强大多了，除了之前的信息外，还包括了：</p>
<ul>
<li>CPU占用率；</li>
<li>内存使用情况；</li>
<li>Page Fault 情况；</li>
<li>进程切换情况；</li>
<li>文件系统IO；</li>
<li>Socket 使用情况；</li>
<li>……</li>
</ul>
<h2 id="场景2：-如何分析golang程序的内存使用情况？"><a href="#场景2：-如何分析golang程序的内存使用情况？" class="headerlink" title="场景2： 如何分析golang程序的内存使用情况？"></a>场景2： 如何分析golang程序的内存使用情况？</h2><h3 id="内存占用情况查看"><a href="#内存占用情况查看" class="headerlink" title="内存占用情况查看"></a>内存占用情况查看</h3><p>我们先写一段demo例子代码<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"runtime"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//slice 会动态扩容，用slice来做堆内存申请</span></span><br><span class="line">    container := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">32</span>*<span class="number">1000</span>*<span class="number">1000</span>; i++ &#123;</span><br><span class="line">        container = <span class="built_in">append</span>(container, i)</span><br><span class="line">    &#125;</span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(<span class="string">"Start."</span>)</span><br><span class="line"></span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">"force gc."</span>)</span><br><span class="line">    runtime.GC() <span class="comment">//强制调用gc回收</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">"Done."</span>)</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3600</span> * time.Second) <span class="comment">//睡眠，保持程序不退出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$go</span> build -o snippet_mem &amp;&amp; ./snippet_mem</span><br></pre></td></tr></table></figure></p>
<p>然后在./snippet_mem进程没有执行完，我们再开一个窗口，通过 <code>top</code> 命令查看进程的内存占用情况<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$top</span> -p $(pidof snippet_mem)</span><br></pre></td></tr></table></figure></p>
<p>得到结果如下：<br><img src="/images/go/godebug_1.png" alt="go debug top"></p>
<p>我们看出来，没有退出的snippet_mem进程有约830m的内存被占用。</p>
<p>直观上来说，这个程序在 <code>test()</code> 函数执行完后，切片 <code>contaner</code> 的内存应该被释放，不应该占用830M那么大。</p>
<p>下面让我们使用GODEBUG来分析程序的内存使用情况。</p>
<h3 id="GODEBUG与gctrace"><a href="#GODEBUG与gctrace" class="headerlink" title="GODEBUG与gctrace"></a>GODEBUG与gctrace</h3><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><p>执行<code>snippet_mem</code>程序之前添加环境变量<code>GODEBUG=&#39;gctrace=1&#39;</code>来跟踪打印垃圾回收器信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GODEBUG=<span class="string">'gctrace=1'</span> ./snippet_mem</span><br></pre></td></tr></table></figure></p>
<p>设置<code>gctrace=1</code>会使得垃圾回收器在每次回收时汇总所回收内存的大小以及耗时，<br>并将这些内容汇总成单行内容打印到标准错误输出中。</p>
<h4 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gc <span class="comment"># @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #-&gt;#-&gt;# MB, # MB goal, # P</span></span><br></pre></td></tr></table></figure>
<p>含义<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gc <span class="comment">#        GC次数的编号，每次GC时递增</span></span><br><span class="line">@<span class="comment">#s         距离程序开始执行时的时间</span></span><br><span class="line"><span class="comment">#%          GC占用的执行时间百分比</span></span><br><span class="line"><span class="comment">#+...+#     GC使用的时间</span></span><br><span class="line"><span class="comment">#-&gt;#-&gt;# MB  GC开始，结束，以及当前活跃堆内存的大小，单位M</span></span><br><span class="line"><span class="comment"># MB goal   全局堆内存大小</span></span><br><span class="line"><span class="comment"># P         使用processor的数量</span></span><br></pre></td></tr></table></figure></p>
<p>如果每条信息最后，以<code>(forced)</code>结尾，那么该信息是由<code>runtime.GC()</code>调用触发</p>
<p>我们来选择其中一行来解释一下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gc 17 @0.149s 1%: 0.004+36+0.003 ms clock, 0.009+0/0.051/36+0.006 ms cpu, 181-&gt;181-&gt;101 MB, 182 MB goal, 2 P</span><br></pre></td></tr></table></figure></p>
<p>该条信息含义如下：</p>
<ul>
<li><code>gc 17</code>: Gc 调试编号为17</li>
<li><code>@0.149s</code>: 此时程序已经执行了0.149s</li>
<li><code>1%</code>: 0.149s中其中gc模块占用了1%的时间</li>
<li><code>0.004+36+0.003 ms clock</code>: 垃圾回收的时间，分别为STW（stop-the-world）清扫的时间+并发标记和扫描的时间+STW标记的时间</li>
<li><code>0.009+0/0.051/36+0.006 ms cpu</code>: 垃圾回收占用cpu时间</li>
<li><code>181-&gt;181-&gt;101 MB</code>: GC开始前堆内存181M， GC结束后堆内存181M，当前活跃的堆内存101M</li>
<li><code>182 MB goal</code>: 全局堆内存大小</li>
<li><code>2 P</code>: 本次GC使用了2个P(调度器中的Processer)</li>
</ul>
<p>了解了GC的调试信息读法后，接下来我们来分析一下本次GC的结果。</p>
<p>我们还是执行GODEBUG调试<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GODEBUG=<span class="string">'gctrace=1'</span> ./snippet_mem</span><br></pre></td></tr></table></figure></p>
<p>结果如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">2020/03/02 11:22:37 Start.</span><br><span class="line">2020/03/02 11:22:37  ===&gt; loop begin.</span><br><span class="line">gc 1 @0.002s 5%: 0.14+0.45+0.002 ms clock, 0.29+0/0.042/0.33+0.005 ms cpu, 4-&gt;4-&gt;0 MB, 5 MB goal, 2 P</span><br><span class="line">gc 2 @0.003s 4%: 0.13+3.7+0.019 ms clock, 0.27+0/0.037/2.8+0.038 ms cpu, 4-&gt;4-&gt;2 MB, 5 MB goal, 2 P</span><br><span class="line">gc 3 @0.008s 3%: 0.002+1.1+0.001 ms clock, 0.005+0/0.083/1.0+0.003 ms cpu, 6-&gt;6-&gt;2 MB, 7 MB goal, 2 P</span><br><span class="line">gc 4 @0.010s 3%: 0.003+0.99+0.002 ms clock, 0.006+0/0.041/0.82+0.004 ms cpu, 5-&gt;5-&gt;2 MB, 6 MB goal, 2 P</span><br><span class="line">gc 5 @0.011s 4%: 0.079+0.80+0.003 ms clock, 0.15+0/0.046/0.51+0.006 ms cpu, 6-&gt;6-&gt;3 MB, 7 MB goal, 2 P</span><br><span class="line">gc 6 @0.013s 4%: 0.15+3.7+0.002 ms clock, 0.31+0/0.061/3.3+0.005 ms cpu, 8-&gt;8-&gt;8 MB, 9 MB goal, 2 P</span><br><span class="line">gc 7 @0.019s 3%: 0.004+2.5+0.005 ms clock, 0.008+0/0.051/2.1+0.010 ms cpu, 20-&gt;20-&gt;6 MB, 21 MB goal, 2 P</span><br><span class="line">gc 8 @0.023s 5%: 0.014+3.7+0.002 ms clock, 0.029+0.040/1.2/0+0.005 ms cpu, 15-&gt;15-&gt;8 MB, 16 MB goal, 2 P</span><br><span class="line">gc 9 @0.031s 4%: 0.003+1.6+0.001 ms clock, 0.007+0.094/0/0+0.003 ms cpu, 19-&gt;19-&gt;10 MB, 20 MB goal, 2 P</span><br><span class="line">gc 10 @0.034s 3%: 0.006+5.2+0.004 ms clock, 0.013+0/0.045/5.0+0.008 ms cpu, 24-&gt;24-&gt;13 MB, 25 MB goal, 2 P</span><br><span class="line">gc 11 @0.040s 3%: 0.12+2.6+0.002 ms clock, 0.24+0/0.043/2.5+0.004 ms cpu, 30-&gt;30-&gt;16 MB, 31 MB goal, 2 P</span><br><span class="line">gc 12 @0.043s 3%: 0.11+4.4+0.002 ms clock, 0.23+0/0.044/4.1+0.005 ms cpu, 38-&gt;38-&gt;21 MB, 39 MB goal, 2 P</span><br><span class="line">gc 13 @0.049s 3%: 0.008+10+0.040 ms clock, 0.017+0/0.045/10+0.080 ms cpu, 47-&gt;47-&gt;47 MB, 48 MB goal, 2 P</span><br><span class="line">gc 14 @0.070s 2%: 0.004+12+0.002 ms clock, 0.008+0/0.062/12+0.005 ms cpu, 122-&gt;122-&gt;41 MB, 123 MB goal, 2 P</span><br><span class="line">gc 15 @0.084s 2%: 0.11+11+0.038 ms clock, 0.22+0/0.064/3.9+0.076 ms cpu, 93-&gt;93-&gt;93 MB, 94 MB goal, 2 P</span><br><span class="line">gc 16 @0.122s 1%: 0.005+25+0.010 ms clock, 0.011+0/0.12/24+0.021 ms cpu, 238-&gt;238-&gt;80 MB, 239 MB goal, 2 P</span><br><span class="line">gc 17 @0.149s 1%: 0.004+36+0.003 ms clock, 0.009+0/0.051/36+0.006 ms cpu, 181-&gt;181-&gt;101 MB, 182 MB goal, 2 P</span><br><span class="line">gc 18 @0.187s 1%: 0.12+19+0.004 ms clock, 0.25+0/0.049/19+0.008 ms cpu, 227-&gt;227-&gt;126 MB, 228 MB goal, 2 P</span><br><span class="line">gc 19 @0.207s 1%: 0.096+27+0.004 ms clock, 0.19+0/0.077/0.73+0.009 ms cpu, 284-&gt;284-&gt;284 MB, 285 MB goal, 2 P</span><br><span class="line">gc 20 @0.287s 0%: 0.005+944+0.040 ms clock, 0.011+0/0.048/1.3+0.081 ms cpu, 728-&gt;728-&gt;444 MB, 729 MB goal, 2 P</span><br><span class="line">2020/03/02 11:22:38  ===&gt; loop end.</span><br><span class="line">2020/03/02 11:22:38 force gc.</span><br><span class="line">gc 21 @1.236s 0%: 0.004+0.099+0.001 ms clock, 0.008+0/0.018/0.071+0.003 ms cpu, 444-&gt;444-&gt;0 MB, 888 MB goal, 2 P (forced)</span><br><span class="line">2020/03/02 11:22:38 Done.</span><br><span class="line">GC forced</span><br><span class="line">gc 22 @122.455s 0%: 0.010+0.15+0.003 ms clock, 0.021+0/0.025/0.093+0.007 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 23 @242.543s 0%: 0.007+0.075+0.002 ms clock, 0.014+0/0.022/0.085+0.004 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 24 @362.545s 0%: 0.018+0.19+0.006 ms clock, 0.037+0/0.055/0.15+0.013 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 25 @482.548s 0%: 0.012+0.25+0.005 ms clock, 0.025+0/0.025/0.11+0.010 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 26 @602.551s 0%: 0.009+0.10+0.003 ms clock, 0.018+0/0.021/0.075+0.006 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 27 @722.554s 0%: 0.012+0.30+0.005 ms clock, 0.025+0/0.15/0.22+0.011 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 28 @842.556s 0%: 0.027+0.18+0.003 ms clock, 0.054+0/0.11/0.14+0.006 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>先看在<code>test()</code>函数执行完后立即打印的<code>gc 21</code>那行的信息。<code>444-&gt;444-&gt;0 MB, 888 MB goal</code>表示垃圾回收器已经把<code>444M</code>的内存标记为非活跃的内存。</p>
<p>再看下一个记录<code>gc 22。0-&gt;0-&gt;0 MB, 4 MB goal</code>表示垃圾回收器中的全局堆内存大小由<code>888M</code>下降为<code>4M</code>。</p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ol>
<li><strong>在test()函数执行完后，demo程序中的切片容器所申请的堆空间都被垃圾回收器回收了</strong>。</li>
<li>如果此时在<code>top</code>指令查询内存的时候，如果依然先死800+MB，说明<strong>垃圾回收器回收了应用层的内存后，（可能）并不会立即将内存归还给系统</strong>。具体分析原因可见：<a href="https://segmentfault.com/a/1190000022472459" target="_blank" rel="external">踩坑记：go服务内存暴涨</a></li>
</ol>
<h3 id="runtime-ReadMemStats"><a href="#runtime-ReadMemStats" class="headerlink" title="runtime.ReadMemStats"></a>runtime.ReadMemStats</h3><p>接下来我么换另一种方式查看内存的方式 利用 runtime库里的<code>ReadMemStats()</code>方法<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// demo2.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"runtime"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readMemStats</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> ms runtime.MemStats</span><br><span class="line">    runtime.ReadMemStats(&amp;ms)</span><br><span class="line">    log.Printf(<span class="string">" ===&gt; Alloc:%d(bytes) HeapIdle:%d(bytes) HeapReleased:%d(bytes)"</span>, ms.Alloc, ms.HeapIdle, ms.HeapReleased)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//slice 会动态扩容，用slice来做堆内存申请</span></span><br><span class="line">    container := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">32</span>*<span class="number">1000</span>*<span class="number">1000</span>; i++ &#123;</span><br><span class="line">        container = <span class="built_in">append</span>(container, i)</span><br><span class="line">        <span class="keyword">if</span> ( i == <span class="number">16</span>*<span class="number">1000</span>*<span class="number">1000</span>) &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Start]."</span>)</span><br><span class="line"></span><br><span class="line">    readMemStats()</span><br><span class="line">    test()</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [force gc]."</span>)</span><br><span class="line">    runtime.GC() <span class="comment">//强制调用gc回收</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Done]."</span>)</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">            time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3600</span> * time.Second) <span class="comment">//睡眠，保持程序不退出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里我们， 封装了一个函数<code>readMemStats()</code>，这里面主要是调用<code>runtime</code>中的<code>ReadMemStats()</code>方法获得内存信息，然后通过<code>log</code>打印出来。</p>
<p>我们执行一下代码并运行:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ go run demo2.go </span><br><span class="line">2020/03/02 18:21:17  ===&gt; [Start].</span><br><span class="line">2020/03/02 18:21:17  ===&gt; Alloc:71280(bytes) HeapIdle:66633728(bytes) HeapReleased:66600960(bytes)</span><br><span class="line">2020/03/02 18:21:17  ===&gt; loop begin.</span><br><span class="line">2020/03/02 18:21:18  ===&gt; Alloc:132535744(bytes) HeapIdle:336756736(bytes) HeapReleased:155721728(bytes)</span><br><span class="line">2020/03/02 18:21:38  ===&gt; loop end.</span><br><span class="line">2020/03/02 18:21:38  ===&gt; Alloc:598300600(bytes) HeapIdle:609181696(bytes) HeapReleased:434323456(bytes)</span><br><span class="line">2020/03/02 18:21:38  ===&gt; [force gc].</span><br><span class="line">2020/03/02 18:21:38  ===&gt; [Done].</span><br><span class="line">2020/03/02 18:21:38  ===&gt; Alloc:55840(bytes) HeapIdle:1207427072(bytes) HeapReleased:434266112(bytes)</span><br><span class="line">2020/03/02 18:21:38  ===&gt; Alloc:56656(bytes) HeapIdle:1207394304(bytes) HeapReleased:434266112(bytes)</span><br><span class="line">2020/03/02 18:21:48  ===&gt; Alloc:56912(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(bytes)</span><br><span class="line">2020/03/02 18:21:58  ===&gt; Alloc:57488(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(bytes)</span><br><span class="line">2020/03/02 18:22:08  ===&gt; Alloc:57616(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(bytes)</span><br><span class="line">c2020/03/02 18:22:18  ===&gt; Alloc:57744(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(by</span><br></pre></td></tr></table></figure></p>
<p>可以看到，打印<code>[Done].</code>之后那条trace信息，Alloc已经下降，即内存已被垃圾回收器回收。在<code>2020/03/02 18:21:38</code>和<code>2020/03/02 18:21:48</code>的两条trace信息中，HeapReleased开始上升，即垃圾回收器把内存归还给系统。</p>
<p>另外，MemStats还可以获取其它哪些信息以及字段的含义可以参见官方文档：<a href="http://golang.org/pkg/runtime/#MemStats" target="_blank" rel="external">http://golang.org/pkg/runtime/#MemStats</a></p>
<h3 id="pprof工具"><a href="#pprof工具" class="headerlink" title="pprof工具"></a>pprof工具</h3><p>pprof工具支持网页上查看内存的使用情况，需要在代码中添加一个协程即可。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>(</span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>))</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure></p>
<p>具体添加的完整代码如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//demo3.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"runtime"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readMemStats</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> ms runtime.MemStats</span><br><span class="line">    runtime.ReadMemStats(&amp;ms)</span><br><span class="line">    log.Printf(<span class="string">" ===&gt; Alloc:%d(bytes) HeapIdle:%d(bytes) HeapReleased:%d(bytes)"</span>, ms.Alloc, ms.HeapIdle, ms.HeapReleased)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//slice 会动态扩容，用slice来做堆内存申请</span></span><br><span class="line">    container := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">32</span>*<span class="number">1000</span>*<span class="number">1000</span>; i++ &#123;</span><br><span class="line">        container = <span class="built_in">append</span>(container, i)</span><br><span class="line">        <span class="keyword">if</span> ( i == <span class="number">16</span>*<span class="number">1000</span>*<span class="number">1000</span>) &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//启动pprof</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        log.Println(http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>))</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Start]."</span>)</span><br><span class="line"></span><br><span class="line">    readMemStats()</span><br><span class="line">    test()</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [force gc]."</span>)</span><br><span class="line">    runtime.GC() <span class="comment">//强制调用gc回收</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Done]."</span>)</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">            time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3600</span> * time.Second) <span class="comment">//睡眠，保持程序不退出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们正常运行程序，然后同时打开浏览器，</p>
<p>输入地址：<a href="http://127.0.0.1:10000/debug/pprof/heap?debug=1" target="_blank" rel="external">http://127.0.0.1:10000/debug/pprof/heap?debug=1</a></p>
<p>浏览器的内容其中有一部分如下，记录了目前的内存情况<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># runtime.MemStats</span></span><br><span class="line"><span class="comment"># Alloc = 228248</span></span><br><span class="line"><span class="comment"># TotalAlloc = 1293696976</span></span><br><span class="line"><span class="comment"># Sys = 834967896</span></span><br><span class="line"><span class="comment"># Lookups = 0</span></span><br><span class="line"><span class="comment"># Mallocs = 2018</span></span><br><span class="line"><span class="comment"># Frees = 671</span></span><br><span class="line"><span class="comment"># HeapAlloc = 228248</span></span><br><span class="line"><span class="comment"># HeapSys = 804913152</span></span><br><span class="line"><span class="comment"># HeapIdle = 804102144</span></span><br><span class="line"><span class="comment"># HeapInuse = 811008</span></span><br><span class="line"><span class="comment"># HeapReleased = 108552192</span></span><br><span class="line"><span class="comment"># HeapObjects = 1347</span></span><br><span class="line"><span class="comment"># Stack = 360448 / 360448</span></span><br><span class="line"><span class="comment"># MSpan = 28288 / 32768</span></span><br><span class="line"><span class="comment"># MCache = 3472 / 16384</span></span><br><span class="line"><span class="comment"># BuckHashSys = 1449617</span></span><br><span class="line"><span class="comment"># GCSys = 27418976</span></span><br><span class="line"><span class="comment"># OtherSys = 776551</span></span><br><span class="line"><span class="comment"># NextGC = 4194304</span></span><br><span class="line"><span class="comment"># LastGC = 1583203571137891390</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure></p>
<h2 id="场景3-如何分析Golang程序的CPU性能情况？"><a href="#场景3-如何分析Golang程序的CPU性能情况？" class="headerlink" title="场景3: 如何分析Golang程序的CPU性能情况？"></a>场景3: 如何分析Golang程序的CPU性能情况？</h2><h3 id="性能分析注意事项"><a href="#性能分析注意事项" class="headerlink" title="性能分析注意事项"></a>性能分析注意事项</h3><ul>
<li>性能分析必须在一个可重复的、稳定的环境中来进行。</li>
<li>机器必须闲置<ul>
<li>不要在共享硬件上进行性能分析;</li>
<li>不要在性能分析期间，在同一个机器上去浏览网页</li>
</ul>
</li>
<li>注意省电模式和过热保护，如果突然进入这些模式，会导致分析数据严重不准确</li>
<li><strong>不要使用虚拟机、共享的云主机</strong>，太多干扰因素，分析数据会很不一致</li>
<li>不要在 macOS 10.11 及以前的版本运行性能分析，有 bug，之后的版本修复了</li>
</ul>
<p>如果承受得起，购买专用的性能测试分析的硬件设备，上架。</p>
<ul>
<li>关闭电源管理、过热管理</li>
<li>绝不要升级，以保证测试的一致性，以及具有可比性</li>
</ul>
<p>如果没有这样的环境，那就一定要在多个环境中，执行多次，以取得可参考的、具有相对一致性的测试结果。</p>
<h3 id="CPU性能分析"><a href="#CPU性能分析" class="headerlink" title="CPU性能分析"></a>CPU性能分析</h3><p>我们来用下面的代码进行测试<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//demo4.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"bytes"</span></span><br><span class="line">    <span class="string">"math/rand"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line">        log.Println(genSomeBytes())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//生成一个随机字符串</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">genSomeBytes</span><span class="params">()</span> *<span class="title">bytes</span>.<span class="title">Buffer</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> buff bytes.Buffer</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">20000</span>; i++ &#123;</span><br><span class="line">        buff.Write([]<span class="keyword">byte</span>&#123;<span class="string">'0'</span> + <span class="keyword">byte</span>(rand.Intn(<span class="number">10</span>))&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &amp;buff</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            test()</span><br><span class="line">            time.Sleep(time.Second * <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动pprof</span></span><br><span class="line">    http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里面还是启动了pprof的坚挺,有关<code>pprof</code>启动的代码如下:<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">//启动pprof</span></span><br><span class="line">  http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>main()</code>里的流程很简单,启动一个goroutine去无限循环调用<code>test()</code>方法,休眠1s.</p>
<p><code>test()</code>的流程是生成1000个20000个字符的随机字符串.并且打印.</p>
<p>我们将上面的代码编译成可执行的二进制文件 <code>demo4</code>(记住这个名字,稍后我们能用到)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ go build demo4.go</span><br></pre></td></tr></table></figure></p>
<p>接下来我们启动程序,程序会无限循环的打印字符串.</p>
<p>接下来我们通过几种方式来查看进程的cpu性能情况.</p>
<h4 id="A-Web界面查看"><a href="#A-Web界面查看" class="headerlink" title="A. Web界面查看"></a>A. Web界面查看</h4><p>浏览器访问: <a href="http://127.0.0.1:10000/debug/pprof/" target="_blank" rel="external">http://127.0.0.1:10000/debug/pprof/</a></p>
<p>我们会看到如下画面<br><img src="/images/go/godebug_2.png" alt="go pprof web"></p>
<p>这里面能够通过pprof查看包括(阻塞信息、cpu信息、内存堆信息、锁信息、goroutine信息等等), 我们这里关心的cpu的性能的<code>profile</code>信息.</p>
<p>有关<code>profile</code>下面的英文解释大致如下:</p>
<blockquote>
<p>“CPU配置文件。您可以在秒GET参数中指定持续时间。获取概要文件后，请使用go tool pprof命令调查概要文件。”</p>
</blockquote>
<p>所以我们要是想得到cpu性能,就是要获取到当前进程的<code>profile</code>文件,这个文件默认是30s生成一个,所以你的程序要至少运行30s以上(这个参数也可以修改,稍后我们介绍)</p>
<p>我们可以直接点击网页的<code>profile</code>,浏览器会给我们下载一个<code>profile</code>文件. 记住这个文件的路径, 可以拷贝到与<code>demo4</code>所在的同一文件夹下.</p>
<h4 id="B-使用pprof工具查看"><a href="#B-使用pprof工具查看" class="headerlink" title="B. 使用pprof工具查看"></a>B. 使用pprof工具查看</h4><p>pprof 的格式如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof [binary] [profile]</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>binary</code>: 必须指向生成这个性能分析数据的那个二进制可执行文件；</li>
<li><code>profile</code>: 必须是该二进制可执行文件所生成的性能分析数据文件。</li>
</ul>
<p><code>binary</code> 和 <code>profile</code> <strong>必须严格匹配</strong>。</p>
<p>我们来查看一下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof ./demo4 profile</span><br><span class="line"></span><br><span class="line">File: demo4</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Mar 3, 2020 at 11:18pm (CST)</span><br><span class="line">Duration: 30.13s, Total samples = 6.27s (20.81%)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p><strong>help</strong>可以查看一些指令,我么可以通过<strong>top</strong>来查看cpu的性能情况.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 5090ms, 81.18% of 6270ms total</span><br><span class="line">Dropped 80 nodes (cum &lt;= 31.35ms)</span><br><span class="line">Showing top 10 nodes out of 60</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">    1060ms 16.91% 16.91%     2170ms 34.61%  math/rand.(*lockedSource).Int63</span><br><span class="line">     850ms 13.56% 30.46%      850ms 13.56%  sync.(*Mutex).Unlock (inline)</span><br><span class="line">     710ms 11.32% 41.79%     2950ms 47.05%  math/rand.(*Rand).Int31n</span><br><span class="line">     570ms  9.09% 50.88%      990ms 15.79%  bytes.(*Buffer).Write</span><br><span class="line">     530ms  8.45% 59.33%      540ms  8.61%  syscall.Syscall</span><br><span class="line">     370ms  5.90% 65.23%      370ms  5.90%  runtime.procyield</span><br><span class="line">     270ms  4.31% 69.54%     4490ms 71.61%  main.genSomeBytes</span><br><span class="line">     250ms  3.99% 73.52%     3200ms 51.04%  math/rand.(*Rand).Intn</span><br><span class="line">     250ms  3.99% 77.51%      250ms  3.99%  runtime.memmove</span><br><span class="line">     230ms  3.67% 81.18%      690ms 11.00%  runtime.suspendG</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p>这里面有几列数据,需要说明一下.</p>
<ul>
<li>flat：当前函数占用CPU的耗时</li>
<li>flat%：:当前函数占用CPU的耗时百分比</li>
<li>sun%：函数占用CPU的耗时累计百分比</li>
<li>cum：当前函数加上调用当前函数的函数占用CPU的总耗时</li>
<li>cum%：当前函数加上调用当前函数的函数占用CPU的总耗时百分比</li>
<li>最后一列：函数名称</li>
</ul>
<p>通过结果我们可以看出, 该程序的大部分cpu性能消耗在 <code>main.getSoneBytes()</code>方法中,其中math/rand取随机数消耗比较大.</p>
<h4 id="C-通过go-tool-pprof得到profile文件"><a href="#C-通过go-tool-pprof得到profile文件" class="headerlink" title="C. 通过go tool pprof得到profile文件"></a>C. 通过go tool pprof得到profile文件</h4><p>我们上面的profile文件是通过web浏览器下载的,这个profile的经过时间是30s的,默认值我们在浏览器上修改不了,如果你想得到时间更长的cpu利用率,可以通过<code>go tool pprof</code>指令与程序交互来获取到</p>
<p>首先,我们先启动程序<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./demo4</span><br></pre></td></tr></table></figure></p>
<p>然后再打开一个终端<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof http://localhost:10000/debug/pprof/profile?seconds=60</span><br></pre></td></tr></table></figure></p>
<p>这里制定了生成profile文件的时间间隔60s</p>
<p>等待60s之后, 终端就会有结果出来,我们继续使用top来查看.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof http://localhost:10000/debug/pprof/profile?seconds=60</span><br><span class="line">Fetching profile over HTTP from http://localhost:10000/debug/pprof/profile?seconds=60</span><br><span class="line">Saved profile <span class="keyword">in</span> /home/itheima/pprof/pprof.demo4.samples.cpu.005.pb.gz</span><br><span class="line">File: demo4</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Mar 3, 2020 at 11:59pm (CST)</span><br><span class="line">Duration: 1mins, Total samples = 12.13s (20.22%)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 9940ms, 81.95% of 12130ms total</span><br><span class="line">Dropped 110 nodes (cum &lt;= 60.65ms)</span><br><span class="line">Showing top 10 nodes out of 56</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">    2350ms 19.37% 19.37%     4690ms 38.66%  math/rand.(*lockedSource).Int63</span><br><span class="line">    1770ms 14.59% 33.97%     1770ms 14.59%  sync.(*Mutex).Unlock (inline)</span><br><span class="line">    1290ms 10.63% 44.60%     6040ms 49.79%  math/rand.(*Rand).Int31n</span><br><span class="line">    1110ms  9.15% 53.75%     1130ms  9.32%  syscall.Syscall</span><br><span class="line">     810ms  6.68% 60.43%     1860ms 15.33%  bytes.(*Buffer).Write</span><br><span class="line">     620ms  5.11% 65.54%     6660ms 54.91%  math/rand.(*Rand).Intn</span><br><span class="line">     570ms  4.70% 70.24%      570ms  4.70%  runtime.procyield</span><br><span class="line">     500ms  4.12% 74.36%     9170ms 75.60%  main.genSomeBytes</span><br><span class="line">     480ms  3.96% 78.32%      480ms  3.96%  runtime.memmove</span><br><span class="line">     440ms  3.63% 81.95%      440ms  3.63%  math/rand.(*rngSource).Uint64</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p>依然会得到cpu性能的结果, 我们发现这次的结果与上次30s的结果百分比类似.</p>
<h4 id="D-可视化查看"><a href="#D-可视化查看" class="headerlink" title="D.可视化查看"></a>D.可视化查看</h4><p>我们还是通过<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof ./demo4 profile</span><br></pre></td></tr></table></figure></p>
<p>进入profile文件查看,然后我们输入<code>web</code>指令.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof ./demo4 profileFile: demo4</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Mar 3, 2020 at 11:18pm (CST)</span><br><span class="line">Duration: 30.13s, Total samples = 6.27s (20.81%)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof) web</span><br></pre></td></tr></table></figure></p>
<p>这里如果报找不到<code>graphviz</code>工具，需要安装一下。这里自行百度如何安装</p>
<p>然后我们得到一个<code>svg</code>的可视化文件在<code>/tmp</code>路径下<br><img src="/images/go/godebug_3.png" alt="go pprof web"></p>
<p>这样我们就能比较清晰的看到函数之间的调用关系,方块越大的表示cpu的占用越大.</p>
<p>来源：刘丹冰Aceld 简书</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes Etcd 数据备份与恢复]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-etcd-backup-restore.html</url>
      <content type="html"><![CDATA[<h2 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h2><blockquote>
<p>Etcd 版本：3.4.3<br>Kubernetes 版本：1.17.4<br>Kubernetes 安装方式：Kubeadm</p>
</blockquote>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kubernetes 使用 Etcd 数据库实时存储集群中的数据，可以说 Etcd 是 Kubernetes 的核心组件，犹如人类的大脑。如果 Etcd 数据损坏将导致 Kubernetes 不可用，在生产环境中 Etcd 数据是一定要做好高可用与数据备份，这里介绍下如何备份与恢复 Etcd 数据。</p>
<h2 id="备份-Etcd-数据"><a href="#备份-Etcd-数据" class="headerlink" title="备份 Etcd 数据"></a>备份 Etcd 数据</h2><p>采用镜像方式部署的 Etcd，所以操作 Etcd 需要使用 Etcd 镜像提供的 Etcdctl 工具。如果你是非镜像方式部署 Etcd，可以直接使用 Etcdctl 命令备份数据。</p>
<p>运行 Etcd 镜像，并且使用镜像内部的 etcdctl 工具连接 etcd 集群，执行数据快照备份：</p>
<ul>
<li>/bin/sh -c：执行 shell 命令</li>
<li>–env：设置环境变量，指定 etcdctl 工具使用的 API 版本</li>
<li>-v：docker 挂载选项，用于挂载 Etcd 证书相关目录以及备份数据存放的目录</li>
<li>–cacert：etcd CA 证书</li>
<li>–key：etcd 客户端证书 key</li>
<li>–cert：etcd 客户端证书 crt</li>
<li>–endpoints：指定 ETCD 连接地址</li>
<li>etcdctl snapshot save：etcd 数据备份</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm                                    \</span><br><span class="line">-v /data/backup:/backup                              \</span><br><span class="line">-v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \</span><br><span class="line">--env ETCDCTL_API=3                                  \</span><br><span class="line">k8s.gcr.io/etcd:3.4.3-0                              \</span><br><span class="line">/bin/sh -c <span class="string">"etcdctl --endpoints=https://192.168.2.11:2379 \</span><br><span class="line">--cacert=/etc/kubernetes/pki/etcd/ca.crt                  \</span><br><span class="line">--key=/etc/kubernetes/pki/etcd/healthcheck-client.key     \</span><br><span class="line">--cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt    \</span><br><span class="line">snapshot save /backup/etcd-snapshot.db"</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="恢复-ETCD-数据"><a href="#恢复-ETCD-数据" class="headerlink" title="恢复 ETCD 数据"></a>恢复 ETCD 数据</h2><p>在 Etcd 数据损坏时，可以通过 Etcd 备份数据进行数据恢复，先暂停 Kubernetes 相关组件，然后进入 Etcd 镜像使用 etcdctl 工具执行恢复操作。</p>
<h3 id="暂停-Kube-Apiserver-与-Etcd-镜像"><a href="#暂停-Kube-Apiserver-与-Etcd-镜像" class="headerlink" title="暂停 Kube-Apiserver 与 Etcd 镜像"></a>暂停 Kube-Apiserver 与 Etcd 镜像</h3><p>在恢复 Etcd 数据前，需要停止 <code>kube-apiserver</code> 与 <code>etcd</code> 镜像，因为当这俩镜像停止后 Kubernetes 会自动重启这俩镜像，所以我们可以先暂时移除 <code>/etc/kubernetes/manifests</code> 目录，Kubernetes 检测这个目录文件不存在时会停止 Kubernetes 系统相关镜像，使其不能重启，方便我们进行后续的操作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 移除且备份 /etc/kubernetes/manifests 目录</span></span><br><span class="line">$ mv /etc/kubernetes/manifests /etc/kubernetes/manifests.bak</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 kube-apiserver、etcd 镜像是否停止</span></span><br><span class="line">$ docker ps|grep etcd &amp;&amp; docker ps|grep kube-apiserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份现有 Etcd 数据</span></span><br><span class="line">$ mv /var/lib/etcd /var/lib/etcd.bak</span><br></pre></td></tr></table></figure></p>
<h3 id="恢复-Etcd-数据"><a href="#恢复-Etcd-数据" class="headerlink" title="恢复 Etcd 数据"></a>恢复 Etcd 数据</h3><p>运行 Etcd 镜像，然后执行数据恢复，默认会恢复到 <code>/default.etcd/member/</code> 目录下，这里使用 <code>mv</code> 命令在移动到挂载目录 <code>/var/lib/etcd/</code> 下。</p>
<ul>
<li>/bin/sh -c：执行 shell 命令</li>
<li>–env：设置环境变量，指定 etcdctl 工具使用的 API 版本</li>
<li>-v：docker 挂载选项，用于挂载 Etcd 证书相关目录以及备份数据存放的目录</li>
<li>etcdctl snapshot restore：etcd 数据恢复。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm              \</span><br><span class="line">-v /data/backup:/backup        \</span><br><span class="line">-v /var/lib/etcd:/var/lib/etcd \</span><br><span class="line">--env ETCDCTL_API=3            \</span><br><span class="line">k8s.gcr.io/etcd:3.4.3-0        \</span><br><span class="line">/bin/sh -c <span class="string">"etcdctl snapshot restore /backup/etcd-snapshot.db; mv /default.etcd/member/ /var/lib/etcd/"</span></span><br></pre></td></tr></table></figure>
<h3 id="恢复-Kube-Apiserver-与-Etcd-镜像"><a href="#恢复-Kube-Apiserver-与-Etcd-镜像" class="headerlink" title="恢复 Kube-Apiserver 与 Etcd 镜像"></a>恢复 Kube-Apiserver 与 Etcd 镜像</h3><p>将 <code>/etc/kubernetes/manifests</code> 目录恢复，使 Kubernetes 重启 <code>Kube-Apiserver</code> 与 <code>Etcd</code> 镜像：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests</span><br></pre></td></tr></table></figure></p>
<h3 id="执行-Kubectl-命令进行检测"><a href="#执行-Kubectl-命令进行检测" class="headerlink" title="执行 Kubectl 命令进行检测"></a>执行 Kubectl 命令进行检测</h3><p>执行 Kubectl 命令进行检测，查看命令是否能够正常执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure></p>
<p>参考：mydlq.club</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm 添加新 Master 节点到集群出现 ETCD 健康检查失败错误]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-kubeadm-upgrade-master-problem.html</url>
      <content type="html"><![CDATA[<h2 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h2><blockquote>
<p>Docker 版本：18.06.3<br>Kubeadm 版本：1.17.4<br>Kubernetes 版本：1.17.4<br>Kubernetes Master 数量：3<br>Kubernetes 安装方式：Kubeadm</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>Kubernetes 集群中总共有三台 Master，分别是：</p>
<p>k8s-master-2-11、k8s-master-2-12、k8s-master-2-13</p>
<p>对其中 k8s-master-2-11 Master 节点服务器进行了内核和软件升级操作，从而先将其暂时剔出集群，然后进行升级，完成后准备重新加入到 Kubernetes 集群，通过 Kubeadm 执行，输入下面命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm join mydlq.club:16443 \</span><br><span class="line">--token 6w0nwi.zag57qgfcdhi76vd \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:efa49231e4ffd836ff996921741c98ac4c5655dc729d7c32aa48c608232f0f08 \</span><br><span class="line">--control-plane --certificate-key a64e9da7346153bd64dba1e5126a644a97fdb63c878bb73de07911d1add8e26b</span><br></pre></td></tr></table></figure></p>
<p>在执行过程中，输出下面日志，提示 etcd 监控检查失败：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">W0329 00:01:51.364121   19209 manifests.go:214] the default kube-apiserver authorization-mode is <span class="string">"Node,RBAC"</span>; using <span class="string">"Node,RBAC"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">W0329 00:01:51.373807   19209 manifests.go:214] the default kube-apiserver authorization-mode is <span class="string">"Node,RBAC"</span>; using <span class="string">"Node,RBAC"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line"></span><br><span class="line">error execution phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://10.8.18.105:2379 </span><br><span class="line">with maintenance client: context deadline exceeded</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure></p>
<p>根据关键信息 <code>&quot;error execution phase check-etcd&quot;</code> 可知，可能是在执行加入 <code>etcd</code> 时候出现的错误，导致 <code>master</code> 无法加入原先的 <code>kubernetes</code> 集群。</p>
<a id="more"></a>
<h2 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h2><h3 id="查看集群节点列表"><a href="#查看集群节点列表" class="headerlink" title="查看集群节点列表"></a>查看集群节点列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node</span><br><span class="line"></span><br><span class="line">NAME              STATUS    ROLES     VERSION</span><br><span class="line">k8s-master-2-12    Ready    master    v1.17.4</span><br><span class="line">k8s-master-2-13    Ready    master    v1.17.4</span><br><span class="line">k8s-node-2-14      Ready    &lt;none&gt;    v1.17.4</span><br><span class="line">k8s-node-2-15      Ready    &lt;none&gt;    v1.17.4</span><br><span class="line">k8s-node-2-16      Ready    &lt;none&gt;    v1.17.4</span><br></pre></td></tr></table></figure>
<p>可以看到，k8s-master-2-11 节点确实不在节点列表中</p>
<h3 id="查看-Kubeadm-配置信息"><a href="#查看-Kubeadm-配置信息" class="headerlink" title="查看 Kubeadm 配置信息"></a>查看 Kubeadm 配置信息</h3><p>在看看 Kubernetes 集群中的 kubeadm 配置信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe configmaps kubeadm-config -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>获取到的内容如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Name:         kubeadm-config</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">...</span><br><span class="line">ClusterStatus:</span><br><span class="line">----</span><br><span class="line">apiEndpoints:</span><br><span class="line">  k8s-master-2-11:</span><br><span class="line">    advertiseAddress: 192.168.2.11</span><br><span class="line">    <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  k8s-master-2-12:</span><br><span class="line">    advertiseAddress: 192.168.2.12</span><br><span class="line">    <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  k8s-master-2-13:</span><br><span class="line">    advertiseAddress: 192.168.2.13</span><br><span class="line">    <span class="built_in">bind</span>Port: 6443</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterStatus</span><br></pre></td></tr></table></figure></p>
<p>可也看到 <code>k8s-master-2-11</code> 节点信息还存在与 <code>kubeadm</code> 配置中，说明 <code>etcd</code> 中还存储着 <code>k8s-master-2-11</code> 相关信息。</p>
<h3 id="分析问题所在及解决方案"><a href="#分析问题所在及解决方案" class="headerlink" title="分析问题所在及解决方案"></a>分析问题所在及解决方案</h3><p>因为集群是通过 <code>kubeadm</code> 工具搭建的，且使用了 <code>etcd</code> 镜像方式与 <code>master</code> 节点一起，所以每个 <code>Master</code> 节点上都会存在一个 <code>etcd</code> 容器实例。当剔除一个 <code>master</code> 节点时 <code>etcd</code> 集群未删除剔除的节点的 <code>etcd</code> 成员信息，该信息还存在 <code>etcd</code> 集群列表中。</p>
<p>所以，我们需要 <strong>进入 etcd 手动删除 etcd 成员信息</strong>。</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><h3 id="获取-Etcd-镜像列表"><a href="#获取-Etcd-镜像列表" class="headerlink" title="获取 Etcd 镜像列表"></a>获取 Etcd 镜像列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system | grep etcd</span><br><span class="line"></span><br><span class="line">etcd-k8s-master-2-12   1/1   Running   0</span><br><span class="line">etcd-k8s-master-2-13   1/1   Running   0</span><br></pre></td></tr></table></figure>
<h3 id="进入-Etcd-容器并删除节点信息"><a href="#进入-Etcd-容器并删除节点信息" class="headerlink" title="进入 Etcd 容器并删除节点信息"></a>进入 Etcd 容器并删除节点信息</h3><p>选择上面两个 etcd 中任意一个 pod，通过 kubectl 工具进入 pod 内部：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> -it etcd-k8s-master-2-12 sh -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>进入容器后，按下面步执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置环境</span></span><br><span class="line">$ <span class="built_in">export</span> ETCDCTL_API=3</span><br><span class="line">$ <span class="built_in">alias</span> etcdctl=<span class="string">'etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 etcd 集群成员列表</span></span><br><span class="line">$ etcdctl member list</span><br><span class="line"></span><br><span class="line">63bfe05c4646fb08, started, k8s-master-2-11, https://192.168.2.11:2380, https://192.168.2.11:2379, <span class="literal">false</span></span><br><span class="line">8e41efd8164c6e3d, started, k8s-master-2-12, https://192.168.2.12:2380, https://192.168.2.12:2379, <span class="literal">false</span></span><br><span class="line">a61d0bd53c1cbcb6, started, k8s-master-2-13, https://192.168.2.13:2380, https://192.168.2.13:2379, <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 etcd 集群成员 k8s-master-2-11</span></span><br><span class="line">$ etcdctl member remove 63bfe05c4646fb08</span><br><span class="line"></span><br><span class="line">Member 63bfe05c4646fb08 removed from cluster ed984b9o8w35<span class="built_in">cap</span>2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看 etcd 集群成员列表</span></span><br><span class="line">$ etcdctl member list</span><br><span class="line"></span><br><span class="line">8e41efd8164c6e3d, started, k8s-master-2-12, https://192.168.2.12:2380, https://192.168.2.12:2379, <span class="literal">false</span></span><br><span class="line">a61d0bd53c1cbcb6, started, k8s-master-2-13, https://192.168.2.13:2380, https://192.168.2.13:2379, <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出容器</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<h3 id="通过-kubeadm-命令再次尝试加入集群"><a href="#通过-kubeadm-命令再次尝试加入集群" class="headerlink" title="通过 kubeadm 命令再次尝试加入集群"></a>通过 kubeadm 命令再次尝试加入集群</h3><p>通过 <code>kubeadm</code> 命令再次尝试将 <code>k8s-master-2-11</code> 节点加入集群，在执行前首先进入到 <code>k8s-master-2-11</code> 节点服务器，执行 <code>kubeadm</code> 的清除命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm reset</span><br></pre></td></tr></table></figure></p>
<p>然后尝试加入 kubernetes 集群：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm join mydlq.club:16443 \</span><br><span class="line">--token 6w0nwi.zag57qgfcdhi76vd \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:efa49231e4ffd836ff996921741c98ac4c5655dc729d7c32aa48c608232f0f08 \</span><br><span class="line">--control-plane --certificate-key a64e9da7346153bd64dba1e5126a644a97fdb63c878bb73de07911d1add8e26b</span><br></pre></td></tr></table></figure></p>
<p>参考：mydlq.club</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes 1.18+ 使用ipvs后coredns无法解析域名采坑记]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-1-18-ipvs-problem.html</url>
      <content type="html"><![CDATA[<h2 id="环境现象"><a href="#环境现象" class="headerlink" title="环境现象"></a>环境现象</h2><blockquote>
<p>CoreDNS 版本：1.6.7<br>Docker 版本：18.06.3-ce<br>Kubernetes 版本：1.18.1<br>操作系统版本：CentOS 7.x<br>CentOS 内核版本：3.10.0-693.2.2.el7.x86_64</p>
</blockquote>
<h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>由于最近要向组内演示 Istio 1.5，故在云服务器上安装了kubernetes 1.18.1 (使用ipvs) 来配合 Istio 1.5 的演示，kubernetes 与 istio 安装完后一切顺利，bookinfo demo跑的也非常的顺利。但在一次重启整个集群后，一切都变的不那么美好了。istio-proxy起不来了，提示pilot not ready，更可悲的是service name 无法解析了，导致很多服务之间的调用都没法正常工作，有依赖的服务都不能正常启动了，为了更好的定位问题，于是卸载 Istio 1.5，重新回到k8s 1.18.1上，在一步步排查中发现，在pod中使用pod ip相互访问是可以的，由此判断，初步怀疑很可能是 DNS 出现了问题，后来慢慢发现 kube-proxy 中的错误，再定位到 IPVS parseIP Error 错误，再到解决问题。以下是整个问题定位分析</p>
<a id="more"></a>
<h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>为了更好的重现问题，我重新安装了一套新的 kubernetes 1.18.1，安装完后验证一切都是ok，接下来我就重启整个集群，问题就产生了，在pod中无法解析service name了，所有的pod运行都是ok的，一切都变的”很神奇”，好吧，开启定位之旅。</p>
<h3 id="部署DNS调试工具"><a href="#部署DNS调试工具" class="headerlink" title="部署DNS调试工具"></a>部署DNS调试工具</h3><p>为了探针是否为 DNS 问题，这里需要提前部署用于测试 DNS 问题的 dnsutils 镜像，该镜像中包含了用于测试 DNS 问题的工具包，非常利于我们分析与发现问题。</p>
<p>部署 ndsutils.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> dnsutils</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> dnsutils</span><br><span class="line"><span class="attr">    image:</span> mydlqclub/dnsutils:<span class="number">1.3</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"sleep"</span>,<span class="string">"3600"</span>]</span><br></pre></td></tr></table></figure></p>
<h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><h4 id="1-进入-DNS-工具-Pod-的命令行"><a href="#1-进入-DNS-工具-Pod-的命令行" class="headerlink" title="1. 进入 DNS 工具 Pod 的命令行"></a>1. 进入 DNS 工具 Pod 的命令行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it dnsutils sh</span><br></pre></td></tr></table></figure>
<h4 id="2-通过-Ping-和-Nsloopup-命令测试"><a href="#2-通过-Ping-和-Nsloopup-命令测试" class="headerlink" title="2. 通过 Ping 和 Nsloopup 命令测试"></a>2. 通过 Ping 和 Nsloopup 命令测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ping 集群外部，例如这里 ping 一下百度</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line">ping: bad address <span class="string">'www.baidu.com'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 集群内部 kube-apiserver 的 Service 地址</span></span><br><span class="line">$ ping kubernetes.default</span><br><span class="line">ping: bad address <span class="string">'kubernetes.default'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 nslookup 命令查询域名信息</span></span><br><span class="line">$ nslookup kubernetes.default</span><br><span class="line">;; connection timed out; no servers could be reached</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接 Ping 集群内部 kube-apiserver 的 IP 地址</span></span><br><span class="line">$ ping 10.96.0.1</span><br><span class="line">PING 10.96.0.1 (10.96.0.1): 56 data bytes</span><br><span class="line">64 bytes from 10.96.0.1: seq=0 ttl=64 time=0.096 ms</span><br><span class="line">64 bytes from 10.96.0.1: seq=1 ttl=64 time=0.050 ms</span><br><span class="line">64 bytes from 10.96.0.1: seq=2 ttl=64 time=0.068 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 dnsutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>可以观察到两次 ping 域名都不能 ping 通，且使用 nsloopup 分析域名信息时超时。然而，使用 ping kube-apiserver 的 IP 地址 “10.96.0.1” 则可以正常通信，所以，排除网络插件（flannel、calico 等）的问题。初步判断，很可能是 CoreDNS 组件的错误引起的某些问题，所以接下来我们测试 CoreDNS 是否正常。</p>
<h4 id="3-检测-CoreDNS-应用是否正常运行"><a href="#3-检测-CoreDNS-应用是否正常运行" class="headerlink" title="3. 检测 CoreDNS 应用是否正常运行"></a>3. 检测 CoreDNS 应用是否正常运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods <span class="_">-l</span> k8s-app=kube-dns -n kube-system</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-669f77d7cc-8pkpw   1/1     Running   2          6h5m</span><br><span class="line">coredns-669f77d7cc-jk9wk   1/1     Running   2          6h5m</span><br></pre></td></tr></table></figure>
<p>可也看到 CoreDNS 两个 Pod 均正常启动，所以再查看两个 Pod 中的日志信息，看看有无错误日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br></pre></td></tr></table></figure></p>
<p>通过上面信息可以观察到，日志中信息也是正常启动没有问题。再接下来，查看下 CoreDNS 的入口 Service “kube-dns” 是否存在：</p>
<blockquote>
<p>kube-dns 的 IP 为 10.96.0.10，集群内的 Pod 都是通过该 IP 与 DNS 组件进行交互，查询 DNS 信息。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get service -n kube-system | grep kube-dns</span><br><span class="line">NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)               </span><br><span class="line">kube-dns                    ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP</span><br></pre></td></tr></table></figure>
<p>上面显示 Service “kube-dns” 也存在，但是 Service 是通过 endpoints 和 Pod 进行绑定的，所以看看这个 CoreDNS 的 endpoints 是否存在，及信息是否正确：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get endpoints kube-dns -n kube-system</span><br><span class="line">NAME       ENDPOINTS                                      </span><br><span class="line">kube-dns   10.244.0.21:53,d10.244.2.82:53,10.244.0.21:9153</span><br></pre></td></tr></table></figure></p>
<p>可以看到 endpoints 配置也是正常的，正确的与两个 CporeDNS Pod 进行了关联。</p>
<p>经过上面一系列检测 CoreDNS 组件确实是正常运行。接下来，观察 CoreDNS 域名解析日志，进而确定 Pod 中的域名解析请求是否能够正常进入 CoreDNS。</p>
<h4 id="4-观察-CoreDNS-域名解析日志信息"><a href="#4-观察-CoreDNS-域名解析日志信息" class="headerlink" title="4. 观察 CoreDNS 域名解析日志信息"></a>4. 观察 CoreDNS 域名解析日志信息</h4><p>使用 <code>kubectl edit</code> 命令来修改存储于 Kubernetes <code>ConfigMap</code> 中的 CoreDNS 配置参数信息，添加 <code>log</code> 参数，让 CoreDNS 日志中显示域名解析信息：</p>
<blockquote>
<p>CoreDNS 配置参数说明：</p>
<ul>
<li>errors: 输出错误信息到控制台。</li>
<li>health：CoreDNS 进行监控检测，检测地址为 <a href="http://localhost:8080/health" target="_blank" rel="external">http://localhost:8080/health</a> 如果状态为不健康则让 Pod 进行重启。</li>
<li>ready: 全部插件已经加载完成时，将通过 endpoints 在 8081 端口返回 HTTP 状态 200。</li>
<li>kubernetes：CoreDNS 将根据 Kubernetes 服务和 pod 的 IP 回复 DNS 查询。</li>
<li>prometheus：是否开启 CoreDNS Metrics 信息接口，如果配置则开启，接口地址为 <a href="http://localhost:9153/metrics" target="_blank" rel="external">http://localhost:9153/metrics</a></li>
<li>forward：任何不在Kubernetes 集群内的域名查询将被转发到预定义的解析器 (/etc/resolv.conf)。</li>
<li>cache：启用缓存，30 秒 TTL。</li>
<li>loop：检测简单的转发循环，如果找到循环则停止 CoreDNS 进程。</li>
<li>reload：监听 CoreDNS 配置，如果配置发生变化则重新加载配置。</li>
<li>loadbalance：DNS 负载均衡器，默认 round_robin。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑 coredns 配置</span></span><br><span class="line">$ kubectl edit configmap coredns -n kube-system</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        <span class="built_in">log</span>            <span class="comment">#添加log</span></span><br><span class="line">        errors</span><br><span class="line">        health &#123;</span><br><span class="line">           lameduck 5s</span><br><span class="line">        &#125;</span><br><span class="line">        ready</span><br><span class="line">        kubernetes cluster.local <span class="keyword">in</span>-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           fallthrough <span class="keyword">in</span>-addr.arpa ip6.arpa</span><br><span class="line">           ttl 30</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        forward . /etc/resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">        loop</span><br><span class="line">        reload</span><br><span class="line">        loadbalance</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>保存更改后 CoreDNS 会自动重新加载配置信息，不过可能需要等上一两分钟才能将这些更改传播到 CoreDNS Pod。等一段时间后，再次查看 CoreDNS 日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] 127.0.0.1:47278 - 55171 <span class="string">"HINFO IN 4940754309314083739.5160468069505858354. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.040844011s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 127.0.0.1:32896 - 49064 <span class="string">"HINFO IN 1027842207973621585.7098421666386159336. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.044098742s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br></pre></td></tr></table></figure></p>
<p>可以看到 CoreDNS 已经重新加载了配置，我们再次进入 dnsuitls Pod 中执行 ping 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 DNSutils Pod 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils sh </span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 ping 命令</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 dnsutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>然后，再次查看 CoreDNS 的日志信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] 127.0.0.1:47278 - 55171 <span class="string">"HINFO IN 4940754309314083739.5160468069505858354. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.040844011s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 127.0.0.1:32896 - 49064 <span class="string">"HINFO IN 1027842207973621585.7098421666386159336. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.044098742s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br></pre></td></tr></table></figure></p>
<p>发现和之前没有执行 ping 命令时候一样，没有 DNS 域名解析的日志信息，说明 Pod 执行域名解析时，请求并没有进入 CoreDNS 中。接下来在查看 Pod 中 DNS 配置信息，进而分析问题。</p>
<h4 id="5-查看-Pod-中的-DNS-配置信息"><a href="#5-查看-Pod-中的-DNS-配置信息" class="headerlink" title="5. 查看 Pod 中的 DNS 配置信息"></a>5. 查看 Pod 中的 DNS 配置信息</h4><p>一般 Pod 中的 <code>DNS</code> 策略默认为 <code>ClusterFirst</code>，该参数起到的作用是，优先从 <code>Kubernetes DNS</code> 插件地址读取 <code>DNS</code> 配置。所以，我们正常创建的 Pod 中，DNS 配置 DNS 服务器地址应该指定为 Kubernetes 集群的 DNS 插件 Service 提供的虚拟 IP 地址。</p>
<blockquote>
<p>注：其中 DNS 策略（dnsPolicy）支持四种类型：</p>
<ul>
<li>Default： 从 DNS 所在节点继承 DNS 配置，即该 Pod 的 DNS 配置与宿主机完全一致。</li>
<li>ClusterFirst：预先从 Kubenetes 的 DNS 插件中进行 DNS 解析，如果解析不成功，才会使用宿主机的 DNS 进行解析。</li>
<li>ClusterFirstWithHostNet：Pod 是用 HOST 模式启动的（hostnetwork），用 HOST 模式表示 Pod 中的所有容器，都使用宿主机的 /etc/resolv.conf 配置进行 DNS 解析，但如果使用了 HOST 模式，还继续使用 Kubernetes 的 DNS 服务，那就将 dnsPolicy 设置为 ClusterFirstWithHostNet。</li>
<li>None：完全忽略 kubernetes 系统提供的 DNS，以 Pod Spec 中 dnsConfig 配置为主。</li>
</ul>
</blockquote>
<p>为了再分析原因，我们接着进入 dnsutils Pod 中，查看 Pod 中 DNS 配置文件 <code>/etc/resolv.conf</code> 配置参数是否正确：</p>
<blockquote>
<p>resolv.conf 配置参数说明：</p>
<ul>
<li>search： 指明域名查询顺序。</li>
<li>nameserver： 指定 DNS 服务器的 IP 地址，可以配置多个 nameserver。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils sh </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 resolv.conf 配置文件</span></span><br><span class="line">$ cat /etc/resolv.conf</span><br><span class="line"></span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search kube-system.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 DNSutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>可以看到 Pod 内部的 resolv.conf 内容，其中 nameserver 指定 DNS 解析服务器 IP 为 “10.96.0.10” ，这个 IP 地址正是本人 Kubernetes 集群 CoreDNS 的 Service “kube-dns” 的 cluterIP，说明当 Pod 内部进行域名解析时，确实是将查询请求发送到 Service “kube-dns” 提供的虚拟 IP 进行域名解析。</p>
<p>那么，既然 Pod 中 DNS 配置文件没问题，且 CoreDNS 也没问题，会不会是 Pod 本身域名解析不正常呢？或者 Service “kube-dns” 是否能够正常转发域名解析请求到 CoreDNS Pod 中？</p>
<p>当然，猜想是没有用的，进行一下测试来观察问题到底出在哪里。</p>
<h4 id="6-进行观察来定位问题所在"><a href="#6-进行观察来定位问题所在" class="headerlink" title="6. 进行观察来定位问题所在"></a>6. 进行观察来定位问题所在</h4><p>上面怀疑是 Pod 本身解析域名有问题，不能正常解析域名。或者 Pod 没问题，但是请求域名解析时将请求发送到 Service “kube-dns” 后不能正常转发请求到 CoreDNS Pod。 为了验证这两点，我们可以修改 Pod 中的 /etc/resolv.conf 配置来进行测试验证。</p>
<p>修改 <code>resolv.conf</code> 中 <code>DNS</code> 解析请求地址为 <code>阿里云 DNS</code> 服务器地址，然后执行 <code>ping</code> 命令验证是否为 <code>Pod</code> 解析域名是否有问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编辑 /etc/resolv.conf 文件，修改 nameserver 参数为阿里云提供的 DNS 服务器地址</span></span><br><span class="line">$ vi /etc/resolv.conf</span><br><span class="line"></span><br><span class="line">nameserver 223.5.5.5</span><br><span class="line"><span class="comment">#nameserver 10.96.0.10</span></span><br><span class="line">search kube-system.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改完后再进行 ping 命令测试，看看是否能够解析 www.baidu.com 网址</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line"></span><br><span class="line">PING www.a.shifen.com (180.101.49.11) 56(84) bytes of data.</span><br><span class="line">64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=1 ttl=48 time=14.0 ms</span><br><span class="line">64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=2 ttl=48 time=14.0 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 DNSutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>上面可也观察到 Pod 中更换 DNS 服务器地址后，域名解析正常，说明 Pod 本身域名解析是没有问题的。</p>
<p>接下来再修改 <code>resolv.conf</code> 中 <code>DNS</code> 解析请求地址为 <code>CoreDNS Pod</code> 的 <code>IP</code> 地址，这样让 <code>Pod</code> 直接连接 <code>CoreDNS Pod</code> 的 <code>IP</code>，而不通过 <code>Service</code> 进行转发，再进行 <code>ping</code> 命令测试，进而判断 <code>Service kube-dns</code> 是否能够正常转发请求到 <code>CoreDNS Pod</code> 的问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 CoreDNS Pod 的 IP 地址</span></span><br><span class="line">$ kubectl get pods -n kube-system -o wide | grep coredns</span><br><span class="line"></span><br><span class="line">coredns-669f77d7cc-rss5f     1/1     Running   0     10.244.2.155   k8s-node-2-13</span><br><span class="line">coredns-669f77d7cc-rt8l6     1/1     Running   0     10.244.1.163   k8s-node-2-12</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编辑 /etc/resolv.conf 文件，修改 nameserver 参数为阿里云提供的 DNS 服务器地址</span></span><br><span class="line">$ vi /etc/resolv.conf</span><br><span class="line"></span><br><span class="line">nameserver 10.244.2.155</span><br><span class="line">nameserver 10.244.1.163</span><br><span class="line"><span class="comment">#nameserver 10.96.0.10</span></span><br><span class="line">search kube-system.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改完后再进行 ping 命令测试，看看是否能够解析 www.baidu.com 网址</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line"></span><br><span class="line">PING www.baidu.com (39.156.66.18): 56 data bytes</span><br><span class="line">64 bytes from 39.156.66.18: seq=0 ttl=127 time=6.054 ms</span><br><span class="line">64 bytes from 39.156.66.18: seq=1 ttl=127 time=4.678 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 DNSutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察 CoreDNS 日志信息，查看有无域名解析相关日志</span></span><br><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] 127.0.0.1:47278 - 55171 <span class="string">"HINFO IN 4940754309314083739.5160468069505858354. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.040844011s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 10.244.1.162:40261 - 21083 <span class="string">"AAAA IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000398875s</span><br><span class="line">[INFO] 10.244.1.162:40261 - 20812 <span class="string">"A IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000505793s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53460 <span class="string">"AAAA IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000215384s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53239 <span class="string">"A IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000267642s</span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 127.0.0.1:32896 - 49064 <span class="string">"HINFO IN 1027842207973621585.7098421666386159336. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.044098742s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 10.244.1.162:40261 - 21083 <span class="string">"AAAA IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000217299s</span><br><span class="line">[INFO] 10.244.1.162:40261 - 20812 <span class="string">"A IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000264552s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53460 <span class="string">"AAAA IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000144795s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53239 <span class="string">"A IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000221163s</span><br></pre></td></tr></table></figure></p>
<p>经过上面两个测试，已经可以得知，如果 <code>Pod DNS</code> 配置中直接修改 <code>DNS</code> 服务器地址为 <code>CoreDNS Pod</code> 的 <code>IP</code> 地址，<code>DNS</code> 解析确实没有问题，能够正常解析。不过，正常的情况下 Pod 中 DNS 配置的服务器地址一般是 CoreDNS 的 Service 地址，不直接绑定 Pod IP（因为 Pod 每次重启 IP 都会发生变化）。 所以问题找到了，正是在 Pod 向 CoreDNS 的 Service “kube-dns” 进行域名解析请求转发时，出现了问题，一般 <code>Service</code> 的问题都跟 <code>Kube-proxy</code> 组件有关，接下来观察该组件是否存在问题。</p>
<h4 id="7-分析-Kube-Proxy-是否存在问题"><a href="#7-分析-Kube-Proxy-是否存在问题" class="headerlink" title="7. 分析 Kube-Proxy 是否存在问题"></a>7. 分析 Kube-Proxy 是否存在问题</h4><p>观察 Kube-proxy 的日志，查看是否存在问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 kube-proxy Pod 列表</span></span><br><span class="line">$ kubectl get pods -n kube-system | grep kube-proxy</span><br><span class="line"></span><br><span class="line">kube-proxy-6kdj2          1/1     Running   3          9h</span><br><span class="line">kube-proxy-lw2q6          1/1     Running   3          9h</span><br><span class="line">kube-proxy-mftlt          1/1     Running   3          9h</span><br><span class="line"> 选择一个 kube-proxy Pod，查看最后 5 条日志内容</span><br><span class="line">$ kubectl logs kube-proxy-6kdj2 --tail=5  -n kube-system</span><br><span class="line"></span><br><span class="line">E0326 15:20:23.159364  1 proxier.go:1950] Failed to list IPVS destinations, error: parseIP Error ip=[10 96 0 10 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159388  1 proxier.go:1192] Failed to sync endpoint <span class="keyword">for</span> service: 10.8.0.10:53/UPD, err: parseIP Error ip=[10 96 0 16 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159479  1 proxier.go:1950] Failed to list IPVS destinations, error: parseIP Error ip=[10 96 0 10 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159501  1 proxier.go:1192] Failed to sync endpoint <span class="keyword">for</span> service: 10.8.0.10:53/TCP, err: parseIP Error ip=[10 96 0 16 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159595  1 proxier.go:1950] Failed to list IPVS destinations, error: parseIP Error ip=[10 96 0 10 0 0 0 0 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure></p>
<p>通过 kube-proxy Pod 的日志可以看到，里面有很多 Error 级别的日志信息，根据关键字 <code>IPVS</code>、<code>parseIP Error</code> 可知，可能是由于 IPVS 模块对 IP 进行格式化导致出现问题。</p>
<p>因为这个问题是升级到 kubernetes 1.18 版本才出现的，所以去 Kubernetes Github 查看相关 issues，发现有人在升级 Kubernetes 版本到 1.18 后，也遇见了相同的问题，经过 issue 中 Kubernetes 维护人员讨论，分析出原因可能为新版 Kubernetes 使用的 IPVS 模块是比较新的，需要系统内核版本支持，本人使用的是 CentOS 系统，内核版本为 3.10，里面的 IPVS 模块比较老旧，缺少新版 Kubernetes IPVS 所需的依赖。</p>
<p>根据该 issue 讨论结果，解决该问题的办法是，更新内核为新的版本。</p>
<blockquote>
<p>注：该 issues 地址为 <a href="https://github.com/kubernetes/kubernetes/issues/89520" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/issues/89520</a></p>
</blockquote>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><h3 id="升级系统内核版本"><a href="#升级系统内核版本" class="headerlink" title="升级系统内核版本"></a>升级系统内核版本</h3><p>升级 Kubernetes 集群各个节点的 CentOS 系统内核版本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入公钥</span></span><br><span class="line">$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 ELRepo 最新版本</span></span><br><span class="line">$ yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出可以使用的 kernel 包版本</span></span><br><span class="line">$ yum list available --disablerepo=* --enablerepo=elrepo-kernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装指定的 kernel 版本：</span></span><br><span class="line">$ yum install -y kernel<span class="_">-lt</span>-4.4.222-1.el7.elrepo --enablerepo=elrepo-kernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统可用内核</span></span><br><span class="line">$ cat /boot/grub2/grub.cfg | grep menuentry</span><br><span class="line"></span><br><span class="line">menuentry <span class="string">'CentOS Linux (3.10.0-1062.el7.x86_64) 7 (Core)'</span> --class centos （略）</span><br><span class="line">menuentry <span class="string">'CentOS Linux (4.4.222-1.el7.elrepo.x86_64) 7 (Core)'</span> --class centos ...（略）</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置开机从新内核启动</span></span><br><span class="line">$ grub2-set-default <span class="string">"CentOS Linux (4.4.222-1.el7.elrepo.x86_64) 7 (Core)"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看内核启动项</span></span><br><span class="line">$ grub2-editenv list</span><br><span class="line">saved_entry=CentOS Linux (4.4.222-1.el7.elrepo.x86_64) 7 (Core)</span><br></pre></td></tr></table></figure></p>
<p>重启系统使内核生效，启动完成查看内核版本是否更新：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uname -r</span><br><span class="line">4.4.222-1.el7.elrepo.x86_64</span><br></pre></td></tr></table></figure></p>
<h3 id="测试-Pod-中-DNS-是否能够正常解析"><a href="#测试-Pod-中-DNS-是否能够正常解析" class="headerlink" title="测试 Pod 中 DNS 是否能够正常解析"></a>测试 Pod 中 DNS 是否能够正常解析</h3><p>进入 Pod 内部使用 ping 命令测试 DNS 是否能正常解析：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 集群外部，例如这里 ping 一下百度</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line">64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=1 ttl=127 time=7.20 ms</span><br><span class="line">64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=2 ttl=127 time=6.60 ms</span><br><span class="line">64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=3 ttl=127 time=6.38 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 集群内部 kube-api 的 Service 地址</span></span><br><span class="line">$ ping kubernetes.default</span><br><span class="line">64 bytes from kubernetes.default.svc.cluster.local (10.96.0.1): icmp_seq=1 ttl=64 time=0.051 ms</span><br><span class="line">64 bytes from kubernetes.default.svc.cluster.local (10.96.0.1): icmp_seq=2 ttl=64 time=0.051 ms</span><br><span class="line">64 bytes from kubernetes.default.svc.cluster.local (10.96.0.1): icmp_seq=3 ttl=64 time=0.064 ms</span><br></pre></td></tr></table></figure></p>
<p>可以看到 Pod 中的域名解析已经恢复正常。</p>
<p>参考: mydlq.club</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes 在 aws 上的应用]]></title>
      <url>http://team.jiunile.com/blog/2020/01/k8s-k8s-in-aws-arch.html</url>
      <content type="html"><![CDATA[<h2 id="kubernetes-in-aws-Overview（EKS）"><a href="#kubernetes-in-aws-Overview（EKS）" class="headerlink" title="kubernetes in aws Overview（EKS）"></a>kubernetes in aws Overview（EKS）</h2><p><img src="/images/k8s/aws_eks_arch.png" alt="eks_in_aws"><br><a id="more"></a><br>上图是 kubernetes(EKS) 在 aws 上的整体情况，下面我会对每块进行展开说明</p>
<h2 id="分解说明"><a href="#分解说明" class="headerlink" title="分解说明"></a>分解说明</h2><h3 id="kubernetes-or-eks"><a href="#kubernetes-or-eks" class="headerlink" title="kubernetes or eks"></a>kubernetes or eks</h3><p>kubernetes 原生支持在 aws 上使用，具体如何启用可参考<a href="http://team.jiunile.com/blog/2019/03/k8s-cloud.html">kuberenetes 云应用实践</a>，这里不在累赘说明。既然选择在云上使用 k8s ，那就直接使用云本身提供的能力（<a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a>），就没必要自己再去适配一遍，毕竟云厂商已经提供了这样的服务，为什么不去使用呢？不过自己也可以去了解EKS为我们做了什么？怎么去管理它？</p>
<h4 id="EKS"><a href="#EKS" class="headerlink" title="EKS"></a>EKS</h4><p><a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 其实就是帮我们托管了master节点（3 master集群），其次更好的与aws资源进行了集成，帮助我们在上层更轻松的使用，<a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 默认会集成以下组件:  </p>
<ul>
<li>kube-proxy</li>
<li>coredns</li>
<li><a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a> (网络插件-替代calico，下面会介绍为何使用它)</li>
</ul>
<hr>
<p>使用 <a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 后，我们就不需要在关注 k8s master 的安装以及高可用，以及怎么在云上（aws）的集成，同时在开源的 kubernetes 上做了优化。当然，你可以可以选择自建，毕竟这些你都可以搭建出来，我们在 aws 中国就是自建的，毕竟 <a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 在国内还未上线，但如果是在国外或者国内上线的前提下需衡量付出的时间成本是否合算？</p>
<h4 id="如何管理"><a href="#如何管理" class="headerlink" title="如何管理"></a>如何管理</h4><p><a href="https://www.terraform.io" target="_blank" rel="external">terrafrom</a> or <a href="https://eksctl.io" target="_blank" rel="external">eksctl</a>，最终我选择了后者，理由也很简单，在我看来一个就好比是”学生”，另一个则是”亲儿子”，<a href="https://eksctl.io" target="_blank" rel="external">eksctl</a> 由 aws 官方提供专门为 <a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 提供的命令行工具，它提供了更多的参数同时管理起来也非常的方便，如果你是深度管理，则使用它，<a href="https://www.terraform.io" target="_blank" rel="external">terrafrom</a> 则是大众普照中的一个，提供的参数也是有限，如果简单管理可以使用它。</p>
<h3 id="负载均衡器ELB"><a href="#负载均衡器ELB" class="headerlink" title="负载均衡器ELB"></a>负载均衡器ELB</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>aws 官方提供了三种负载均衡器，我简单成为 <code>clb</code>、<code>alb</code>、<code>nlb</code>，<code>clb</code> 即将淘汰也是最早的一代，这里主要阐述 <code>alb</code> 7层负载  与 <code>nlb</code> 4层负载 ，该如何选择，则取决于你的业务，他们之间的特征也很明显，如果你是需要关注 <code>header</code> 头信息的，如 <code>x-forward-for</code> 或 <code>x-forward-proto</code> 等信息的则使用 <code>alb</code>，如果不需要则使用 <code>nlb</code> 性能高。具体想了解，可参考aws官方文档：<a href="https://aws.amazon.com/cn/elasticloadbalancing/" target="_blank" rel="external">https://aws.amazon.com/cn/elasticloadbalancing/</a></p>
<h4 id="管理"><a href="#管理" class="headerlink" title="管理"></a>管理</h4><p>同样我们也面临着管理的问题，是使用 <a href="https://www.terraform.io" target="_blank" rel="external">terrafrom</a> 还是别的呢？毕竟我们使用了 k8s ，它有很好的 <code>ingress</code> 帮我们管理着对外的访问不是么？最终，我选择了 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">alb-ingress-controller</a>，同样也是 aws 出品，它很好的与 ELB 进行了集成，我们只需要编写 <code>ingress</code> 文件，加上对应的注解，就可以很方便的使用 ELB，目前支持 <code>alb</code> 与 <code>nlb</code> 这两个负载均衡器的使用。</p>
<p>对 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">alb-ingress-controller</a> 的使用也有一定的注意，推荐是一个对外暴露服务一个 <code>ingress</code>, <strong>同时注意不要人为去 aws consol 界面去修改已有通过 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">alb-ingress-controller</a>  创建出来的负载均衡器，如果你修改了，你会发现过一会就会还原了</strong>，因为在 k8s 中的 alb 控制器会帮你复原，所以一切的变更请使用 <code>ingress</code>, 同时还有一个不好的弊端就是在 alb/nlb 目标群组中，后端协议只能二选一，无法 http/https 同时存在，唯一的方法是使用两个 <code>ingress</code> ，不过我们大多数后端访问都是 http 协议。</p>
<p>详细了解可参考：</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/" target="_blank" rel="external">https://github.com/kubernetes-sigs/aws-alb-ingress-controller/</a></li>
<li><a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/</a></li>
<li><a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/alb-ingress.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/alb-ingress.html</a></li>
<li><a href="https://aws.amazon.com/cn/blogs/china/kubernetes-ingress-aws-alb-ingress-controller/" target="_blank" rel="external">https://aws.amazon.com/cn/blogs/china/kubernetes-ingress-aws-alb-ingress-controller/</a></li>
</ul>
<h3 id="网络插件"><a href="#网络插件" class="headerlink" title="网络插件"></a>网络插件</h3><p>同样，在网络插件上，我也面临着选择的问题，不过现在回想起来，居然选择了云，那就基本大概率上去拥抱云原生提供的插件，不是么，毕竟它和云结合的很好，这里我介绍下 <a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a>。</p>
<p><a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a> 我就说一件事，<strong>pod 与你现有的 vpc 打通</strong>，但劣势也很明显，<code>就是 pod 的 ip 会占有你划分的 vpc ip池</code>，如果你的 <code>subnet</code> 网段规划的不好的话，你的 ip 将会很快用光，所有我建议你的 <code>vpc</code> 网段是 <code>16</code> 位，<code>subnet</code> 网段是 <code>20</code> 位的，具体不同的网段可以放多少个 pod 可以参考这边文章了解下：<a href="https://www.yiibai.com/ipv4/ipv4_subnetting.html，" target="_blank" rel="external">https://www.yiibai.com/ipv4/ipv4_subnetting.html，</a> 这里也不在累赘。 </p>
<p>同时 <a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a> 对主机类型上能起多少 pod 也是有限制的，不同的机型上启动的 pod 数量是不一样的，这里我推荐尽可能的使用中等规模的机型来取缔多个小机型。具体可参考：<a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/f7a7d53baf769846c20467cfa27f0172eca570c1/pkg/awsutils/vpc_ip_resource_limit.go" target="_blank" rel="external">vpc_ip_resource_limit</a> </p>
<p>当然如果你是本地机房或者云未给提供网络插件的话，那我还是推荐你使用 <a href="https://www.projectcalico.org/" target="_blank" rel="external">calico</a>，毕竟它还是非常的强大！有兴趣的伙伴还是可以好好了解下。</p>
<h3 id="访问安全"><a href="#访问安全" class="headerlink" title="访问安全"></a>访问安全</h3><p>容器访问安全也是令我非常困扰的一个问题，aws 在资源访问方面有着非常好的权限控制 <code>iam role</code> ，但放到 k8s 容器中，一切都变的非常不美好了， 直到最近 aws eks 推出了 <a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">service account for iam role</a>， 在 eks 没正式推出 <a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">service account for iam role</a> 之前，有两种方式可以进行访问控制：</p>
<ul>
<li>access key/secret key</li>
<li><a href="https://github.com/jtblin/kube2iam" target="_blank" rel="external">kube2iam</a></li>
</ul>
<hr>
<p>针对以上两个访问，都有一定的缺陷，用key来控制的问题在于本身管理key就是一件头痛的事情，key的泄漏和定期更换非常痛苦，使用 <a href="https://github.com/jtblin/kube2iam" target="_blank" rel="external">kube2iam</a> 问题在于你需要保证这个插件的稳定性和性能，一旦出现问题，你会发现那就是灾难性的，而且也不易排查，毕竟多层组件就多提升了出问题的概率，毕竟还是如此重要的组件。</p>
<p>最后，我隆重推荐大家使用 <a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">service account for iam role</a> ，具体教程可点击链接查看。 但对于自建的小伙们只能退而求其次 使用 key 或者 <a href="https://github.com/jtblin/kube2iam" target="_blank" rel="external">kube2iam</a> 来进行管理控制了</p>
<h3 id="Why-kong"><a href="#Why-kong" class="headerlink" title="Why kong?"></a>Why kong?</h3><p>为什么选型 <a href="http://getkong.org" target="_blank" rel="external">kong</a> ，我想它的官方文档能更好的为你阐述，我从0.9开始就使用它了，直到现在维持在0.14.1，往后引入了 <code>service mesh</code> 的概念，选型 <a href="http://getkong.org" target="_blank" rel="external">kong</a> 的理由也很简单，开源、插件丰富、易于二次开发。后面可以专门分享一下我们是如何使用  <a href="http://getkong.org" target="_blank" rel="external">kong</a> 作为我们的业务网关，同时我们是如何改造 <code>JWT</code> 插件来实现鉴授权，以及通过网关审计日志定位问题，对外核心服务进行了限流等控制。如果你的需求也是如此，让业务保持足够简单专注于业务的实现，跨语言，微服务化，但同时需要一些服务治理相关的需求，还没有上 k8s 或者还没到 <a href="https://istio.io" target="_blank" rel="external">isito</a> 这种重 <code>service mesh</code> 的地步，那  <a href="http://getkong.org" target="_blank" rel="external">kong</a> 就是你最好的选择。</p>
<h3 id="资源池划分"><a href="#资源池划分" class="headerlink" title="资源池划分"></a>资源池划分</h3><p>在资源池划分上，我将网关和业务的池子单独分开，同时开辟了一块竞价池，原因如下：</p>
<ol>
<li>网关池划分： 网关作为业务对外流量的统一入口，提现出非常重要的地步，故而单独分配了一块资源池，同时需要与负载均衡器的配置，对机器权限的控制力度也是和普通业务池是不同的。</li>
<li>业务池划分：这个就什么好说的，用于承载你的业务，跑长期运行的服务。</li>
<li>竞价池划分： 这里需要配合 <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank" rel="external">cluster autoscaler</a> 一起使用更佳，用于跑一些离线任务，随用随起，用完即毁，方便至极。当然如何你使用的是 EKS 直接可以配合 <a href="https://aws.amazon.com/cn/fargate/" target="_blank" rel="external">fargate</a> 来直接使用。</li>
</ol>
<hr>
<p>Cluster Autoscaler：</p>
<ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/cluster-autoscaler.html#ca-ng-considerations" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/cluster-autoscaler.html#ca-ng-considerations</a></li>
<li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank" rel="external">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</a></li>
</ul>
<h3 id="未来展望"><a href="#未来展望" class="headerlink" title="未来展望"></a>未来展望</h3><p>目前，我们通过 <a href="http://getkong.org" target="_blank" rel="external">kong</a> 来实现了对外流量的管理与控制，但对内部服务之间的访问控制还未开始，也缺乏一些服务治理的手段，当然，使用 <a href="http://getkong.org" target="_blank" rel="external">kong</a> 也可以达到，但这样我们就会过渡依赖这个集中网关了，一旦故障，那是灾难性的，<code>service mesh</code> 的到来正式解决此类问题，让服务治理变的更游刃有余，后面我们会上 <a href="https://istio.io" target="_blank" rel="external">isito</a> 来实现诸如灰度、蓝绿发布以及对内部服务调用限流、监控等控制能力的加强。</p>
<h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><ul>
<li><a href="https://docs.aws.amazon.com/" target="_blank" rel="external">https://docs.aws.amazon.com/</a></li>
<li><a href="http://getkong.org" target="_blank" rel="external">http://getkong.org</a></li>
<li><a href="https://eksctl.io/" target="_blank" rel="external">https://eksctl.io/</a></li>
<li><a href="https://eksworkshop.com" target="_blank" rel="external">https://eksworkshop.com</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[分析并解决AWS服务从ec2迁移至Kubernetes后延迟增加的问题]]></title>
      <url>http://team.jiunile.com/blog/2020/01/k8s-k8s-in-aws-latency.html</url>
      <content type="html"><![CDATA[<h2 id="问题概要"><a href="#问题概要" class="headerlink" title="问题概要"></a>问题概要</h2><p>上周我们将一个微服务迁移到中央平台上，包括CI/CD，Kubernetes运行时，metric和其他一些程序。这次实验是为了之后一个月里大概150个微服务的迁移作准备，所有这些服务支撑着西班牙在线市场的运营。</p>
<p>当我们将应用程序部署到Kubernetes上，并且将一些生产流量导入其中之后，事情开始有些不妙了。Kubernetes上的请求延迟比EC2上的高10倍左右。除非我们能找到解决方案，不然这会是微服务迁移的最大障碍，甚至可能彻底摧毁整个项目。<br><a id="more"></a></p>
<h2 id="为什么Kubernetes上的延时比EC2高那么多？"><a href="#为什么Kubernetes上的延时比EC2高那么多？" class="headerlink" title="为什么Kubernetes上的延时比EC2高那么多？"></a>为什么Kubernetes上的延时比EC2高那么多？</h2><p>为了找到系统瓶颈，我们收集了整个请求路径的metric。架构很简单，一个API网关（Zuul）将请求路由到EC2或者Kubernetes的微服务里。在Kubernetes上，我们使用NGINX Ingress控制器，后台是常规的Deployment运行一个基于Spring的JVM应用程序。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">                                  EC2</span><br><span class="line">                            +---------------+</span><br><span class="line">                            |  +---------+  |</span><br><span class="line">                            |  |         |  |</span><br><span class="line">                       +-------&gt; BACKEND |  |</span><br><span class="line">                       |    |  |         |  |</span><br><span class="line">                       |    |  +---------+  |                   </span><br><span class="line">                       |    +---------------+</span><br><span class="line">             +------+  |</span><br><span class="line">Public       |      |  |</span><br><span class="line">      -------&gt; ZUUL +--+</span><br><span class="line">traffic      |      |  |              Kubernetes</span><br><span class="line">             +------+  |    +-----------------------------+</span><br><span class="line">                       |    |  +-------+      +---------+ |</span><br><span class="line">                       |    |  |       |  xx  |         | |</span><br><span class="line">                       +-------&gt; NGINX +------&gt; BACKEND | |</span><br><span class="line">                            |  |       |  xx  |         | |</span><br><span class="line">                            |  +-------+      +---------+ |</span><br><span class="line">                            +-----------------------------+</span><br></pre></td></tr></table></figure></p>
<p>问题看上去是后台的上游延迟（上图用xx表示）。当应用程序部署在EC2上时，响应时间大概20ms。在Kubernetes上则需要100～200ms。</p>
<p>我们很快排除了运行时变更的影响。JVM版本是一致的。容器化的影响也被排除了，因为EC2上也是运行在容器里。也和压力无关，因为即使每秒只有1个请求仍然能看到很高的延时。也不是GC的影响。</p>
<p>一个Kubernetes管理员问应用程序是否有外部的依赖，比如以前DNS解析曾经导致过类似的问题，这是目前为止最可能的猜想。</p>
<h2 id="猜想1：DNS解析"><a href="#猜想1：DNS解析" class="headerlink" title="猜想1：DNS解析"></a>猜想1：DNS解析</h2><p>每次请求里，我们的应用程序会向AWS ElasticSearch实例（域名类似 elastic.spain.adevinta.com）发送1～3次请求。我们在容器内放置了一个shell脚本可以验证这个域名从DNS解析需要多长时间。</p>
<p>容器内的DNS查询：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@be-851c76f696-alf8z /]<span class="comment"># while true; do dig "elastic.spain.adevinta.com" | grep time; sleep 2; done</span></span><br><span class="line">;; Query time: 22 msec</span><br><span class="line">;; Query time: 22 msec</span><br><span class="line">;; Query time: 29 msec</span><br><span class="line">;; Query time: 21 msec</span><br><span class="line">;; Query time: 28 msec</span><br><span class="line">;; Query time: 43 msec</span><br><span class="line">;; Query time: 43 msec</span><br></pre></td></tr></table></figure></p>
<p>运行着对应相同用程序的EC2实例里的同样的查询：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bash-4.4<span class="comment"># while true; do dig "elastic.spain.adevinta.com" | grep time; sleep 2; done</span></span><br><span class="line">;; Query time: 77 msec</span><br><span class="line">;; Query time: 0 msec</span><br><span class="line">;; Query time: 0 msec</span><br><span class="line">;; Query time: 0 msec</span><br><span class="line">;; Query time: 0 msec</span><br></pre></td></tr></table></figure></p>
<p>大概30ms的解析时间，似乎我们的应用程序在和ElasticSearch通信时增加了DNS解析的额外消耗。</p>
<p>但是这里有两点很奇怪：</p>
<ul>
<li>在Kubernetes里已经有很多应用程序和AWS资源通信，但是并没有这个问题。</li>
<li>我们知道JVM实现了内存内的DNS缓存。查看这些镜像的配置，在 $JAVA_HOME/jre/lib/security/java.security里配置了TTL为 networkaddress.cache.ttl=10。JVM应该能够缓存10秒内的所有DNS查询。</li>
</ul>
<p>为了确认是DNS的影响，我们决定避免DNS解析并且查看问题是否会消失。首先尝试让应用程序直接和Elasticsearch的IP通信，而不是域名。这要求代码变更并且重新部署，因此我们只是简单地在 /etc/hosts文件里添加了域名和IP的映射：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">34.55.5.111 elastic.spain.adevinta.com</span><br></pre></td></tr></table></figure></p>
<p>这样容器可以立刻解析IP。我们确实观察到了延时的改善，但是离我们的最终目标还是很远。即使DNS解析足够快，但是真实原因还是没有找到。</p>
<h2 id="网络plumbing"><a href="#网络plumbing" class="headerlink" title="网络plumbing"></a>网络plumbing</h2><p>我们决定在容器里执行 tcpdump，这样可以看到网络到底干了些什么。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@be-851c76f696-alf8z /]<span class="comment"># tcpdump -leni any -w capture.pcap</span></span><br></pre></td></tr></table></figure></p>
<p>然后发送了一些请求并且下载了capture文件（ <code>kubectl cpmy-service:/capture.pcap capture.pcap</code>）在<a href="https://wiki.wireshark.org/FrontPage" target="_blank" rel="external">Wireshark</a>里查看。</p>
<p>DNS查询看上去很正常（除了一些细节，之后会提到）但是我们的服务处理请求的时候很奇怪。下图是capture的截图，显示一个请求从开始到响应的全过程。<br><img src="/images/wireshark.jpg" alt="Wireshark"></p>
<p>第一列是packet序号。我用不同的颜色标示不同的TCP流。</p>
<p>绿色的流从<code>packet 328</code>开始，显示客户端（172.17.22.150）开启了容器（172.17.36.147）的TCP连接。最初的握手（328-330）之后，<code>packet 331</code>开始 <code>HTTP GET/v1/..</code>，这是对我们自己服务的入站请求。整个流程花了<strong>1ms</strong>。</p>
<p>灰色流从<code>packet 339</code>开始，展示了我们的服务发送一个HTTP请求给Elasticsearch实例（这里看不到TCP握手过程因为它使用了一个已有的TCP连接）。这里花了<strong>18ms</strong>。</p>
<p>至此都没有什么问题，所花的时间和预计差不多（<strong>～20-30ms</strong>）。</p>
<p>但是在这两次交互之间，紫色部分花了<strong>86ms</strong>。这里发生了什么？在<code>packet 333</code>， 我们的服务发送了HTTP GET到 <code>/latest/meta-data/iam/security-credentials</code>，之后，在同一个TCP连接里，另一个GET发送到 <code>/latest/meta-data/iam/security-credentials/arn:..</code>。</p>
<p>我们发现每次请求里都会这样做。DNS解析在容器里确实有一点慢（解释很有意思，我会在另一篇文章里介绍）。但是高延迟的实际原因是每次请求里对<code>AWS Instance Metadata</code>的查询。</p>
<h2 id="猜想2：AWS调用"><a href="#猜想2：AWS调用" class="headerlink" title="猜想2：AWS调用"></a>猜想2：AWS调用</h2><p>这两个endpoint都是<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials" target="_blank" rel="external"><code>AWS Instance Metadata API</code></a>的一部分。我们的微服务从Elasticsearch里读取时会用到这个服务。这两个调用都是基础的授权工作流。</p>
<p>第一个请求里查询的endpoint得到和该实例相关联的IAM角色。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># curl http://169.254.169.254/latest/meta-data/iam/security-credentials/</span></span><br><span class="line">arn:aws:iam::&lt;account_id&gt;:role/some_role</span><br></pre></td></tr></table></figure></p>
<p>第二个请求查询第二个endpoint得到该实例的临时credential。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># curl http://169.254.169.254/latest/meta-data/iam/security-credentials/arn:aws:iam::&lt;account_id&gt;:role/some_role</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="string">"Code"</span>: <span class="string">"Success"</span>,</span><br><span class="line">	<span class="string">"LastUpdated"</span>: <span class="string">"2012-04-26T16:39:16Z"</span>,</span><br><span class="line">	<span class="string">"Type"</span>: <span class="string">"AWS-HMAC"</span>,</span><br><span class="line">    <span class="string">"AccessKeyId"</span>: <span class="string">"ASIAIOSFODNN7EXAMPLE"</span>,</span><br><span class="line">    <span class="string">"SecretAccessKey"</span>: <span class="string">"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"</span>,</span><br><span class="line">    <span class="string">"Token"</span>: <span class="string">"token"</span>,</span><br><span class="line">    <span class="string">"Expiration"</span>: <span class="string">"2017-05-17T15:09:54Z"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>客户端可以在短时间内使用它们，并且需要周期性地（在 Expiration之前）去获取新的credencial。模型很简单：AWS为了安全考虑经常轮询临时密钥，但是客户端可以将密钥缓存几分钟来弥补获得新credencial所带来的性能影响。</p>
<p>AWS Java SDK应该处理这些，但是，因为某种原因，它没有这么做。</p>
<p>在GitHub issue里搜索后找到了<a href="https://github.com/aws/aws-sdk-java/issues/1921" target="_blank" rel="external">#1921</a>，里面有我们需要的线索。</p>
<p>AWS SDK在下面两种情况的某一种满足时就会刷新credential：</p>
<ul>
<li>Expiration在 EXPIRATION_THRESHOLD内，硬编码为15分钟。</li>
<li>前一次刷新credential的尝试所花时间大于 REFRESH_THRESHOLD，硬编码为60分钟。</li>
</ul>
<p>我们需要查看得到的证书里的实际过期时间，因此运行了两个 cURL命令调用AWS API，一次从容器里，一次从EC2实例里。从容器里获得的证书过期时间短得多，是15分钟。</p>
<p>问题变得清晰了：我们的服务在第一个请求里会获取临时credential。因为它有15分钟的过期时间，在下一次请求里，AWS SDK会重新刷新credential。每次请求都会这样。</p>
<h2 id="为什么credential过期时间变短了？"><a href="#为什么credential过期时间变短了？" class="headerlink" title="为什么credential过期时间变短了？"></a>为什么credential过期时间变短了？</h2><p>AWS Intance Metadata Service设计上是在EC2实例里使用，而不是Kubernetes上。我们希望应用程序保留相同的接口。因此使用了<a href="https://github.com/uswitch/kiam" target="_blank" rel="external">Kiam</a>，在每个Kubernetes节点上运行一个agent，允许用户（部署应用程序到集群里的工程师）将IAM角色关联到Pod容器上，就像它是个EC2实例一样。它会截获发送到AWS Instance Metadata服务的调用，并且使用agent提前从AWS获取并放在缓存里的内容响应。从应用程序的角度来看，和运行在EC2上没什么区别。</p>
<p>Kiam给Pod提供的正是短期的credencial，这有道理，因为它假定Pod的平均生命周期比EC2实例要短。默认值就是15分钟。</p>
<p>但是如果两处都使用默认值就有问题了。提供给应用程序的证书过期时间为15分钟。AWS Java SDK会强制刷新任何过期时间少于15分钟的证书。</p>
<p>结果就是每个请求都会强制刷新临时证书，这需要两次调用AWS API，给每次请求都带来了巨大的延迟。之后我们找到了<a href="https://github.com/aws/aws-sdk-java/issues/1893" target="_blank" rel="external">AWS Java SDK的一个功能请求</a>，里面提到了同样的问题。</p>
<p>解决办法很简单，我们重新配置了Kiam，请求更长过期时间的credencial。当这一变更生效后，请求就不用每次都调用AWS Metadata服务了，而且延迟比EC2还要小。</p>
<h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>从我们迁移的经验里，最经常遇到的问题不是Kubernetes的bug或者平台的问题。也不是微服务本身的问题。问题通常只是因为集成。我们将以前从来没有一起集成过的复杂系统混合在一起，并且期望它们组成单个的大系统。可移动组件越多，可能发生问题的地方就越多。</p>
<p>在这个问题里，高延迟并不是因为bug或者Kubernetes、Kiam、AWS Java SDK或我们自己微服务本身有什么问题。它是Kiam和AWS Java SDK里两个独立的默认值组合在一起导致的问题。独立来看，两个默认值都没什么问题：AWS Java SDK强制credential刷新策略和Kiam比较低的默认过期时间。但是组合起来就导致了问题。<strong>两个单独看都是正确的决定合在一起并不一定是正确的</strong>。</p>
<blockquote>
<p>如果使用的是aws的eks，现在可以不用使用kiam这个插件来实现role访问aws服务，aws eks 实现了service accounts 与 role 绑定，具体参考：<a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html</a></p>
</blockquote>
<p>来源：kubernetes-added-a-0-to-my-latency</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[LPR是什么，央行给有放贷的人带来什么新年礼物]]></title>
      <url>http://team.jiunile.com/blog/2020/01/lpr.html</url>
      <content type="html"><![CDATA[<h2 id="part-1"><a href="#part-1" class="headerlink" title="part 1"></a>part 1</h2><p><code>每一个背负了房贷的人，绝对想不到，进入2020年头的时候，央行会送来一份大礼。</code></p>
<p>2019年12月28日，中国人民银行（央行）发布年内第30号公告。公告中的文字非常专业、晦涩，不是金融专业出身的人，会觉得云里雾里，也体会不到这则公告的史诗级作用。</p>
<p>第一：<strong>从2020年1月1日开始，商业银行不得和买房人，签订参考贷款基准利率的浮动利率贷款合同。</strong></p>
<p>第二：<strong>从2020年3月1日开始，商业银行必须和存量房贷的借款人，废除原有房贷合同，让借款人重新二选一，要么：固定利率；要么：「LPR利率+基点加成」模式。</strong></p>
<a id="more"></a>
<p>也就是说，从今年的3月1日开始至8月份的5个月时间里，当初和买房人签订了房贷合同的银行，会联系买房人，协商废除原有的「基准利率+浮动比例」利率合同，再二选一。</p>
<p>一定要记住，无论银行怎么巧舌如簧，口吐莲花：</p>
<p><code>不要选择固定利率模式，选择「LPR利率+基点加成」模式！</code></p>
<p>央行公告中说得很清楚，这种变更，一辈子只有一次机会，选错了的话，是无法更改的。</p>
<p>不要小看两种利率模式的差别，我们可以大致估算下，在某些年份，利率差距可能达到1-5个百分点。如果房贷余额有300万元的话，每年还款差异可能达到3-15万元。</p>
<p>在过去几年，数百万的年轻人夫妻，掏空了父母辈的钱包，背负了房贷，不敢辞职和跳槽，过上了不敢消费的佛系生活。</p>
<p>翻阅了央行的定期报告，截止2019年8月份，个人贷款余额总量在29万亿元。你的房贷，就在其中。</p>
<p>现在，央行要给你松绑了，卸下重担了，就看你3个月之后，要不要接收这份大礼，改变一家子的命运了。</p>
<h2 id="part-2"><a href="#part-2" class="headerlink" title="part 2"></a>part 2</h2><p>央行在2019年8月份，进行房贷变革的细节。</p>
<p><strong>变革前，房贷利率=央行基准利率+浮动比例。</strong></p>
<p>基准利率由央行统一公布，商业银行无权控制，只能被动跟随。如果央行加息了，或者降息了，那么在来年的固定日期1月1日，购房者的基准利率就统一改变。当前的基准利率是4.90%。</p>
<p>商业银行能够改变的就是“浮动比例”。有些城市的银行货币宽松，鼓励首套房置业，地方政府财源依赖于土地出让金，那么可能在基准利率的基础上打折扣，但是大多数城市，是没有浮动比例，或者上浮的。</p>
<p>对购房者而言，和银行签订的“浮动比例”一旦确认，是无法变更的。</p>
<p><strong>变革后，房贷利率=贷款市场报价（LPR）利率+基点加成。</strong></p>
<p>LPR利率，专业说法是“全国银行间同业拆借中心”，统计18家银行报出的各自的1年期和5年期以上贷款利率，剔除最低价、最高价后，计算算术平均价。</p>
<p>说得更加直白一点</p>
<p><code>LPR利率更加市场化，在反映全社会无风险利率的程度上，更加逼真。</code></p>
<p>当经济过热时，通胀时，利率高；</p>
<p>当经济过冷时，通缩时，利率低。</p>
<p>这种市场化的利率，将贷款人和借款人之间的利益、风险，均衡了下。</p>
<p>央行高屋建瓴，将1年期LPR利率，用于实体经济；将5年期LPR利率，用于了房地产市场。<br><img src="/images/other/1.jpg" alt="1"></p>
<p>当需要给实体经济降息时，就调低1年期LPR利率，不让漫灌的大水进入房地产行业。</p>
<p>当需要单独对房地产调控时，只需要提高5年期LPR的利率，就不会对实体经济产生成本压力。</p>
<p>但是从长期来看，<strong>5年期LPR利率，一定会向1年期LPR利率进行回归和靠拢的。</strong></p>
<p>2019年11月20日，央行授权全国银行间同业拆借中心公布了最新一期贷款市场报价利率（LPR）。1年期LPR利率为4.15%，5年期以上LPR利率为4.80%。同10月21日公布的上期数值相比，均下调了5个基点。</p>
<p>特别地，5年期LPR利率4.80%，比原有的贷款基准利率4.90%，低了10个基点，也就是0.1个百分点。</p>
<p>如果2020年1月以后买房子，签订的是变革后的房贷利率模式，未来LPR利率长期下行，那么买房人和贷款机构的房贷利率，也必然下行，这样的话，买房人承担的利率成本，和全社会的利率成本大致相当。</p>
<p>如果是2019年8月份之前买的房子，买房人和贷款机构签订的是基准利率，或者固定利率，那么未来社会无风险利率下行后，买房人承担的压力，将前所未有的大。这批买房人，完全将自己的人生上贡给了银行帝国，和后来轻装上阵的买房者，泾渭分明，社会的不公平性就拉大了。</p>
<p>有些人可能会说，变革前后，二种利率的差异，就在10个基点，0.1个百分点，差异不大。</p>
<p>你要这么说，只能证明，<code>对我们国家未来利率下行趋势的力量，一无所知！</code></p>
<h2 id="part-3"><a href="#part-3" class="headerlink" title="part 3"></a>part 3</h2><p>你也许会问：</p>
<p><strong>央行和放贷银行，居然给我们买房者这么好的福利，会把吃进肚子里的肉，吐了出来，它们图什么呢？</strong></p>
<p>呵呵。先让明哥笑几声。</p>
<p>如果你能这么想，说明你真的太天真了。</p>
<p>央行和放贷银行，给借款人松绑，绝对不是为了把吃进肚子里的肉吐出来，而是担心未来进入低利率时代以后，借款人弃房而去，那样会引发全社会系统性的大风险。</p>
<p>因为，它们是极其专业和前瞻的机构，已经提前预测到了：</p>
<p><code>在未来5-20年的时间长度内，中国一定会步发达国家的后尘，进入低利率，甚至是负利率时代。</code></p>
<p>这是坏事吗？绝对不是，这是任何经济体发展到一定水平后，必然要进入的常态。</p>
<p>当经济体的生产力已经充分发达，人口不再继续增长，物质充分丰富以后，一个市场的消费能力是有限的。也就是说，企业生产出来的实体商品、提供的服务和虚拟商品，终将超过该国或者全球所有人口的消费潜力。</p>
<p>假如没有突破性的科技进展，那么企业家没有多余利润可图，将不再扩大生产，不将利润再投资，不向银行间接贷款融资，不向股票市场直接股权融资，社会的GDP原地踏步，年轻人薪资20年不上涨。</p>
<p>这样将导致，社会上的存款资金和可投资资金，将无处可去。商业银行收储了海量的居民存款，找不到适格的企业去发放贷款，自然也赚取不了息差。</p>
<p>整体社会的利率，将显著下行。</p>
<p>统计过，2020年初，全球26个国家或经济体当中，有20个国家或经济体利率处于下降趋势。</p>
<p>欧盟央行：-0.5%；</p>
<p>日本央行：-0.1%；</p>
<p>德国10年期国债：-0.675%。</p>
<p>2019年8月5号，丹麦的第三大银行日德兰银行，推出了人类历史上首笔负利率按揭贷款业务，房贷利率为-0.5%。什么意思呢？</p>
<p>如果你借丹麦这家银行100万元去买房，一年后你只需还99.5万元就可以了。</p>
<p>这家银行向买房人发放贷款，贷款买房人，最终还的钱，比当初银行借出的钱，还要少！</p>
<p>听起来，像不像天方夜谭？绝对不是，这在西方发达经济体之中，已经成为了常识。</p>
<p>2019年8月6号，瑞士银行宣布对50万欧元以下的存款账户收取年费，并且不支付利息。</p>
<p>同样地，由于欧盟组织金融体系的利率是-0.5%，这意味着，一个人在欧盟所属的银行存款1万欧元，一年后，只能取回9950欧元。<br><img src="/images/other/2.jpg" alt="2"></p>
<p>2019年11月21日，已经卸任了中国人民银行行长职位的周小川，在创新经济论坛上表示：</p>
<p><strong>中国可以尽量避免快速地进入到负利率时代。</strong></p>
<p>言下之意，我们社会可以推迟进入到负利率时代的时间长度，延迟它的到来，但是改变不了历史归途。</p>
<p>在2017年之前的20年时间里，我们已经习惯了房价的上涨、物价的上涨，人民币购买力的贬值。其根本原因在于，我们国家依然处于发展阶段，各行各业的商品和服务，还有着巨大的空白市场去等着发掘。</p>
<p>房贷利率5.53%、民间借贷利率10-30%、P2P网贷利率20-50%。银行理财产品收益率低于6%，都没有大妈大爷看得上。</p>
<p>从2017年开始，高收益的幻象，逐一破灭了。房价下跌、P2P平台爆雷、上市公司多元化扩张一地鸡毛。那些追逐高收益的人，不仅没有得到高利率，连自己的本金都损失掉了。</p>
<p>股市收益率预期降低、年轻人就业心态放平、中年人只求保住饭碗，这才是未来我国经济的「新常态」。</p>
<p>进入低利率时代后，我们去银行存钱，不仅没有利息，还要倒付费用给银行来负责保管我们的资产数据。</p>
<p>为什么呢？</p>
<p>因为银行收储之后，将居民资产保管在银行内部，它放贷不出去，没有人来借款，自然赚取不了利息差带来的利润。</p>
<p>所以，负利率时代到来以后，谁放钱在银行，谁就要支付给银行一定的费用。</p>
<p>如果借款人发现，自己的存款没有利息，全社会的无风险利率都接近于零，企业不愿意去投资，社会GDP增速原地踏步，自己反而要承担4.9%的利率，那这种负担，绝对是压力山大，不可承受的。</p>
<p>等到那个时候，别说奢望房价上涨成为了白日梦，每年下跌2-10%，都是很有可能的，结果还要凭空承担-4.9%的利率亏损，傻子都知道，应该抛弃房子，直接断供。</p>
<p>如果大批量的买房人，集体断供，最终会引发什么后果呢？那就是2008年美国次贷危机的翻版，银行将倒闭，经济继续萧条，年轻人失业。那种后果不是每个社会都能承受的。</p>
<p>对于那些在2020年，就选择了「LPR利率+基点加成」模式的买房人而言，会张灯结彩、敲锣打鼓地迎接低利率时代、甚至是负利率时代的到来。</p>
<p>理想情况下，假如5年期LPR利率降低到了零，那么当初的买房人承担的利率就只是当初的「基点加成」，无论是上浮还是下浮，都可以认为轻如鸿毛。</p>
<p>这个时候，原有传统模式买房人，承担的利率是4.90%，接受了新模式的买房人，几乎没有房贷利率！</p>
<p>年化4.90%利率的差别，如果房贷余额还剩下200万，那就是年化9.8万元的差距。</p>
<p>这钱不香吗？为什么要凭空贡献给银行呢？</p>
<h2 id="part-4"><a href="#part-4" class="headerlink" title="part 4"></a>part 4</h2><p>站在2020年的门槛上，央行一眼洞穿了未来20年的经济轨迹，参考了西方发达国家的成熟经验，引导各大商业银行贷款人，和借款人，重新签订锚定于LPR利率的贷款合同。</p>
<p>这不仅是给予借款人的大红包，也是站在全社会的角度，未雨绸缪，将收益和风险，在借款人和贷款机构之间分摊，促进整个社会的稳定运行。</p>
<p>但是，央行出台的房贷改革新政，让贷款人和借款人重新签订利率合同，充满着大量的金融术语和晦涩的概念。</p>
<p>商业银行的目的是为了逐利。它们作为贷款人，赚取利息差，让借款人承担的利率越高，银行的利润就越高。<br><img src="/images/other/3.jpg" alt="3"></p>
<p>收取智商税的最佳方式是，制造信息不对称，利用自己的信息优势，来收割对方。</p>
<p>商业银行一定会巧舌如簧、口吐莲花一般，故意将两种利率模式，说得借款人如云山雾里，眼冒金星。</p>
<p>站在银行的角度来说，它们希望借款人像无头苍蝇一样，跟着他们的节奏走，不知不觉就躺在砧板上，摆好了待宰的体位。</p>
<p>只有明哥，惦记着你的钱包。</p>
<p>千言万语，汇成一句话！</p>
<p><code>从2020年3月1日开始的5个月时间里，当银行联系你重新协商房贷利率时，一定要坚持「LPR利率+基点加成」模式，并且利率一年自动更新一次。</code></p>
<p>这个机会，涉及到你家庭数十万，甚至上百万的资金，是用于自己，还是贡献给银行。</p>
<p>可千万别选错了，因为央行只给了一次机会，没有后悔药可吃！</p>
<p>来源：明哥在路上</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes coredns域名解析5秒记录]]></title>
      <url>http://team.jiunile.com/blog/2019/12/k8s-coredns-debug.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>近期线上 <code>k8s</code> 时不时就会出现一些内部服务间的调用超时问题，通过日志可以得知超时的原因都是出现在域名解析上，并且都是 <code>k8s</code> 内部的域名解析超时，于是直接先将内部域名替换成 <code>k8s service</code> 的 IP，观察一段时间发现没有超时的情况发生了，但是由于使用 <code>service IP</code> 不是长久之计，所以还要去找解决办法。<br><a id="more"></a></p>
<h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p>一开始运维同事在调用方 pod 中使用ab工具对目标服务进行了多次压测，并没有发现有超时的请求，我介入之后分析ab这类 http 压测工具应该都会有 dns 缓存，而我们主要是要测试 dns 服务的性能，于是直接动手撸了一个压测工具只做域名解析，代码如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"context"</span></span><br><span class="line">	<span class="string">"flag"</span></span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"net"</span></span><br><span class="line">	<span class="string">"sync/atomic"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> host <span class="keyword">string</span></span><br><span class="line"><span class="keyword">var</span> connections <span class="keyword">int</span></span><br><span class="line"><span class="keyword">var</span> duration <span class="keyword">int64</span></span><br><span class="line"><span class="keyword">var</span> limit <span class="keyword">int64</span></span><br><span class="line"><span class="keyword">var</span> timeoutCount <span class="keyword">int64</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="comment">// os.Args = append(os.Args, "-host", "www.baidu.com", "-c", "200", "-d", "30", "-l", "5000")</span></span><br><span class="line"></span><br><span class="line">	flag.StringVar(&amp;host, <span class="string">"host"</span>, <span class="string">""</span>, <span class="string">"Resolve host"</span>)</span><br><span class="line">	flag.IntVar(&amp;connections, <span class="string">"c"</span>, <span class="number">100</span>, <span class="string">"Connections"</span>)</span><br><span class="line">	flag.Int64Var(&amp;duration, <span class="string">"d"</span>, <span class="number">0</span>, <span class="string">"Duration(s)"</span>)</span><br><span class="line">	flag.Int64Var(&amp;limit, <span class="string">"l"</span>, <span class="number">0</span>, <span class="string">"Limit(ms)"</span>)</span><br><span class="line">	flag.Parse()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> count <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">var</span> errCount <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">	pool := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;, connections)</span><br><span class="line">	exit := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line">	<span class="keyword">var</span> (</span><br><span class="line">		min <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">		max <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">		sum <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		time.Sleep(time.Second * time.Duration(duration))</span><br><span class="line">		exit &lt;- <span class="literal">true</span></span><br><span class="line">	&#125;()</span><br><span class="line">endD:</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> pool &lt;- <span class="literal">nil</span>:</span><br><span class="line">			<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">				<span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">					&lt;-pool</span><br><span class="line">				&#125;()</span><br><span class="line">				resolver := &amp;net.Resolver&#123;&#125;</span><br><span class="line">				now := time.Now()</span><br><span class="line">				_, err := resolver.LookupIPAddr(context.Background(), host)</span><br><span class="line">				use := time.Since(now).Nanoseconds() / <span class="keyword">int64</span>(time.Millisecond)</span><br><span class="line">				<span class="keyword">if</span> min == <span class="number">0</span> || use &lt; min &#123;</span><br><span class="line">					min = use</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">if</span> use &gt; max &#123;</span><br><span class="line">					max = use</span><br><span class="line">				&#125;</span><br><span class="line">				sum += use</span><br><span class="line">				<span class="keyword">if</span> limit &gt; <span class="number">0</span> &amp;&amp; use &gt;= limit &#123;</span><br><span class="line">					timeoutCount++</span><br><span class="line">				&#125;</span><br><span class="line">				atomic.AddInt64(&amp;count, <span class="number">1</span>)</span><br><span class="line">				<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">					fmt.Println(err.Error())</span><br><span class="line">					atomic.AddInt64(&amp;errCount, <span class="number">1</span>)</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;()</span><br><span class="line">		<span class="keyword">case</span> &lt;-exit:</span><br><span class="line">			<span class="keyword">break</span> endD</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">"request count：%d\nerror count：%d\n"</span>, count, errCount)</span><br><span class="line">	fmt.Printf(<span class="string">"request time：min(%dms) max(%dms) avg(%dms) timeout(%dn)\n"</span>, min, max, sum/count, timeoutCount)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译好<a href="http://www.jiunile.com/k8s/ab-dns" target="_blank" rel="external">二进制</a>程序直接丢到对应的 pod 容器中进行压测：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 200个并发,持续30秒</span></span><br><span class="line"></span><br><span class="line">$ ./ab-dns -host &#123;service&#125;.&#123;namespace&#125; -c 200 <span class="_">-d</span> 30</span><br></pre></td></tr></table></figure></p>
<p>这次可以发现最大耗时有5s多，多次测试结果都是类似：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ./ab-dns -host s-inno-bpm.inno-ci -c 200 -d 30</span></span><br><span class="line">request count：109061</span><br><span class="line">error count：0</span><br><span class="line">request time：min(1ms) max(5082ms) avg(53ms) timeout(0n)</span><br></pre></td></tr></table></figure></p>
<p>而我们内部服务间 HTTP 调用的超时一般都是设置在3s左右，以此推断出与线上的超时情况应该是同一种情况，在并发高的情况下会出现部分域名解析超时而导致 HTTP 请求失败。</p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>起初一直以为是 <code>coredns</code> 的问题，于是找运维升级了下 <code>coredns</code> 版本再进行压测，发现问题还是存在，说明不是版本的问题，难道是 <code>coredns</code> 本身的性能就差导致的？想想也不太可能啊，才 200 的并发就顶不住了那性能也未免太弱了吧，结合之前的压测数据，平均响应都挺正常的(53ms)，但是就有个别请求会延迟，而且都是 5 秒左右，所以就又带着k8s dns 5s的关键字去 google 搜了一下，这不搜不知道一搜吓一跳啊，原来是 k8s 里的一个大坑啊(其实和 k8s 没有太大的关系，只是 k8s 层面没有提供解决方案)。</p>
<h2 id="5s-超时原因"><a href="#5s-超时原因" class="headerlink" title="5s 超时原因"></a>5s 超时原因</h2><p>linux 中 <code>glibc</code> 的 resolver 的缺省超时时间是 5s，而导致超时的原因是内核 <code>conntrack</code> 模块的 bug。</p>
<blockquote>
<p>Weave works 的工程师 Martynas Pumputis 对这个问题做了很详细的分析：<a href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts" target="_blank" rel="external">https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts</a></p>
</blockquote>
<p>这里再引用下 <a href="https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/" target="_blank" rel="external">https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/</a> 文章中的解释：</p>
<blockquote>
<p>DNS client (glibc 或 musl libc) 会并发请求 A 和 AAAA 记录，跟 DNS Server 通信自然会先 connect (建立 fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议， connect 时并不会发包，也就不会创建 conntrack 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，send 时各自发的包它们源 Port 相同(因为用的同一个 socket 发送)，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们分别创建 conntrack 表项，而集群内请求 kube-dns 或 coredns 都是访问的 CLUSTER-IP，报文最终会被 DNAT 成一个 endpoint 的 POD IP，当两个包恰好又被 DNAT 成同一个 POD IP 时，它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 dns 的 pod 副本只有一个实例的情况就很容易发生(始终被 DNAT 成同一个 POD IP)，现象就是 dns 请求超时，client 默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 dns 请求有 5s 的延时。</p>
</blockquote>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="方案（一）：使用-TCP-协议发送-DNS-请求"><a href="#方案（一）：使用-TCP-协议发送-DNS-请求" class="headerlink" title="方案（一）：使用 TCP 协议发送 DNS 请求"></a>方案（一）：使用 TCP 协议发送 DNS 请求</h3><p>通过 <code>resolv.conf</code> 的 <code>use-vc</code> 选项来开启 TCP 协议, 修改 <code>/etc/resolv.conf</code> 文件，在最后加入一行文本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver ...</span><br><span class="line">search ...</span><br><span class="line">options ndots:5 use-vc</span><br></pre></td></tr></table></figure></p>
<p>经过测试，确实没有出现 5s 的超时问题了，但是部分请求耗时还是比较高，在 4s 左右，而且平均耗时比 UPD 协议的还高，效果并不好。</p>
<h3 id="方案（二）：避免相同五元组-DNS-请求的并发"><a href="#方案（二）：避免相同五元组-DNS-请求的并发" class="headerlink" title="方案（二）：避免相同五元组 DNS 请求的并发"></a>方案（二）：避免相同五元组 DNS 请求的并发</h3><p>通过 <code>resolv.conf</code> 的 <code>single-request-reopen</code> 和 <code>single-request</code> 选项来避免：</p>
<ul>
<li>single-request-reopen (glibc&gt;=2.9) 发送 A 类型请求和 AAAA 类型请求使用不同的源端口。这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突。</li>
<li>single-request (glibc&gt;=2.10) 避免并发，改为串行发送 A 类型和 AAAA 类型请求，没有了并发，从而也避免了冲突。</li>
</ul>
<p>修改 /etc/resolv.conf 文件，在最后加入一行文本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver ...</span><br><span class="line">search ...</span><br><span class="line">options ndots:5 timeout:2 single-request-reopen</span><br></pre></td></tr></table></figure></p>
<p>通过压测结果可以看到 <code>single-request-reopen</code> 和 <code>single-request</code> 选项确实可以显著的降低域名解析耗时。</p>
<h2 id="关于方案（一）和方案（二）的实施步骤和缺点"><a href="#关于方案（一）和方案（二）的实施步骤和缺点" class="headerlink" title="关于方案（一）和方案（二）的实施步骤和缺点"></a>关于方案（一）和方案（二）的实施步骤和缺点</h2><p>其实就是要给容器的 <code>/etc/resolv.conf</code> 文件添加选项，目前有两个方案比较合适：</p>
<ol>
<li><p>通过修改 pod 的 postStart hook 来设置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span></span><br><span class="line"><span class="attr">  postStart:</span></span><br><span class="line"><span class="attr">    exec:</span></span><br><span class="line"><span class="attr">      command:</span></span><br><span class="line"><span class="bullet">        -</span> /bin/sh</span><br><span class="line"><span class="bullet">        -</span> -c</span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/echo 'options single-request-reopen' &gt;&gt; /etc/resolv.conf"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>通过修改 pod 的 template.spec.dnsConfig 来设置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">template:</span></span><br><span class="line"><span class="attr">  spec:</span></span><br><span class="line"><span class="attr">    dnsConfig:</span></span><br><span class="line"><span class="attr">      options:</span></span><br><span class="line"><span class="attr">        - name:</span> single-request-reopen</span><br></pre></td></tr></table></figure>
</li>
</ol>
<blockquote>
<p><code>注: 需要 k8s 版本&gt;=1.9</code></p>
</blockquote>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>不支持 <code>alpine</code> 基础镜像的容器，因为 <code>apline</code> 底层使用的 <code>musl libc</code> 库并不支持这些 <code>resolv.conf</code> 选项，所以如果使用 <code>alpine</code> 基础镜像构建的应用，还是无法规避超时的问题。</p>
<h2 id="方案（三）：本地-DNS-缓存"><a href="#方案（三）：本地-DNS-缓存" class="headerlink" title="方案（三）：本地 DNS 缓存"></a>方案（三）：本地 DNS 缓存</h2><p>其实 k8s 官方也意识到了这个问题比较常见，给出了 coredns 以 cache 模式作为 daemonset 部署的解决方案: <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns</a></p>
<p>大概原理就是：</p>
<blockquote>
<p>本地 DNS 缓存以 DaemonSet 方式在每个节点部署一个使用 hostNetwork 的 Pod，创建一个网卡绑上本地 DNS 的 IP，本机的 Pod 的 DNS 请求路由到本地 DNS，然后取缓存或者继续使用 TCP 请求上游集群 DNS 解析 (由于使用 TCP，同一个 socket 只会做一遍三次握手，不存在并发创建 conntrack 表项，也就不会有 conntrack 冲突)</p>
</blockquote>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><ol>
<li><p>获取当前 <code>kube-dns service</code> 的 <code>clusterIP</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get svc kube-dns -o jsonpath=<span class="string">"&#123;.spec.clusterIP&#125;"</span></span><br><span class="line"></span><br><span class="line">10.96.0.10</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载官方提供的 yaml 模板进行关键字替换</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ wget -O nodelocaldns.yaml <span class="string">"https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"</span> &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__DNS__SERVER__/10.96.0.10/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__LOCAL__DNS__/169.254.20.10/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__DNS__DOMAIN__/cluster.local/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__CLUSTER__DNS__/10.96.0.10/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__UPSTREAM__SERVERS__/\/etc\/resolv.conf/g'</span> nodelocaldns.yaml</span><br></pre></td></tr></table></figure>
</li>
<li><p>最终 yaml 文件如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> node-local-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kube-dns-upstream</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> kube-dns</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">    kubernetes.io/name: <span class="string">"KubeDNSUpstream"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> dns</span><br><span class="line"><span class="attr">      port:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">      protocol:</span> UDP</span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">    - name:</span> dns-tcp</span><br><span class="line"><span class="attr">      port:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">      protocol:</span> TCP</span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> kube-dns</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> node-local-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  Corefile:</span> <span class="string">|</span><br><span class="line">    cluster.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache &#123;</span><br><span class="line">                success 9984 30</span><br><span class="line">                denial 9984 5</span><br><span class="line">        &#125;</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . 10.96.0.10 &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">        &#125;</span><br><span class="line">    in-addr.arpa:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . 10.96.0.10 &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        &#125;</span><br><span class="line">    ip6.arpa:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . 10.96.0.10 &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        &#125;</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . /etc/resolv.conf &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        &#125;</span><br><span class="line">---</span><br><span class="line"></span><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> DaemonSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> node-local-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> node-local-dns</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">    rollingUpdate:</span></span><br><span class="line"><span class="attr">      maxUnavailable:</span> <span class="number">10</span>%</span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> node-local-dns</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> node-local-dns</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      priorityClassName:</span> system-node-critical</span><br><span class="line"><span class="attr">      serviceAccountName:</span> node-local-dns</span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      dnsPolicy:</span> Default <span class="comment"># Don't use cluster DNS.</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">"CriticalAddonsOnly"</span></span><br><span class="line"><span class="attr">          operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> node-cache</span><br><span class="line"><span class="attr">          image:</span> k8s.gcr.io/k8s-dns-node-cache:<span class="number">1.15</span><span class="number">.7</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">25</span>m</span><br><span class="line"><span class="attr">              memory:</span> <span class="number">5</span>Mi</span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line">            [</span><br><span class="line">              <span class="string">"-localip"</span>,</span><br><span class="line">              <span class="string">"169.254.20.10,10.96.0.10"</span>,</span><br><span class="line">              <span class="string">"-conf"</span>,</span><br><span class="line">              <span class="string">"/etc/Corefile"</span>,</span><br><span class="line">              <span class="string">"-upstreamsvc"</span>,</span><br><span class="line">              <span class="string">"kube-dns-upstream"</span>,</span><br><span class="line">            ]</span><br><span class="line"><span class="attr">          securityContext:</span></span><br><span class="line"><span class="attr">            privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">              name:</span> dns</span><br><span class="line"><span class="attr">              protocol:</span> UDP</span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">              name:</span> dns-tcp</span><br><span class="line"><span class="attr">              protocol:</span> TCP</span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">9253</span></span><br><span class="line"><span class="attr">              name:</span> metrics</span><br><span class="line"><span class="attr">              protocol:</span> TCP</span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line"><span class="attr">              path:</span> /health</span><br><span class="line"><span class="attr">              port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - mountPath:</span> /run/xtables.lock</span><br><span class="line"><span class="attr">              name:</span> xtables-lock</span><br><span class="line"><span class="attr">              readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">            - name:</span> config-volume</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/coredns</span><br><span class="line"><span class="attr">            - name:</span> kube-dns-config</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/kube-dns</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> xtables-lock</span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> /run/xtables.lock</span><br><span class="line"><span class="attr">            type:</span> FileOrCreate</span><br><span class="line"><span class="attr">        - name:</span> kube-dns-config</span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> kube-dns</span><br><span class="line"><span class="attr">            optional:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - name:</span> config-volume</span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> node-local-dns</span><br><span class="line"><span class="attr">            items:</span></span><br><span class="line"><span class="attr">              - key:</span> Corefile</span><br><span class="line"><span class="attr">                path:</span> Corefile.base</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>通过 yaml 可以看到几个细节：</p>
<ol>
<li>部署类型是使用的 <code>DaemonSet</code>，即在每个 <code>k8s node</code> 节点上运行一个 dns 服务</li>
<li><code>hostNetwork</code> 属性为 <code>true</code>，即直接使用 node 物理机的网卡进行端口绑定，这样在此 node 节点中的 pod 可以直接访问 dns 服务，不通过 service 进行转发，也就不会有 DNAT</li>
<li><code>dnsPolicy</code> 属性为 <code>Default</code>，不使用 <code>cluster DNS</code>，在解析外网域名时直接使用本地的 DNS 设置</li>
<li>绑定在 node 节点 <code>169.254.20.10</code> 和 <code>10.96.0.10</code> IP 上，这样节点下面的 pod 只需要将 dns 设置为<code>169.254.20.10</code> 即可直接访问宿主机上的 dns 服务。</li>
</ol>
<h3 id="实施"><a href="#实施" class="headerlink" title="实施"></a>实施</h3><ol>
<li><p>通过修改 <code>pod</code> 的 <code>template.spec.dnsConfig</code> 来设置，并将 <code>dnsPolicy</code> 设置为 <code>None</code></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line"><span class="attr">  spec:</span></span><br><span class="line"><span class="attr">    dnsConfig:</span></span><br><span class="line">	  nameservers:</span><br><span class="line"><span class="bullet">        -</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line"><span class="attr">      searches:</span></span><br><span class="line"><span class="bullet">        -</span> public.svc.cluster.local</span><br><span class="line">		- svc.cluster.local</span><br><span class="line">		- cluster.local</span><br><span class="line"><span class="attr">      options:</span></span><br><span class="line"><span class="attr">        - name:</span> ndots</span><br><span class="line"><span class="attr">          value:</span> <span class="string">"5"</span></span><br><span class="line"><span class="attr">    dnsPolicy:</span> None</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改默认的 cluster-dns，在 node 节点上将 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 文件中的 <code>--cluster-dns</code> 参数值修改为 <code>169.254.20.10</code>，然后重启 <code>kubelet</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>参考：</p>
<ul>
<li>monkeywie.github.io</li>
<li>imroc.io</li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[白话Kubernetes核心组件及原理]]></title>
      <url>http://team.jiunile.com/blog/2019/12/k8s-k8s-intro.html</url>
      <content type="html"><![CDATA[<h2 id="Kubernetes是什么？"><a href="#Kubernetes是什么？" class="headerlink" title="Kubernetes是什么？"></a>Kubernetes是什么？</h2><p>Kubernetes其实就是一个集群，从我们此前运维的角度来理解，它就是一个集群，组合多台主机的资源（内存、CPU、磁盘等）整合成一个大的资源池并统一对外提供计算存储等能力的集群。我们找很多台主机，每台主机上面安装Kubernetes的相关程序，而不同的主机程序之间相互通信，从而完成彼此之间的协调，并且通过这些应用程序之间的协同工作，把多个主机当成一个主机来使用，形成一个集群，仅此而已，但是在kubernetes集群当中主机是分角色的，即所谓的有中心结点架构的集群系统，master/nodes模型, 由一组节点用于master不需要太多，一般高可用的话需要三个（根据集群规模来判断），nodes节点（worker节点）就是干活的，Kubernetes上提供的各种资源服务，运行在node节点上面。<br><a id="more"></a></p>
<h2 id="用户如何在kubernetes运行容器，逻辑过程是什么？"><a href="#用户如何在kubernetes运行容器，逻辑过程是什么？" class="headerlink" title="用户如何在kubernetes运行容器，逻辑过程是什么？"></a>用户如何在kubernetes运行容器，逻辑过程是什么？</h2><p>用户把创建启动容器的请求首先发给master，具体来说是发给了master节点上面的<code>API Server</code>组件， <code>API Server</code>通过调度器，按照预设的调度算法及策略，去分析各node节点上面现有的可用资源状态，然后找一个最佳适配，来运行用户所请求容器的结点，并把它调度上去（这里需要与node节点上面的<code>kubelet</code>交互），并由这个node节点上面本地的docker或其它容器引擎负责把这个容器启动起来，要启动容器，需要有镜像，镜像在哪里呢？在仓库上面，node上面启动容器是会先检查本地是否有镜像（根据镜像拉取策略），如果没有会docker pull下来，然后再启动，kubernetes自身并没有托管自动所依赖的每一个容器镜像，而是需要到仓库中去下载的，仓库可以是私有的，也可以是公有的。</p>
<p>集群在master节点上面提供一个<code>API Server</code>组件，它负责接受请求，解析请求，处理请求的，至于当用户请求的是创建一个容器，最好不要运行在master节点上面，而应该运行在node节点上面，确定哪个node更合适，这个时间就需要<code>scheder</code>调度器，它负责监控每个node节点上面总可用的计算、内存、存储等资源，并根据用户所请求创建的这个容器所需要的资源，docker容器可以做资源限制 ，但在kubernetes上面不但可以设定容器使用资源的上限（阀值），还可以设定资源使用的下限（资源请求量），调度器就是根据容器的最低需求来进行评估，哪一个节点最合适；当然了，资源的评估不只是一个维度，而是从多个维度考虑，这都是调度器<code>scheduler</code>根据调度策略和算法需要考虑的，如果在一个node上面把容器启动起来了，我们还需要对容器中的应用程序的健康状态做监测，我们不但能根据容器中应用程序是否运行判断它的健康状况，还可以根据额外的健康状态探测方式来探测，我们叫做可用性探测机制来探测服务的可用性。如果一旦容器中的应用挂了，我们又需要确保容器中的一个容器要运行，此时怎么办？node节点之上有一个应用程序，这个应用程序就是<code>kebulet</code>，这个应用程序确保容器始终处于健康状态，如果出现问题，它就会通知<code>API Server</code>，然后重新调度；但是有一点，很不幸，这个node节点如果宕机了，那么此前拖管在此node上面的所有容器就挂了，我们知道kubernetes具有自愈的能力，无论是单个容器，还是node节点上面的所有容器，一旦容器不见了，是不需要人工参与的，kubernetes会使用新的个体来取代它，它会在其它node上面创建出来一模一样的容器出来。如何确保这个容器是健康的呢？以及一旦出问题就可以及时被发现呢? 其实kubernetes是通过控制器组件来负责监控它所管理的每一个容器的健康状态，一旦发现不健康了，控制器向master上面 <code>API server</code>发请求，容器挂了一个，你帮我重新调度再启动一个，这里控制器需要在本地不停的loop循环，周期性，持续性的探测所管理的容器的健康状况，一旦不健康，或者不符合用户所定义（期望）的目标，此是调度器就会向用户期待的状态向前移，确保符合用户期望的状态；其实在kubernetes集群中我们有很多很多的控制器，假设我们有一个控制器挂了呢，用于监控容器健康的控制器不健康了，容器的健康状态就无法保证，怎么办？我们在master节点上面有一个控制器管理器，控制器管理器负责控制监控每个控制器的健康状况，控制器管理器如果出现问题怎么办，因此在这里，我们需要对控制器管理器做冗余。</p>
<p>以上我们通过在集群上面创建一个容器的例子，讲解了<code>API Server</code>、<code>scheduler</code>、<code>控制器</code>、<code>控制管理器</code>等。</p>
<h2 id="什么是Pod？"><a href="#什么是Pod？" class="headerlink" title="什么是Pod？"></a>什么是Pod？</h2><p>Pod，英文意思是豆荚。大家都知道这种植物，一个豆荚中有几个豆粒。</p>
<p>Kubernetes上面运行的最小单元是<code>Pod</code>,  kubernetes并不直接调度容器的运行，而调度的目标是<code>Pod</code>，<code>Pod</code>可以理解为容器的外壳，给容器做了一层抽象的封装，因此<code>Pod</code>成为了Kubernetes集群之上最小的调度单位（逻辑单元），<code>Pod</code>内部主要是用来放容器的，<code>Pod</code>有一个特点，一个<code>Pod</code>中可以运行多个容器，多个容器共享同一个底层的网络命名空间（底层的<code>net</code>, <code>uts</code>, <code>IPC</code>三个网络命名空间），另外三个命名空间相互隔离（<code>User</code>, <code>mnt</code>, <code>pid</code>），这样一来，同一个<code>Pod</code>上面的多个容器，每个容器上面跑应用程序 ，对外更像是同一台“虚拟机”，这也是kubernetes组织容器的一个非常精巧的办法，基于此我们可以构建较为精细的容器间通信，并且同一个Pod上面的容器，还共享第二种资源，叫做存储卷，存储卷不属于容器，属于<code>Pod</code>，<code>Pod</code>的磁盘，相同<code>Pod</code>的容器共享。</p>
<p>各个node节点主要是用来运行<code>Pod</code>的，一般说来，一个<code>Pod</code>上面只放一个容器，除非有特别紧密的关系，需要放在同一个<code>Pod</code>上面，否则，不要放在同一<code>Pod</code>上面；如果确实有需要，将多个容器需要放在一个<code>Pod</code>中，通常有一个容器是主容器，其它的容器为辅助主容器，辅助容器中的应用程序主要是为了完成更多功能来辅佐主容器工作，这里我们调度器也是调度的<code>Pod</code>, node节点上面也是<code>Pod</code>, <code>Pod</code>是一个原子单元，也就意味着一个<code>Pod</code>中有一个容器，还是有多个容器，一旦被调度之后，相同<code>Pod</code>上面的容器，只能在同一个node节点上面。</p>
<p>创建<code>Pod</code>时，可以直接创建，并且自主管理的，但它仍然要提交给<code>API Server</code>，由<code>API Server</code>接收以后，通过调度器调度到指定的Node节点上，而node节点启动此Pod，此后如果pod上面的容器出现故障，需要重要重启容器，需要<code>kubelet</code>完成，但是node节点故障了，节点就消失了。还有一种Pod的创建方式，是通过控制器来创建的，后面为讲什么是控制器，它的作用是什么？</p>
<h2 id="Node节点是做什么的？"><a href="#Node节点是做什么的？" class="headerlink" title="Node节点是做什么的？"></a>Node节点是做什么的？</h2><p>刚才说了node是kubernetes集群中的工作节点，负责运行由master节点上面指派的各种任务，而最核心的任务是以Pod的形式运行容器的，理解上讲node可以是任何形式的资源设备，只要有传统意义上的内存、CPU、存储资源即可，并且可以安装上Kubernetes集群的应用程序 ，都可以做为k8s集群的一个份子来工作，它是承载资源的。</p>
<p>这样一来终端用户不需要关心应用程序（Pod）在哪个Node节点上面，它就这样脱离了终端用户的视线，终端用户也无需关注应用程序部署在哪个node节点上面的pod，从而真正意义上实现了资源池，从而进行统一管理。</p>
<h2 id="什么是标签，标签选择器是做什么的？"><a href="#什么是标签，标签选择器是做什么的？" class="headerlink" title="什么是标签，标签选择器是做什么的？"></a>什么是标签，标签选择器是做什么的？</h2><p>如何让一个控制器管理指定的Pod，例如，我们创建了5个Pod，Pod中运行tomcat容器，我们让一个控制器来管理这一组Pod，为了让Pod能够实现被控制器管理识别，我们需要在Pod上面附加一些元数据（标签），用标签来识别Pod，在创建Pod的时候，或者人为的打上一个标签，让控制器能够识别出标签，进而识别出Pod。我们前面创建了5个 Pod，我们在每一个pod上面加一个标签app, 标签的值叫tomcat (<code>标签：值====&gt; app:tomcat</code>)，我们想把这一类找出来，怎么找，我们先找拥有key是app，并且值是tomcat的pod分拣出来。标签是Kubernetes大规模集群管理、分类、识别资源使用的，标签是非常非常重要的凭证，我们是如何把我们感兴趣的标签找到的呢，我们有一个标签选择器/挑选器（selector）组件，标签选择器，简单来讲就是根据标签，过滤符合条件的资源对象的机制，其实标签不只是Pod有，很多其它资源都有，因此这种选择器叫做标签选择器，而不叫pod标签选择器，Kubernetes是Restfull 风格的API，通过http或者https对外提供服务，所以所有Restfull对外提供的服务资源都称为对象，所有的对象都可以拥有标签，所有的标签都可以使用标签选择器来选择，只不过pod是其中一种比较重要的。</p>
<h2 id="什么是控制器，控制器是做什么的，有哪些控制器？"><a href="#什么是控制器，控制器是做什么的，有哪些控制器？" class="headerlink" title="什么是控制器，控制器是做什么的，有哪些控制器？"></a>什么是控制器，控制器是做什么的，有哪些控制器？</h2><p>我们刚才讲Pod的时候 ，讲到了创建Pod时，一种是直接创建Pod，Pod删除后，不会自动创建，还有一种创建Pod的方式，是通过控制器创建的Pod，这种<code>控制器管理的Pod</code>， 正是控制器管理器机制的使用。在Kubernetes设计中，Pod完全可以叫做有生命周期的对象，而后由调度器将其调度至集群中的某节点，运行以后，任务终止也就被删除停掉了，但是有一些任务，比如nginx，或者运行一个tomcat，他们是做为守护进程来运行的，这种程序，我们要确保这种pod随时运行，一旦出现故障，需要第一时间发现，要么取代它，要么重启它，要么重建一个新的pod，这种靠人的右眼是无法保证的，而Kubernetes提供了具备这种工作能力的组件叫<code>Pod控制器</code>。</p>
<p>Pod控制器最早的一种叫<code>ReplicationController</code> (副本控制器，早期版本的控制器，也称为Pod控制器)， 当我们启动一个pod时，一个不够了，可以再启动一个副本，控制器就是控制同一类资源对象的副本，一旦副本数量少了，就会自动加一个，能够定义要求的副本数，多了就删除，精确符合人们期望的数量，它还可以实现滚动更新，它允许临时添加副本，然后把旧版本的去掉，实现滚动更新；它也允许回滚操作，后来的版本中新加了<code>ReplicaSetController</code>（副本集控制器），而<code>ReplicaSetController</code>也不直接使用，而是有一个声明式更新的控制器叫<code>Deployment</code>，用它来进行管理控制， 我们使用的最多的也是<code>Deployment</code>控制器，而<code>Deployment</code>控制器只能管理哪些无状态的应用，哪么有状态的应用如何控制管理呢？我们使用新的控制器，叫<code>StatefulSet</code>有状态副本集，另外如果我们需要在每个node上面运行一个Pod，而不是随意运行，我们还需要一个<code>DaemonSet</code>，如果我们运行作业，还需要Job, 周期性作业，<code>Cronjob</code>,  这些是常见的Pod控制器；后面的这些控制器都是实现一种特定的应用管理，比如临时运行一个容器去完成删除日志的功能，这个运行完就删除了，我们就可以使用Job控制器管理Pod, 但是如果Job没有运行完挂了，需要重新启动，如果运行完，就删除了，再比如nginx一直需要处于运行状态，就不能使用Job控制器，所以说这么多的控制器是用于确保不同类型的Pod资源，来符合用户所期望的方式来运行，像<code>Deployment</code>控制器还支持二级控制器，叫<code>HPA</code>，叫水平Pod，自动伸缩控制器，比如我们一个控制器控制两个副本在运行，但在资源利用率高的时候，可以自动的伸缩控制，就是使用<code>HPA</code>进行控制，一旦利用率低了，可以自动减少，但要符合我们预期的最小值。</p>
<h2 id="Serveice是什么，为什么需要Service？"><a href="#Serveice是什么，为什么需要Service？" class="headerlink" title="Serveice是什么，为什么需要Service？"></a>Serveice是什么，为什么需要Service？</h2><p>到这里我们想到一个问题，Pod是由生命周期的，万一Pod所在Node节点宕机了，Pod有可能需要在其它的 Node节点上面重新创建，而重新创建完成后的pod，跟之前的不是一个，只不过是应用程序一样而已，提供相同的服务，由于每个pod中容器的IP地址就不一样，这样一来就有一个问题，我们客户端怎么去访问这些Pod呢？是利用服务发现机制，首先客户端每一次去访问服务时，客户端是不知道后端的服务是谁的（不知道pod的存在），他需要找一个地方问一句，发现一下，有没有这种服务，这些服务是Pod启动的时候注册到一个类似总线地址上，客户端直接去总线位置去问，有没有，有的话，给一个Pod地址，客户端与Pod地址进行通信；因此尽可能降低这种复杂度，Kubernetes为每一组提供相同功能的Pod和客户端之间添加了一个中间层，这个中间层是固定的，这个中间层就叫service，只要service不删除，它就是固定的，名称也是固定的，当客户端需要访问时，只需要在客户端写上service 主机名|服务器|地址即可，也不需要发现，而这个服务service 是一个调度器，不但提供一个固定稳定的访问入口，只要不删除，它就是稳定的，客户端只需要写service名称即可，服务再把请求代理到后面的pod上面，那么Pod宕机了，新创建的pod会被service立即给关联进来；还会把新加的pod作为service后面的可用资源对象之一，怎么实现的呢？我们知道 客户端与服务器通信是通过<code>IP：Port</code>或者<code>域名:Port</code>形式，而service与后面的pod不是依靠<code>IP:Port</code>的形式（因为pod的主机名和IP经常要变），而是通过Pod上面固定的标签来识别，只要是相同标签的pod，不管主机名和IP怎么变，都会被service通过标签识别，service是通过标签选择器来关联pod的；这样一来，只要pod属于这个标签选择器，就能立即被service能选中，并且做为service后端组件存在，关联进来以后，再动态探测这个pod的IP地址是什么，端口是什么，并做为自己后端可调度的服务器，资源对象， 最后，客户端是通过service代理至后端pod进行通信；意味着客户端看到的地址就是service的地址，而在kubernetes集群上service可不是什么应用程序 ，也不是一个实体组件，它只不过是一个<code>iptables DNAT</code>规则；我们创建一个<code>DNAT</code> 规则 ，我们所有到达xxx地址的，都统统被目标地址转换成yyy地址，<code>DNAT</code>规则只是一个规则 ，而service地址，事实上并没有配置到任何一张网卡上，是不存在的，它仅仅出在规则中，可以ping通的，并且可以做请求中转，能ping通是因为有<code>TCP/IP</code>协议栈。这个IP地址，仅出现在规则中，更重要的是service做为Kubernetes中的对象来讲，它有名称，就相当于这个服务的名字，名称可以被解析，就是把service名称解析成IP，名称解析靠DNS，没错，我们安装完k8s后，第一件事，就是让部署一个<code>DNS Pod</code>，以确保service被解析，这个Pod是Kubernetes自身的服务就需要的pod，所以我们称之为基础性的系统架构级的pod或对象，而且称他们叫集群的附件。</p>
<h2 id="集群附件DNS"><a href="#集群附件DNS" class="headerlink" title="集群附件DNS"></a>集群附件DNS</h2><p>DNS附件只是Kubernetes集群中纵多附件中的一个，并且这种DNS有一个很有意义的特点，可以动态的创建，动态的改变，动态的更新，动态的变动，比如，你更新了service名称，DNS中的记录即就会被改变，再比如我们手动修改了service IP ,他会自动触发DNS 服务中的解析记录的更改，所以以后客户端访问service时，可以直接访问服务的名称 ，而由集群中专门的DNS服务来负责解析，解析的是service的地址，不是pod地址，再由service代理访问pod，刚才也说了，这种代理是端口代理，由DNAT实现，可不能忘记service后面中两个或者多个pod，这里的DNAT就是多个目标了，多目标调度，对于linux来讲，大家知道对于iptables来讲，已经把负载均衡的功能主要交给了IPVS，因此如果service背后的同一个服务有多个Pod，并且由DNAT来实现，可能在调度效果上并不尽人意，因此在 Kubernetes 1.11版本以后，已经把iptables规则改成了IPVS规则，也就相当于，当你创建一条service规则时，就创建了一条IPVS规则 ，只不过是NAT模型的IPVS规则，因此还支持用户可以指定任意的调度算法，如轮询、加权、最小连接等，所以你就会发现LVS是我们的基础性服务，以上就是我们所讲的service组件。</p>
<p>来源：Linux点滴运维实践</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes修改主机ip]]></title>
      <url>http://team.jiunile.com/blog/2019/12/k8s-kubeadm-edit-hostip.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于外部等不确定因素印象，kubernetes集群中的主机IP修改了，会导致集群受影响，故出以下教程来保证集群的稳定性。此教程适用于kubeadm安装的kubernetes集群，使用版本kubeadm1.15+ </p>
<a id="more"></a>
<h2 id="修改master主机ip"><a href="#修改master主机ip" class="headerlink" title="修改master主机ip"></a>修改master主机ip</h2><h3 id="方式一：通过kubeadm命令进行调整"><a href="#方式一：通过kubeadm命令进行调整" class="headerlink" title="方式一：通过kubeadm命令进行调整"></a>方式一：通过kubeadm命令进行调整</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kubeadm.conf 配置文件如下</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.3 <span class="comment">#--&gt;这里改成你集群对应的版本</span></span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers </span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成/etc/kubernetes目录下的conf文件</span></span><br><span class="line">kubeadm init phase kubeconfig all --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下文件</span></span><br><span class="line"><span class="comment">#admin.conf	</span></span><br><span class="line"><span class="comment">#controller-manager.conf	</span></span><br><span class="line"><span class="comment">#kubelet.conf	</span></span><br><span class="line"><span class="comment">#scheduler.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成manifests目录下的yaml配置</span></span><br><span class="line">kubeadm init phase control-plane all --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下文件</span></span><br><span class="line"><span class="comment">#manifests/kube-apiserver.yaml</span></span><br><span class="line"><span class="comment">#manifests/kube-controller-manager.yaml</span></span><br><span class="line"><span class="comment">#manifests/kube-scheduler.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成etcd.yaml,如果是外部etcd则略过</span></span><br><span class="line">kubeadm init phase etcd <span class="built_in">local</span> --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下文件，外部etcd则不会影响</span></span><br><span class="line"><span class="comment">#manifests/etcd.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成组件配置</span></span><br><span class="line">kubeadm init phase addon all --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下组件</span></span><br><span class="line"><span class="comment">#kube-proxy</span></span><br><span class="line"><span class="comment">#coredns</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#备份/etc/kubernetes/pki目录</span></span><br><span class="line">cp -r /etc/kubernetes/pki /etc/kubernetes/pki-old</span><br><span class="line">rm -rf /etc/kubernetes/pki/apiserver.*  /etc/kubernetes/pki/front-proxy-client* </span><br><span class="line">rm -rf /etc/kubernetes/pki/etcd/healthcheck-client* /etc/kubernetes/pki/etcd/peer* /etc/kubernetes/pki/etcd/server*</span><br><span class="line"><span class="comment">#重新生成证书</span></span><br><span class="line">kubeadm init phase certs all --config=/root/kubeadm.conf</span><br></pre></td></tr></table></figure>
<h3 id="方式二：通过sed-命令进行调整"><a href="#方式二：通过sed-命令进行调整" class="headerlink" title="方式二：通过sed 命令进行调整"></a>方式二：通过sed 命令进行调整</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改/etc/kubernetes 目录下的配置文件</span></span><br><span class="line">sed -i <span class="string">'s/oldip/newip/g'</span> `grep <span class="string">"oldip"</span> -rl /etc/kubernetes`</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改kube-proxy configmap配置</span></span><br><span class="line">kubectl  get cm kube-proxy -n kube-system -o yaml &gt; proxy.yaml</span><br><span class="line">sed -i <span class="string">'s/oldip/newip/g'</span> proxy.yaml</span><br><span class="line">kubectl apply <span class="_">-f</span> proxy.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#重新renew证书</span></span><br><span class="line">kubeadm alpha certs renew all --config=/root/kubeadm.conf</span><br></pre></td></tr></table></figure>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在 Kubernetes 中配置 Container Capabilities]]></title>
      <url>http://team.jiunile.com/blog/2019/12/capabilities.html</url>
      <content type="html"><![CDATA[<p>我们在使用 Kubernetes 过程中，偶尔会遇到如下所示的一段配置：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">  capabilities:</span></span><br><span class="line"><span class="attr">    drop:</span></span><br><span class="line"><span class="bullet">    -</span> ALL</span><br><span class="line"><span class="attr">    add:</span></span><br><span class="line"><span class="bullet">    -</span> NET_BIND_SERVICE</span><br></pre></td></tr></table></figure></p>
<p>实际上这是配置对应的容器的 <code>Capabilities</code>，在我们使用 <code>docker run</code> 的时候可以通过 <code>--cap-add</code> 和 <code>--cap-drop</code> 命令来给容器添加 <code>Linux Capabilities</code>。对于大部分同学可能又要疑问 <code>Linux Capabilities</code> 是什么呢？<br><a id="more"></a></p>
<h2 id="Linux-Capabilities"><a href="#Linux-Capabilities" class="headerlink" title="Linux Capabilities"></a>Linux Capabilities</h2><p>要了解 <code>Linux Capabilities</code>，这就得从 Linux 的权限控制发展来说明。在 Linux 2.2 版本之前，当内核对进程进行权限验证的时候，Linux 将进程划分为两类：特权进程（UID=0，也就是超级用户）和非特权进程（UID!=0），特权进程拥有所有的内核权限，而非特权进程则根据进程凭证（effective UID, effective GID，supplementary group 等）进行权限检查。</p>
<p>比如我们以常用的 <code>passwd</code> 命令为例，修改用户密码需要具有 root 权限，而普通用户是没有这个权限的。但是实际上普通用户又可以修改自己的密码，这是怎么回事呢？在 Linux 的权限控制机制中，有一类比较特殊的权限设置，比如 SUID(Set User ID on execution)，允许用户以可执行文件的 owner 的权限来运行可执行文件。因为程序文件 <code>/bin/passwd</code> 被设置了 <code>SUID</code> 标识，所以普通用户在执行 passwd 命令时，进程是以 passwd 的所有者，也就是 root 用户的身份运行，从而就可以修改密码了。</p>
<p>但是使用 <code>SUID</code> 却带来了新的安全隐患，当我们运行设置了 <code>SUID</code> 的命令时，通常只是需要很小一部分的特权，但是 <code>SUID</code> 却给了它 root 具有的全部权限，一旦 被设置了 <code>SUID</code> 的命令出现漏洞，是不是就很容易被利用了。</p>
<p>为此 Linux 引入了 <code>Capabilities</code> 机制来对 root 权限进行了更加细粒度的控制，实现按需进行授权，这样就大大减小了系统的安全隐患。</p>
<h3 id="什么是-Capabilities"><a href="#什么是-Capabilities" class="headerlink" title="什么是 Capabilities"></a>什么是 Capabilities</h3><p>从内核 2.2 开始，Linux 将传统上与超级用户 root 关联的特权划分为不同的单元，称为 <code>capabilites</code>。<code>Capabilites</code> 每个单元都可以独立启用和禁用。这样当系统在作权限检查的时候就变成了：在执行特权操作时，如果进程的有效身份不是 <strong>root</strong>，就去检查是否具有该特权操作所对应的 <strong>capabilites</strong>，并以此决定是否可以进行该特权操作。比如如果我们要设置系统时间，就得具有 <code>CAP_SYS_TIME</code> 这个 capabilites。下面是从 <a href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank" rel="external">capabilities man page</a> 中摘取的 capabilites 列表：<br><img src="/images/capabilities.png" alt="capabilities"></p>
<h3 id="如何使用-Capabilities"><a href="#如何使用-Capabilities" class="headerlink" title="如何使用 Capabilities"></a>如何使用 Capabilities</h3><p>我们可以通过 <code>getcap</code> 和 <code>setcap</code> 两条命令来分别查看和设置程序文件的 <code>capabilities</code> 属性。比如当前我们是<code>zuiapp</code> 这个用户，使用 <code>getcap</code> 命令查看 <code>ping</code> 命令目前具有的 <code>capabilities</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ ll /bin/ping</span><br><span class="line">-rwxr-xr-x 1 root root 62088 11月  7 2016 /bin/ping</span><br><span class="line">❯ <span class="built_in">getcap</span> /bin/ping</span><br><span class="line">/bin/ping = <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw+p</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到具有 <code>cap_net_admin</code> 这个属性，所以我们现在可以执行 <code>ping</code> 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ ping team.jiunile.com</span><br><span class="line">PING icyxp.github.io (185.199.109.153): 56 data bytes</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=0 ttl=58 time=46.651 ms</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=1 ttl=58 time=35.604 ms</span><br></pre></td></tr></table></figure></p>
<p>但是如果我们把命令的 <code>capabilities</code> 属性移除掉：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❯ sudo <span class="built_in">setcap</span> <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw-p /bin/ping</span><br><span class="line">❯ <span class="built_in">getcap</span> /bin/ping</span><br><span class="line">/bin/ping =</span><br></pre></td></tr></table></figure></p>
<p>这个时候我们执行 <code>ping</code> 命令可以发现已经没有权限了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ ping team.jiunile.com</span><br><span class="line">ping: socket: Operation not permitted</span><br></pre></td></tr></table></figure></p>
<p>因为 ping 命令在执行时需要访问网络，所需的 <code>capabilities</code> 为 <code>cap_net_admin</code> 和 <code>cap_net_raw</code>，所以我们可以通过 <code>setcap</code> 命令可来添加它们：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❯ sudo <span class="built_in">setcap</span> <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw+p /bin/ping</span><br><span class="line">❯ <span class="built_in">getcap</span> /bin/ping</span><br><span class="line">/bin/ping = <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw+p</span><br><span class="line">❯ ping team.jiunile.com</span><br><span class="line">PING icyxp.github.io (185.199.109.153): 56 data bytes</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=0 ttl=58 time=46.651 ms</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=1 ttl=58 time=35.604 ms</span><br></pre></td></tr></table></figure></p>
<p>命令中的 <code>p</code> 表示 <code>Permitted</code> 集合(接下来会介绍)，<code>+</code> 号表示把指定的 <code>capabilities</code> 添加到这些集合中，<code>-</code> 号表示从集合中移除。</p>
<p>对于可执行文件的属性中有三个集合来保存三类 <code>capabilities</code>，它们分别是：</p>
<ul>
<li>Permitted：在进程执行时，Permitted 集合中的 capabilites 自动被加入到进程的 Permitted 集合中。</li>
<li>Inheritable：Inheritable 集合中的 capabilites 会与进程的 Inheritable 集合执行与操作，以确定进程在执行 execve 函数后哪些 capabilites 被继承。</li>
<li>Effective：Effective 只是一个 bit。如果设置为开启，那么在执行 execve 函数后，Permitted 集合中新增的 capabilities 会自动出现在进程的 Effective 集合中。</li>
</ul>
<p>对于进程中有五种 <code>capabilities</code> 集合类型，相比文件的 <code>capabilites</code>，进程的 <code>capabilities</code> 多了两个集合，分别是 <code>Bounding</code> 和 <code>Ambient</code>。</p>
<p>我们可以通过下面的命名来查看当前进程的 <code>capabilities</code> 信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ cat /proc/7029/status | grep <span class="string">'Cap'</span>  <span class="comment">#7029为PID</span></span><br><span class="line">CapInh:	0000000000000000</span><br><span class="line">CapPrm:	0000000000000000</span><br><span class="line">CapEff:	0000000000000000</span><br><span class="line">CapBnd:	0000001fffffffff</span><br><span class="line">CapAmb:	0000000000000000</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以使用 <code>capsh</code> 命令把它们转义为可读的格式，这样基本可以看出进程具有的 <code>capabilities</code> 了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ capsh --decode=0000001fffffffff</span><br><span class="line">0x0000001fffffffff=<span class="built_in">cap</span>_chown,<span class="built_in">cap</span>_dac_override,<span class="built_in">cap</span>_dac_<span class="built_in">read</span>_search,<span class="built_in">cap</span>_fowner,<span class="built_in">cap</span>_fsetid,<span class="built_in">cap</span>_<span class="built_in">kill</span>,<span class="built_in">cap</span>_setgid,<span class="built_in">cap</span>_setuid,<span class="built_in">cap</span>_setpcap,<span class="built_in">cap</span>_linux_immutable,<span class="built_in">cap</span>_net_<span class="built_in">bind</span>_service,<span class="built_in">cap</span>_net_broadcast,<span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw,<span class="built_in">cap</span>_ipc_lock,<span class="built_in">cap</span>_ipc_owner,<span class="built_in">cap</span>_sys_module,<span class="built_in">cap</span>_sys_rawio,<span class="built_in">cap</span>_sys_chroot,<span class="built_in">cap</span>_sys_ptrace,<span class="built_in">cap</span>_sys_pacct,<span class="built_in">cap</span>_sys_admin,<span class="built_in">cap</span>_sys_boot,<span class="built_in">cap</span>_sys_nice,<span class="built_in">cap</span>_sys_resource,<span class="built_in">cap</span>_sys_time,<span class="built_in">cap</span>_sys_tty_config,<span class="built_in">cap</span>_mknod,<span class="built_in">cap</span>_lease,<span class="built_in">cap</span>_audit_write,<span class="built_in">cap</span>_audit_control,<span class="built_in">cap</span>_setfcap,<span class="built_in">cap</span>_mac_override,<span class="built_in">cap</span>_mac_admin,<span class="built_in">cap</span>_syslog,35,36</span><br></pre></td></tr></table></figure></p>
<h2 id="Docker-Container-Capabilities"><a href="#Docker-Container-Capabilities" class="headerlink" title="Docker Container Capabilities"></a>Docker Container Capabilities</h2><p>我们说 Docker 容器本质上就是一个进程，所以理论上容器就会和进程一样会有一些默认的开放权限，默认情况下 Docker 会删除必须的 <code>capabilities</code> 之外的所有 <code>capabilities</code>，因为在容器中我们经常会以 root 用户来运行，使用 <code>capabilities</code> 现在后，容器中的使用的 root 用户权限就比我们平时在宿主机上使用的 root 用户权限要少很多了，这样即使出现了安全漏洞，也很难破坏或者获取宿主机的 root 权限，所以 Docker 支持 <code>Capabilities</code> 对于容器的安全性来说是非常有必要的。</p>
<p>不过我们在运行容器的时候可以通过指定 <code>--privileded</code> 参数来开启容器的超级权限，这个参数一定要慎用，因为他会获取系统 root 用户所有能力赋值给容器，并且会扫描宿主机的所有设备文件挂载到容器内部，所以是非常危险的操作。</p>
<p>但是如果你确实需要一些特殊的权限，我们可以通过 <code>--cap-add</code> 和 <code>--cap-drop</code> 这两个参数来动态调整，可以最大限度地保证容器的使用安全。下面表格中列出的 <code>Capabilities</code> 是 Docker 默认给容器添加的，我们可以通过 <code>--cap-drop</code> 去除其中一个或者多个：<br><img src="/images/docker-add-capabilities.png" alt="docker-add-capabilities"></p>
<p>下面表格中列出的 <code>Capabilities</code> 是 Docker 默认删除的，我们可以通过<code>--cap-add</code>添加其中一个或者多个：<br><img src="/images/docker-drop-capabilities.png" alt="docker-drop-capabilities"></p>
<blockquote>
<p><code>--cap-add</code>和<code>--cap-drop</code> 这两参数都支持<code>ALL</code>值，比如如果你想让某个容器拥有除了<code>MKNOD</code>之外的所有内核权限，那么可以执行下面的命令： <code>$ sudo docker run --cap-add=ALL --cap-drop=MKNOD ...</code></p>
</blockquote>
<p>比如现在我们需要修改网络接口数据，默认情况下是没有权限的，因为需要的 <code>NET_ADMIN</code> 这个 <code>Capabilities</code> 默认被移除了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ docker run -it --rm busybox /bin/sh</span><br><span class="line">/ <span class="comment"># ip link add dummy0 type dummy</span></span><br><span class="line">ip: RTNETLINK answers: Operation not permitted</span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>所以在不使用 <code>--privileged</code> 的情况下（不建议）我们可以使用 <code>--cap-add=NET_ADMIN</code> 将这个 <code>Capabilities</code> 添加回来：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❯ docker run -it --rm --cap-add=NET_ADMIN busybox /bin/sh</span><br><span class="line">/ <span class="comment"># ip link add dummy0 type dummy</span></span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到已经 OK 了。</p>
<h2 id="Kubernetes-配置-Capabilities"><a href="#Kubernetes-配置-Capabilities" class="headerlink" title="Kubernetes 配置 Capabilities"></a>Kubernetes 配置 Capabilities</h2><p>上面我介绍了在 Docker 容器下如何来配置 <code>Capabilities</code>，在 Kubernetes 中也可以很方便的来定义，我们只需要添加到 Pod 定义的 <code>spec.containers.sercurityContext.capabilities</code>中即可，也可以进行 <code>add</code> 和 <code>drop</code> 配置，同样上面的示例，我们要给 busybox 容器添加 <code>NET_ADMIN</code> 这个 <code>Capabilities</code>，对应的 YAML 文件可以这样定义：(cpb-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> cpb-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> cpb</span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> sleep</span><br><span class="line"><span class="bullet">    -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      capabilities:</span></span><br><span class="line"><span class="attr">        add:</span> <span class="comment"># 添加</span></span><br><span class="line"><span class="bullet">        -</span> NET_ADMIN</span><br><span class="line"><span class="attr">        drop:</span>  <span class="comment"># 删除</span></span><br><span class="line"><span class="bullet">        -</span> KILL</span><br></pre></td></tr></table></figure></p>
<p>我们在 <code>securityContext</code> 下面添加了 <code>capabilities</code> 字段，其中添加了 <code>NET_ADMIN</code> 并且删除了 <code>KILL</code> 这个默认的容器 <code>Capabilities</code>，这样我们就可以在 Pod 中修改网络接口数据了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❯ kubectl apply <span class="_">-f</span> cpb-demo.yaml</span><br><span class="line">❯ kubectl get pods</span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">cpb-demo                  1/1     Running   0          2m9s</span><br><span class="line">❯ kubectl <span class="built_in">exec</span> -it cpb-demo /bin/sh</span><br><span class="line">/ <span class="comment"># ip link add dummy0 type dummy</span></span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>在 Kubernetes 中通过 <code>sercurityContext.capabilities</code> 进行配置容器的 <code>Capabilities</code>，当然最终还是通过 Docker 的 <code>libcontainer</code> 去借助 <code>Linux kernel capabilities</code> 实现的权限管理。</p>
<h3 id="应用一：在kubernetes容器中使用perf命令"><a href="#应用一：在kubernetes容器中使用perf命令" class="headerlink" title="应用一：在kubernetes容器中使用perf命令"></a>应用一：在kubernetes容器中使用perf命令</h3><p>在容器中运行 <code>perf</code> 会报如下错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">┌─Error:───────────────────────────────────────────────────────────┐</span><br><span class="line">│No permission to <span class="built_in">enable</span> cycles event.                             │</span><br><span class="line">│                                                                  │</span><br><span class="line">│You may not have permission to collect system-wide stats.         │</span><br><span class="line">│                                                                  │</span><br><span class="line">│Consider tweaking /proc/sys/kernel/perf_event_paranoid,           │</span><br><span class="line">│<span class="built_in">which</span> controls use of the performance events system by            │</span><br><span class="line">│unprivileged users (without CAP_SYS_ADMIN).                       │</span><br><span class="line">│                                                                  │</span><br><span class="line">│The current value is 3:                                           │</span><br><span class="line">│                                                                  │</span><br><span class="line">│  -1: Allow use of (almost) all events by all users               │</span><br><span class="line">│&gt;= 0: Disallow raw tracepoint access by users without CAP_IOC_LOCK│</span><br><span class="line">│&gt;= 1: Disallow CPU event access by users without CAP_SYS_ADMIN    │</span><br><span class="line">│&gt;= 2: Disallow kernel profiling by users without C                │</span><br><span class="line">│                                                                  │</span><br><span class="line">│                                                                  │</span><br><span class="line">│Press any key...                                                  │</span><br><span class="line">└──────────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure></p>
<p>提示没有权限，需要更改 <code>/proc/sys/kernel/perf_event_paranoid</code> 文件，然后尝试执行:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 0 &gt; /proc/sys/kernel/perf_event_paranoid</span><br><span class="line">bash: /proc/sys/kernel/perf_event_paranoid: Read-only file system</span><br></pre></td></tr></table></figure></p>
<p>解决办法，修改容器的deployment文件，加入 <code>SYS_ADMIN</code><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">    capabilities:</span></span><br><span class="line"><span class="attr">        add:</span> [<span class="string">"SYS_ADMIN"</span>]</span><br></pre></td></tr></table></figure></p>
<p>参考链接:</p>
<ul>
<li><a href="https://www.cnblogs.com/sparkdev/p/11417781.html" target="_blank" rel="external">https://www.cnblogs.com/sparkdev/p/11417781.html</a></li>
<li><a href="https://hustcat.github.io/docker-config-capabilities/" target="_blank" rel="external">https://hustcat.github.io/docker-config-capabilities/</a></li>
<li><a href="https://blog.csdn.net/WaltonWang/article/details/62226738" target="_blank" rel="external">https://blog.csdn.net/WaltonWang/article/details/62226738</a></li>
<li><a href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank" rel="external">http://man7.org/linux/man-pages/man7/capabilities.7.html</a></li>
<li>www.qikqiak.com</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[golang如何取消子 goroutine]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-cancellation-of-goroutines.html</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前写了个工具，用于检测gitlab runner是否能承受住当前runner job的构建，根据Prometheus的监控，在资源使用过载的情况下，就临期启动服务器加入到集群中用于分担runner job构建时的压力。在运行一段时间后发现内存有时占用有点高（<code>goroutine</code>过多），于是就有了下面一步步的优化。<br> <a id="more"></a></p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先我们一开始有以下一段代码逻辑：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"sync"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">    wg sync.WaitGroup</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">	<span class="comment">//假设这个任务要干1000次，一次任务需要做2秒完成</span></span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-time.After(<span class="number">2</span> * time.Second):</span><br><span class="line">            fmt.Println(<span class="string">"Doing some work "</span>, i)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line"></span><br><span class="line">    wg.Add(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> work()</span><br><span class="line">    wg.Wait()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Finished. I'm going home"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing some work  0</span><br><span class="line">Doing some work  1</span><br><span class="line">Doing some work  2</span><br><span class="line">Doing some work  3</span><br><span class="line">...</span><br><span class="line">Doing some work  999</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>现在我们假设下我们调用的<code>work</code>这个方式是来自用户的交互或者一个http请求，我们可能不想一直等待直到<code>goroutine</code>完成，因此，常见的做法是采用超时机制，代码如下：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-time.After(<span class="number">2</span> * time.Second):</span><br><span class="line">            fmt.Println(<span class="string">"Doing some work "</span>, i)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line"></span><br><span class="line">    ch := <span class="built_in">make</span>(<span class="keyword">chan</span> error, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        ch &lt;- work()</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> err := &lt;-ch:</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            log.Fatal(<span class="string">"Something went wrong :("</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">case</span> &lt;-time.After(<span class="number">4</span> * time.Second):</span><br><span class="line">        fmt.Println(<span class="string">"等的不耐烦了，就这样吧..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Finished. I'm going home"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing some work  0</span><br><span class="line">Doing some work  1</span><br><span class="line">Life is to short to wait that long</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>现在情况比上一个好一些，main的执行不在需要等待work完成。</p>
<p>但上述代码还存在一些问题，比如这段代码写在一个http服务中，即使利用超时机制不等待<code>work</code>的完成，但<code>work</code> 这个<code>goroutine</code>还是会在后台一直运行并消耗资源。这时候就需要想个办法来取消这个子<code>goroutine</code>。于是我想到了<code>context</code> 这个包，于是又有了如下的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;sync&quot;</span><br><span class="line">    &quot;time&quot;</span><br><span class="line"></span><br><span class="line">    &quot;golang.org/x/net/context&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">var (</span><br><span class="line">    wg sync.WaitGroup</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func work(ctx context.Context) error &#123;</span><br><span class="line">    defer wg.Done()</span><br><span class="line"></span><br><span class="line">    for i := 0; i &lt; 1000; i++ &#123;</span><br><span class="line">        select &#123;</span><br><span class="line">        case &lt;-time.After(2 * time.Second):</span><br><span class="line">            fmt.Println(&quot;Doing some work &quot;, i)</span><br><span class="line"></span><br><span class="line">        // we received the signal of cancelation in this channel    </span><br><span class="line">        case &lt;-ctx.Done():</span><br><span class="line">            fmt.Println(&quot;Cancel the context &quot;, i)</span><br><span class="line">            return ctx.Err()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func main() &#123;   </span><br><span class="line">    ctx, cancel := context.WithTimeout(context.Background(), 4*time.Second)</span><br><span class="line">    defer cancel()</span><br><span class="line"></span><br><span class="line">    fmt.Println(&quot;Hey, I&apos;m going to do some work&quot;)</span><br><span class="line"></span><br><span class="line">    wg.Add(1)</span><br><span class="line">    go work(ctx)</span><br><span class="line">    wg.Wait()</span><br><span class="line"></span><br><span class="line">    fmt.Println(&quot;Finished. I&apos;m going home&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing some work  0</span><br><span class="line">Cancel the context  1</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>这看上去非常的好，代码看上去也易于管理超时，现在我们确保函数正常工作也不会浪费任何资源。现在为了让例子更加真实，我们在实际的http服务中来进行模拟。</p>
<p>以下是http server代码，模拟有部分概率会有慢响应：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="comment">// Lazy and Very Random Server </span></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"math/rand"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    http.HandleFunc(<span class="string">"/"</span>, LazyServer)</span><br><span class="line">    http.ListenAndServe(<span class="string">":1111"</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// sometimes really fast server, sometimes really slow server</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LazyServer</span><span class="params">(w http.ResponseWriter, req *http.Request)</span></span> &#123;</span><br><span class="line">    headOrTails := rand.Intn(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> headOrTails == <span class="number">0</span> &#123;</span><br><span class="line">        time.Sleep(<span class="number">6</span> * time.Second)</span><br><span class="line">        fmt.Fprintf(w, <span class="string">"Go! slow %v"</span>, headOrTails)</span><br><span class="line">        fmt.Printf(<span class="string">"Go! slow %v"</span>, headOrTails)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fmt.Fprintf(w, <span class="string">"Go! quick %v"</span>, headOrTails)</span><br><span class="line">    fmt.Printf(<span class="string">"Go! quick %v"</span>, headOrTails)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用curl来请求查看结果；<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://localhost:1111/</span><br><span class="line">Go! quick 1</span><br><span class="line">$ curl http://localhost:1111/</span><br><span class="line">Go! quick 1</span><br><span class="line">$ curl http://localhost:1111/</span><br><span class="line">*some seconds later*</span><br><span class="line">Go! slow 0</span><br></pre></td></tr></table></figure></p>
<p>现在，我们将在<code>goroutine</code>中向该服务器发出http请求，但是如果服务器速度较慢，我们将取消该请求并快速返回，以便我们可以管理取消并释放连接。 代码如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"io/ioutil"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    <span class="string">"sync"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"golang.org/x/net/context"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">    wg sync.WaitGroup</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// main is not changed</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    ctx, cancel := context.WithTimeout(context.Background(), <span class="number">2</span>*time.Second)</span><br><span class="line">    <span class="keyword">defer</span> cancel()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line"></span><br><span class="line">    wg.Add(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> work(ctx)</span><br><span class="line">    wg.Wait()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Finished. I'm going home"</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work</span><span class="params">(ctx context.Context)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">    tr := &amp;http.Transport&#123;&#125;</span><br><span class="line">    client := &amp;http.Client&#123;Transport: tr&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// anonymous struct to pack and unpack data in the channel</span></span><br><span class="line">    c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">        r   *http.Response</span><br><span class="line">        err error</span><br><span class="line">    &#125;, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    req, _ := http.NewRequest(<span class="string">"GET"</span>, <span class="string">"http://localhost:1111"</span>, <span class="literal">nil</span>)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        resp, err := client.Do(req)</span><br><span class="line">        fmt.Println(<span class="string">"Doing http request is a hard job"</span>)</span><br><span class="line">        pack := <span class="keyword">struct</span> &#123;</span><br><span class="line">            r   *http.Response</span><br><span class="line">            err error</span><br><span class="line">        &#125;&#123;resp, err&#125;</span><br><span class="line">        c &lt;- pack</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">        tr.CancelRequest(req)</span><br><span class="line">        &lt;-c <span class="comment">// Wait for client.Do</span></span><br><span class="line">        fmt.Println(<span class="string">"Cancel the context"</span>)</span><br><span class="line">        <span class="keyword">return</span> ctx.Err()</span><br><span class="line">    <span class="keyword">case</span> ok := &lt;-c:</span><br><span class="line">        err := ok.err</span><br><span class="line">        resp := ok.r</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Println(<span class="string">"Error "</span>, err)</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">defer</span> resp.Body.Close()</span><br><span class="line">        out, _ := ioutil.ReadAll(resp.Body)</span><br><span class="line">        fmt.Printf(<span class="string">"Server Response: %s\n"</span>, out)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing http request is a hard job</span><br><span class="line">Server Response: Go! quick 1</span><br><span class="line">Finished. I'</span>m going home</span><br><span class="line"></span><br><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing http request is a hard job</span><br><span class="line">Cancel the context</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>如您在输出中所看到的，我们避免了服务器的缓慢响应。在客户端中，tcp连接已取消，因此不会忙于等待响应缓慢，因此我们不会浪费资源。</p>
<p>还有一个例子，有一个常驻的任务，即要控制<code>goroutine</code>的增长，又需要防止在<code>goroutine</code>超时后<code>goroutine</code>在后台运行造成资源的浪费，让我们来看下如何实现：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">	<span class="string">"golang.org/x/net/context"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">//var wg sync.WaitGroup</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work1</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//defer wg.Done()</span></span><br><span class="line">	ch &lt;- <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//任务超过3秒就超时</span></span><br><span class="line">	ctx, cancel := context.WithTimeout(context.Background(), <span class="number">3</span>*time.Second)</span><br><span class="line">	<span class="keyword">defer</span> cancel()</span><br><span class="line">	chT := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> <span class="built_in">close</span>(chT)</span><br><span class="line">		<span class="comment">//具体的任务，这里模拟做的任务需要5秒完成</span></span><br><span class="line">		time.Sleep(time.Second * <span class="number">5</span>)</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> &lt;-chT:</span><br><span class="line">		fmt.Println(<span class="string">"job1 finsh..."</span>, runtime.NumGoroutine())</span><br><span class="line">	<span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">		fmt.Println(<span class="string">"job1 timeout..."</span>, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Println(<span class="string">"job1 exit.."</span>)</span><br><span class="line">	&lt;-ch  <span class="comment">//释放chanel</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work2</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//defer wg.Done()</span></span><br><span class="line">	ch &lt;- <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">	ctx, cancel := context.WithTimeout(context.Background(), <span class="number">3</span>*time.Second)</span><br><span class="line">	<span class="keyword">defer</span> cancel()</span><br><span class="line">	chT := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> <span class="built_in">close</span>(chT)</span><br><span class="line">		<span class="comment">//具体的任务，这里模拟做的任务需要1秒完成</span></span><br><span class="line">		time.Sleep(time.Second * <span class="number">1</span>)</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> &lt;-chT:</span><br><span class="line">		fmt.Println(<span class="string">"job2 finsh..."</span>, runtime.NumGoroutine())</span><br><span class="line">	<span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">		fmt.Println(<span class="string">"job2 timeout..."</span>, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&lt;-ch <span class="comment">//释放chanel</span></span><br><span class="line">	fmt.Println(<span class="string">"job2 exit.."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//控制goroutine数量</span></span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//永久运行</span></span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="comment">//因为是永久运行，所以这里的sync.Waitgroup可以不再需要</span></span><br><span class="line">		<span class="comment">//wg.Add(2)</span></span><br><span class="line">		<span class="keyword">go</span> work1(ch)</span><br><span class="line">		<span class="keyword">go</span> work2(ch)</span><br><span class="line">		time.Sleep(<span class="number">2</span> * time.Second)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//wg.Wait()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>希望以上例子可以给你带来一些帮助！Happy coding gophers!.</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 程序占用大量内存问题分析]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-debug-memory.html</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在运行一段时间的go程序后内存竟然达到4G左右，几乎可以肯定是由于某段方法操作不规范引起的问题，于是对go程序进行分析<br><a id="more"></a></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>分析内存光靠手撕代码是比较困难的，总要借助一些工具。Golang <code>pprof</code>是Go官方的<code>profiling</code>工具，非常强大，使用起来也很方便。</p>
<p>代码改造：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> _ <span class="string">"net/http/pprof"</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">	http.ListenAndServe(<span class="string">"0.0.0.0:8888"</span>, <span class="literal">nil</span>)</span><br><span class="line"> &#125;()</span><br></pre></td></tr></table></figure></p>
<p>改造后程序启动后，浏览器中输入<a href="http://ip:8899/debug/pprof/就可以看到一个汇总分析页面，显示如下信息：" target="_blank" rel="external">http://ip:8899/debug/pprof/就可以看到一个汇总分析页面，显示如下信息：</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/debug/pprof/</span><br><span class="line"></span><br><span class="line">profiles:</span><br><span class="line">0    block</span><br><span class="line">32    goroutine</span><br><span class="line">552    heap</span><br><span class="line">0    mutex</span><br><span class="line">51    threadcreate</span><br><span class="line"></span><br><span class="line">full goroutine stack dump</span><br></pre></td></tr></table></figure></p>
<p>点击heap，在汇总分析页面的最上方可以看到如下图所示，红色箭头所指的就是当前已经使用的堆内存是25M！！<br><img src="/images/go/debug_memory_1.png" alt="内存分析"></p>
<p>接下来我们需要借助<code>go tool pprof</code>来分析：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof -inuse_space http://本机Ip:8888/debug/pprof/heap</span><br></pre></td></tr></table></figure></p>
<p>这个命令进入后，是一个类似<code>gdb</code>的交互式界面，输入 <code>top</code> 命令可以前10大的内存分配，<code>flat</code> 是堆栈中当前层的inuse内存值，cum是堆栈中本层级的累计inuse内存值（包括调用的函数的inuse内存值，上面的层级）<br><img src="/images/go/debug_memory_2.png" alt="内存分析"></p>
<p>可以看到，<code>bytes.makeSlice</code>这个内置方法竟然使用了24M内存，继续往下看，可以看到<code>ReadFrom</code>这个方法，搜了一下，发现 <code>ioutil.ReadAll()</code> 里会调用 <code>bytes.Buffer.ReadFrom</code>, 而 <code>bytes.Buffer.ReadFrom</code> 会进行 <code>makeSlice</code>。再回头看一下<code>io/ioutil.readAll</code>的代码实现，<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readAll</span><span class="params">(r io.Reader, capacity <span class="keyword">int64</span>)</span> <span class="params">(b []<span class="keyword">byte</span>, err error)</span></span> &#123;</span><br><span class="line">    buf := bytes.NewBuffer(<span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">0</span>, capacity))</span><br><span class="line">    <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        e := <span class="built_in">recover</span>()</span><br><span class="line">        <span class="keyword">if</span> e == <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> panicErr, ok := e.(error); ok &amp;&amp; panicErr == bytes.ErrTooLarge &#123;</span><br><span class="line">            err = panicErr</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">panic</span>(e)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line">    _, err = buf.ReadFrom(r)</span><br><span class="line">    <span class="keyword">return</span> buf.Bytes(), err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// bytes.MinRead = 512</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadAll</span><span class="params">(r io.Reader)</span> <span class="params">([]<span class="keyword">byte</span>, error)</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> readAll(r, bytes.MinRead)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，<code>ioutil.ReadAll</code> 每次都会分配初始化一个大小为 <code>bytes.MinRead</code> 的 buffer ，<code>bytes.MinRead</code> 在 Golang 里是一个常量，值为 <code>512</code> 。就是说每次调用 <code>ioutil.ReadAll</code> 都会分配一块大小为 <code>512</code> 字节的内存，目前看起来是正常的，但我们再看一下<code>ReadFrom</code>的实现，<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ReadFrom reads data from r until EOF and appends it to the buffer, growing</span></span><br><span class="line"><span class="comment">// the buffer as needed. The return value n is the number of bytes read. Any</span></span><br><span class="line"><span class="comment">// error except io.EOF encountered during the read is also returned. If the</span></span><br><span class="line"><span class="comment">// buffer becomes too large, ReadFrom will panic with ErrTooLarge.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(b *Buffer)</span> <span class="title">ReadFrom</span><span class="params">(r io.Reader)</span> <span class="params">(n <span class="keyword">int64</span>, err error)</span></span> &#123;</span><br><span class="line">    b.lastRead = opInvalid</span><br><span class="line">    <span class="comment">// If buffer is empty, reset to recover space.</span></span><br><span class="line">    <span class="keyword">if</span> b.off &gt;= <span class="built_in">len</span>(b.buf) &#123;</span><br><span class="line">        b.Truncate(<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> free := <span class="built_in">cap</span>(b.buf) - <span class="built_in">len</span>(b.buf); free &lt; MinRead &#123;</span><br><span class="line">            <span class="comment">// not enough space at end</span></span><br><span class="line">            newBuf := b.buf</span><br><span class="line">            <span class="keyword">if</span> b.off+free &lt; MinRead &#123;</span><br><span class="line">                <span class="comment">// not enough space using beginning of buffer;</span></span><br><span class="line">                <span class="comment">// double buffer capacity</span></span><br><span class="line">                newBuf = makeSlice(<span class="number">2</span>*<span class="built_in">cap</span>(b.buf) + MinRead)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">copy</span>(newBuf, b.buf[b.off:])</span><br><span class="line">            b.buf = newBuf[:<span class="built_in">len</span>(b.buf)-b.off]</span><br><span class="line">            b.off = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        m, e := r.Read(b.buf[<span class="built_in">len</span>(b.buf):<span class="built_in">cap</span>(b.buf)])</span><br><span class="line">        b.buf = b.buf[<span class="number">0</span> : <span class="built_in">len</span>(b.buf)+m]</span><br><span class="line">        n += <span class="keyword">int64</span>(m)</span><br><span class="line">        <span class="keyword">if</span> e == io.EOF &#123;</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> n, e</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> n, <span class="literal">nil</span> <span class="comment">// err is EOF, so return nil explicitly</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个函数主要作用就是从 <code>io.Reader</code> 里读取的数据放入 <code>buffer</code> 中，如果 buffer 空间不够，就按照每次 <code>2x + MinRead</code> 的算法递增，这里 <code>MinRead</code> 的大小也是 512 Bytes ，也就是说如果我们一次性读取的文件过大，就会导致所使用的内存倍增，假设我们的爬虫文件总共有500M,那么所用的内存就有500M * 2 + 512B，况且爬虫文件中还带了那么多log文件，那看看crawlab源码中是哪一段使用<code>ioutil.ReadAll</code>读了爬虫文件，定位到了这里：<br><img src="/images/go/debug_memory_3.jpg" alt="内存分析"></p>
<p>这里直接将全部的文件内容，以二进制的形式读了进来，导致内存倍增，令人窒息的操作。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>其实在读大文件的时候，把文件内容全部读到内存，直接就翻车了，正确是处理方法有两种</p>
<h3 id="流式处理"><a href="#流式处理" class="headerlink" title="流式处理"></a>流式处理</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadFile</span><span class="params">(filePath <span class="keyword">string</span>, handle <span class="keyword">func</span>(<span class="keyword">string</span>)</span>) <span class="title">error</span></span> &#123;</span><br><span class="line">    f, err := os.Open(filePath)</span><br><span class="line">    <span class="keyword">defer</span> f.Close()</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    buf := bufio.NewReader(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        line, err := buf.ReadLine(<span class="string">"\n"</span>)</span><br><span class="line">        line = strings.TrimSpace(line)</span><br><span class="line">        handle(line)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> err == io.EOF&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="分片处理"><a href="#分片处理" class="headerlink" title="分片处理"></a>分片处理</h3><blockquote>
<p>当读取的是二进制文件，没有换行符的时候，使用这种方案比较合适</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadBigFile</span><span class="params">(fileName <span class="keyword">string</span>, handle <span class="keyword">func</span>([]<span class="keyword">byte</span>)</span>) <span class="title">error</span></span> &#123;</span><br><span class="line">    f, err := os.Open(fileName)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"can't opened this file"</span>)</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> f.Close()</span><br><span class="line">    s := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">4096</span>)</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">switch</span> nr, err := f.Read(s[:]); <span class="literal">true</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> nr &lt; <span class="number">0</span>:</span><br><span class="line">            fmt.Fprintf(os.Stderr, <span class="string">"cat: error reading: %s\n"</span>, err.Error())</span><br><span class="line">            os.Exit(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">case</span> nr == <span class="number">0</span>: <span class="comment">// EOF</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">        <span class="keyword">case</span> nr &gt; <span class="number">0</span>:</span><br><span class="line">            handle(s[<span class="number">0</span>:nr])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>参考：</strong></p>
<ul>
<li>juejin.im/post/5d5be347f265da03b94ff66b</li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[慎用<-time.After()，容易导致内存泄漏]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-select-timer.html</url>
      <content type="html"><![CDATA[<h2 id="问题代码"><a href="#问题代码" class="headerlink" title="问题代码"></a>问题代码</h2><a id="more"></a>
<blockquote>
<p>顺带说明下select 监听是如何工作的</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//go start</span></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> &#123;</span><br><span class="line">			timerC := time.After(<span class="number">2</span> * time.Second)</span><br><span class="line">			<span class="comment">//timerC 每次都是重新创建的，什么意思呢？简单说来，当 select 成功监听 ch 并进入它的处理分支，下次循环 timerC 重新创建了，时间肯定就重置了。</span></span><br><span class="line">			<span class="keyword">select</span> &#123;</span><br><span class="line">			<span class="comment">//如果有多个 case 都可以运行，select 会随机公平选择出一个执行。其余的则不会执行</span></span><br><span class="line">			<span class="keyword">case</span> num := &lt;-ch:</span><br><span class="line">				fmt.Println(<span class="string">"get num is"</span>, num)</span><br><span class="line">			<span class="keyword">case</span> &lt;-timerC:</span><br><span class="line">				<span class="comment">//等价于 case &lt;-time.After(2 * time.Second)</span></span><br><span class="line">				fmt.Println(<span class="string">"time's up!!!"</span>)</span><br><span class="line">				<span class="comment">//done&lt;-true</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">5</span>; i++ &#123;</span><br><span class="line">		ch &lt;- i</span><br><span class="line">		time.Sleep(time.Second * <span class="number">2</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h2><p>以上代码会导致内存泄漏，其罪魁祸首是<code>&lt;-time.After()</code>，在官方文档中有此说明：如果定时器没有到达定时时间，<code>gc</code> 就不会启动垃圾回收。标准库文档中有说明：</p>
<blockquote>
<p>The underlying Timer is not recovered by the garbage collector until the timer fires</p>
</blockquote>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>在不使用 <code>time.After</code> 来实现超时的前提下，可通过创建 <code>timer</code> 配合 <code>reset</code> 来实现超时机制，具体代码示例如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">	idleDuration := <span class="number">2</span> * time.Second</span><br><span class="line">	idleDelay := time.NewTimer(idleDuration)</span><br><span class="line">	<span class="keyword">defer</span> idleDelay.Stop()</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		idleDelay.Reset(idleDuration)</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> num := &lt;-ch:</span><br><span class="line">			fmt.Println(<span class="string">"get num is"</span>, num)</span><br><span class="line">		<span class="keyword">case</span> &lt;-idleDelay.C:</span><br><span class="line">			fmt.Println(<span class="string">"time's up!!!"</span>)</span><br><span class="line">			<span class="comment">//done&lt;-ture</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">....</span><br></pre></td></tr></table></figure></p>
<p><strong>参考：</strong></p>
<ul>
<li>studygolang.com/articles/22617</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 陷阱系列]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-trap.html</url>
      <content type="html"><![CDATA[<h2 id="陷阱一-interface问题"><a href="#陷阱一-interface问题" class="headerlink" title="陷阱一 interface问题"></a>陷阱一 interface问题</h2><a id="more"></a>
<h3 id="问题代码"><a href="#问题代码" class="headerlink" title="问题代码"></a>问题代码</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//DetaildError ...</span></span><br><span class="line"><span class="keyword">type</span> DetaildError <span class="keyword">struct</span> &#123;</span><br><span class="line">	code    <span class="keyword">int</span></span><br><span class="line">	message <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e DetaildError)</span> <span class="title">Error</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> fmt.Sprintf(<span class="string">"Error occured at (%d,%s)"</span>, e.code, e.message)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">handle</span><span class="params">(x <span class="keyword">int</span>)</span> *<span class="title">DetaildError</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> x != <span class="number">1</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> &amp;DetaildError&#123;code: <span class="number">1000</span>, message: <span class="string">"whoami?"</span>&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> err error</span><br><span class="line">	err = handle(<span class="number">0</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"I am Error 1 of %s\n"</span>, err)</span><br><span class="line">	&#125;</span><br><span class="line">	err = handle(<span class="number">1</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"I am Error 2 of %s\n"</span>, err)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I am Error 1 of Error occured at (1000,whoami?)</span><br><span class="line">I am Error 2 of &lt;nil&gt;</span><br></pre></td></tr></table></figure>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><blockquote>
<p>err 是一个接口，接口在Go中保存了两个值，一个是<code>类型T</code>，一个<code>值V</code><br>只有当<code>T</code>和<code>V</code> <strong>同时</strong> 为<code>nil</code>时，接口才是<code>nil</code><br>在Go中，接口是隐式实现，<strong>因此当我们用一个接口类型去接收一个nil结构体的时候，那么这个接口将不再是nil</strong><br>此时的err值为<code>(T=*DetaildError, V=nil)</code>，不满足接口为nil条件</p>
</blockquote>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><h4 id="修正一：将var-err-Error-修改为接收Struct-而不是接口"><a href="#修正一：将var-err-Error-修改为接收Struct-而不是接口" class="headerlink" title="修正一：将var err Error 修改为接收Struct 而不是接口"></a>修正一：将var err Error 修改为接收Struct 而不是接口</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> err *DetaildError</span><br></pre></td></tr></table></figure>
<h4 id="修正二：将handle方法的返回类型为interface"><a href="#修正二：将handle方法的返回类型为interface" class="headerlink" title="修正二：将handle方法的返回类型为interface"></a>修正二：将handle方法的返回类型为interface</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">handle</span><span class="params">(x <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> x != <span class="number">1</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> &amp;DetaildError&#123;code: <span class="number">1000</span>, message: <span class="string">"whoami?"</span>&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>参考：</strong></p>
<ul>
<li>mp.weixin.qq.com/s/0bJOzNxoQhdVjFOunhmVKQ</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[控制 Goroutine 的并发数量]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-control-goroutine-number.html</url>
      <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在 Go 语言中创建协程（Goroutine）的成本非常低，因此稍不注意就可能创建出大量的协程，一方面会造成资源不断增长负载变高，另一方面不容易控制这些协程的状态。</p>
<p>不过，“能力越大，越需要克制”。网络上已经存在一些讲控制 Goroutine 数目的文章，本文通过图示的方式再简单总结一下其基本理念，以便于记忆。</p>
<a id="more"></a>
<h2 id="Goroutine-数量不受控示例"><a href="#Goroutine-数量不受控示例" class="headerlink" title="Goroutine 数量不受控示例"></a>Goroutine 数量不受控示例</h2><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"sync"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	jobsCount := <span class="number">10</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; jobsCount; j++ &#123;</span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">go</span> do(j)</span><br><span class="line">		fmt.Printf(<span class="string">"index: %d,goroutine Num: %d \n"</span>, j, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Println(<span class="string">"done!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">do</span><span class="params">(i <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">"hello %d!\n"</span>, i)</span><br><span class="line">	time.Sleep(time.Second * <span class="number">2</span>) <span class="comment">// 刻意睡 2 秒钟，模拟耗时</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码假设有 <code>jobsCount</code> 个任务，通过 <code>for-range 给每个任务创建了一个 Goroutine</code>。为了让主协程等待所有的子协程执行完毕后再退出，使用 <code>sync.WaitGroup</code> 监控所有协程的状态，从而保证主协程结束时所有的子协程已经退出。为了说明问题，上面的代码还输出了 <code>runtime.NumGoroutine()</code> 的值用以表征协程的数量。</p>
<p>运行上面的代码，可以得到类似下面的输出。从下面的输出中我们可以得到两点信息：① 协程的执行顺序是随机的（比如 hello 3 在 hello 6 后面出现）；② 协程的数量递增，最后到了 11 个之多。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">index: 0,goroutine Num: 2</span><br><span class="line">hello 0!</span><br><span class="line">index: 1,goroutine Num: 3</span><br><span class="line">index: 2,goroutine Num: 4</span><br><span class="line">hello 1!</span><br><span class="line">index: 3,goroutine Num: 5</span><br><span class="line">index: 4,goroutine Num: 6</span><br><span class="line">index: 5,goroutine Num: 7</span><br><span class="line">index: 6,goroutine Num: 8</span><br><span class="line">hello 6!</span><br><span class="line">hello 3!</span><br><span class="line">index: 7,goroutine Num: 9</span><br><span class="line">hello 7!</span><br><span class="line">index: 8,goroutine Num: 10</span><br><span class="line">index: 9,goroutine Num: 11</span><br><span class="line">hello 2!</span><br><span class="line">hello 9!</span><br><span class="line">hello 8!</span><br><span class="line">hello 4!</span><br><span class="line"><span class="keyword">done</span>!</span><br><span class="line">hello 5!</span><br></pre></td></tr></table></figure></p>
<h3 id="Goroutine-数量不受控制的图示"><a href="#Goroutine-数量不受控制的图示" class="headerlink" title="Goroutine 数量不受控制的图示"></a>Goroutine 数量不受控制的图示</h3><p>我们应该怎么理解例一的代码呢？</p>
<p>假如 CPU 只有 <strong>两个</strong> 核，下图展示了为每个 job 创建一个 goroutine 的情况（换句话说，goroutine 的数量是不受控制的）。此种情况虽然生成了很多的 goroutine，但是 <strong><code>每个 CPU 核上同一时间只能执行一个 goroutine</code></strong> ；当 job 很多且生成了相应数目的 goroutine 后，会出现很多等待执行的 goroutine，从而造成资源上的浪费。</p>
<p><img src="/images/go/figure-goroutine-controll-1.png" alt="goroutine"></p>
<h2 id="Goroutine-数量受控制示例"><a href="#Goroutine-数量受控制示例" class="headerlink" title="Goroutine 数量受控制示例"></a>Goroutine 数量受控制示例</h2><h3 id="Goroutine-数量受到限制的图示"><a href="#Goroutine-数量受到限制的图示" class="headerlink" title="Goroutine 数量受到限制的图示"></a>Goroutine 数量受到限制的图示</h3><p>给每个 job 生成一个 goroutine 的方式显得粗暴了很多，那么可以通过什么样的方式控制 goroutine 的数目呢？其实上面的代码通过一个 for-range 循环完成了两件事情：①为每个 job 创建 goroutine；②把任务相关的标识传给相应的 goroutine 执行。为了控制 goroutine 的数目，可以通过 buffered channel 给每个 goroutine 传递任务相关的信息 或者 可以把上面的两个过程拆分开：a）先通过一个 for-range 循环创建指定数目的 goroutine，b）然后通过 channel/buffered channel 给每个 goroutine 传递任务相关的信息（这里的channel是否缓冲无所谓，主要用到的是 channel 的线程安全特性）。如下图所示。</p>
<p><img src="/images/go/figure-goroutine-controll-2.png" alt="goroutine"></p>
<h3 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h3><p>方式一：通过有阻塞的 <code>buffer channel</code> 来控制 <code>goroutine</code> 增长<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"sync"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	jobsCount := <span class="number">10</span></span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>, <span class="number">2</span>) <span class="comment">//通过 channel 控制 goroutine 数量，防止无休止增长</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; jobsCount; j++ &#123;</span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line">		ch &lt;- <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">go</span> do(ch, j)</span><br><span class="line">		fmt.Printf(<span class="string">"index: %d,goroutine Num: %d \n"</span>, j, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	wg.Wait()</span><br><span class="line">	fmt.Println(<span class="string">"done!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">do</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">bool</span>, i <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">"hello %d!\n"</span>, i)</span><br><span class="line">	time.Sleep(time.Second * <span class="number">2</span>) <span class="comment">// 刻意睡 2 秒钟，模拟耗时</span></span><br><span class="line"></span><br><span class="line">	&lt;-ch  <span class="comment">//必须在任务完成后从channel取出，通过channel来阻塞</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>方式二：通过拆任务实现：</p>
<ul>
<li>a）先通过一个 for-range 循环创建指定数目的 goroutine</li>
<li>b）然后通过 channel/buffered channel 给每个 goroutine 传递任务相关的信息</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"sync"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	jobsCount := <span class="number">10</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> jobsChan = <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// a) 生成指定数目的 goroutine，每个 goroutine 消费 jobsChan 中的数据</span></span><br><span class="line">	poolCount := <span class="number">2</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; poolCount; i++ &#123;</span><br><span class="line">		<span class="keyword">go</span> job(jobsChan)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// b) 把 job 依次推送到 jobsChan 供 goroutine 消费</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; jobsCount; i++ &#123;</span><br><span class="line">		jobsChan &lt;- i</span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line">		fmt.Printf(<span class="string">"index: %d,goroutine Num: %d\n"</span>, i, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	wg.Wait()</span><br><span class="line">	</span><br><span class="line">	fmt.Println(<span class="string">"done!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">job</span><span class="params">(jobsChan <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> j := <span class="keyword">range</span> jobsChan &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"hello %d\n"</span>, j)</span><br><span class="line">		time.Sleep(time.Second * <span class="number">2</span>) <span class="comment">// 刻意睡 2 秒钟，模拟耗时</span></span><br><span class="line">		wg.Done()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行上面的代码可以得到下面类似的输出（可以看到 goroutine 的数量控制在了 3 个）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hello 0!</span><br><span class="line">index: 0,goroutine Num: 2</span><br><span class="line">index: 1,goroutine Num: 3</span><br><span class="line">hello 1!</span><br><span class="line">hello 2!</span><br><span class="line">index: 2,goroutine Num: 3</span><br><span class="line">index: 3,goroutine Num: 3</span><br><span class="line">hello 3!</span><br><span class="line">index: 4,goroutine Num: 3</span><br><span class="line">hello 4!</span><br><span class="line">index: 5,goroutine Num: 3</span><br><span class="line">hello 5!</span><br><span class="line">index: 6,goroutine Num: 3</span><br><span class="line">index: 7,goroutine Num: 3</span><br><span class="line">hello 7!</span><br><span class="line">hello 6!</span><br><span class="line">index: 8,goroutine Num: 3</span><br><span class="line">index: 9,goroutine Num: 3</span><br><span class="line">hello 8!</span><br><span class="line">hello 9!</span><br><span class="line"><span class="keyword">done</span>!</span><br></pre></td></tr></table></figure></p>
<p>参考：</p>
<ul>
<li>jingwei.link</li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Podman、Skopeo和Buildah下一代容器代替Docker]]></title>
      <url>http://team.jiunile.com/blog/2019/09/containers-podman-skopeo-buildah.html</url>
      <content type="html"><![CDATA[<h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>在Docker实践中，很多人应该都遇到过开机重启时，由于Docker守护程序在占用多核CPU使用100%C使用的情况，此时所有容器都无法正常工作，所有服务都不能用。解决唯一方法只能杀掉所有容器并重启守护进程，才能恢复。经过了解该问题是由于Docker守护进程引起，而且Docker守护进程是以root特权权限启动的，是一个安全问题，那么有什么方法解决呢？本文介绍一下基于CRI 等标准（Docker新架构也符合CRI标准）的新一代容器工具<code>Podman</code>、<code>Skopero</code>和<code>Buiddah</code>套件。</p>
<h2 id="OCI"><a href="#OCI" class="headerlink" title="OCI"></a>OCI</h2><p>为了防止容器被Docker一家垄断，巨头们（谷歌，Redhat、微软、IBM、Intel、思科）聚在一起决定要成立一个组织（OCI），大家一起商量指定了一套规范（CRI、CNI），大家一致统一只兼容符合这套规范的工具。Docker虽然心有不甘但是毕竟胳膊拧不过大腿，只能该架构兼容规范。</p>
<p>在该规范的指导下就有了本文的三个主人公，这三个工具都是符合OCI计划下的工具（<a href="https://github.com/containers" target="_blank" rel="external">https://github.com/containers</a>）。他们主要是由RedHat推动，三者各司其职，配合完成Docker所有的功能和新扩展功能，并且对docker的问题进行了改良：包括不需要守护程序或访问有root权限的组；容器架构基于fork/exec模型创建容器，更加安全可靠；所以是更先进、高效和安全的下一代容器容器工具。</p>
<a id="more"></a>
<h2 id="Podman"><a href="#Podman" class="headerlink" title="Podman"></a>Podman</h2><p><img src="/images/containers/podman.jpg" alt="podman"></p>
<p><a href="https://github.com/containers/libpod" target="_blank" rel="external">Podman</a>是该工具套件的核心，用来替换Docker中了大多数子命令（RUN，PUSH，PULL等）。Podman无需守护进程，使用用户命名空间来模拟容器中的root，无需连接到具有root权限的套接字保证容器的体系安全。</p>
<p>Podman专注于维护和修改OCI镜像的所有命令和功能，例如拉动和标记。它还允许我们创建，运行和维护从这些镜像创建的容器。</p>
<h2 id="Buildah"><a href="#Buildah" class="headerlink" title="Buildah"></a>Buildah</h2><p><a href="https://github.com/containers/buildah" target="_blank" rel="external">Buildah</a>用来构建OCI图像。虽然Podman也可以用户构建Docker镜像，但是构建速度超慢，并且默认情况下使用vfs存储驱动程序会耗尽大量磁盘空间。 buildah bud（使用Dockerfile构建）则会非常快，并使用覆盖存储驱动程序。</p>
<p><img src="/images/containers/buildah.jpg" alt="buildah"></p>
<p>Buildah专注于构建OCI镜像。 Buildah的命令复制了Dockerfile中的所有命令。可以使用Dockerfiles构建镜像，并且不需要任何root权限。 Buildah的最终目标是提供更低级别的coreutils界面来构建图像。Buildah也支持非Dockerfiles构建镜像，可以允许将其他脚本语言集成到构建过程中。 Buildah遵循一个简单的fork-exec模型，不以守护进程运行，但它基于golang中的综合API，可以存储到其他工具中。</p>
<h2 id="Skopeo"><a href="#Skopeo" class="headerlink" title="Skopeo"></a>Skopeo</h2><p><a href="https://github.com/containers/skopeo" target="_blank" rel="external">Skopeo</a>是一个工具，允许我们通过推，拉和复制镜像来处理Docker和OC镜像。</p>
<p><img src="/images/containers/skopeo.jpg" alt="skopeo"></p>
<h2 id="三个工具对比"><a href="#三个工具对比" class="headerlink" title="三个工具对比"></a>三个工具对比</h2><p>Buildah构建容器，Podman运行容器，Skopeo传输容器镜像。这些都是由Github容器组织维护的开源工具（<a href="https://github.com/containers" target="_blank" rel="external">https://github.com/containers</a>）。这些工具都不需要运行守护进程，并且大多数情况下也不需要root访问权限。</p>
<p>Podman和Buildah之间的一个主要区别是他们的容器概念。 Podman允许用户创建”传统容器”。虽然Buildah容器实际上只是为了允许将内容添加回容器图像而创建的。一种简单方法是buildah run命令模拟Dockerfile中的RUN命令，而podman run命令模拟功能中的docker run命令。</p>
<p>总之，Buildah是创建OCI镜像的有效方式，而Podman允许我们使用熟悉的容器cli命令在生产环境中管理和维护这些镜像和容器。</p>
<h2 id="容器迁移"><a href="#容器迁移" class="headerlink" title="容器迁移"></a>容器迁移</h2><h3 id="套件安装"><a href="#套件安装" class="headerlink" title="套件安装"></a>套件安装</h3><p>各大Linux发行版都提供了二进制安装包， 可以使用发行版的系统包管理工具一键安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Fedora, CentOS：sudo yum -y install podman</span><br><span class="line">Arch &amp; Manjaro Linux： sudo pacman -S podman</span><br></pre></td></tr></table></figure></p>
<p>Ubuntu不支持一键安装，需要先添加第三方私有ppa仓库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update -qq</span><br><span class="line">sudo apt-get install -qq -y software-properties-common uidmap</span><br><span class="line">sudo add-apt-repository -y ppa:projectatomic/ppa</span><br><span class="line">sudo apt-get update -qq</span><br><span class="line">sudo apt-get -qq -y install podman</span><br></pre></td></tr></table></figure></p>
<h2 id="实践迁移"><a href="#实践迁移" class="headerlink" title="实践迁移"></a>实践迁移</h2><p>安装了套件的三个工具后，就可以对docker实例进行迁移了。</p>
<blockquote>
<p>替换cron或者CI作业（脚本）中的所有docker实例，把docker替换为podman，可以使用后面提到的别名的方式。</p>
</blockquote>
<h3 id="1、停止和删除所有的运行的docker。"><a href="#1、停止和删除所有的运行的docker。" class="headerlink" title="1、停止和删除所有的运行的docker。"></a>1、停止和删除所有的运行的docker。</h3><p>为了确保没有有差错，可以使用sysdig来捕获系统中docker的引用，看看是否还有其他东西在调用docker。</p>
<h3 id="2、删除docker"><a href="#2、删除docker" class="headerlink" title="2、删除docker"></a>2、删除docker</h3><p>现在就可以删除docker了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum remove docker || apt remove -y docker-ce</span><br></pre></td></tr></table></figure></p>
<h3 id="3、环境清理"><a href="#3、环境清理" class="headerlink" title="3、环境清理"></a>3、环境清理</h3><p>最后清理下docker文件，我们建个一目录docker备份目录，把以下目录mv到备份目录即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/docker/*，/etc/default/docker和/var/lib/ docker中的任何遗留文件</span><br></pre></td></tr></table></figure></p>
<p>删除docker组<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delgroup docker</span><br></pre></td></tr></table></figure></p>
<p>现在可以吧docker别名成podman来无缝使用了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> docker = podman</span><br></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了符合CIR标准的 cri-o 容器套件Podman，Skopeo和Buildah。该新一代容器套件架构基于*nix传统的fork-exec模型，解决了由于docker守护程序导致的启动和安全问题，提提高了容器的性能和安全。</p>
<p>来源：</p>
<ul>
<li>zhuanlan.zhihu.com</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubelet 状态更新机制]]></title>
      <url>http://team.jiunile.com/blog/2019/08/k8s-kubelet-sync-node-status.html</url>
      <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。</p>
<ol>
<li>kubelet 自身会定期更新状态到 apiserver，通过参数<code>--node-status-update-frequency</code>指定上报频率，默认是 10s 上报一次。</li>
<li>kube-controller-manager 会每隔<code>--node-monitor-period</code>时间去检查 kubelet 的状态，默认是 5s。</li>
<li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>notready</code> 状态，这段时长通过<code>--node-monitor-grace-period</code>参数配置，默认 40s。</li>
<li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>unhealthy</code>状态，这段时长通过<code>--node-startup-grace-period</code>参数配置，默认 1m0s。</li>
<li>当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过<code>--pod-eviction-timeout</code>参数配置，默认 5m0s。</li>
</ol>
<blockquote>
<p>kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果<code>--node-status-update-frequency</code>设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。<br><a id="more"></a><br>Kubelet在更新状态失败时，会进行<code>nodeStatusUpdateRetry</code>次重试，默认为<strong><code>5 次</code></strong>。</p>
</blockquote>
<p>Kubelet 会在函数<code>tryUpdateNodeStatus</code>中尝试进行状态更新。Kubelet 使用了 Golang 中的<code>http.Client()</code>方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。</p>
<p>因此，在<code>nodeStatusUpdateRetry * --node-status-update-frequency</code>时间后才会更新一次节点状态。</p>
<p>同时，Kubernetes 的 controller manager 将尝试每<code>--node-monitor-period</code>时间周期内检查<code>nodeStatusUpdateRetry</code>次。在<code>--node-monitor-grace-period</code>之后，会认为节点 unhealthy，然后会在<code>--pod-eviction-timeout</code>后删除 Pod。</p>
<p>kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>对于这些参数的配置，需要根据不通的集群规模场景来进行配置。</p>
<h3 id="社区默认的配置"><a href="#社区默认的配置" class="headerlink" title="社区默认的配置"></a>社区默认的配置</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">10s</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">5s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">40s</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">5m</td>
</tr>
</tbody>
</table>
<h3 id="快速更新和快速响应"><a href="#快速更新和快速响应" class="headerlink" title="快速更新和快速响应"></a>快速更新和快速响应</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">4s</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">2s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">20s</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">30s</td>
</tr>
</tbody>
</table>
<p>在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，<code>--pod-eviction-timeout</code> 在 30s 之后发生，Kubelet将尝试每4秒更新一次状态。因此，在Kubernetes控制器管理器考虑节点的不健康状态之前，它将是 (20s / 4s * 5) = 25 次尝试，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。</p>
<p>如果环境有1000个节点，那么每分钟将有(60s / 4s * 1000) = 15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。</p>
<blockquote>
<p>如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。</p>
</blockquote>
<h3 id="中等更新和平均响应"><a href="#中等更新和平均响应" class="headerlink" title="中等更新和平均响应"></a>中等更新和平均响应</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">20s</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">5s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">2m</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">1m</td>
</tr>
</tbody>
</table>
<p>这种场景下会，Pod 将在 3m 被驱逐。 Kubelet将尝试每20秒更新一次状态。因此，在Kubernetes控制器管理器考虑节点的不健康状态之前，它将是 (2m <em> 60 / 20s </em> 5) = 30 次尝试</p>
<p>如果有 1000 个节点，1分钟之内就会有 (60s / 20s * 1000) = 3000 次的节点状态更新操作。</p>
<h3 id="低更新和慢响应"><a href="#低更新和慢响应" class="headerlink" title="低更新和慢响应"></a>低更新和慢响应</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">1m</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">5s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">5m</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">1m</td>
</tr>
</tbody>
</table>
<p>这种场景下会，Pod 将在 6m 被驱逐。 Kubelet将尝试每1分钟更新一次状态。因此，在Kubernetes控制器管理器考虑节点的不健康状态之前，它将是 (5m / 1m * 5) = 25 次尝试</p>
<p>如果有 1000 个节点，1分钟之内就会有 (1m / 60s * 1000) = 1000 次的节点状态更新操作。</p>
<p>参考链接：</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md" target="_blank" rel="external">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md</a></li>
<li><a href="https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md" target="_blank" rel="external">https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Calico 3.5+ 根据节点标签分配 IP 地址]]></title>
      <url>http://team.jiunile.com/blog/2019/08/k8s-calico-assigning-ip.html</url>
      <content type="html"><![CDATA[<h2 id="关于-IP-地址的分配"><a href="#关于-IP-地址的分配" class="headerlink" title="关于 IP 地址的分配"></a>关于 IP 地址的分配</h2><p>Calico 能够进行配置，为不同拓扑指定 IP 地址池。例如可能希望某些机架、地区、或者区域能够从同一个 IP 池中获取地址。这对于降低路由数量或者配合防火墙策略的要求会很有帮助。</p>
<p><a href="https://docs.projectcalico.org/v3.5/reference/cni-plugin/configuration#ipam" target="_blank" rel="external">cni 插件配置参考中的 IP 地址管理章节</a>中包含了三种分配 IP 地址的方式。Kubernetes 注解方式只能用于 Namespace 或者 Pod 一级。剩下的只有两个办法，CNI 配置或者是基于节点选择器的 IP 池，相对于 CNI 配置的方式来说，节点选择器方案省去了修改本地文件的麻烦。</p>
<p>在更高层次上，基于节点选择器的 IP 地址分配方法就是给节点设置标签，然后用节点选择器选择对应的 IP 地址池进行分配。后面的内容中将给出一个详细的例子，用这种方式来设置一种机架亲和方式的 IP 地址分配方案。</p>
<blockquote>
<p>如果 Calico 无法根据上述顺序来决定一个 IP 地址池，或者在选定的地址池中找不到可用的 IP 地址，那么这一工作负载就不会分到 IP 地址，无法启动。为了防止这种情况的发生，我们建议所有节点至少有一个合适的地址池。<br><a id="more"></a></p>
<h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p><strong><code>这一功能需要 Calico 在 ETCD 模式下工作</code></strong></p>
</blockquote>
<h2 id="示例：Kubernetes"><a href="#示例：Kubernetes" class="headerlink" title="示例：Kubernetes"></a>示例：Kubernetes</h2><p>本例中，我们会创建一个集群，其中包含四个节点，分布在两个机架上，每个机架各两台。示意如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">       -------------------</span><br><span class="line">       |    router       |</span><br><span class="line">       -------------------</span><br><span class="line">       |                 |</span><br><span class="line">---------------   ---------------</span><br><span class="line">| rack-0      |   | rack-1      |</span><br><span class="line">---------------   ---------------</span><br><span class="line">| kube-node-0 |   | kube-node-2 |</span><br><span class="line">- - - - - - - -   - - - - - - - -</span><br><span class="line">| kube-node-1 |   | kube-node-3 |</span><br><span class="line">- - - - - - - -   - - - - - - - -</span><br></pre></td></tr></table></figure></p>
<p>Pod IP 地址范围为 <code>192.168.0.0/16</code>，我们进行如下设计：保留 <code>192.168.0.0/24</code> 给 <code>rack-0</code>,  <code>192.168.1.0/24</code> 给 <code>rack-1</code>。</p>
<p>要设置一个没有缺省地址池的的 Calico，首先运行 <code>calicoctl get ippool -o wide</code>，会看到已经创建了一个 <code>192.168.0.0/16</code> 的地址池：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME                  CIDR             NAT    IPIPMODE   DISABLED   SELECTOR</span><br><span class="line">default-ipv4-ippool   192.168.0.0/16   <span class="literal">true</span>   Always     <span class="literal">false</span>      all()</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>删除缺省地址池<br><code>default-ipv4-ippool</code> 地址池已经存在，并占据了整个 <code>/16</code> 块，因此必须删除：<code>calicoctl delete ippools default-ipv4-ippool</code></p>
</li>
<li><p>给 Node 打标签。<br>要给特定节点分配地址池，节点必须用标签进行标识：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl label nodes kube-node-0 rack=0</span><br><span class="line">kubectl label nodes kube-node-1 rack=0</span><br><span class="line">kubectl label nodes kube-node-2 rack=1</span><br><span class="line">kubectl label nodes kube-node-3 rack=1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>为每个机架创建地址池</strong></p>
<p>rack-0-ippool<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">calicoctl create -f -&lt;&lt;EOF</span><br><span class="line"><span class="attr">apiVersion:</span> projectcalico.org/v3</span><br><span class="line"><span class="attr">kind:</span> IPPool</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> rack<span class="bullet">-0</span>-ippool</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  cidr:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line"><span class="attr">  ipipMode:</span> Always</span><br><span class="line"><span class="attr">  natOutgoing:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  nodeSelector:</span> rack == <span class="string">"0"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>rack-1-ippool<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">calicoctl create -f -&lt;&lt;EOF</span><br><span class="line"><span class="attr">apiVersion:</span> projectcalico.org/v3</span><br><span class="line"><span class="attr">kind:</span> IPPool</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> rack<span class="bullet">-1</span>-ippool</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  cidr:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line"><span class="attr">  ipipMode:</span> Always</span><br><span class="line"><span class="attr">  natOutgoing:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  nodeSelector:</span> rack == <span class="string">"1"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>现在就创建了两个地址池，使用 <code>calicoctl get ippool -o wide</code> 进行查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NAME                  CIDR             NAT    IPIPMODE   DISABLED   SELECTOR</span><br><span class="line">rack-1-ippool         192.168.0.0/24   <span class="literal">true</span>   Always     <span class="literal">false</span>      rack == <span class="string">"0"</span></span><br><span class="line">rack-2-ippool         192.168.1.0/24   <span class="literal">true</span>   Always     <span class="literal">false</span>      rack == <span class="string">"1"</span></span><br></pre></td></tr></table></figure></p>
<ol>
<li>检查地址池的工作状态<br>创建一个 Nginx 的 Deployment，其中包含五个副本，保证分配到每一个节点上。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image nginx --replicas 5</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>检查新的 Pod 是否已经根据所在机架获得了应有的 IP 地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAME                   READY   STATUS    RESTARTS   AGE    IP             NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-5c7588df-prx4z   1/1     Running   0          6m3s   192.168.0.64   kube-node-0   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df<span class="_">-s</span>7qw6   1/1     Running   0          6m7s   192.168.0.129  kube-node-1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df-w7r7g   1/1     Running   0          6m3s   192.168.1.65   kube-node-2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df-62lnf   1/1     Running   0          6m3s   192.168.1.1    kube-node-3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df-pnsvv   1/1     Running   0          6m3s   192.168.1.64   kube-node-2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，IP 地址的是根据节点（所在的机架）来选择了对应的地址池进行分配的。</p>
<p>参考地址</p>
<ul>
<li><a href="https://docs.projectcalico.org/v3.5/usage/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.5</a></li>
<li><a href="https://docs.projectcalico.org/v3.6/networking/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.6</a></li>
<li><a href="https://docs.projectcalico.org/v3.7/networking/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.7</a></li>
<li><a href="https://docs.projectcalico.org/v3.8/networking/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.8</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[镜像漏洞检测工具 -- Trivy]]></title>
      <url>http://team.jiunile.com/blog/2019/08/security-trivy.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>道路千万条，安全第一条； 镜像不规范，同事两行泪。</p>
</blockquote>
<p><a href="https://github.com/aquasecurity/trivy" target="_blank" rel="external">Trivy</a> 是一个面向镜像的漏洞检测工具，具备如下特点：</p>
<ol>
<li>开源</li>
<li>免费</li>
<li>易用</li>
<li>准确度高</li>
<li>CI 友好</li>
</ol>
<p>相对于老前辈 <a href="https://github.com/coreos/clair" target="_blank" rel="external">Clair</a>，Trivy 的使用非常直观方便，适用于更多的场景。</p>
<p>下面是官方出具的对比表格：</p>
<table>
<thead>
<tr>
<th>扫描器</th>
<th style="text-align:center">操作系统</th>
<th style="text-align:center">依赖检测</th>
<th style="text-align:center">适用性</th>
<th style="text-align:center">准确度</th>
<th style="text-align:center">CI 友好</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trivy</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◎</td>
<td style="text-align:center">◯</td>
</tr>
<tr>
<td>Clair</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">△</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">△</td>
</tr>
<tr>
<td>Anchore Engine</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">△</td>
<td style="text-align:center">△</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">△</td>
</tr>
<tr>
<td>Quay</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td>MicroScanner</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
</tr>
<tr>
<td>Docker Hub</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td>GCR</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="MacOS"><a href="#MacOS" class="headerlink" title="MacOS"></a>MacOS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew tap knqyf263/trivy</span><br><span class="line">$ brew install knqyf263/trivy/trivy</span><br></pre></td></tr></table></figure>
<h3 id="RHEL-CentOS"><a href="#RHEL-CentOS" class="headerlink" title="RHEL/CentOS"></a>RHEL/CentOS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/yum.repos.d/trivy.repo</span><br><span class="line">[trivy]</span><br><span class="line">name=Trivy repository</span><br><span class="line">baseurl=https://aquasecurity.github.io/trivy-repo/rpm/releases/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">$ sudo yum -y update</span><br><span class="line">$ sudo yum -y install trivy</span><br></pre></td></tr></table></figure>
<p>or<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.0.15/trivy_0.0.15_Linux-64bit.rpm</span><br></pre></td></tr></table></figure></p>
<h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><p>这个工具的最大闪光点就是提供了很多适合用在自动化场景的用法。详细使用帮助可参考官方文档：<a href="https://github.com/aquasecurity/trivy" target="_blank" rel="external">Trivy</a></p>
<h3 id="扫描镜像："><a href="#扫描镜像：" class="headerlink" title="扫描镜像："></a>扫描镜像：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy centos</span><br></pre></td></tr></table></figure>
<h3 id="扫描镜像文件"><a href="#扫描镜像文件" class="headerlink" title="扫描镜像文件"></a>扫描镜像文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker save ruby:2.3.0-alpine3.9 -o ruby-2.3.0.tar</span><br><span class="line">$ trivy --input ruby-2.3.0.tar</span><br></pre></td></tr></table></figure>
<h3 id="根据严重程度进行过滤"><a href="#根据严重程度进行过滤" class="headerlink" title="根据严重程度进行过滤"></a>根据严重程度进行过滤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy --severity HIGH,CRITICAL ruby:2.3.0</span><br></pre></td></tr></table></figure>
<h3 id="忽略未修复问题"><a href="#忽略未修复问题" class="headerlink" title="忽略未修复问题"></a>忽略未修复问题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy --ignore-unfixed ruby:2.3.0</span><br></pre></td></tr></table></figure>
<h3 id="忽略特定问题"><a href="#忽略特定问题" class="headerlink" title="忽略特定问题"></a>忽略特定问题</h3><p>使用 <code>.trivyignore</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat .trivyignore</span><br><span class="line"><span class="comment"># Accept the risk</span></span><br><span class="line">CVE-2018-14618</span><br><span class="line"></span><br><span class="line"><span class="comment"># No impact in our settings</span></span><br><span class="line">CVE-2019-1543</span><br><span class="line"></span><br><span class="line">$ trivy python:3.4-alpine3.9</span><br></pre></td></tr></table></figure></p>
<h3 id="使用-JSON-输出结果"><a href="#使用-JSON-输出结果" class="headerlink" title="使用 JSON 输出结果"></a>使用 JSON 输出结果</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy <span class="_">-f</span> json dustise/translat-chatbot:20190428-5</span><br></pre></td></tr></table></figure>
<h3 id="定义返回值"><a href="#定义返回值" class="headerlink" title="定义返回值"></a>定义返回值</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ trivy --exit-code 0 --severity MEDIUM,HIGH ruby:2.3.0</span><br><span class="line">$ trivy --exit-code 1 --severity CRITICAL ruby:2.3.0</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>相对于其它同类工具，Trivy 非常适合自动化操作，从 CircleCI 之类的公有服务，到企业内部使用的 Jenkins、Gitlab 等私有工具，或者作为开发运维人员的自测环节，都有 Trivy 的用武之地。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 语言的错误处理：从拒绝到接受]]></title>
      <url>http://team.jiunile.com/blog/2019/08/go-errors-in-go.html</url>
      <content type="html"><![CDATA[<h3 id="学习如何在go中不在担心并且爱上错误处理"><a href="#学习如何在go中不在担心并且爱上错误处理" class="headerlink" title="学习如何在go中不在担心并且爱上错误处理"></a>学习如何在go中不在担心并且爱上错误处理</h3><p>正如一位英国诗人所说的，“犯错是人，宽恕是神”。错误处理是编程实践中非常重要的一部分，但在很多流行语言中并没有对它给予足够的重视。</p>
<p>作为众多语言的鼻祖，C 语言从一开始就没有一个完善的错误处理和异常机制。在 C 语言中，错误处理完全由程序员来负责，要么通过设置一个错误码，或者程序直接就崩溃了（segment fault）。</p>
<p>虽然异常处理机制早在 C 语言发明之前就出现了（最早由 LISP 1.5在1962年支持），但直到19世纪80年代它才流行开来。C++ 和 Java 让程序员熟悉了 <code>try...catch</code> 这一模式，所有的解释型语言也沿用了它。</p>
<p>尽管在语法上略有差异（比如是用 <code>try</code> 还是 <code>begin</code>），我之前遇到的每一种语言在一开始学习的时候都不会让你注意到错误处理的概念。通常，在你刚开始写着玩的时候根本用不到它，只有当你开始写一个真正的项目时才会意识到需要有错误处理。至少对于我而言，一直如此。</p>
<p>然后我遇到了 Golang ：一开始大家都是从《a Tour of Go》来认识它的。<br><a id="more"></a></p>
<p>在学习《a Tour of Go》的过程中，不断的有 <code>err</code> 这样代表错误的变量映入眼帘。不管一个 Go 项目有多大，一种模式总是存在：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f, err := os.Open(filename)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  <span class="comment">// 在这里处理错误</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据 Go 的惯例，每一个会出现错误的函数都需要在最后一个返回值中定义它（Go 允许多返回值），程序员需要在每一次调用函数后都对返回的错误进行处理，因此就出现了随处可见的 <code>if err != nil</code> 代码片段。</p>
<p>一开始，每个函数调用后都进行一次错误检查让人感觉很崩溃。对于许多 Go 新手来说，这非常痛苦。在我们刚接触到错误处理的时候就开始为它这种繁琐的处理方式感到厌恶了。</p>
<p>有一个著名的用于处理悲痛和失去的模型，它是由美籍瑞士心理学家 Elisabeth Kübler-Ross 在1969年提出。它包含了五个阶段：拒绝，愤怒，讨价还价，失落，接受。虽然起初它主要是用于解决跟死亡和伤痛有关的问题，但事实已经证明，它在处理当一个人遇到重大变故而产生内心抵抗时都是有效的。学习一门新的编程语言显然属于这一范畴。</p>
<p>在我拥抱 Go 的错误处理模式的过程中，我经历了所有这五个阶段，下面我就跟你分享一下我的旅程。</p>
<p>那么，一切都从拒绝开始说起吧。</p>
<h3 id="拒绝"><a href="#拒绝" class="headerlink" title="拒绝"></a>拒绝</h3><p>“一定是哪里出错了，不应该出现这么多错误检查…”</p>
<p>这是我刚开始写 Go 代码时的想法。我下意识的想找 Go 里的异常机制，但我没找到。Go 有意地去掉了对异常的支持。</p>
<p>使用异常的一个问题是你永远都不知道一个函数是否会抛异常。当然了，Java 通过 <code>throws</code> 关键字来显式声明了一个函数可能会抛出异常，这解决了异常不明确的问题，但同时也使得代码变得非常啰嗦。有人说我可以在文档中把这个问题说清楚，但文档也不是银弹，通常更新不及时的文档是大部分项目永远的痛。</p>
<p>Go 中的错误处理机制体现了一致性：每个可能产生错误的函数都应在最后一个返回值中返回一个 <code>error</code> 类型。</p>
<p>如果你正常处理了错误，那一切相安无事，代码会继续运行。在某些情况下，如果你觉得没有必要进行错误检查，你完全可以忽略它。当出现错误时，其它的返回值默认则是零值，这样有些错误你完全不必理会。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Let's convert a string into int64.</span></span><br><span class="line"><span class="comment">// We don't care whether strconv.ParseInt returns an error</span></span><br><span class="line"><span class="comment">// as the first returned value will be 0 if it fails to convert.</span></span><br><span class="line">i, _ := strconv.ParseInt(strVal, <span class="number">10</span>, <span class="number">64</span>)</span><br><span class="line">log.Printf(<span class="string">"Parsed value is: %d"</span>, i)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>例如上面的 ParseInt 函数，当转换错误时 i 默认为0，因为对于转换的错误你即使不做处理也是 ok 的，但是对于后续的处理逻辑则无法区分是因为转换错误导致 i 等于0，还是本身转换的内容就是0这两种情况。</p>
</blockquote>
<p>但是如果你仅仅是调用这个函数，并没有处理它的返回值，那么你很容易忘记这个函数可能还会返回一个错误。因此，在使用之前在文档中查看一下这个函数是否会返回错误永远都是明智的做法。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// http.ListenAndServe returns an error, but we don't check for it.</span></span><br><span class="line"><span class="comment">// Since we don't use returned values further, this code will compile.</span></span><br><span class="line">http.ListenAndServe(<span class="string">":8080"</span>, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// However, it is still better to check the returned error for consistency.</span></span><br><span class="line">err := http.ListenAndServe(<span class="string">":8080"</span>, <span class="literal">nil</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  log.Fatalf(<span class="string">"Can't start the server: %s"</span>, err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="愤怒"><a href="#愤怒" class="headerlink" title="愤怒"></a>愤怒</h3><p>有许多编程语言都有所谓“正常”的错误处理（类似于 <code>try catch</code>），那我为什么要用这种奇怪的像垃圾碎片一样的“把错误作为函数结果”来返回呢？</p>
<p>作为作者，这两种处理机制我都使用过。Go 不仅仅是将那些我习以为常的 exception 当成 error 来替换，这让我对这门语言感到愤慨。然而更好的做法是将这种 error 视为方法是否被成功执行的指示。</p>
<p>如果你曾经在 Rails 中用过 <code>active record</code>，你可能会熟悉这样的代码：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">user = User.new(user_params)</span><br><span class="line"><span class="keyword">if</span> user.save</span><br><span class="line">  head <span class="symbol">:ok</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  render <span class="symbol">json:</span> user.errors, <span class="symbol">status:</span> <span class="number">422</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><code>if</code> 后面的 <code>user.save</code> 是一个 bool 值，它表示是否成功保存了用户的实例。<code>user.errors</code> 则返回了可能发生的 error 列表结果。当时保存用户实例失败的时候， <code>user</code> 就会包含 <code>errors</code>，这种方法经常被批评为反模式。</p>
<p>然而，Go 语言自带报告方法”失败细节“的内置模式，并且还没有什么副作用。毕竟，Go 的 error 只是一个含有单个方法的接口：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> error <span class="keyword">interface</span> &#123;</span><br><span class="line">  Error() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以任意去集成这个接口。如果想要提供一些验证错误的信息，可以定义如下的结构体类型：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> ValidationErr <span class="keyword">struct</span> &#123;</span><br><span class="line">  <span class="comment">// We will store validation errors here.</span></span><br><span class="line">  <span class="comment">// The key is a field name, and the value is a slice of validation messages.</span></span><br><span class="line">  ErrorMessages <span class="keyword">map</span>[<span class="keyword">string</span>][]<span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *ValidationErr)</span> <span class="title">Error</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> FormatErrors(e.ErrorMessages)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于如何格式化错误信息并不是本文的重点， 所以我省去了 <code>FormatErrors</code> 方法的具体实现。我们只说如何将错误信息合并成单个的字符串。</p>
<p>现在假设我们用 Go 语言写了一个类似 Rails 的框架，<code>actions handler</code> 的处理方式就像这样：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(a * Action)</span> <span class="title">Handle</span><span class="params">()</span></span> &#123;</span><br><span class="line">  user := NewUser(a.Params[<span class="string">"user"</span>])</span><br><span class="line">  <span class="keyword">if</span> err := user.Save(); err == <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="comment">// No errors, yay! Respond with 200.</span></span><br><span class="line">    a.Respond(<span class="number">200</span>, <span class="string">"OK"</span>, <span class="string">"text/plain"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> verr, ok := err.(*ValidationErr); ok &#123;</span><br><span class="line">    <span class="comment">// err was successfully typecast to ValidationErr.</span></span><br><span class="line">    <span class="comment">// Let's respond with 422.</span></span><br><span class="line">    resp, _ := json.Marshal(verr.ErrorMessages)</span><br><span class="line">    a.Respond(<span class="number">422</span>, <span class="keyword">string</span>(resp), <span class="string">"application/json"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Unexpected error, respond with 500.</span></span><br><span class="line">    a.Respond(<span class="number">500</span>, err.Error(), <span class="string">"text/plain"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>就这样，错误验证是函数返回的合法部分，我们减少了 <code>user.Save</code> 方法的副作用。所有非预期的错误都是在显式的进行处理，而不是隐藏在框架里面。如果还出现问题，我们可以在处理后续逻辑之前采取其他必要的措施。</p>
<p>返回错误的时候如果有额外的信息，这总归是好的。许多流行的 Go 包都会用他们自己实现的 error 接口，比如我的 imgproxy 也不例外。此处，我用了自定义的 imgproxyError 结构体，它来告诉 HTTP handler 应该返回什么状态码，返回给上层调用者什么消息，在日志中应该打印什么信息。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> imgproxyError <span class="keyword">struct</span> &#123;</span><br><span class="line">  StatusCode    <span class="keyword">int</span></span><br><span class="line">  Message       <span class="keyword">string</span></span><br><span class="line">  PublicMessage <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *imgproxyError)</span> <span class="title">Error</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> e.Message</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来演示一下我是如何用这种方式的:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ierr, ok := err.(*imgproxyError); ok &#123;</span><br><span class="line">  respondWithError(ierr)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  msg := fmt.Sprintf(<span class="string">"Unexpected error: %s"</span>, err)</span><br><span class="line">  respondWithError(&amp;imgproxyError&#123;<span class="number">500</span>, msg, <span class="string">"Internal error"</span>&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而在之前，我所做的就是检查错误类型是否是我所定义的类型，不是我定义的类型说明不是预期的错误。那么就将它转化成 imgproxyError 实例，以此来告诉 HTTP handler 去响应500的状态码并让程序在日志中打印错误信息。</p>
<p>这里有必要说一个 Go 中类型转换的注意事项，毕竟它总是让新手困扰。你可以通过两种方式进行类型转换，不过建议最好还是用相对安全的方式：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Unsafe. If err is not *imgproxyError, Go will panic.</span></span><br><span class="line">ierr := err.(*imgproxyError)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Safe way. ok indicates if interface was typecast successfully or not.</span></span><br><span class="line"><span class="comment">// Go will not panic even if the interface represents the wrong type.</span></span><br><span class="line">ierr, ok := err.(*imgproxyError)</span><br></pre></td></tr></table></figure>
<p>现在，我们可以看到 Go 惯用的错误处理可以非常的灵活，接下来就可以进入下一个环节 - 讨价还价。</p>
<h3 id="讨价还价"><a href="#讨价还价" class="headerlink" title="讨价还价"></a>讨价还价</h3><p>“哪里出现错误，哪里处理错误”，这种错误处理的方式依旧对我来说很陌生，也许我能做些什么让它更像我喜欢的语言。</p>
<p>在代码每个可能出现的地方都进行错误处理是一件很麻烦的事情。很多时候，我们都想把错误提升到某些可以批量或集中处理的地方。这种方式，最显而易见的就是函数嵌套调用，在最上层处理掉来自底层的方法所产生的错误。</p>
<p>看一下这个公认的函数调用函数的例子，期望在最顶层处理掉所有的error：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"errors"</span></span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The principal function to be called where all errors will end up.</span></span><br><span class="line"><span class="comment">// Takes a numeric string, logs a square root of that number.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  f, err := StringToSqrt(str)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    HandleError(err) <span class="comment">// a function where he handle all errors</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, f)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Tries to parse a float64 out of a string and returns its square root.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  f, err := strconv.ParseFloat(str, <span class="number">64</span>)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, err</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  f, err = Sqrt(f)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, err</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> f, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Calculates a square root of the float.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calc sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这是 Go 语言惯用的方式，是的，看起来又臭又长。好在写 Go 语言的人好像也承认了这个问题。目前他们正在就 Go 2 的错误检查和处理问题发起讨论。官方错误处理草案引入了一个新的 construct <code>check ... handle</code>，关于它是如何工作的，草案是这么说的：</p>
<ul>
<li>check 语句适用于 error 类型的表达式或者函数返回以 error 类型值结尾的函数调用。如果 error 非 nil，check 语句将会返回闭包方法的结果，而这个闭包方法是通过 error 值调用处理程序链触发的。</li>
<li>handle 语句定义的代码块就是 handler，用来处理 check 语句检测到的 error。handler 中的 return 语句会导致闭包函数立刻返回给定的返回值。只有闭包函数没有结果或使用命名结果的时候， 才允许不带返回值。在后一种情况下，函数返回那些结果的当前值。</li>
</ul>
<p>依旧是 square 的例子，现在用另一种方式来进行错误处理。Go 2 已经发布，官方建议的写法如下：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"errors"</span></span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  handle err &#123; HandleError(err) &#125; <span class="comment">// where the magic happens</span></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, check StringToSqrt(str))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  handle err &#123; <span class="keyword">return</span> <span class="number">0</span>, err &#125; <span class="comment">// no need to explicitly if...else</span></span><br><span class="line">  <span class="keyword">return</span> check math.Sqrt(check strconv.ParseFloat(str, <span class="number">64</span>)), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calculate sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看上去好一些了，但是距离真正用 Go 2 做实际开发仍旧有一段距离。</p>
<p>与此同时，其实我们可以用另一种错误处理的方式，他可以显著减少 <code>if ... else</code> 语句，并且允许出现单点的 error。我叫这种方法为“Panic 驱动的错误处理”。</p>
<p>为了做到“Panic 驱动”，将依赖内置于 Go 语言的三个关键词：defer，panic，recover。这里稍微回顾一下他们分时是什么：</p>
<ul>
<li>defer 将函数 push 到本函数返回后执行的堆栈中，当你需要一些清理时候会派上用场。在我们的这个 case 里面，什么时候会用到 defer 呢？就是从 panic 中 recover 的时候，需要用到 defer。</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Foo</span><span class="params">()</span></span> &#123;</span><br><span class="line">f, _ := os.Open(<span class="string">"filename"</span>)</span><br><span class="line"><span class="comment">// defer ensures that f.Close() will be executed when Foo returns.</span></span><br><span class="line"><span class="keyword">defer</span> f.Close()</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>panic 会停止普通的程序流控制并开始 panicking。当函数开始 panic，程序的正常执行会被中止，程序开始调用堆栈执行所有的 defer 方法，同时在当前的 goroutine 的 root goroutine 程序开始崩溃。</li>
<li>recover 重新获取正在 panic 的 goroutine 的控制，并返回触发 panic 的 interface。recover 仅在 defer 中有效，在其他地方将返回 nil。</li>
</ul>
<p>BTW，纯粹的讲，下面的代码不代表最常见的 Go。灵感来自于 Gin 的源码（Gin 是当前比较流行的Go 领域的 web 框架）我自己并没有完全想的出它。 在 Gin 框架里面，如果一个 critical error 发生了，你可以在 handler 程序中调用 panic，然后 Gin 会 recover，打印错误日志并且返回500状态码。</p>
<p>由 Panic 驱动错误处理的想法很简单：只要嵌套调用返回 error 引发的 panic（译者注：checkErr 封装作为 reference，有多处地方调用），在 recover 的时候有单独的地方进行错误处理：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"errors"</span></span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// A simple helper to panic on errors.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErr</span><span class="params">(err error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="comment">// It is important to defer the anonymous function that wraps around error handling.</span></span><br><span class="line">  <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> r := <span class="built_in">recover</span>(); r != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> err, ok := r.(error); ok &#123;</span><br><span class="line">        <span class="comment">// Recover returned an error, handle it somehow.</span></span><br><span class="line">        HandleError(err)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Recover returned something that is not an error, so "re-panic".</span></span><br><span class="line">        <span class="built_in">panic</span>(r)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A call that starts a chain of events that might go wrong</span></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, StringToSqrt(str))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line">  f, err := strconv.ParseFloat(str, <span class="number">64</span>)</span><br><span class="line">  checkErr(err)</span><br><span class="line"></span><br><span class="line">  f, err = Sqrt(f)</span><br><span class="line">  checkErr(err)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> f</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calculate sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>的确， 这看起来不像其他语言上的 <code>try catch</code>，但是却让我们将错误处理这样的责任移动到对应的调用链上。</p>
<p>在 imgproxy 这个模块里面，我用这种方式实现当达到 timeout 就停止图片加载。回到之前说的，如果在每个方法中达到timeout就要进行 timeout error 的处理，这是很让人烦恼的，现在，我可以在任何地方用一行代码进行 timeout 的 check。</p>
<p>关于 error 的内容，我们也同样希望能添加更多的信息，但是golang的标准错误类型并没有提供堆栈跟踪信息。好在可以直接用 github.com/pkg/errors 来替换内置的 errors 包。你只需要用 <code>import “github.com/pkg/errors”</code> 替换 <code>import “errors”</code>，然后你的 errors 就可以包含堆栈跟踪信息了。注意现在起，你可不是在处理默认的 error 类型。下面就是标准类库的替代方案所建议的：</p>
<ul>
<li><code>func New(message string)</code> 是类似于内置 errors 包的同名函数。它实现并返回了包含堆栈信息的 error 类型</li>
<li><code>func WithMessage(err error,message string)</code> 将你的 error 封装到另一个类型里面， 并且这个类型包含了一些额外的信息。</li>
<li><code>fuc WithStack(err error) error</code> 封装了你的 error 到另一个类型， 这个类型包含了堆栈信息。当你用第三方包时，相当当前类型的 error 添加到第三方包的 error；或者想要添加堆栈信息到第三方包的 error。</li>
<li><code>func Wrap(err error,messag string) error</code> 是 WithStack+WitchMessage 的缩写。</li>
</ul>
<p>试着用刚才说的方法改进一下之前的代码：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line"></span><br><span class="line">  <span class="string">"github.com/pkg/errors"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErr</span><span class="params">(err error, msg <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(errors.WithMessage(err, msg))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErrWithStack</span><span class="params">(err error, msg <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(errors.Wrap(err, msg))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> r := <span class="built_in">recover</span>(); r != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> err, ok := r.(error); ok &#123;</span><br><span class="line">        <span class="comment">// Print the error to the log before handling.</span></span><br><span class="line">        <span class="comment">// %+v prints formatted error with additional messages and a stack trace:</span></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        <span class="comment">// Failed to sqrt: Can't calc sqrt of a negative number</span></span><br><span class="line">        <span class="comment">// main.main /app/main.go:14</span></span><br><span class="line">        <span class="comment">// runtime.main /goroot/libexec/src/runtime/proc.go:198</span></span><br><span class="line">        <span class="comment">// runtime.goexit /goroot/libexec/src/runtime/asm_amd64.s:2361</span></span><br><span class="line">        log.Printf(<span class="string">"%+v"</span>, err)</span><br><span class="line">        HandleError(err)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">panic</span>(r)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;()</span><br><span class="line"></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, StringToSqrt(str))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line">  f, err := strconv.ParseFloat(str, <span class="number">64</span>)</span><br><span class="line">  checkErrWithStack(err, <span class="string">"Failed to parse"</span>)</span><br><span class="line"></span><br><span class="line">  f, err = Sqrt(f)</span><br><span class="line">  checkErr(err, <span class="string">"Failed to sqrt"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> f</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="comment">// We use New from https://github.com/pkg/errors,</span></span><br><span class="line">    <span class="comment">// so our error will contain a stack trace.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calc sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>重要提示</strong>：也许你已经注意到了，errors.WithMessage 和 errors.WithStack 将 github.com/pkg/errors 封装进了定义类型里面。 这同时意味着你不能对自己的 error 实现直接的进行类型转化了。为了能将 github.com/pkg/errors 类型转化成你自己的 error 类型，首先需要用errors.Cause 对 github.com/pkg/errors 进行解包：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">err := PerformValidation()</span><br><span class="line"><span class="keyword">if</span> verr, ok := errors.Cause(err).(*ValidationErr); ok &#123;</span><br><span class="line">  <span class="comment">// Do something with the validation error</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在看似有强大的机制在一个地方集中处理相关的错误。但是别高兴的太早，Go 语言中最强大的就是 goroutine，goroutine 在并发的情况下，这种方法将会失败。</p>
<p>接下来我们就讲讲这种让人沮丧的时刻 - 失落。</p>
<h3 id="失落"><a href="#失落" class="headerlink" title="失落"></a>失落</h3><p>我努力的在我的代码中采用集中处理错误的方式，但是当我在使用 goroutines 的时候它却失效了。这种错误处理机制变得毫无意义。。。</p>
<p>不要 panic，将 panic 留给你的代码。在 goroutines 中的固定位置处理错误依然可行，此处我将使用不止一种方法（实际上是两种）。</p>
<h3 id="Channels-和-sync-WaitGroup"><a href="#Channels-和-sync-WaitGroup" class="headerlink" title="Channels 和 sync.WaitGroup"></a>Channels 和 sync.WaitGroup</h3><p>你可以将 Go 的 channel 和内置的 sync.Waitgroup 结合起来使用，这样就可以在特定的 channel 中处理相应的 errors，同时在异步进程处理完成后可以一个个的处理它们。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">errCh := <span class="built_in">make</span>(<span class="keyword">chan</span> error, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"><span class="comment">// We will launch two goroutines.</span></span><br><span class="line">wg.Add(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #1</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="comment">// We are done on return</span></span><br><span class="line">  <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If any error has occurred, put it into the channel.</span></span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    errCh &lt;- err</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #2</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    errCh &lt;- err</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wait till all goroutines are done and close the channel.</span></span><br><span class="line">wg.Wait()</span><br><span class="line"><span class="built_in">close</span>(errCh)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Loop over the channel to collect all errors.</span></span><br><span class="line"><span class="keyword">for</span> err := <span class="keyword">range</span> errCh &#123;</span><br><span class="line">  HandleErr(err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当你需要在多个 goroutines 中收集所有错误时，这种方法将非常有用。</p>
<p>通常情况下，我们很少需要处理每个一个错误。多数情况下，要么全处理要么不处理：我们需要知道是否其中一些 goroutines 失败了。因此，我们准备使用 Golang 官方的子代码库中的 errgroup 包。下面代码展示了如何使用它：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> g errgroup.Group</span><br><span class="line"></span><br><span class="line"><span class="comment">// g.Go takes a function that returns error.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #1</span></span><br><span class="line">g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">  <span class="comment">// If any error has occurred, return it.</span></span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #2</span></span><br><span class="line">g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// g.Wait waits till all goroutines are done</span></span><br><span class="line"><span class="comment">// and returns only the first error.</span></span><br><span class="line"><span class="keyword">if</span> err := g.Wait(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">  HandleErr(err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只有 errgroup.Group 内部启动的 subroutines 中的第一个非零错误（如果有）才会被返回。而所有繁重的工作都是在幕后完成的。</p>
<h3 id="开始你自己的-PanicGroup"><a href="#开始你自己的-PanicGroup" class="headerlink" title="开始你自己的 PanicGroup"></a>开始你自己的 PanicGroup</h3><p>正如之前提到的，所有的 goroutines 在他们自己的范围里发生 panic。如果你想在 goroutines 中使用“panic 驱动错误处理”模式，你还需要做一点点其他的工作。糟糕的是 errgroup 不会有所帮助。然而，没有任何人阻止我们实现一遍我们自己的 PanicGroup！下面试一下完整的实现：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> PanicGroup <span class="keyword">struct</span> &#123;</span><br><span class="line">  wg      sync.WaitGroup</span><br><span class="line">  errOnce sync.Once</span><br><span class="line">  err     error</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *PanicGroup)</span> <span class="title">Wait</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">  g.wg.Wait()</span><br><span class="line">  <span class="keyword">return</span> g.err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *PanicGroup)</span> <span class="title">Go</span><span class="params">(f <span class="keyword">func</span>()</span>)</span> &#123;</span><br><span class="line">  g.wg.Add(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> g.wg.Done()</span><br><span class="line">    <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">      <span class="keyword">if</span> r := <span class="built_in">recover</span>(); r != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> err, ok := r.(error); ok &#123;</span><br><span class="line">          <span class="comment">// 我们仅仅需要第一个错误, sync.Onece 在这里很有帮助.</span></span><br><span class="line">          g.errOnce.Do(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">            g.err = err</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="built_in">panic</span>(r)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    f()</span><br><span class="line">  &#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在，我们可以像下面这样，使用我们自己的 PanicGroup：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErr</span><span class="params">(err error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Foo</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">var</span> g PanicGroup</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Goroutine #1</span></span><br><span class="line">  g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 如果在这里发生了任何错误, panic.</span></span><br><span class="line">    checkErr(dangerous.Action())</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Goroutine #2</span></span><br><span class="line">  g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    checkErr(dangerous.Action())</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> err := g.Wait(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    HandleErr(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以， 当我们需要处理多个 goroutines，并且每个 goroutines 还需要抛出它自定义的 panic 时， 我们仍然可以通过上面的方式， 来保证代码清晰，简练。</p>
<h3 id="接受-并且完美"><a href="#接受-并且完美" class="headerlink" title="接受(并且完美)"></a>接受(并且完美)</h3><p>感谢您看完了我的文章。现在，我们就能了解到为什么 Go 语言里的错误处理是这个样子，什么才是大家最关心的问题，以及当 Go 2 仅仅出现一点点苗头的时候，我们怎么去克服这些困难。我们的”疗法”很完整。</p>
<p>当浏览完我所有的5个悲伤的阶段，我意识到，Go 里面的错误处理不应该被当成一种痛苦，反而相对于流程控制而言，是一种强大的，灵活的工具。</p>
<p>无论任何时候，在错误刚刚出现的后面，通过 <code>if err != nil</code> 来处理是一种完美的选择。如果你需要在一个地方集中处理所有的错误，将错误向上逐层返回到调用者。在这一点上，为错误添加上下文将是有益的，因此您不会忘记正在发生的事情并且可以正确处理每种错误。</p>
<p>如果您需要在发生错误后完全停止程序流程，请随便使用我所描述的“panic 驱动错误处理”，并且不要忘记通过 Twitter 与我分享您的经验。</p>
<p>最后一个要点，请记住，当事情真的发生了错误，保证总会有 <code>log.Fatal</code> 去记录一下。</p>
<p>来源：evilmartians.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一份快速实用的 tcpdump 命令参考手册]]></title>
      <url>http://team.jiunile.com/blog/2019/06/tcpdump.html</url>
      <content type="html"><![CDATA[<h2 id="tcpdump-简介"><a href="#tcpdump-简介" class="headerlink" title="tcpdump 简介"></a>tcpdump 简介</h2><p>对于 <code>tcpdump</code> 的使用，大部分管理员会分成两类。有一类管理员，他们熟知  <code>tcpdump</code> 和其中的所有标记；另一类管理员，他们仅了解基本的使用方法，剩下事情都要借助参考手册才能完成。出现这种情况的原因在于， <code>tcpdump</code> 是一个相当高级的命令，使用的时候需要对网络的工作机制有相当深入的了解。</p>
<p>在今天的文章中，我想提供一个快速但相当实用的 <code>tcpdump</code> 参考。我会谈到基本的和一些高级的使用方法。我敢肯定我会忽略一些相当酷的命令，欢迎你补充在评论部分。</p>
<p>在我们深入了解以前，最重要的是了解  <code>tcpdump</code> 是用来做什么的。 tcpdump 命令用来保存和记录网络流量。你可以用它来观察网络上发生了什么，并可用来解决各种各样的问题，包括和网络通信无关的问题。除了网络问题，我经常用 <code>tcpdump</code> 解决应用程序的问题。如果你发现两个应用程序之间无法很好工作，可以用  <code>tcpdump</code>  观察出了什么问题。 <code>tcpdump</code> 可以用来抓取和读取数据包，特别是当通信没有被加密的时候。</p>
<a id="more"></a>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p>了解 <code>tcpdump</code> ，首先要知道 <code>tcpdump</code> 中使用的标记（flag）。在这个章节中，我会涵盖到很多基本的标记，这些标记在很多场合下会被用到。</p>
<h3 id="不转换主机名、端口号等"><a href="#不转换主机名、端口号等" class="headerlink" title="不转换主机名、端口号等"></a>不转换主机名、端口号等</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -n</span></span><br></pre></td></tr></table></figure>
<p>通常情况下， tcpdump  会尝试查找和转换主机名和端口号。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p>你可以通过 <code>-n</code> 标记关闭这个功能。我个人总是使用这个标记，因为我喜欢使用 IP 地址而不是主机名，主机名和端口号的转换经常会带来困扰。但是，知道利用  <code>tcpdump</code>  转换或者不转换的功能还是相当有用的，特别是有些时候，知道源流量（source traffic）来自哪个服务器是相当重要的。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -n</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:23:47.934665 IP 10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], seq 2546457621:2546457817, ack 1824684201, win 355, options [nop,nop,TS val 621010158 ecr 621010055], length 196</span><br></pre></td></tr></table></figure></p>
<h3 id="增加详细信息"><a href="#增加详细信息" class="headerlink" title="增加详细信息"></a>增加详细信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -v</span></span><br></pre></td></tr></table></figure>
<p>增加一个简单 <code>-v</code> 标记，输出会包含更多信息，例如一个 IP 包的生存时间(ttl, time to live)、长度和其他的选项。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p><code>tcpdump</code>  的详细信息有三个等级，你可以通过在命令行增加 <code>v</code> 标记的个数来获取更多的信息。通常我在使用 <code>tcpmdump</code> 的时候，总是使用最高等级的详细信息，因为我希望看到所有信息，以免后面会用到。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -vvv -c 1</span></span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:36:13.873456 IP (tos 0x10, ttl 64, id 121, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    blog.ssh &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0x0dfd), seq 2546458841:2546458973, ack 1824684869, win 355, options [nop,nop,TS val 621196643 ecr 621196379], length 132</span><br></pre></td></tr></table></figure></p>
<h3 id="指定网络接口"><a href="#指定网络接口" class="headerlink" title="指定网络接口"></a>指定网络接口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -i eth0</span></span><br></pre></td></tr></table></figure>
<p>通常情况下，如果不指定网络接口， <code>tcpdump</code>  在运行时会选择编号最低的网络接口，一般情况下是 <code>eth0</code>，不过因系统不同可能会有所差异。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p>你可以用 <code>-i</code> 标记来指定网络接口。在大多数 Linux 系统上，<code>any</code> 这一特定的网络接口名用来让  <code>tcpdump</code>  监听所有的接口。我发现这在排查服务器（拥有多个网络接口）的问题特别有用，尤其是牵扯到路由的时候。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -i any</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">16:45:59.312046 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2547763641:2547763837, ack 1824693949, win 355, options [nop,nop,TS val 621343002 ecr 621342962], length 196</span><br></pre></td></tr></table></figure></p>
<h3 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -w /path/to/file</span></span><br></pre></td></tr></table></figure>
<p><code>tcpdump</code>  运行结果会输出在屏幕上。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p>但很多时候，你希望把  <code>tcpdump</code>  的输出结果保存在文件中，最简单的方法就是利用 <code>-w</code> 标记。如果你后续还会检查这些网络数据，这样做就特别有用。将这些数据存成一个文件的好处，就是你可以多次读取这个保存下来的文件，并且可以在这个网络流量的快照上使用其它标记或者过滤器（我们后面会讨论到）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -w /var/tmp/tcpdata.pcap</span></span><br><span class="line"> tcpdump : listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">1 packet captured</span><br><span class="line">2 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure></p>
<p>通常这些数据被缓存而不会被写入文件，直到你用 <code>CTRL+C</code> 结束 <code>tcpdump</code> 命令的时候。</p>
<h3 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -r /path/to/file</span></span><br></pre></td></tr></table></figure>
<p>一旦你将输出存成文件，就必然需要读取这个文件。要做到这点，你只需要在 <code>-r</code> 标记后指定这个文件的存放路径。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -r /var/tmp/tcpdata.pcap </span></span><br><span class="line">reading from file /var/tmp/tcpdata.pcap, link-type EN10MB (Ethernet)</span><br><span class="line">16:56:01.610473 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2547766673:2547766805, ack 1824696181, win 355, options [nop,nop,TS val 621493577 ecr 621493478], length 132</span><br></pre></td></tr></table></figure></p>
<p>一个小提醒，如果你熟悉 <a href="https://www.wireshark.org/" target="_blank" rel="external">wireshark</a> 这类网络诊断工具，也可以利用它们来读取  <code>tcpdump</code>  保存的文件。</p>
<h3 id="指定抓包大小"><a href="#指定抓包大小" class="headerlink" title="指定抓包大小"></a>指定抓包大小</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -s 100</span></span><br></pre></td></tr></table></figure>
<p>较新版本的  <code>tcpdump</code>  通常可以截获 <strong>65535</strong> 字节，但某些情况下你不需要截获默认大小的数据包。运行  <code>tcpdump</code>  时，你可以通过 <code>-s</code> 标记来指定快照长度。</p>
<h3 id="指定抓包数量"><a href="#指定抓包数量" class="headerlink" title="指定抓包数量"></a>指定抓包数量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -c 10</span></span><br></pre></td></tr></table></figure>
<p><code>tcpdump</code>  会一直运行，直至你用 <code>CTRL+C</code> 让它退出。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  host google.com</span></span><br><span class="line"> tcpdump : verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">^C</span><br><span class="line">0 packets captured</span><br><span class="line">4 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure></p>
<p>你也可以通过 <code>-c</code> 标记后面加上抓包的数量，让  <code>tcpdump</code> 在抓到一定数量的数据包后停止操作。当你不希望看到  <code>tcpdump</code>  的输出大量出现在屏幕上，以至于你无法阅读的时候，就会希望使用这个标记。当然，通常更好的方法是借助过滤器来截获特定的流量。</p>
<h3 id="基础知识汇总"><a href="#基础知识汇总" class="headerlink" title="基础知识汇总"></a>基础知识汇总</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 100 -s 100</span></span><br></pre></td></tr></table></figure>
<p>你可以将以上这些基础的标记组合起来使用，来让  <code>tcpdump</code>  提供你所需要的信息。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -w /var/tmp/tcpdata.pcap -i any -c 10 -vvv</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">10 packets captured</span><br><span class="line">10 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br><span class="line"><span class="comment"># tcpdump -r /var/tmp/tcpdata.pcap -nvvv -c 5</span></span><br><span class="line">reading from file /var/tmp/tcpdata.pcap, link-type LINUX_SLL (Linux cooked)</span><br><span class="line">17:35:14.465902 IP (tos 0x10, ttl 64, id 5436, offset 0, flags [DF], proto TCP (6), length 104)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1b51 (incorrect -&gt; 0x72bc), seq 2547781277:2547781329, ack 1824703573, win 355, options [nop,nop,TS val 622081791 ecr 622081775], length 52</span><br><span class="line">17:35:14.466007 IP (tos 0x10, ttl 64, id 52193, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x4950), seq 1, ack 52, win 541, options [nop,nop,TS val 622081791 ecr 622081791], length 0</span><br><span class="line">17:35:14.470239 IP (tos 0x10, ttl 64, id 5437, offset 0, flags [DF], proto TCP (6), length 168)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1b91 (incorrect -&gt; 0x98c3), seq 52:168, ack 1, win 355, options [nop,nop,TS val 622081792 ecr 622081791], length 116</span><br><span class="line">17:35:14.470370 IP (tos 0x10, ttl 64, id 52194, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x48da), seq 1, ack 168, win 541, options [nop,nop,TS val 622081792 ecr 622081792], length 0</span><br><span class="line">17:35:15.464575 IP (tos 0x10, ttl 64, id 5438, offset 0, flags [DF], proto TCP (6), length 104)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1b51 (incorrect -&gt; 0xc3ba), seq 168:220, ack 1, win 355, options [nop,nop,TS val 622082040 ecr 622081792], length 52</span><br></pre></td></tr></table></figure></p>
<h2 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h2><p>介绍完基础的标记后，我们该介绍过滤器了。 <code>tcpdump</code>  可以通过各式各样的表达式，来过滤所截取或者输出的数据。我在这篇文章里会给出一些简单的例子，以便让你们了解语法规则。你们可以查询  <code>tcpdump</code>  帮助中的 <a href="http://www.tcpdump.org/manpages/pcap-filter.7.html" target="_blank" rel="external">pcap-filter</a> 章节，了解更为详细的信息。</p>
<h3 id="查找特定主机的流量"><a href="#查找特定主机的流量" class="headerlink" title="查找特定主机的流量"></a>查找特定主机的流量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 3 host 10.0.3.1</span></span><br></pre></td></tr></table></figure>
<p>运行上述命令， <code>tcpdump</code>  会像前面一样把结果输出到屏幕上，不过只会显示源 IP 或者目的 IP 地址是 <code>10.0.3.1</code> 的数据包。通过增加主机 <code>10.0.3.1</code> 参数，我们可以让  <code>tcpdump</code>  过滤掉源和目的地址不是 <code>10.0.3.1</code> 的数据包。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 host 10.0.3.1</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">17:54:15.067496 IP (tos 0x10, ttl 64, id 5502, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0x9f75), seq 2547785621:2547785753, ack 1824705637, win 355, options [nop,nop,TS val 622366941 ecr 622366923], length 132</span><br><span class="line">17:54:15.067613 IP (tos 0x10, ttl 64, id 52315, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x7c34), seq 1, ack 132, win 540, options [nop,nop,TS val 622366941 ecr 622366941], length 0</span><br><span class="line">17:54:15.075230 IP (tos 0x10, ttl 64, id 5503, offset 0, flags [DF], proto TCP (6), length 648)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1d71 (incorrect -&gt; 0x3443), seq 132:728, ack 1, win 355, options [nop,nop,TS val 622366943 ecr 622366941], length 596</span><br></pre></td></tr></table></figure></p>
<h3 id="只显示源地址为特定主机的流量"><a href="#只显示源地址为特定主机的流量" class="headerlink" title="只显示源地址为特定主机的流量"></a>只显示源地址为特定主机的流量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 3 src host 10.0.3.1</span></span><br></pre></td></tr></table></figure>
<p>前面的例子显示了源和目的地址是 <code>10.0.3.1</code> 的流量，而上面的命令只显示数据包源地址是 <code>10.0.3.1</code> 的流量。这是通过在 <code>host</code> 前面增加 <code>src</code> 参数来实现的。这个额外的过滤器告诉  <code>tcpdump</code>  查找特定的源地址。 反过来通过 <code>dst</code> 过滤器，可以指定目的地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 src host 10.0.3.1</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">17:57:12.194902 IP (tos 0x10, ttl 64, id 52357, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x1707), seq 1824706545, ack 2547787717, win 540, options [nop,nop,TS val 622411223 ecr 622411223], length 0</span><br><span class="line">17:57:12.196288 IP (tos 0x10, ttl 64, id 52358, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x15c5), seq 0, ack 325, win 538, options [nop,nop,TS val 622411223 ecr 622411223], length 0</span><br><span class="line">17:57:12.197677 IP (tos 0x10, ttl 64, id 52359, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x1491), seq 0, ack 633, win 536, options [nop,nop,TS val 622411224 ecr 622411224], length 0</span><br><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 dst host 10.0.3.1</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">17:59:37.266838 IP (tos 0x10, ttl 64, id 5552, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0x586d), seq 2547789725:2547789857, ack 1824707577, win 355, options [nop,nop,TS val 622447491 ecr 622447471], length 132</span><br><span class="line">17:59:37.267850 IP (tos 0x10, ttl 64, id 5553, offset 0, flags [DF], proto TCP (6), length 392)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1c71 (incorrect -&gt; 0x462e), seq 132:472, ack 1, win 355, options [nop,nop,TS val 622447491 ecr 622447491], length 340</span><br><span class="line">17:59:37.268606 IP (tos 0x10, ttl 64, id 5554, offset 0, flags [DF], proto TCP (6), length 360)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1c51 (incorrect -&gt; 0xf469), seq 472:780, ack 1, win 355, options [nop,nop,TS val 622447491 ecr 622447491], length 308</span><br></pre></td></tr></table></figure></p>
<h3 id="过滤源和目的端口"><a href="#过滤源和目的端口" class="headerlink" title="过滤源和目的端口"></a>过滤源和目的端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 3 port 22 and port 60738</span></span><br></pre></td></tr></table></figure>
<p>通过类似 <code>and</code> 操作符，你可以在  <code>tcpdump</code>  上使用更为复杂的过滤器描述。这个就类似 <code>if</code> 语句，你就这么想吧。这个例子中，我们使用 <code>and</code> 操作符告诉  <code>tcpdump</code>  只输出端口号是 <code>22</code> 和 <code>60738</code> 的数据包。这点在分析网络问题的时候很有用，因为可以通过这个方法来关注某一个特定会话（session）的数据包。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 port 22 and port 60738</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:05:54.069403 IP (tos 0x10, ttl 64, id 64401, offset 0, flags [DF], proto TCP (6), length 104)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [P.], cksum 0x1b51 (incorrect -&gt; 0x5b3c), seq 917414532:917414584, ack 1550997318, win 353, options [nop,nop,TS val 622541691 ecr 622538903], length 52</span><br><span class="line">18:05:54.072963 IP (tos 0x10, ttl 64, id 13601, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.60738: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0xb0b1), seq 1:133, ack 52, win 355, options [nop,nop,TS val 622541692 ecr 622541691], length 132</span><br><span class="line">18:05:54.073080 IP (tos 0x10, ttl 64, id 64402, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x1e3b), seq 52, ack 133, win 353, options [nop,nop,TS val 622541692 ecr 622541692], length 0</span><br></pre></td></tr></table></figure></p>
<p>你可以用两种方式来表示 <code>and</code> 操作符，<code>and</code> 或者 <code>&amp;&amp;</code> 都可以。我个人倾向于两个都使用，特别要记住在使用 <code>&amp;&amp;</code> 的时候，要用单引号或者双引号包住表达式。在 BASH 中，你可以使用 <code>&amp;&amp;</code> 运行一个命令，该命令成功后再执行后面的命令。通常，最好将表达式用引号包起来，这样会避免不预期的结果，特别当过滤器中有一些特殊字符的时候。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 'port 22 &amp;&amp; port 60738'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:06:16.062818 IP (tos 0x10, ttl 64, id 64405, offset 0, flags [DF], proto TCP (6), length 88)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [P.], cksum 0x1b41 (incorrect -&gt; 0x776c), seq 917414636:917414672, ack 1550997518, win 353, options [nop,nop,TS val 622547190 ecr 622541776], length 36</span><br><span class="line">18:06:16.065567 IP (tos 0x10, ttl 64, id 13603, offset 0, flags [DF], proto TCP (6), length 120)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.60738: Flags [P.], cksum 0x1b61 (incorrect -&gt; 0xaf2d), seq 1:69, ack 36, win 355, options [nop,nop,TS val 622547191 ecr 622547190], length 68</span><br><span class="line">18:06:16.065696 IP (tos 0x10, ttl 64, id 64406, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0xf264), seq 36, ack 69, win 353, options [nop,nop,TS val 622547191 ecr 622547191], length 0</span><br></pre></td></tr></table></figure></p>
<h3 id="查找两个端口号的流量"><a href="#查找两个端口号的流量" class="headerlink" title="查找两个端口号的流量"></a>查找两个端口号的流量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 20 'port 80 or port 443'</span></span><br></pre></td></tr></table></figure>
<p>你可以用 <code>or</code> 或者 <code>||</code> 操作符来过滤结果。在这个例子中，我们使用 <code>or</code> 操作符去截获发送和接收端口为 <code>80</code> 或 <code>443</code> 的数据流。这在 Web 服务器上特别有用，因为服务器通常有两个开放的端口，端口号 <code>80</code> 表示 <code>http</code> 连接，<code>443</code> 表示 <code>https</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 20 'port 80 or port 443'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:24:28.817940 IP (tos 0x0, ttl 64, id 39930, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.1.50524 &gt; 10.0.3.246.443: Flags [S], cksum 0x1b25 (incorrect -&gt; 0x8611), seq 3836995553, win 29200, options [mss 1460,sackOK,TS val 622820379 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:24:28.818052 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 40)</span><br><span class="line">    10.0.3.246.443 &gt; 10.0.3.1.50524: Flags [R.], cksum 0x012c (correct), seq 0, ack 3836995554, win 0, length 0</span><br><span class="line">18:24:32.721330 IP (tos 0x0, ttl 64, id 48510, offset 0, flags [DF], proto TCP (6), length 475)</span><br><span class="line">    10.0.3.1.60374 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1cc4 (incorrect -&gt; 0x3a4e), seq 580573019:580573442, ack 1982754038, win 237, options [nop,nop,TS val 622821354 ecr 622815632], length 423</span><br><span class="line">18:24:32.721465 IP (tos 0x0, ttl 64, id 1266, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.246.80 &gt; 10.0.3.1.60374: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x45d7), seq 1, ack 423, win 243, options [nop,nop,TS val 622821355 ecr 622821354], length 0</span><br><span class="line">18:24:32.722098 IP (tos 0x0, ttl 64, id 1267, offset 0, flags [DF], proto TCP (6), length 241)</span><br><span class="line">    10.0.3.246.80 &gt; 10.0.3.1.60374: Flags [P.], cksum 0x1bda (incorrect -&gt; 0x855c), seq 1:190, ack 423, win 243, options [nop,nop,TS val 622821355 ecr 622821354], length 189</span><br><span class="line">18:24:32.722232 IP (tos 0x0, ttl 64, id 48511, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.60374 &gt; 10.0.3.246.80: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x4517), seq 423, ack 190, win 245, options [nop,nop,TS val 622821355 ecr 622821355], length 0</span><br></pre></td></tr></table></figure></p>
<h3 id="查找两个特定端口和来自特定主机的数据流"><a href="#查找两个特定端口和来自特定主机的数据流" class="headerlink" title="查找两个特定端口和来自特定主机的数据流"></a>查找两个特定端口和来自特定主机的数据流</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 20 '(port 80 or port 443) and host 10.0.3.169'</span></span><br></pre></td></tr></table></figure>
<p>前面的例子用来排查多端口的协议问题，是非常有效的。如果 Web 服务器的数据流量相当大， <code>tcpdump</code>  的输出可能有点混乱。我们可以通过增加 <code>host</code> 参数进一步限定输出。在这种情况下，我们通过把 <code>or</code> 表达式放在括号中来保持 <code>or</code> 描述。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 20 '(port 80 or port 443) and host 10.0.3.169'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:38:05.551194 IP (tos 0x0, ttl 64, id 63169, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.169.33786 &gt; 10.0.3.246.443: Flags [S], cksum 0x1bcd (incorrect -&gt; 0x0d96), seq 4173164403, win 29200, options [mss 1460,sackOK,TS val 623024562 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:38:05.551310 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 40)</span><br><span class="line">    10.0.3.246.443 &gt; 10.0.3.169.33786: Flags [R.], cksum 0xa64a (correct), seq 0, ack 4173164404, win 0, length 0</span><br><span class="line">18:38:05.717130 IP (tos 0x0, ttl 64, id 51574, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.169.35629 &gt; 10.0.3.246.80: Flags [S], cksum 0x1bcd (incorrect -&gt; 0xdf7c), seq 1068257453, win 29200, options [mss 1460,sackOK,TS val 623024603 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:38:05.717255 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.80 &gt; 10.0.3.169.35629: Flags [S.], cksum 0x1bcd (incorrect -&gt; 0xed80), seq 2992472447, ack 1068257454, win 28960, options [mss 1460,sackOK,TS val 623024603 ecr 623024603,nop,wscale 7], length 0</span><br><span class="line">18:38:05.717474 IP (tos 0x0, ttl 64, id 51575, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.169.35629 &gt; 10.0.3.246.80: Flags [.], cksum 0x1bc5 (incorrect -&gt; 0x8c87), seq 1, ack 1, win 229, options [nop,nop,TS val 623024604 ecr 623024603], length 0</span><br></pre></td></tr></table></figure></p>
<p>在一个过滤器中，你可以多次使用括号。在下面的例子中，下面命令可以限定截获满足如下条件的数据包：发送或接收端口号为 <code>80</code> 或 <code>443</code>，主机来源于 <code>10.0.3.169</code> 或者 <code>10.0.3.1</code>，且目的地址是 <code>10.0.3.246</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 20 '((port 80 or port 443) and (host 10.0.3.169 or host 10.0.3.1)) and dst host 10.0.3.246'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:53:30.349306 IP (tos 0x0, ttl 64, id 52641, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [S], cksum 0x1b25 (incorrect -&gt; 0x4890), seq 3026316656, win 29200, options [mss 1460,sackOK,TS val 623255761 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:53:30.349558 IP (tos 0x0, ttl 64, id 52642, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x3454), seq 3026316657, ack 3657995297, win 229, options [nop,nop,TS val 623255762 ecr 623255762], length 0</span><br><span class="line">18:53:30.354056 IP (tos 0x0, ttl 64, id 52643, offset 0, flags [DF], proto TCP (6), length 475)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1cc4 (incorrect -&gt; 0x10c2), seq 0:423, ack 1, win 229, options [nop,nop,TS val 623255763 ecr 623255762], length 423</span><br><span class="line">18:53:30.354682 IP (tos 0x0, ttl 64, id 52644, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x31e6), seq 423, ack 190, win 237, options [nop,nop,TS val 623255763 ecr 623255763], length 0</span><br></pre></td></tr></table></figure></p>
<h2 id="理解输出结果"><a href="#理解输出结果" class="headerlink" title="理解输出结果"></a>理解输出结果</h2><p>打开  <code>tcpdump</code>  的所有选项去截获网络流量是相当困难的，但一旦你拿到这些数据你就要对它进行解读。在这个章节，我们将涉及如何判断源/目的 IP 地址，源/目的端口号，以及 <code>TCP</code> 协议类型的数据包。当然这些是相当基础的，你从  <code>tcpdump</code>  里面获取的信息也远不止这些。不过这篇文章主要是粗略的介绍，我们会关注在这些基础知识上。我建议你们可以通过<a href="http://www.tcpdump.org/manpages/" target="_blank" rel="external">帮助页</a>获取更为详细的信息。</p>
<h3 id="判断源和目的地址"><a href="#判断源和目的地址" class="headerlink" title="判断源和目的地址"></a>判断源和目的地址</h3><p>判断源和目的地址和端口号相当简单。</p>
<p>从上面的输出，我们可以看到源 IP 地址是 <code>10.0.3.246</code>，源端口号是 <code>56894</code>， 目的 IP 地址是 <code>192.168.0.92</code>，端口号是 <code>22</code>。一旦你理解  <code>tcpdump</code>  格式后，这些信息很容易判断。如果你还没有猜到格式，你可以按照 <code>src-ip.src-port &gt; dest-ip.dest-port: Flags[S]</code> 格式来分析。源地址位于 <code>&gt;</code> 前面，后面则是目的地址。你可以把 <code>&gt;</code> 想象成一个指向目的地址的箭头符号。</p>
<h3 id="判断数据包类型"><a href="#判断数据包类型" class="headerlink" title="判断数据包类型"></a>判断数据包类型</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x0388), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989005 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure>
<p>从上面的例子，我们可以判断这个数据包是一个 <code>SYN</code> 数据包。我们是通过  <code>tcpdump</code>  输出中的 <code>[S]</code> 标记字段得出这个结论，不同类型的数据包有不同类型的标记。不需要深入了解 <code>TCP</code> 协议中的数据包类型，你就可以通过下面的速查表来加以判断。</p>
<ul>
<li>[S] – SYN (开始连接)</li>
<li>[.] – 没有标记</li>
<li>[P] – PSH (数据推送)</li>
<li>[F] – FIN (结束连接)</li>
<li>[R] – RST (重启连接)</li>
</ul>
<p>在这个版本的  <code>tcpdump</code>  输出中，<code>[S.]</code> 标记代表这个数据包是 <code>SYN-ACK</code> 数据包。</p>
<h3 id="不好的例子"><a href="#不好的例子" class="headerlink" title="不好的例子"></a>不好的例子</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">15:15:43.323412 IP (tos 0x0, ttl 64, id 51051, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x0388), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989005 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:15:44.321444 IP (tos 0x0, ttl 64, id 51052, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x028e), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989255 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:15:46.321610 IP (tos 0x0, ttl 64, id 51053, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x009a), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989755 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure>
<p>上面显示了一个不好的通信例子，在这个例子中“不好”，代表通信没有建立起来。我们可以看到 <code>10.0.3.246</code> 发出一个 <code>SYN</code> 数据包给 主机 <code>192.168.0.92</code>，但是主机并没有应答。</p>
<h3 id="好的例子"><a href="#好的例子" class="headerlink" title="好的例子"></a>好的例子</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">15:18:25.716453 IP (tos 0x10, ttl 64, id 53344, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.34908 &gt; 192.168.0.110.22: Flags [S], cksum 0xcf3a (incorrect -&gt; 0xc838), seq 1943877315, win 29200, options [mss 1460,sackOK,TS val 620029603 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:18:25.716777 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    192.168.0.110.22 &gt; 10.0.3.246.34908: Flags [S.], cksum 0x594a (correct), seq 4001145915, ack 1943877316, win 5792, options [mss 1460,sackOK,TS val 18495104 ecr 620029603,nop,wscale 2], length 0</span><br><span class="line">15:18:25.716899 IP (tos 0x10, ttl 64, id 53345, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.246.34908 &gt; 192.168.0.110.22: Flags [.], cksum 0xcf32 (incorrect -&gt; 0x9dcc), ack 1, win 229, options [nop,nop,TS val 620029603 ecr 18495104], length 0</span><br></pre></td></tr></table></figure>
<p>好的例子应该向上面这样，我们看到典型的 TCP 3次握手。第一数据包是 <code>SYN</code> 包，从主机 <code>10.0.3.246</code> 发送给 主机<code>192.168.0.110</code>，第二个包是 <code>SYN-ACK</code> 包，主机<code>192.168.0.110</code> 回应 <code>SYN</code> 包。最后一个包是一个 <code>ACK</code> 或者 <code>SYN – ACK – ACK</code> 包，是主机 <code>10.0.3.246</code> 回应收到了 <code>SYN – ACK</code> 包。从上面看到一个 TCP/IP 连接成功建立。</p>
<h2 id="数据包检查"><a href="#数据包检查" class="headerlink" title="数据包检查"></a>数据包检查</h2><h3 id="用十六进制和-ASCII-码打印数据包"><a href="#用十六进制和-ASCII-码打印数据包" class="headerlink" title="用十六进制和 ASCII 码打印数据包"></a>用十六进制和 ASCII 码打印数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 1 -XX 'port 80 and host 10.0.3.1'</span></span><br></pre></td></tr></table></figure>
<p>排查应用程序网络问题的通常做法，就是用  <code>tcpdump</code>  的 <code>-XX</code> 标记打印出 16 进制和 ASCII 码格式的数据包。这是一个相当有用的命令，它可以让你看到源地址，目的地址，数据包类型以及数据包本身。但我不是这个命令输出的粉丝，我认为它太难读了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 1 -XX 'port 80 and host 10.0.3.1'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">19:51:15.697640 IP (tos 0x0, ttl 64, id 54313, offset 0, flags [DF], proto TCP (6), length 483)</span><br><span class="line">    10.0.3.1.45732 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1ccc (incorrect -&gt; 0x2ce8), seq 3920159713:3920160144, ack 969855140, win 245, options [nop,nop,TS val 624122099 ecr 624117334], length 431</span><br><span class="line">        0x0000:  0000 0001 0006 fe0a e2d1 8785 0000 0800  ................</span><br><span class="line">        0x0010:  4500 01e3 d429 4000 4006 49f5 0a00 0301  E....)@.@.I.....</span><br><span class="line">        0x0020:  0a00 03f6 b2a4 0050 e9a8 e3e1 39ce d0a4  .......P....9...</span><br><span class="line">        0x0030:  8018 00f5 1ccc 0000 0101 080a 2533 58f3  ............%3X.</span><br><span class="line">        0x0040:  2533 4656 4745 5420 2f73 6f6d 6570 6167  %3FVGET./somepag</span><br><span class="line">        0x0050:  6520 4854 5450 2f31 2e31 0d0a 486f 7374  e.HTTP/1.1..Host</span><br><span class="line">        0x0060:  3a20 3130 2e30 2e33 2e32 3436 0d0a 436f  :.10.0.3.246..Co</span><br><span class="line">        0x0070:  6e6e 6563 7469 6f6e 3a20 6b65 6570 2d61  nnection:.keep<span class="_">-a</span></span><br><span class="line">        0x0080:  6c69 7665 0d0a 4361 6368 652d 436f 6e74  live..Cache-Cont</span><br><span class="line">        0x0090:  726f 6c3a 206d 6178 2d61 6765 3d30 0d0a  rol:.max-age=0..</span><br><span class="line">        0x00a0:  4163 6365 7074 3a20 7465 7874 2f68 746d  Accept:.text/htm</span><br><span class="line">        0x00b0:  6c2c 6170 706c 6963 6174 696f 6e2f 7868  l,application/xh</span><br><span class="line">        0x00c0:  746d 6c2b 786d 6c2c 6170 706c 6963 6174  tml+xml,applicat</span><br><span class="line">        0x00d0:  696f 6e2f 786d 6c3b 713d 302e 392c 696d  ion/xml;q=0.9,im</span><br><span class="line">        0x00e0:  6167 652f 7765 6270 2c2a 2f2a 3b71 3d30  age/webp,*/*;q=0</span><br><span class="line">        0x00f0:  2e38 0d0a 5573 6572 2d41 6765 6e74 3a20  .8..User-Agent:.</span><br><span class="line">        0x0100:  4d6f 7a69 6c6c 612f 352e 3020 284d 6163  Mozilla/5.0.(Mac</span><br><span class="line">        0x0110:  696e 746f 7368 3b20 496e 7465 6c20 4d61  intosh;.Intel.Ma</span><br><span class="line">        0x0120:  6320 4f53 2058 2031 305f 395f 3529 2041  c.OS.X.10_9_5).A</span><br><span class="line">        0x0130:  7070 6c65 5765 624b 6974 2f35 3337 2e33  ppleWebKit/537.3</span><br><span class="line">        0x0140:  3620 284b 4854 4d4c 2c20 6c69 6b65 2047  6.(KHTML,.like.G</span><br><span class="line">        0x0150:  6563 6b6f 2920 4368 726f 6d65 2f33 382e  ecko).Chrome/38.</span><br><span class="line">        0x0160:  302e 3231 3235 2e31 3031 2053 6166 6172  0.2125.101.Safar</span><br><span class="line">        0x0170:  692f 3533 372e 3336 0d0a 4163 6365 7074  i/537.36..Accept</span><br><span class="line">        0x0180:  2d45 6e63 6f64 696e 673a 2067 7a69 702c  -Encoding:.gzip,</span><br><span class="line">        0x0190:  6465 666c 6174 652c 7364 6368 0d0a 4163  deflate,sdch..Ac</span><br><span class="line">        0x01a0:  6365 7074 2d4c 616e 6775 6167 653a 2065  cept-Language:.e</span><br><span class="line">        0x01b0:  6e2d 5553 2c65 6e3b 713d 302e 380d 0a49  n-US,en;q=0.8..I</span><br><span class="line">        0x01c0:  662d 4d6f 6469 6669 6564 2d53 696e 6365  f-Modified-Since</span><br><span class="line">        0x01d0:  3a20 5375 6e2c 2031 3220 4f63 7420 3230  :.Sun,.12.Oct.20</span><br><span class="line">        0x01e0:  3134 2031 393a 3430 3a32 3020 474d 540d  14.19:40:20.GMT.</span><br><span class="line">        0x01f0:  0a0d 0a                                  ...</span><br></pre></td></tr></table></figure></p>
<h3 id="只打印-ASCII-码格式的数据包"><a href="#只打印-ASCII-码格式的数据包" class="headerlink" title="只打印 ASCII 码格式的数据包"></a>只打印 ASCII 码格式的数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 1 -A 'port 80 and host 10.0.3.1'</span></span><br></pre></td></tr></table></figure>
<p>我倾向于只打印 ASCII 格式数据，这可以帮助我快速定位数据包中发送了什么，哪些是正确的，哪些是错误的。你可以通过 <code>-A</code> 标记来实现这一点。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 1 -A 'port 80 and host 10.0.3.1'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">19:59:52.011337 IP (tos 0x0, ttl 64, id 53757, offset 0, flags [DF], proto TCP (6), length 406)</span><br><span class="line">    10.0.3.1.46172 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1c7f (incorrect -&gt; 0xead1), seq 1552520173:1552520527, ack 428165415, win 237, options [nop,nop,TS val 624251177 ecr 624247749], length 354</span><br><span class="line">E.....@.@.Ln</span><br><span class="line">...</span><br><span class="line">....\.P\.....I<span class="string">'...........</span><br><span class="line">%5Q)%5C.GET /newpage HTTP/1.1</span><br><span class="line"> </span><br><span class="line">Host: 10.0.3.246</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8</span><br><span class="line">User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36</span><br><span class="line">Accept-Encoding: gzip,deflate,sdch</span><br><span class="line">Accept-Language: en-US,en;q=0.8</span></span><br></pre></td></tr></table></figure></p>
<p>从上面的输出，你可以看到我们成功获取了一个 http 的 <code>GET</code> 请求包。如果网络通信没有被加密，用人类可阅读的格式打出包中数据，对于解决应用程序的问题是很有帮助。如果你排查一个网络通信被加密的问题，打印包中数据就不是很有用。不过如果你有证书的话，你还是可以使用 <code>ssldump</code> 或者 <code>wireshark</code>。</p>
<h2 id="非-TCP-数据流"><a href="#非-TCP-数据流" class="headerlink" title="非 TCP 数据流"></a>非 TCP 数据流</h2><p>虽然这篇文章主要采用 TCP 传输来讲解  <code>tcpdump</code> ，但是  <code>tcpdump</code>  绝对不是只能抓 TCP 数据包。它还可以用来获取其他类型的数据包，例如 ICMP、 UDP 和 ARP 包。下面是一些简单的例子，说明  <code>tcpdump</code>  可以截获非 TCP 数据包。</p>
<h3 id="ICMP-数据包"><a href="#ICMP-数据包" class="headerlink" title="ICMP 数据包"></a>ICMP 数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 2 icmp</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">20:11:24.627824 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.0.3.169 &gt; 10.0.3.246: ICMP <span class="built_in">echo</span> request, id 15683, seq 1, length 64</span><br><span class="line">20:11:24.627926 IP (tos 0x0, ttl 64, id 31312, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.0.3.246 &gt; 10.0.3.169: ICMP <span class="built_in">echo</span> reply, id 15683, seq 1, length 64</span><br></pre></td></tr></table></figure>
<h3 id="UDP-数据包"><a href="#UDP-数据包" class="headerlink" title="UDP 数据包"></a>UDP 数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 2 udp</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">20:12:41.726355 IP (tos 0xc0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 76)</span><br><span class="line">    10.0.3.246.123 &gt; 198.55.111.50.123: [bad udp cksum 0x43a9 -&gt; 0x7043!] NTPv4, length 48</span><br><span class="line">        Client, Leap indicator: clock unsynchronized (192), Stratum 2 (secondary reference), poll 6 (64s), precision -22</span><br><span class="line">        Root Delay: 0.085678, Root dispersion: 57.141830, Reference-ID: 199.102.46.75</span><br><span class="line">          Reference Timestamp:  3622133515.811991035 (2014/10/12 20:11:55)</span><br><span class="line">          Originator Timestamp: 3622133553.828614115 (2014/10/12 20:12:33)</span><br><span class="line">          Receive Timestamp:    3622133496.748308420 (2014/10/12 20:11:36)</span><br><span class="line">          Transmit Timestamp:   3622133561.726278364 (2014/10/12 20:12:41)</span><br><span class="line">            Originator - Receive Timestamp:  -57.080305658</span><br><span class="line">            Originator - Transmit Timestamp: +7.897664248</span><br><span class="line">20:12:41.748948 IP (tos 0x0, ttl 54, id 9285, offset 0, flags [none], proto UDP (17), length 76)</span><br><span class="line">    198.55.111.50.123 &gt; 10.0.3.246.123: [udp sum ok] NTPv4, length 48</span><br><span class="line">        Server, Leap indicator:  (0), Stratum 3 (secondary reference), poll 6 (64s), precision -20</span><br><span class="line">        Root Delay: 0.054077, Root dispersion: 0.058944, Reference-ID: 216.229.0.50</span><br><span class="line">          Reference Timestamp:  3622132887.136984840 (2014/10/12 20:01:27)</span><br><span class="line">          Originator Timestamp: 3622133561.726278364 (2014/10/12 20:12:41)</span><br><span class="line">          Receive Timestamp:    3622133618.830113530 (2014/10/12 20:13:38)</span><br><span class="line">          Transmit Timestamp:   3622133618.830129086 (2014/10/12 20:13:38)</span><br><span class="line">            Originator - Receive Timestamp:  +57.103835195</span><br><span class="line">            Originator - Transmit Timestamp: +57.103850722</span><br></pre></td></tr></table></figure>
<p>如果你觉得有好例子进一步说明  <code>tcpdump</code>  命令，请在评论中补充。</p>
<p>来源：bencane.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Calico 问题排障]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-calico-troubleshooting.html</url>
      <content type="html"><![CDATA[<h2 id="故障一"><a href="#故障一" class="headerlink" title="故障一"></a>故障一</h2><h3 id="问题表现"><a href="#问题表现" class="headerlink" title="问题表现"></a>问题表现</h3><p>集群中有台node服务器因为资源达到上限出现假死现状，重启后发现calico node 无法启动成功，提示如下信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:Readiness probe failed: caliconode is not ready: BIRD is not ready: BGP not established with 172.18.0.1</span><br></pre></td></tr></table></figure></p>
<p>使用<code>calicoctl node status</code> 命令查看node状态信息提示如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+--------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |              INFO              |</span><br><span class="line">+--------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">| 172.18.0.1   | node-to-node mesh | start | 01:56:41 | Connect Socket: Network        |</span><br><span class="line">|              |                   |       |          | unreachable                    |</span><br><span class="line">+--------------+-------------------+-------+----------+--------------------------------+</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h3><p>在BGP网络中出现了一个未知的IP地址172.18.0.1，我们集群中的机器都是10开头的网络地址，所以登录有问题的机器上查看对应的网络信息 <code>ip a</code>，结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">14: br-3a10a9384428: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:3a:f3:45:18 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-3a10a9384428</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:3aff:fef3:4518/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>后来咨询了相关同事，结果是同事在node机器上用docker启动了<a href="https://github.com/google/cadvisor" target="_blank" rel="external"><code>advisor</code></a> 这个容器监控程序，所以会产生一块虚拟网卡出来。</p>
<h3 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h3><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>将<code>advisor</code> 改成使用二进制文件启动，不使用容器启动，则不会产生虚拟网络设备</p>
<h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p>调整calicao 网络插件的网卡发现机制，修改<code>IP_AUTODETECTION_METHOD</code>对应的value值。官方提供的yaml文件中，ip识别策略（IPDETECTMETHOD）没有配置，即默认为first-found，这会导致一个网络异常的ip作为nodeIP被注册，从而影响<code>node-to-node mesh</code>。我们可以修改成<code>can-reach</code>或者<code>interface</code>的策略，尝试连接某一个Ready的node的IP，以此选择出正确的IP。</p>
<ul>
<li><p><strong>can-reach</strong> 使用您的本地路由来确定将使用哪个IP地址到达提供的目标。可以使用IP地址和域名。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using IP addresses</span></span><br><span class="line">IP_AUTODETECTION_METHOD=can-reach=8.8.8.8</span><br><span class="line">IP6_AUTODETECTION_METHOD=can-reach=2001:4860:4860::8888</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using domain names</span></span><br><span class="line">IP_AUTODETECTION_METHOD=can-reach=www.google.com</span><br><span class="line">IP6_AUTODETECTION_METHOD=can-reach=www.google.com</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>interface</strong> 使用提供的接口正则表达式（golang语法）枚举匹配的接口并返回第一个匹配接口上的第一个IP地址。列出接口和IP地址的顺序取决于系统。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Valid IP address on interface eth0, eth1, eth2 etc.</span></span><br><span class="line">IP_AUTODETECTION_METHOD=interface=eth.*</span><br><span class="line">IP6_AUTODETECTION_METHOD=interface=eth.*</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="故障二"><a href="#故障二" class="headerlink" title="故障二"></a>故障二</h2><h3 id="问题表现-1"><a href="#问题表现-1" class="headerlink" title="问题表现"></a>问题表现</h3><p>将某台新机器（aws美东区域）加到容器集群之后，发现该节点没法加入到容器集群里面。</p>
<p>该节点的calico-node启动后不久反复的crash重启。crash前的log如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl logs -f calico-node-wm6bb  -n kube-system -c calico-node</span></span><br><span class="line">Skipping datastore connection <span class="built_in">test</span></span><br><span class="line">Using autodetected IPv4 address 10.12.13.12/24 on matching interface eth0</span><br></pre></td></tr></table></figure></p>
<p>恰巧新加坡和美东区域各有一台闲置机器，尝试将这两台机器加入calico网络。发现新加坡机器成功加入到calico网络，而美东机器加入calico失败，且失败表现相同。</p>
<p>这究竟是什么鬼原因？</p>
<h3 id="问题排查-1"><a href="#问题排查-1" class="headerlink" title="问题排查"></a>问题排查</h3><h4 id="排除ETCD连接问题"><a href="#排除ETCD连接问题" class="headerlink" title="排除ETCD连接问题"></a>排除ETCD连接问题</h4><p>calico-node启动阶段会访问etcd获取集群网络配置，所以首先怀疑会不会是节点连接etcd失败。</p>
<p>在问题节点上，尝试访问etcd：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  curl --cacert /etc/cni/net.d/calico-tls/etcd-ca --cert /etc/cni/net.d/calico-tls/etcd-cert --key /etc/cni/net.d/calico-tls/etcd-key https://[ETCD服务IP]:2379/health</span></span><br><span class="line">&#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>发现可以连上etcd。那么可以排除etcd连接问题。</p>
<h4 id="排除AWS路由表限制"><a href="#排除AWS路由表限制" class="headerlink" title="排除AWS路由表限制"></a>排除AWS路由表限制</h4><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-route-tables" target="_blank" rel="external">AWS EC2路由表有50条的数量限制</a> ，这有可能会限制集群的节点数上限。<a href="https://docs.projectcalico.org/v3.2/reference/public-cloud/aws" target="_blank" rel="external">但根据calico官方文档，calico应该不受aws 50节点的限制</a> :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No 50 Node Limit: Calico allows you to surpass the 50 node <span class="built_in">limit</span>, <span class="built_in">which</span> exists as a consequence of the AWS 50 route <span class="built_in">limit</span> when using the VPC routing table.</span><br></pre></td></tr></table></figure></p>
<p>而且，我们的集群节点数已经超出50（目前节点数53）。</p>
<p>至此，排除AWS路由表原因。</p>
<h4 id="排除aws-security-groups影响"><a href="#排除aws-security-groups影响" class="headerlink" title="排除aws security groups影响"></a>排除aws security groups影响</h4><p>仔细看了<a href="https://docs.projectcalico.org/v2.6/reference/public-cloud/aws" target="_blank" rel="external">calico的aws部署说明，其中提到需要配置aws安全组，允许BGP和IPIP通信</a>，会不会因为公司aws美东区域安全组配置的原因？</p>
<p>查看美东那两台问题机器的security-groups是 DY-Default-10.12，而其他机器的 security-groups是DY-Default-10.12。很失望。可以排除aws security group的影响。</p>
<h4 id="最终原因"><a href="#最终原因" class="headerlink" title="最终原因"></a>最终原因</h4><p>至此，问题原因仍毫无头绪。只好找来<a href="https://github.com/projectcalico/node/blob/4db4e815e47885db77957e113a18269fa1ce0ffd/pkg/startup/startup.go#L233" target="_blank" rel="external">calico node的启动代码</a>来看看。</p>
<p>期间发现，calico node启动时是依据<code>CALICO_STARTUP_LOGLEVEL</code>环境变量来设置log级别。考虑到出问题的calico node输出的log实在是太少，修改calico node的daemonset配置，将log等级设置成最低的DEBUG级别：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - env:</span></span><br><span class="line"><span class="attr">    - name:</span> CALICO_STARTUP_LOGLEVEL</span><br><span class="line"><span class="attr">      value:</span> DEBUG</span><br></pre></td></tr></table></figure></p>
<p>重启问题节点的calico-node，果然输出了更多的log：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl logs -f calico-node-jz7cz -n kube-system -c calico-node </span></span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=info msg=<span class="string">"Early log level set to debug"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=info msg=<span class="string">"NODENAME environment not specified - check HOSTNAME"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=info msg=<span class="string">"Loading config from environment"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=debug msg=<span class="string">"Using datastore type 'etcdv2'"</span> </span><br><span class="line">Skipping datastore connection <span class="built_in">test</span></span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=debug msg=<span class="string">"Validate name: AMZ-IAD12-OpsResPool-13-33"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=debug msg=<span class="string">"Get Key: /calico/v1/host/AMZ-IAD12-OpsResPool-13-33/metadata"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=info msg=<span class="string">"Building new node resource"</span> Name=AMZ-IAD12-OpsResPool-13-33 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=info msg=<span class="string">"Initialise BGP data"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Querying interface addresses"</span> Interface=eth0 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Found valid IP address and network"</span> CIDR=10.12.13.33/24 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Check interface"</span> Name=eth0 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Check address"</span> CIDR=10.12.13.33/24 </span><br><span class="line">Using autodetected IPv4 address 10.12.13.33/24 on matching interface eth0</span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=info msg=<span class="string">"Node IPv4 changed, will check for conflicts"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Listing all host metadatas"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Parse host directories."</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Get Node key from /calico/v1/host/AMZ-IAD12-Coupon-35-221/metadata"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Get Node key from /calico/v1/host/AMZ-IAD12-Coupon-35-222/metadata"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Get Node key from /calico/v1/host/AMZ-IAD12-OpsResPool-13-31/metadata"</span> </span><br><span class="line"></span><br><span class="line">.... 省略类似<span class="built_in">log</span> ....</span><br><span class="line"></span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/ip_addr_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/network_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/ip_addr_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/network_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/as_num"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:27Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:27Z"</span> level=debug msg=<span class="string">"Get Key: /calico/v1/host/AMZ-OR39-CTR-135-60/orchestrator_refs"</span> </span><br><span class="line"></span><br><span class="line">.... 省略类似<span class="built_in">log</span> ....</span><br><span class="line"></span><br><span class="line">time=<span class="string">"2018-10-31T07:48:12Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/ip_addr_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/network_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/ip_addr_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/network_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/as_num"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:14Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:14Z"</span> level=debug msg=<span class="string">"Get Key: /calico/v1/host/AMZ-SIN8-OpsResPool-33-62/orchestrator_refs"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:14Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-63/ip_addr_v4"</span></span><br></pre></td></tr></table></figure></p>
<p>输出完上面的log后，calico-node就被重启了，是什么原因呢？</p>
<p>根据calico node的代码，<a href="https://github.com/projectcalico/node/blob/4db4e815e47885db77957e113a18269fa1ce0ffd/pkg/startup/startup.go#L1003" target="_blank" rel="external">如果calico node启动失败，那退出前会先打一行log：Terminating</a>，但在上面的log中并未发现有Terminating。</p>
<p>从log看出，calico-node终止前是在查询etcd中的节点数据。于是又试了试在问题节点上查询etcd：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># etcdctl  --ca-file=/var/lib/kubernetes/ca.pem --cert-file=/var/lib/kubernetes/kubernetes.pem --key-file=/var/lib/kubernetes/kubernetes-key.pem --endpoints=https://[etcd服务ip]:2379 get /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-63/ip_addr_v4</span></span><br><span class="line">10.8.33.63</span><br></pre></td></tr></table></figure></p>
<p>看结果很正常，绝不会因此而出错。</p>
<p>纠结之际，突然想到：会不会calico-node启动超时了？</p>
<h3 id="问题处理-1"><a href="#问题处理-1" class="headerlink" title="问题处理"></a>问题处理</h3><p>马上看了下calico-node daemonset的livenessProbe：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line"><span class="attr">  failureThreshold:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  httpGet:</span></span><br><span class="line"><span class="attr">    path:</span> /liveness</span><br><span class="line"><span class="attr">    port:</span> <span class="number">9099</span></span><br><span class="line"><span class="attr">    scheme:</span> HTTP</span><br><span class="line"><span class="attr">  initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  timeoutSeconds:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>尝试着把initialDelaySeconds加到60秒，failureThreshold加到10。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line"><span class="attr">  failureThreshold:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  httpGet:</span></span><br><span class="line"><span class="attr">    path:</span> /liveness</span><br><span class="line"><span class="attr">    port:</span> <span class="number">9099</span></span><br><span class="line"><span class="attr">    scheme:</span> HTTP</span><br><span class="line"><span class="attr">  initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">  periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  timeoutSeconds:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>改完重启了问题节点的calico-node。等待了数分钟，发现居然启动成功了。</p>
<p>至此，问题原因就明确了：<strong><code>由于集群节点越来越多，calico-node启动所需时间也随着变长了，超出了liveness probe的重启时间限制，从而被k8s干掉重启</code></strong>。</p>
<p>那么为什么新加坡区域的节点启动成功，而美东的节点启动失败？</p>
<p>原因应该是，我们的master与etcd都在新加坡区域，因此新加坡节点从etcd获取数据较快，calico-node启动速度也就更快，可以在限定的时间内启动完毕。而美东节点访问新加坡etcd的延迟较长，因此美东calico-node启动更慢。</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[谈谈kubernetes Runtime]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-k8s-runtime.html</url>
      <content type="html"><![CDATA[<h2 id="白话kubernetes-Runtime"><a href="#白话kubernetes-Runtime" class="headerlink" title="白话kubernetes Runtime"></a>白话kubernetes Runtime</h2><blockquote>
<p>回想最开始接触 k8s 的时候, 经常搞不懂 CRI 和 OCI 的联系和区别, 也不知道为啥要垫那么多的 “shim”(尤其是 containerd-shim 和 dockershim 这两个完全没啥关联的东西还恰好都叫 shim). 所以嘛, 这篇就写一写 k8s 的 runtime 部分, 争取一篇文章把下面这张 Landscape 里的核心项目给白话明白</p>
</blockquote>
<p><img src="/images/k8s/bf52b77fly1g0wkhtwdlij217c0si44d.jpg" alt="landscape"></p>
<h2 id="典型的-Runtime-架构"><a href="#典型的-Runtime-架构" class="headerlink" title="典型的 Runtime 架构"></a>典型的 Runtime 架构</h2><p>我们从最常见的 runtime 方案 Docker 说起, 现在 Kubelet 和 Docker 的集成还是挺啰嗦的:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws7jziucj21hy0dmtb0.jpg" alt="典型的runtime架构"><br><a id="more"></a><br>当 Kubelet 想要创建一个容器时, 有这么几步:</p>
<ol>
<li>Kubelet 通过 <strong>CRI 接口</strong>(gRPC) 调用 dockershim, 请求创建一个容器. <strong>CRI</strong> 即容器运行时接口(Container Runtime Interface), 这一步中, Kubelet 可以视作一个简单的 CRI Client, 而 dockershim 就是接收请求的 Server. 目前 dockershim 的代码其实是内嵌在 Kubelet 中的, 所以接收调用的凑巧就是 Kubelet 进程;</li>
<li>dockershim 收到请求后, 转化成 Docker Daemon 能听懂的请求, 发到 Docker Daemon 上请求创建一个容器;</li>
<li>Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程: containerd 中了, 因此 Docker Daemon 仍然不能帮我们创建容器, 而是要请求 containerd 创建一个容器;</li>
<li>containerd 收到请求后, 并不会自己直接去操作容器, 而是创建一个叫做 containerd-shim 的进程, 让 containerd-shim 去操作容器. 这是因为容器进程需要一个父进程来做诸如收集状态, 维持 stdin 以及 fd 打开等工作. 而假如这个父进程就是 containerd, 那每次 containerd 挂掉或升级, 整个宿主机上所有的容器都得退出了. 而引入了 containerd-shim 就规避了这个问题(containerd 和 shim 并不需要是父子进程关系, 当 containerd 退出或重启时, shim 会 re-parent 到 systemd 这样的 1 号进程上);</li>
<li>我们知道创建容器需要做一些设置 namespaces 和 cgroups, 挂载 root filesystem 等等操作, 而这些事该怎么做已经有了公开的规范了, 那就是 <a href="https://github.com/opencontainers/runtime-spec" target="_blank" rel="external">OCI(Open Container Initiative, 开放容器标准)</a>. 它的一个参考实现叫做 <a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runc</a>. 于是, containerd-shim 在这一步需要调用 <code>runc</code> 这个命令行工具, 来启动容器;</li>
<li><code>runc</code> 启动完容器后本身会直接退出, containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程;</li>
</ol>
<p>这个过程乍一看像是在搞我们: Docker Daemon 和 dockershim 看上去就是两个不干活躺在中间划水的啊, Kubelet 为啥不直接调用 containerd 呢?</p>
<p>当然是可以的, 不过咱们先不提那个, 先看看为什么现在的架构如此繁冗.</p>
<h2 id="小插曲-容器历史小叙-不负责任版"><a href="#小插曲-容器历史小叙-不负责任版" class="headerlink" title="小插曲: 容器历史小叙(不负责任版)"></a>小插曲: 容器历史小叙(不负责任版)</h2><p>其实 k8s 最开始的 Runtime 架构远没这么复杂: kubelet 想要创建容器直接跟 Docker Daemon 说一声就行, 而那时也不存在 containerd, Docker Daemon 自己调一下 <code>libcontainer</code> 这个库把容器跑起来, 整个过程就搞完了.</p>
<p>而熟悉容器和容器编排历史的读者老爷应该知道, 这之后就是容器圈的一系列政治斗争, 先是大佬们认为运行时标准不能被 Docker 一家公司控制, 于是就撺掇着搞了开放容器标准 OCI. Docker 则把 <code>libcontainer</code> 封装了一下, 变成 runC 捐献出来作为 OCI 的参考实现.</p>
<p>再接下来就是 <a href="https://github.com/rkt/rkt" target="_blank" rel="external">rkt</a> 想从 docker 那边分一杯羹, 希望 k8s 原生支持 rkt 作为运行时, 而且 PR 还真的合进去了. 维护过这块业务同时接两个需求方的读者老爷应该都知道类似的事情有多坑, k8s 中负责维护 kubelet 的小组 sig-node 也是被狠狠坑了一把.</p>
<p>大家一看这么搞可不行, 今天能有 rkt, 明天就能有更多幺蛾子出来, 这么搞下去我们小组也不用干活了, 整天搞兼容性的 bug 就够呛. 于是乎, k8s 1.5 推出了 CRI 机制, 即容器运行时接口(Container Runtime Interface), k8s 告诉大家, 你们想做 Runtime 可以啊, 我们也资瓷欢迎, 实现这个接口就成, 成功反客为主.</p>
<p>不过 CRI 本身只是 k8s 推的一个标准, 当时的 k8s 尚未达到如今这般武林盟主的地位, 容器运行时当然不能说我跟 k8s 绑死了只提供 CRI 接口, 于是就有了 shim(垫片) 这个说法, 一个 shim 的职责就是作为 Adapter 将各种容器运行时本身的接口适配到 k8s 的 CRI 接口上.</p>
<p>接下来就是 Docker 要搞 Swarm 进军 PaaS 市场, 于是做了个架构切分, 把容器操作都移动到一个单独的 Daemon 进程 containerd 中去, 让 Docker Daemon 专门负责上层的封装编排. 可惜 Swarm 在 k8s 面前实在是不够打, 惨败之后 Docker 公司就把 <a href="https://github.com/containerd/containerd" target="_blank" rel="external">containerd 项目</a>捐给 CNCF 缩回去安心搞 Docker 企业版了.</p>
<p>最后就是我们在上一张图里看到的这一坨东西了, 尽管现在已经有 CRI-O, containerd-plugin 这样更精简轻量的 Runtime 架构, dockershim 这一套作为经受了最多生产环境考验的方案, 迄今为止仍是 k8s 默认的 runtime 实现.</p>
<p>了解这些具体的架构有时能在 debug 时候帮我们一些忙, 但更重要的是它们能作为一个例子, 帮助我们更好地理解整个 k8s runtime 背后的设计逻辑, 我们这就言归正传.</p>
<h2 id="OCI-CRI-与被滥用的名词-“Runtime”"><a href="#OCI-CRI-与被滥用的名词-“Runtime”" class="headerlink" title="OCI, CRI 与被滥用的名词 “Runtime”"></a>OCI, CRI 与被滥用的名词 “Runtime”</h2><p>OCI, 也就是前文提到的”开放容器标准”其实就是一坨文档, 其中主要规定了两点:</p>
<ol>
<li>容器镜像要长啥样, 即 <a href="https://github.com/opencontainers/image-spec" target="_blank" rel="external">ImageSpec</a>. 里面的大致规定就是你这个东西需要是一个压缩了的文件夹, 文件夹里以 xxx 结构放 xxx 文件;</li>
<li>容器要需要能接收哪些指令, 这些指令的行为是什么, 即 <a href="https://github.com/opencontainers/runtime-spec" target="_blank" rel="external">RuntimeSpec</a>. 这里面的大致内容就是”容器”要能够执行 “create”, “start”, “stop”, “delete” 这些命令, 并且行为要规范.</li>
</ol>
<p><a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runC</a> 为啥叫参考实现呢, 就是它能按照标准将符合标准的容器镜像运行起来(当然, 这里为了易读性略去了很多细节, 要了解详情建议点前文的链接读文档)</p>
<p>标准的好处就是方便搞创新, 反正只要我符合标准, 生态圈里的其它工具都能和我一起愉快地工作(…当然 OCI 这个标准本身制订得不怎么样, 真正工程上还是要做一些 adapter 的), 那我的镜像就可以用任意的工具去构建, 我的”容器”就不一定非要用 namespace 和 cgroups 来做隔离. 这就让各种虚拟化容器可以更好地参与到游戏当中, 我们暂且不表.</p>
<p>而 CRI 更简单, 单纯是一组 gRPC 接口, 扫一眼 <a href="https://github.com/kubernetes/kubernetes/blob/8327e433590f9e867b1e31a4dc32316685695729/pkg/kubelet/apis/cri/services.go" target="_blank" rel="external">kubelet/apis/cri/services.go</a> 就能归纳出几套核心接口:</p>
<ul>
<li>一套针对容器操作的接口, 包括创建,启停容器等等;</li>
<li>一套针对镜像操作的接口, 包括拉取镜像删除镜像等;</li>
<li>还有一套针对 PodSandbox (容器沙箱环境) 的操作接口, 我们之后再说;</li>
</ul>
<p>现在我们可以找到很多符合 OCI 标准或兼容了 CRI 接口的项目, 而这些项目就大体构成了整个 Kuberentes 的 Runtime 生态:</p>
<ul>
<li>OCI Compatible: <a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runC</a>, <a href="https://github.com/kata-containers/kata-containers" target="_blank" rel="external">Kata</a>(以及它的前身 <a href="https://github.com/hyperhq/runv" target="_blank" rel="external">runV</a> 和 <a href="https://github.com/clearcontainers/runtime" target="_blank" rel="external">Clear Containers</a>), <a href="https://github.com/google/gvisor" target="_blank" rel="external">gVisor</a>. 其它比较偏门的还有 Rust 写的 <a href="https://github.com/oracle/railcar" target="_blank" rel="external">railcar</a></li>
<li>CRI Compatible: Docker(借助 dockershim), <a href="https://github.com/containerd/containerd" target="_blank" rel="external">containerd</a>(借助 CRI-containerd), <a href="https://github.com/kubernetes-sigs/cri-o" target="_blank" rel="external">CRI-O</a>, <a href="https://github.com/kubernetes/frakti" target="_blank" rel="external">frakti</a>, etc.</li>
</ul>
<p>最开始 k8s 的时候我经常弄不清 OCI 和 CRI 的区别与联系, 其中一大原因就是社区里糟糕的命名: 这上面的项目统统可以称为容器运行时(Container Runtime), 彼此之间区分的办法就是给”容器运行时”这个词加上各种定语和从句来进行修饰. Dave Cheney 有条推说:</p>
<blockquote>
<p>Good naming is like a good joke. If you have to explain it, it’s not funny.</p>
</blockquote>
<p>显然 Container Runtime 在这里就不是一个好名字了, 我们接下来换成一个在这篇文章的语境中更准确的说法: <strong>cri-runtime</strong> 和 <strong>oci-runtime</strong>. 通过这个粗略的分类, 我们其实可以总结出整个 runtime 架构万变不离其宗的三层抽象:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Orchestration API -&gt; Container API -&gt; Kernel API</span><br></pre></td></tr></table></figure></p>
<p>这其中 k8s 已经是 Orchestration API 的事实标准, 而在 k8s 中, Container API 的接口标准就是 CRI, 由 cri-runtime 实现, Kernel API 的规范是 OCI, 由 oci-runtime 实现.</p>
<p>根据这个思路, 我们就很容易理解下面这两种东西:</p>
<ul>
<li>各种更为精简的 cri-runtime</li>
<li>各种”强隔离”容器方案</li>
</ul>
<h2 id="containerd-和-CRI-O"><a href="#containerd-和-CRI-O" class="headerlink" title="containerd 和 CRI-O"></a>containerd 和 CRI-O</h2><p>我们在第一节就看到现在的 runtime 实在是有点复杂了, 而复杂是万恶之源, 于是就有了直接拿 containerd 做 oci-runtime 的方案. 当然, 除了 k8s 之外, containerd 还要接诸如 Swarm 等调度系统, 因此它不会去直接实现 CRI, 这个适配工作当然就要交给一个 shim 了.</p>
<p>containerd 1.0 中, 对 CRI 的适配通过一个单独的进程 <code>CRI-containerd</code> 来完成:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws7vtgq2j21de0cmgmo.jpg" alt="containerd 1.0"></p>
<p>containerd 1.1 中做的又更漂亮一点, 砍掉了 CRI-containerd 这个进程, 直接把适配逻辑作为插件放进了 containerd 主进程中:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws87espcj21ao0d00tz.jpg" alt="containerd 1.1"></p>
<p>但在 containerd 做这些事情之情, 社区就已经有了一个更为专注的 cri-runtime: <a href="https://github.com/kubernetes-sigs/cri-o" target="_blank" rel="external">CRI-O</a>, 它非常纯粹, 就是兼容 CRI 和 OCI, 做一个 k8s 专用的运行时:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws8dqqsoj21am0d0q4q.jpg" alt="CRI-O"></p>
<p>其中 <code>conmon</code> 就对应 containerd-shim, 大体意图是一样的.</p>
<p>CRI-O 和 (直接调用)containerd 的方案比起默认的 dockershim 确实简洁很多, 但没啥生产环境的验证案例, 我所知道的仅仅是 containerd 在 GKE 上是 beta 状态. 因此假如你对 docker 没有特殊的政治恨意, 大可不必把 dockershim 这套换掉.</p>
<h2 id="强隔离容器-Kata-gVisor-firecracker"><a href="#强隔离容器-Kata-gVisor-firecracker" class="headerlink" title="强隔离容器: Kata, gVisor, firecracker"></a>强隔离容器: Kata, gVisor, firecracker</h2><p>一直以来 k8s 都有一个被诟病的点: 难以实现真正的多租户.</p>
<p>为什么这么说呢, 我们先考虑一下什么样是理想的多租户状态:</p>
<blockquote>
<p>理想来说, 平台的各个租户(tenant)之间应该无法感受到彼此的存在, 表现得就像每个租户独占这整个平台一样. 具体来说, 我不能看到其它租户的资源, 我的资源跑满了不能影响其它租户的资源使用, 我也无法从网络或内核上攻击其它租户.</p>
</blockquote>
<p>k8s 当然做不到, 其中最大的两个原因是:</p>
<ul>
<li>kube-apiserver 是整个集群中的单例, 并且没有多租户概念</li>
<li>默认的 oci-runtime 是 runC, 而 runC 启动的容器是共享内核的</li>
</ul>
<p>对于第二个问题, 一个典型的解决方案就是提供一个新的 OCI 实现, 用 VM 来跑容器, 实现内核上的硬隔离. <a href="https://github.com/hyperhq/runv" target="_blank" rel="external">runV</a> 和 <a href="https://github.com/clearcontainers/runtime" target="_blank" rel="external">Clear Containers</a> 都是这个思路. 因为这两个项目做得事情是很类似, 后来就合并成了一个项目 <a href="https://github.com/kata-containers/kata-containers" target="_blank" rel="external">Kata Container</a>. Kata 的一张图很好地解释了基于虚拟机的容器与基于 namespaces 和 cgroups 的容器间的区别:</p>
<p><img src="/images/k8s/bf52b77fly1g0wnfdxo9fj20hs0anmyb.jpg" alt="Kata Container"></p>
<blockquote>
<p>当然, 没有系统是完全安全的, 假如 hypervisor 存在漏洞, 那么用户仍有可能攻破隔离. 但所有的事情都要对比而言, 在共享内核的情况下, 暴露的攻击面是非常大的, 做安全隔离的难度就像在美利坚和墨西哥之间修 The Great Wall, 而当内核隔离之后, 只要守住 hypervisor 这道关子就后顾无虞了</p>
</blockquote>
<p>嗯, 一个 VM 里跑一个容器, 听上去隔离性很不错, 但不是说虚拟机又笨重又不好管理才切换到容器的吗, 怎么又要走回去了?</p>
<p>Kata 告诉你, 虚拟机没那么邪恶, 只是以前没玩好:</p>
<ul>
<li><strong>不好管理</strong>是因为没有遵循”不可变基础设施”, 大家都去虚拟机上这摸摸那碰碰, 这台装 Java 8 那台装 Java 6, Admin 是要 angry 的. Kata 则支持 OCI 镜像, 完全可以用上 Dockerfile + 镜像, 让不好管理成为了过去时;</li>
<li><strong>笨重</strong>是因为之前要虚拟化整个系统, 现在我们只着眼于虚拟化应用, 那就可以裁剪掉很多功能, 把 VM 做得很轻量, 因此即便用虚拟机来做容器, Kata 还是可以将容器启动时间压缩得非常短, 启动后在内存上和IO 上的 overhead 也尽可能去优化;</li>
</ul>
<p>不过话说回来, k8s 上的调度单位是 Pod, 是容器组啊, Kata 这样一个虚拟机里一个容器, 同一个 Pod 间的容器还怎么做 namespace 的共享?</p>
<p>这就要说回我们前面讲到的 CRI 中针对 PodSandbox (容器沙箱环境) 的操作接口了. 第一节中, 我们刻意简化了场景, 只考虑创建一个<strong>容器</strong>, 而没有讨论创建一个<strong>Pod</strong>. 大家都知道, 真正启动 Pod 里定义的容器之前, kubelet 会先启动一个 infra 容器, 并执行 /pause 让 infra 容器的主进程永远挂起. 这个容器存在的目的就是维持住整个 pod 的各种 namespace, 真正的业务容器只要加入 infra 容器的 network 等 namespace 就能实现对应 namespace 的共享. 而 infra 容器创造的这个共享环境则被抽象为 <strong>PodSandbox</strong>. 每次 kubelet 在创建 Pod 时, 就会先调用 CRI 的 <code>RunPodSandbox</code> 接口启动一个沙箱环境, 再调用 <code>CreateContainer</code> 在沙箱中创建容器.</p>
<p>这里就已经说出答案了, 对于 <strong>Kata Container</strong> 而言, 只要在 <code>RunPodSandbox</code> 调用中创建一个 VM, 之后再往 VM 中添加容器就可以了. 最后运行 Pod 的样子就是这样的:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws90i7ttj21mq0d440r.jpg" alt="Kata Container"></p>
<p>说完了 Kata, 其实 gVisor 和 firecracker 都不言自明了, 大体上都是类似的, 只是:</p>
<ul>
<li><a href="https://github.com/google/gvisor" target="_blank" rel="external">gVisor</a> 并不会去创建一个完整的 VM, 而是实现了一个叫 “Sentry” 的用户态进程来处理容器的 syscall, 而拦截 syscall 并重定向到 Sentry 的过程则由 KVM 或 ptrace 实现.</li>
<li><a href="https://github.com/firecracker-microvm/firecracker" target="_blank" rel="external">firecracker</a> 称自己为 microVM, 即轻量级虚拟机, 它本身还是基于 KVM 的, 不过 KVM 通常使用 QEMU 来虚拟化除CPU和内存外的资源, 比如IO设备,网络设备. firecracker 则使用 rust 实现了最精简的设备虚拟化, 为的就是压榨虚拟化的开销, 越轻量越好.</li>
</ul>
<h2 id="安全容器与-Serverless"><a href="#安全容器与-Serverless" class="headerlink" title="安全容器与 Serverless"></a>安全容器与 Serverless</h2><p>你可能觉得安全容器对自己而言没什么用: 大不了我给每个产品线都部署 k8s, 机器池也都隔离掉, 从基础设施的层面就隔离掉嘛.</p>
<p>这么做当然可以, 但同时也要知道, 这种做法最终其实是以 IaaS 的方式在卖资源, 是做不了真正的 PaaS 乃至 Serverless 的.</p>
<p>Serverless 要做到所有的用户容器或函数按需使用计算资源, 那必须满足两点:</p>
<ul>
<li><strong>多租户强隔离</strong>: 用户的容器或函数都是按需启动按秒计费, 我们可不能给每个用户预先分配一坨隔离的资源,因此我们要保证整个 Platform 是多租户强隔离的;</li>
<li><strong>极度轻量</strong>: Serverless 的第一个特点是运行时沙箱会更频繁地创建和销毁, 第二个特点是切分的粒度会非常非常细, 细中细就是 FaaS, 一个函数就要一个沙箱. 因此就要求两点: <pre><code>1. 沙箱启动删除必须飞快; 
2. 沙箱占用的资源越少越好. 
</code></pre>这两点在 long-running, 粒度不大的容器运行环境下可能不明显, 但在 Serverless 环境下就会急剧被放大. 这时候去做MicroVM 的 ROI 就比以前要高很多. 想想, 用传统的 KVM 去跑 FaaS, 那还不得亏到姥姥家了?</li>
</ul>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>这次的内容是越写越多, 感觉怎么都写不完的样子, rkt, lxd 其实都还没涉及, 这里就提供下类比, 大家可以自行做拓展阅读: rkt 跟 docker 一样是一个容器引擎, 特点是无 daemon, 目前项目基本不活跃了; lxc 是 docker 最早使用的容器工具集, 位置可以类比 runc, 提供跟 kernel 打交道的库&amp;命令行工具, lxd 则是基于 lxc 的一个容器引擎, 只不过大多数容器引擎的目标是容器化应用, lxd 的目标则是容器化操作系统.</p>
<p>来源：aleiwu.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如何在线关闭一个tcp socket连接]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-online-close-tcp-socket.html</url>
      <content type="html"><![CDATA[<p>你可能会说，简单，<code>netstat -antp</code>找到连接，<code>kill</code>掉这个进程就行了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -antp|grep 6789</span></span><br><span class="line">tcp        0      0 1.1.1.1:59950      1.1.1.2:6789        ESTABLISHED 45059/ceph-fuse</span><br><span class="line"><span class="comment"># kill 45059</span></span><br></pre></td></tr></table></figure>
<p>连接确实关掉了，进程也跟着一起杀死了。达不到“在线”的要求。</p>
<p>有没有办法不杀死进程，但还是可以关闭socket连接呢？</p>
<p>我们知道，在编码的时候，要关闭一个socket，只要调用 close 函数就可以了，但是进程在运行着呢，怎么让它调用 close 呢？<br><a id="more"></a></p>
<p>在<a href="https://superuser.com/" target="_blank" rel="external">superuser</a>上看到一个很棒的方法，原理就是 <code>gdb attach</code> 到进程上下文，然后 <code>call close($fd)</code>。</p>
<p>1、 使用 <code>netstat</code> 找到进程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -antp|grep 6789</span></span><br><span class="line">tcp        0      0 1.1.1.1:59950      1.1.1.2:6789        ESTABLISHED 45059/ceph-fuse</span><br></pre></td></tr></table></figure></p>
<p>如上，进程pid为45059。</p>
<p>2、 使用 lsof 找到进程45059打开的所有文件描述符，并找到对应的socket连接<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lsof -np 45059</span><br><span class="line">COMMAND     PID USER   FD   TYPE             DEVICE SIZE/OFF       NODE NAME</span><br><span class="line">ceph-fuse 45059 root  rtd    DIR                8,2     4096          2 /</span><br><span class="line">ceph-fuse 45059 root  txt    REG                8,2  6694144    1455967 /usr/bin/ceph-fuse</span><br><span class="line">ceph-fuse 45059 root  mem    REG                8,2   510416    2102312 /usr/lib64/libfreeblpriv3.so</span><br><span class="line">...</span><br><span class="line">ceph-fuse 45059 root   12u  IPv4         1377072656      0t0        TCP 1.1.1.1:59950-&gt;1.1.1.2:smc-https (ESTABLISHED)</span><br></pre></td></tr></table></figure></p>
<p>其中 <code>12u</code> 就是上面对应socket连接的文件描述符。</p>
<p>3、 gdb 连接到进程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gdb -p 45059</span><br></pre></td></tr></table></figure></p>
<p>4、 关闭socket连接<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(gdb) call close(12u)</span><br></pre></td></tr></table></figure></p>
<p>socket连接就可以关闭了，但是进程 45059 还是好好着的。</p>
<p>你可能会问，什么时候会用到这个特性呢？场景还是比较多的，比如你想测试下应用是否会自动重连mysql，通过这个办法就可以比较方便的测试了。</p>
<p>Ref:</p>
<ul>
<li><a href="https://superuser.com/questions/127863/manually-closing-a-port-from-commandline" target="_blank" rel="external">Manually closing a port from commandline</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kuberenetes 升级后新加入节点报错]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-kubeadm-up-node-join-failed.html</url>
      <content type="html"><![CDATA[<h2 id="问题简述"><a href="#问题简述" class="headerlink" title="问题简述"></a>问题简述</h2><blockquote>
<p>原本用<code>kubeadm</code>安装的的kubernetes 1.11.x集群升级到1.12.x 后（使用<code>kubeadm upgrade</code>升级）发现无法将新的node加入到集群中，会出现以下报错信息</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[preflight] running pre-flight checks</span><br><span class="line">    [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_sh ip_vs ip_vs_rr ip_vs_wrr] or no <span class="built_in">builtin</span> kernel ipvs support: map[ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125; ip_vs:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line"> 1. Run <span class="string">'modprobe -- '</span> to load missing kernel modules;</span><br><span class="line">2. Provide the missing <span class="built_in">builtin</span> kernel ipvs support</span><br><span class="line"></span><br><span class="line">[discovery] Trying to connect to API Server <span class="string">"172.19.170.254:6443"</span></span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from <span class="string">"https://172.19.170.254:6443"</span></span><br><span class="line">[discovery] Requesting info from <span class="string">"https://172.19.170.254:6443"</span> again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server <span class="string">"172.19.170.254:6443"</span></span><br><span class="line">[discovery] Successfully established connection with API Server <span class="string">"172.19.170.254:6443"</span></span><br><span class="line">[kubelet] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.12"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">configmaps <span class="string">"kubelet-config-1.12"</span> is forbidden: User <span class="string">"system:bootstrap:y1zgt7"</span> cannot get configmaps <span class="keyword">in</span> the namespace <span class="string">"kube-system"</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="修复方法"><a href="#修复方法" class="headerlink" title="修复方法"></a>修复方法</h2><blockquote>
<p>在master节点上执行步骤1-4，在从节点（将要加入集群的机器）上执行步骤5</p>
</blockquote>
<h3 id="步骤一"><a href="#步骤一" class="headerlink" title="步骤一"></a>步骤一</h3><p><strong>从现有的”ConfigMap kubelet-config-1.11” 创建一个新的ConfigMap “kubelet-config-1.12”</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get cm --all-namespaces</span><br><span class="line">$ kubectl -n kube-system get cm kubelet-config-1.11 -o yaml --export &gt; kubelet-config-1.12-cm.yaml</span><br><span class="line">$ vim kubelet-config-1.12-cm.yaml       <span class="comment">#modify at the bottom:</span></span><br><span class="line">                                        <span class="comment">#name: kubelet-config-1.12</span></span><br><span class="line">                                        <span class="comment">#delete selfLink</span></span><br><span class="line">$ kubectl -n kube-system create <span class="_">-f</span> kubelet-config-1.12-cm.yaml</span><br></pre></td></tr></table></figure>
<h3 id="步骤二：获取令牌前缀"><a href="#步骤二：获取令牌前缀" class="headerlink" title="步骤二：获取令牌前缀"></a>步骤二：获取令牌前缀</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm token list           <span class="comment">#if no output, then create a token:</span></span><br><span class="line">$ kubeadm token create</span><br><span class="line">TOKEN                       ...     ...</span><br><span class="line">a0b1c2.svn4my9ifft4zxgg     ...     ...</span><br><span class="line"><span class="comment"># Token prefix is "a0b1c2"</span></span><br></pre></td></tr></table></figure>
<h3 id="步骤三"><a href="#步骤三" class="headerlink" title="步骤三"></a>步骤三</h3><p><strong>从现有角色“kubeadm：kubelet-config-1.11”创建一个新角色“kubeadm：kubelet-config-1.12”</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get roles --all-namespaces</span><br><span class="line">$ kubectl -n kube-system get role kubeadm:kubelet-config-1.11 -o yaml &gt; kubeadm:kubelet-config-1.12-role.yaml</span><br><span class="line">$ vim kubeadm\:kubelet-config-1.12-role.yaml    <span class="comment">#modify the following:</span></span><br><span class="line">                                                <span class="comment">#name: kubeadm:kubelet-config-1.12</span></span><br><span class="line">                                                <span class="comment">#resourceNames: kubelet-config-1.12</span></span><br><span class="line">                                                <span class="comment">#delete creationTimestamp, resourceVersion, selfLink, uid (because --export option is not supported)    </span></span><br><span class="line">$ kubectl -n kube-system create <span class="_">-f</span> kubeadm\:kubelet-config-1.12-role.yaml</span><br></pre></td></tr></table></figure>
<h3 id="步骤四"><a href="#步骤四" class="headerlink" title="步骤四"></a>步骤四</h3><p><strong>从现有角色绑定 “kubeadm：kubelet-config-1.11” 创建一个新角色绑定 “kubeadm：kubelet-config-1.12” </strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get rolebindings --all-namespaces</span><br><span class="line">$ kubectl -n kube-system get rolebinding kubeadm:kubelet-config-1.11 -o yaml &gt; kubeadm:kubelet-config-1.12-rolebinding.yaml</span><br><span class="line">$ vim kubeadm\:kubelet-config-1.12-rolebinding.yaml     <span class="comment">#modify the following:</span></span><br><span class="line">                                                            <span class="comment">#metadata/name: kubeadm:kubelet-config-1.12</span></span><br><span class="line">                                                            <span class="comment">#roleRef/name: kubeadm:kubelet-config-1.12</span></span><br><span class="line">                                                            <span class="comment">#delete creationTimestamp, resourceVersion, selfLink, uid (because --export option is not supported)</span></span><br><span class="line">- apiGroup: rbac.authorization.k8s.io                       <span class="comment">#add these 3 lines as another group in "subjects:" at the bottom, with the 6 character token prefix from STEP 2</span></span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:bootstrap:a0b1c2 </span><br><span class="line">$ kubectl -n kube-system create <span class="_">-f</span> kubeadm\:kubelet-config-1.12-rolebinding.yaml</span><br></pre></td></tr></table></figure>
<h3 id="步骤5：从工作节点启动kubeadm-join"><a href="#步骤5：从工作节点启动kubeadm-join" class="headerlink" title="步骤5：从工作节点启动kubeadm join"></a>步骤5：从工作节点启动kubeadm join</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo kubeadm join --token &lt;token&gt; &lt;master-IP&gt;:6443 --discovery-token-ca-cert-hash sha256:&lt;key-value&gt; </span><br><span class="line"><span class="comment"># If you receive 2 ERRORS, run kubeadm join again with the following options:</span></span><br><span class="line">$ sudo kubeadm join --token &lt;token&gt; &lt;master-IP&gt;:6443 --discovery-token-ca-cert-hash sha256:&lt;key-value&gt; --ignore-preflight-errors=FileAvailable--etc-kubernetes-bootstrap-kubelet.conf,FileAvailable--etc-kubernetes-pki-ca.crt</span><br></pre></td></tr></table></figure>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm1.14 证书调整]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-kubeadm14-ca-upgrade.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>kubeadm部署的kubernets证书一直都是个诟病，默认都只有一年有效期，kubeadm 1.14.x安装后有部分证书还是一年有效期，但个别证书已修改为10年有效期，但对我们使用来说，一年有效期还是一个比较的坑，需要进行调整。</p>
</blockquote>
<h3 id="修改kubeadm-1-14-x源码，调整证书过期时间"><a href="#修改kubeadm-1-14-x源码，调整证书过期时间" class="headerlink" title="修改kubeadm 1.14.x源码，调整证书过期时间"></a>修改kubeadm 1.14.x源码，调整证书过期时间</h3><h4 id="kubeadm1-14-x-安装过后crt证书如下所示"><a href="#kubeadm1-14-x-安装过后crt证书如下所示" class="headerlink" title="kubeadm1.14.x 安装过后crt证书如下所示"></a>kubeadm1.14.x 安装过后crt证书如下所示</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/etc/kubernetes/pki/apiserver.crt</span><br><span class="line">/etc/kubernetes/pki/front-proxy-ca.crt         #10年有效期</span><br><span class="line">/etc/kubernetes/pki/ca.crt                     #10年有效期</span><br><span class="line">/etc/kubernetes/pki/apiserver-etcd-client.crt</span><br><span class="line">/etc/kubernetes/pki/front-proxy-client.crt     #10年有效期</span><br><span class="line">/etc/kubernetes/pki/etcd/server.crt</span><br><span class="line">/etc/kubernetes/pki/etcd/ca.crt                #10年有效期</span><br><span class="line">/etc/kubernetes/pki/etcd/peer.crt              #10年有效期</span><br><span class="line">/etc/kubernetes/pki/etcd/healthcheck-client.crt</span><br><span class="line">/etc/kubernetes/pki/apiserver-kubelet-client.crt</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>如上所示，除了标注说明的证书为10年有效期，其余都是1年有效期，我们查看下原先调整证书有效期的源码，克隆kubernetes 源码，切换到1.14.1 tag 查看：<br>代码目录： <code>staging/src/k8s.io/client-go/util/cert/cert.go</code><br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> duration365d = time.Hour * <span class="number">24</span> * <span class="number">365</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewSelfSignedCACert</span><span class="params">(cfg Config, key crypto.Signer)</span> <span class="params">(*x509.Certificate, error)</span></span> &#123;</span><br><span class="line">    now := time.Now()</span><br><span class="line">    tmpl := x509.Certificate&#123;</span><br><span class="line">        SerialNumber: <span class="built_in">new</span>(big.Int).SetInt64(<span class="number">0</span>),</span><br><span class="line">        Subject: pkix.Name&#123;</span><br><span class="line">            CommonName:   cfg.CommonName,</span><br><span class="line">            Organization: cfg.Organization,</span><br><span class="line">        &#125;,</span><br><span class="line">        NotBefore:             now.UTC(),</span><br><span class="line">        <span class="comment">//这里已经调整为10年有效期</span></span><br><span class="line">        NotAfter:              now.Add(duration365d * <span class="number">10</span>).UTC(),</span><br><span class="line">        KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,</span><br><span class="line">        BasicConstraintsValid: <span class="literal">true</span>,</span><br><span class="line">        IsCA:                  <span class="literal">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;tmpl, &amp;tmpl, key.Public(), key)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x509.ParseCertificate(certDERBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如上所示，通过<code>NewSelfSignedCACert</code>这个方法签发的证书都默认为10年有效期了，但这个只影响部分证书，但这样还没满足我们的需求，个别证书的有效期调整，在经过对源码的分析后，找到了如下的逻辑：</p>
<p>发现部分证书是通过<code>NewSignedCert</code>这个方法签发，而这个方法签发的证书默认只有一年有效期，查看代码逻辑：<br>代码: <code>cmd/kubeadm/app/util/pkiutil/pki_helpers.go</code><br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> duration365d = time.Hour * <span class="number">24</span> * <span class="number">365</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewSignedCert</span><span class="params">(cfg *certutil.Config, key crypto.Signer, caCert *x509.Certificate, caKey crypto.Signer)</span> <span class="params">(*x509.Certificate, error)</span></span> &#123;</span><br><span class="line">    serial, err := rand.Int(rand.Reader, <span class="built_in">new</span>(big.Int).SetInt64(math.MaxInt64))</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cfg.CommonName) == <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">"must specify a CommonName"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cfg.Usages) == <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">"must specify at least one ExtKeyUsage"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    certTmpl := x509.Certificate&#123;</span><br><span class="line">        Subject: pkix.Name&#123;</span><br><span class="line">            CommonName:   cfg.CommonName,</span><br><span class="line">            Organization: cfg.Organization,</span><br><span class="line">        &#125;,</span><br><span class="line">        DNSNames:     cfg.AltNames.DNSNames,</span><br><span class="line">        IPAddresses:  cfg.AltNames.IPs,</span><br><span class="line">        SerialNumber: serial,</span><br><span class="line">        NotBefore:    caCert.NotBefore,</span><br><span class="line">        <span class="comment">// 只有一年有效期</span></span><br><span class="line">        NotAfter:     time.Now().Add(duration365d).UTC(),</span><br><span class="line">        KeyUsage:     x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature,</span><br><span class="line">        ExtKeyUsage:  cfg.Usages,</span><br><span class="line">    &#125;</span><br><span class="line">    certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;certTmpl, caCert, key.Public(), caKey)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x509.ParseCertificate(certDERBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>至此，调整<code>NewSignedCert</code>这个方法，重新进行编译，将证书有效期调整为你想要的任何时间。</p>
<p>如何重新编译kubeadm源码，请参考之前的文章，链接如下：<a href="http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ca-upgdate.html">Kubeadm证书过期时间调整</a></p>
<h3 id="如何一键离线安装kubernetes-1-14教程"><a href="#如何一键离线安装kubernetes-1-14教程" class="headerlink" title="如何一键离线安装kubernetes 1.14教程"></a>如何一键离线安装kubernetes 1.14教程</h3><blockquote>
<p>k8s 1.14.x 离线一键安装包教程&amp;&amp;地址：<a href="http://team.jiunile.com/pro/k8s/">kubernetes 1.14 离线安装地址</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kuberenetes 云应用实践]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-cloud.html</url>
      <content type="html"><![CDATA[<h2 id="如何使用云盘"><a href="#如何使用云盘" class="headerlink" title="如何使用云盘"></a>如何使用云盘</h2><h3 id="aws-ebs"><a href="#aws-ebs" class="headerlink" title="aws ebs"></a>aws ebs</h3><a id="more"></a>
<h4 id="设置IAM角色并分配到机器上"><a href="#设置IAM角色并分配到机器上" class="headerlink" title="设置IAM角色并分配到机器上"></a>设置IAM角色并分配到机器上</h4><blockquote>
<p>具有controlplane(控制面板)角色的节点的IAM策略：</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line"><span class="attr">"Statement"</span>: [</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">    <span class="attr">"Action"</span>: [</span><br><span class="line">      <span class="string">"autoscaling:DescribeAutoScalingGroups"</span>,</span><br><span class="line">      <span class="string">"autoscaling:DescribeLaunchConfigurations"</span>,</span><br><span class="line">      <span class="string">"autoscaling:DescribeTags"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeInstances"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeRegions"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeRouteTables"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeSecurityGroups"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeSubnets"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeVolumes"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateSecurityGroup"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateTags"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:ModifyInstanceAttribute"</span>,</span><br><span class="line">      <span class="string">"ec2:ModifyVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:AttachVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:AuthorizeSecurityGroupIngress"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateRoute"</span>,</span><br><span class="line">      <span class="string">"ec2:DeleteRoute"</span>,</span><br><span class="line">      <span class="string">"ec2:DeleteSecurityGroup"</span>,</span><br><span class="line">      <span class="string">"ec2:DeleteVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:DetachVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:RevokeSecurityGroupIngress"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeVpcs"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:AddTags"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:AttachLoadBalancerToSubnets"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateLoadBalancerPolicy"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateLoadBalancerListeners"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ConfigureHealthCheck"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteLoadBalancerListeners"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeLoadBalancers"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeLoadBalancerAttributes"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DetachLoadBalancerFromSubnets"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeregisterInstancesFromLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ModifyLoadBalancerAttributes"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:RegisterInstancesWithLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:AddTags"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateListener"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateTargetGroup"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteListener"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteTargetGroup"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeListeners"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeLoadBalancerPolicies"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeTargetGroups"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeTargetHealth"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ModifyListener"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ModifyTargetGroup"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:RegisterTargets"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:SetLoadBalancerPoliciesOfListener"</span>,</span><br><span class="line">      <span class="string">"iam:CreateServiceLinkedRole"</span>,</span><br><span class="line">      <span class="string">"kms:DescribeKey"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"Resource"</span>: [</span><br><span class="line">      <span class="string">"*"</span></span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>worker角色的节点的IAM策略</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line"><span class="attr">"Statement"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">        <span class="attr">"Action"</span>: [</span><br><span class="line">            <span class="string">"ec2:DescribeInstances"</span>,</span><br><span class="line">            <span class="string">"ec2:DescribeRegions"</span>,</span><br><span class="line">            <span class="string">"ecr:GetAuthorizationToken"</span>,</span><br><span class="line">            <span class="string">"ecr:BatchCheckLayerAvailability"</span>,</span><br><span class="line">            <span class="string">"ecr:GetDownloadUrlForLayer"</span>,</span><br><span class="line">            <span class="string">"ecr:GetRepositoryPolicy"</span>,</span><br><span class="line">            <span class="string">"ecr:DescribeRepositories"</span>,</span><br><span class="line">            <span class="string">"ecr:ListImages"</span>,</span><br><span class="line">            <span class="string">"ecr:BatchGetImage"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"Resource"</span>: <span class="string">"*"</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="配置ClusterID"><a href="#配置ClusterID" class="headerlink" title="配置ClusterID"></a>配置ClusterID</h4><blockquote>
<p>需要在机器上打上标签，如果不配置会产生以下错误</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed to run Kubelet: could not init cloud provider <span class="string">"aws"</span>: AWS cloud failed to find ClusterID</span><br></pre></td></tr></table></figure>
<p>标 签 Key ： <code>kubernetes.io/cluster/CLUSTERID</code><br>标签Value：  <code>owned</code></p>
<p><code>CLUSTERID</code> 可以为任何值</p>
<h4 id="kubernetes配置"><a href="#kubernetes配置" class="headerlink" title="kubernetes配置"></a>kubernetes配置</h4><p>需要kube-apiserver, kube-controller-manager以及kubelet 启动加上 <code>--cloud-provider=aws</code>参数 , <code>注意</code>需要在<strong>所有node</strong>上的kubelet 开启<code>--cloud-provider=aws</code></p>
<p><strong>使用kubeadm 初始化kubernetes设置</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiServerExtraArgs:</span></span><br><span class="line"><span class="attr">  cloud-provider:</span> aws</span><br><span class="line"><span class="attr">controllerManagerExtraArgs:</span></span><br><span class="line"><span class="attr">  cloud-provider:</span> aws</span><br><span class="line"></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line">  <span class="comment">#建议将Kubelet的名称设置为EC2中节点的私有DNS条目（这可确保它与主机名匹配，如本文前面所述）</span></span><br><span class="line"><span class="attr">  name:</span> ip<span class="bullet">-10</span><span class="bullet">-15</span><span class="bullet">-30</span><span class="bullet">-45.</span>cn-northwest<span class="bullet">-1.</span>compute.internal</span><br><span class="line"><span class="attr">  kubeletExtraArgs:</span></span><br><span class="line"><span class="attr">    cloud-provider:</span> aws</span><br></pre></td></tr></table></figure></p>
<p><strong>使用kubeadm 命令安装后的情况下</strong></p>
<ul>
<li><code>kube-apiserver</code> 修改 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>下的启动参数</li>
<li><code>kube-controller-manager</code> 修改 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code>下的启动参数</li>
<li><code>kubelet</code> 修改 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 下的启动参数</li>
</ul>
<h4 id="开始使用EBS"><a href="#开始使用EBS" class="headerlink" title="开始使用EBS"></a>开始使用EBS</h4><h5 id="创建StorageClass"><a href="#创建StorageClass" class="headerlink" title="创建StorageClass"></a>创建StorageClass</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat standard-storageclass.yaml</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> standard</span><br><span class="line"><span class="attr">provisioner:</span> kubernetes.io/aws-ebs</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> gp2</span><br><span class="line"><span class="attr">  zone:</span> ap-northeast<span class="bullet">-1</span>b</span><br><span class="line"><span class="attr">reclaimPolicy:</span> Delete       <span class="comment"># 在删除 Pod 的同时，也删除 EBS 磁盘</span></span><br><span class="line"><span class="comment"># kubectl apply -f standard-storageclass.yaml</span></span><br></pre></td></tr></table></figure>
<h5 id="创建PVC"><a href="#创建PVC" class="headerlink" title="创建PVC"></a>创建PVC</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat nginx-pv-claim.yaml</span></span><br><span class="line"><span class="attr">kind:</span> PersistentVolumeClaim</span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-pv-claim</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> ReadWriteOnce</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">20</span>Gi</span><br><span class="line"><span class="attr">  storageClassName:</span> standard</span><br><span class="line"><span class="comment"># kubectl apply -f nginx-pv-claim.yaml</span></span><br></pre></td></tr></table></figure>
<h5 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat nginx-pv-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-pv</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">    - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> nginx-pv-claim</span><br><span class="line"><span class="attr">      mountPath:</span> /data</span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx-pv-claim</span><br><span class="line"><span class="attr">    persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">      claimName:</span> nginx-pv-claim</span><br><span class="line"><span class="comment"># kubectl apply -f nginx-pv-pod.yaml</span></span><br></pre></td></tr></table></figure>
<h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul>
<li><a href="https://www.kubernetes.org.cn/4078.html" target="_blank" rel="external">https://www.kubernetes.org.cn/4078.html</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#aws" target="_blank" rel="external">https://kubernetes.io/docs/concepts/storage/storage-classes/#aws</a></li>
<li><a href="https://blog.scottlowe.org/2018/09/28/setting-up-the-kubernetes-aws-cloud-provider/" target="_blank" rel="external">https://blog.scottlowe.org/2018/09/28/setting-up-the-kubernetes-aws-cloud-provider/</a></li>
</ul>
<h3 id="阿里云云盘"><a href="#阿里云云盘" class="headerlink" title="阿里云云盘"></a>阿里云云盘</h3><h4 id="创建阿里云Provisioner-控制器"><a href="#创建阿里云Provisioner-控制器" class="headerlink" title="创建阿里云Provisioner 控制器"></a>创建阿里云Provisioner 控制器</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-common</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> cloud</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-efficiency</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> cloud_efficiency</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-ssd</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> cloud_ssd</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-available</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> available</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller-runner</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"persistentvolumes"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"delete"</span>]</span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"persistentvolumeclaims"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"update"</span>]</span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">"storage.k8s.io"</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"storageclasses"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"events"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"update"</span>, <span class="string">"patch"</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> run-alicloud-disk-controller</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">    name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">    namespace:</span> kube-system</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller-runner</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> cloud-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  special.keyid: 你阿里云的access key</span><br><span class="line">  special.keysecret: 你阿里云的secret key</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">apiVersion:</span> extensions/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    type:</span> Recreate</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">        operator:</span> Exists</span><br><span class="line"><span class="attr">        key:</span> node-role.kubernetes.io/master</span><br><span class="line"><span class="attr">      - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">        operator:</span> Exists</span><br><span class="line"><span class="attr">        key:</span> node.cloudprovider.kubernetes.io/uninitialized</span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line">         node-role.kubernetes.io/master: <span class="string">""</span></span><br><span class="line"><span class="attr">      serviceAccount:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">          image:</span> icyboy/alicloud-disk-controller:<span class="number">3652</span>ddf</span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">          - name:</span> ACCESS_KEY_ID</span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              configMapKeyRef:</span></span><br><span class="line"><span class="attr">                name:</span> cloud-config</span><br><span class="line"><span class="attr">                key:</span> special.keyid</span><br><span class="line"><span class="attr">          - name:</span> ACCESS_KEY_SECRET</span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              configMapKeyRef:</span></span><br><span class="line"><span class="attr">                name:</span> cloud-config</span><br><span class="line"><span class="attr">                key:</span> special.keysecret</span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> cloud-config</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/kubernetes/</span><br><span class="line"><span class="attr">            - name:</span> logdir</span><br><span class="line"><span class="attr">              mountPath:</span> /var/log/alicloud/</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> cloud-config</span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> /etc/kubernetes/</span><br><span class="line"><span class="attr">        - name:</span> logdir</span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> /var/log/alicloud/</span><br></pre></td></tr></table></figure>
<h4 id="创建PVC-1"><a href="#创建PVC-1" class="headerlink" title="创建PVC"></a>创建PVC</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> PersistentVolumeClaim</span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> disk</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> ReadWriteOnce</span><br><span class="line"><span class="comment">#alicloud-disk-common     普通云盘</span></span><br><span class="line"><span class="comment">#alicloud-disk-efficiency 高效云盘</span></span><br><span class="line"><span class="comment">#alicloud-disk-ssd        ssd云盘</span></span><br><span class="line"><span class="comment">#alicloud-disk-available</span></span><br><span class="line"><span class="attr">  storageClassName:</span> alicloud-disk-common</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">20</span>Gi</span><br></pre></td></tr></table></figure>
<h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pvc</span></span><br><span class="line">NAME      STATUS    VOLUME                   CAPACITY   ACCESS MODES   STORAGECLASS           AGE</span><br><span class="line">disk      Bound     d-bp1cz8sslda31ld2snbq   20Gi       RWO            alicloud-disk-common   11s</span><br><span class="line"><span class="comment"># kubectl get pv</span></span><br><span class="line">NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM          STORAGECLASS           REASON    AGE</span><br><span class="line">d-bp1cz8sslda31ld2snbq   20Gi       RWO            Delete           Bound     default/disk   alicloud-disk-common             14s</span><br></pre></td></tr></table></figure>
<h4 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h4><ul>
<li><a href="https://yq.aliyun.com/articles/629001" target="_blank" rel="external">https://yq.aliyun.com/articles/629001</a></li>
<li><a href="https://github.com/AliyunContainerService/alicloud-storage-provisioner" target="_blank" rel="external">https://github.com/AliyunContainerService/alicloud-storage-provisioner</a></li>
</ul>
<h2 id="IAM-使用"><a href="#IAM-使用" class="headerlink" title="IAM 使用"></a>IAM 使用</h2><h3 id="kube2iam-in-aws-iam"><a href="#kube2iam-in-aws-iam" class="headerlink" title="kube2iam in aws iam"></a>kube2iam in aws iam</h3><h4 id="通过AWS-IAM创建一个名为kube2iam的策略，策略规则如下"><a href="#通过AWS-IAM创建一个名为kube2iam的策略，策略规则如下" class="headerlink" title="通过AWS IAM创建一个名为kube2iam的策略，策略规则如下"></a>通过AWS IAM创建一个名为kube2iam的<code>策略</code>，策略规则如下</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line">    <span class="attr">"Statement"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"Action"</span>: [</span><br><span class="line">                <span class="string">"sts:AssumeRole"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">            <span class="attr">"Resource"</span>: <span class="string">"arn:aws-cn:iam::955466075186:role/k8s-*"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="通过AWS-IAM创建一个名为：node-k8s-role的角色，并将kube2iam的策略赋予它，注意！！同时将这个角色赋予到机器上"><a href="#通过AWS-IAM创建一个名为：node-k8s-role的角色，并将kube2iam的策略赋予它，注意！！同时将这个角色赋予到机器上" class="headerlink" title="通过AWS IAM创建一个名为：node-k8s-role的角色，并将kube2iam的策略赋予它，注意！！同时将这个角色赋予到机器上"></a>通过AWS IAM创建一个名为：node-k8s-role的<code>角色</code>，并将<code>kube2iam</code>的<code>策略</code>赋予它，<code>注意！！同时将这个角色赋予到机器上</code></h4><h4 id="通过AWS-IAM创建一个名为：k8s-kube2iam-demo的角色，将AmazonS3FullAccess的策略赋予它，同时编辑信任关系，调整为如下："><a href="#通过AWS-IAM创建一个名为：k8s-kube2iam-demo的角色，将AmazonS3FullAccess的策略赋予它，同时编辑信任关系，调整为如下：" class="headerlink" title="通过AWS IAM创建一个名为：k8s-kube2iam-demo的角色，将AmazonS3FullAccess的策略赋予它，同时编辑信任关系，调整为如下："></a>通过AWS IAM创建一个名为：k8s-kube2iam-demo的<code>角色</code>，将<code>AmazonS3FullAccess</code>的<code>策略</code>赋予它，同时编辑信任关系，调整为如下：</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line">  <span class="attr">"Statement"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">      <span class="attr">"Principal"</span>: &#123;</span><br><span class="line">        <span class="attr">"Service"</span>: <span class="string">"ec2.amazonaws.com.cn"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"Action"</span>: <span class="string">"sts:AssumeRole"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"Sid"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">      <span class="attr">"Principal"</span>: &#123;</span><br><span class="line">        <span class="attr">"AWS"</span>: <span class="string">"arn:aws-cn:iam::955466075186:role/node-k8s-role"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"Action"</span>: <span class="string">"sts:AssumeRole"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="部署kube2iam"><a href="#部署kube2iam" class="headerlink" title="部署kube2iam"></a>部署kube2iam</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Source: kube2iam/templates/serviceaccount.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: kube2iam/templates/clusterrole.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> namespaces</span><br><span class="line"><span class="bullet">      -</span> pods</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: kube2iam/templates/clusterrolebinding.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">    name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">    namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: kube2iam/templates/daemonset.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> extensions/v1beta1</span><br><span class="line"><span class="attr">kind:</span> DaemonSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> kube2iam</span><br><span class="line"><span class="attr">        release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> kube2iam</span><br><span class="line"><span class="attr">          image:</span> <span class="string">"jtblin/kube2iam:0.10.4"</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">"IfNotPresent"</span></span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--host-interface=cali+"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--node=$(NODE_NAME)"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--host-ip=$(HOST_IP)"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--iptables=true"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--base-role-arn=arn:aws-cn:iam::955466075186:role/node-k8s-role"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--app-port=8181"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--use-regional-sts-endpoint"</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> HOST_IP</span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> status.podIP</span><br><span class="line"><span class="attr">            - name:</span> NODE_NAME</span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> spec.nodeName</span><br><span class="line"><span class="attr">            - name:</span> AWS_REGION</span><br><span class="line"><span class="attr">              value:</span> cn-northwest<span class="bullet">-1</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - name:</span> http</span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">8181</span></span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> /healthz</span><br><span class="line"><span class="attr">              port:</span> <span class="number">8181</span></span><br><span class="line"><span class="attr">              scheme:</span> HTTP</span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line">            &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">          securityContext:</span></span><br><span class="line"><span class="attr">            privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line">        []</span><br><span class="line"></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">    type:</span> OnDelete</span><br></pre></td></tr></table></figure>
<h4 id="测试pod"><a href="#测试pod" class="headerlink" title="测试pod"></a>测试pod</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> awscli-deployment</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> awscli</span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      annotations:</span></span><br><span class="line">        iam.amazonaws.com/role: arn:aws-cn:iam::<span class="number">955466075186</span>:role/k8s/k8s-kube2iam-demo</span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> awscli</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> awscli</span><br><span class="line"><span class="attr">        image:</span> mesosphere/aws-cli</span><br><span class="line"><span class="attr">        command:</span> [ <span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>, <span class="string">"--"</span> ]</span><br><span class="line"><span class="attr">        args:</span> [ <span class="string">"while true; do sleep 3000; done;"</span> ]</span><br></pre></td></tr></table></figure>
<h2 id="云应用疑难杂症"><a href="#云应用疑难杂症" class="headerlink" title="云应用疑难杂症"></a>云应用疑难杂症</h2><h3 id="使用云负载均衡器代理APIServer问题"><a href="#使用云负载均衡器代理APIServer问题" class="headerlink" title="使用云负载均衡器代理APIServer问题"></a>使用云负载均衡器代理APIServer问题</h3><blockquote>
<p>阿里云/AWS的负载均衡是四层TCP负责，<strong>不支持后端ECS实例既作为Real Server又作为客户端向所在的负载均衡实例发送请求</strong>。因为返回的数据包只在云服务器内部转发，不经过负载均衡，所以在后端ECS实例上去访问负载均衡的服务地址是不通的。什么意思？就是如果你要使用阿里云的SLB的话，那么你不能在apiserver节点上使用SLB（比如在apiserver 上安装kubectl，然后将apiserver的地址设置为SLB的负载地址使用），因为这样的话就可能造成回环了，所以简单的做法是另外用两个新的节点做HA实例，然后将这两个实例添加到SLB 机器组中。</p>
</blockquote>
<ul>
<li>aws 使用nlb，指定ip</li>
<li>aliyun 在apiserver上搭建keepalived</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes内部DNS剖析]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-dns.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们可以通过 Service 生成的<code>ClusterIP(VIP)</code>来访问 Pod 提供的服务，但是在使用的时候还有一个问题：我们怎么知道某个应用的 VIP 呢？比如我们有两个应用，一个是 api 应用，一个是 db 应用，两个应用都是通过<code>Deployment</code>进行管理的，并且都通过 Service 暴露出了端口提供服务。api 需要连接到 db 这个应用，我们只知道 db 应用的名称和 db 对应的 Service 的名称，但是并不知道它的 VIP 地址，我们前面的 Service 课程中是不是学习到我们通过<code>ClusterIP</code>就可以访问到后面的<code>Pod</code>服务，如果我们知道了 VIP 的地址是不是就行了？</p>
<h2 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h2><p>我们知道可以从 apiserver 中直接查询获取到对应 service 的后端 Endpoints信息，所以最简单的办法是从 apiserver 中直接查询，如果偶尔一个特殊的应用，我们通过<code>apiserver</code>去查询到<code>Service</code>后面的 <code>Endpoints</code>直接使用是没问题的，但是如果每个应用都在启动的时候去查询依赖的服务，这不但增加了应用的复杂度，这也导致了我们的应用需要依赖<code>Kubernetes</code>了，耦合度太高了，不具有通用性。<br><a id="more"></a></p>
<h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>为了解决上面的问题，在之前的版本中，Kubernetes 采用了环境变量的方法，每个 Pod 启动的时候，会通过环境变量设置所有服务的 IP 和 port 信息，这样 Pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息，这种方法使用起来相对简单，但是有一个很大的问题就是依赖的服务必须在 Pod 启动之前就存在，不然是不会被注入到环境变量中的。比如我们首先创建一个 Nginx 服务：(test-nginx.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-deploy</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> nginx-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> nginx</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-service</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> nginx-service</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">5000</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br></pre></td></tr></table></figure></p>
<p>创建上面的服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> <span class="built_in">test</span>-nginx.yaml</span><br><span class="line">deployment.apps <span class="string">"nginx-deploy"</span> created</span><br><span class="line">service <span class="string">"nginx-service"</span> created</span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">...</span><br><span class="line">nginx-deploy-75675f5897-47h4t             1/1       Running   0          53s</span><br><span class="line">nginx-deploy-75675f5897-mmm8w             1/1       Running   0          53s</span><br><span class="line">...</span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">...</span><br><span class="line">nginx-service   ClusterIP   10.107.225.42    &lt;none&gt;        5000/TCP         1m</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到两个 Pod 和一个名为 nginx-service 的服务创建成功了，该 Service 监听的端口是 5000，同时它会把流量转发给它代理的所有 Pod（我们这里就是拥有 app: nginx 标签的两个 Pod）。</p>
<p>现在我们再来创建一个普通的 Pod，观察下该 Pod 中的环境变量是否包含上面的 nginx-service 的服务信息：（test-pod.yaml）<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> test-pod</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> test-service-pod</span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>, <span class="string">"env"</span>]</span><br></pre></td></tr></table></figure></p>
<p>然后创建该测试的 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> <span class="built_in">test</span>-pod.yaml</span><br><span class="line">pod <span class="string">"test-pod"</span> created</span><br></pre></td></tr></table></figure></p>
<p>等 Pod 创建完成后，我们查看日志信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs <span class="built_in">test</span>-pod</span><br><span class="line">...</span><br><span class="line">KUBERNETES_PORT=tcp://10.96.0.1:443</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">HOSTNAME=<span class="built_in">test</span>-pod</span><br><span class="line">HOME=/root</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP_ADDR=10.107.225.42</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP_PORT=5000</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP_PROTO=tcp</span><br><span class="line">KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1</span><br><span class="line">PATH=/usr/<span class="built_in">local</span>/sbin:/usr/<span class="built_in">local</span>/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">NGINX_SERVICE_SERVICE_HOST=10.107.225.42</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP=tcp://10.107.225.42:5000</span><br><span class="line">KUBERNETES_PORT_443_TCP_PORT=443</span><br><span class="line">KUBERNETES_PORT_443_TCP_PROTO=tcp</span><br><span class="line">NGINX_SERVICE_SERVICE_PORT=5000</span><br><span class="line">NGINX_SERVICE_PORT=tcp://10.107.225.42:5000</span><br><span class="line">KUBERNETES_SERVICE_PORT_HTTPS=443</span><br><span class="line">KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.96.0.1</span><br><span class="line">PWD=/</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到打印了很多环境变量处理，其中就包括我们刚刚创建的 nginx-service 这个服务，有 HOST、PORT、PROTO、ADDR 等，也包括其他已经存在的 Service 的环境变量，现在如果我们需要在这个 Pod 里面访问 nginx-service 的服务，我们是不是可以直接通过 NGINX_SERVICE_SERVICE_HOST 和 NGINX_SERVICE_SERVICE_PORT 就可以了，但是我们也知道如果这个 Pod 启动起来的时候如果 nginx-service 服务还没启动起来，在环境变量中我们是无法获取到这些信息的，当然我们可以通过 <code>initContainer</code>之类的方法来确保 nginx-service 启动后再启动 Pod，但是这种方法毕竟增加了 Pod 启动的复杂性，所以这不是最优的方法。</p>
<h2 id="KubeDNS"><a href="#KubeDNS" class="headerlink" title="KubeDNS"></a>KubeDNS</h2><p>由于上面环境变量这种方式的局限性，我们需要一种更加智能的方案，其实我们可以自己想学一种比较理想的方案：那就是可以直接使用 Service 的名称，因为 Service 的名称不会变化，我们不需要去关心分配的<code>ClusterIP</code>的地址，因为这个地址并不是固定不变的，所以如果我们直接使用 Service 的名字，然后对应的<code>ClusterIP</code>地址的转换能够自动完成就很好了。我们知道名字和 IP 直接的转换是不是和我们平时访问的网站非常类似啊？他们之间的转换功能通过<code>DNS</code>就可以解决了，同样的，<code>Kubernetes</code>也提供了<code>DNS</code><br>的方案来解决上面的服务发现的问题。</p>
<h3 id="kubedns-介绍"><a href="#kubedns-介绍" class="headerlink" title="kubedns 介绍"></a>kubedns 介绍</h3><p>DNS 服务不是一个独立的系统服务，而是作为一种 addon 插件而存在，也就是说不是 Kubernetes 集群必须安装的，当然我们强烈推荐安装，可以将这个插件看成是一种运行在 Kubernetes 集群上的一直比较特殊的应用，现在比较推荐的两个插件：<code>kube-dns</code> 和 <code>CoreDNS</code>。我们在前面使用<code>kubeadm</code>搭建集群的时候直接安装的 kube-dns 插件，如果不记得了可以回头去看一看。当然如果我们想使用<code>CoreDNS</code> 的话也很方便，只需要执行下面的命令即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm init --feature-gates=CoreDNS=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>Kubernetes DNS pod 中包括 3 个容器，可以通过 kubectl 工具查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line">NAME                                    READY     STATUS    RESTARTS   AGE</span><br><span class="line">...</span><br><span class="line">kube-dns-5868f69869-zp5kz               3/3       Running   0          19d</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>READY 一栏可以看到是 3/3，用如下命令可以很清楚的看到 kube-dns 包含的3个容器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod kube-dns-5868f69869-zp5kz -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>kube-dns、dnsmasq-nanny、sidecar 这3个容器分别实现了什么功能?</p>
<ul>
<li>kubedns: kubedns 基于 SkyDNS 库，通过 apiserver 监听 Service 和 Endpoints 的变更事件同时也同步到本地 Cache，实现了一个实时的 Kubernetes 集群内 Service 和 Pod 的 DNS服务发现</li>
<li>dnsmasq: dsnmasq 容器则实现了 DNS 的缓存功能(在内存中预留一块默认大小为 1G 的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubedns 中查询，并把结果缓存起来)，通过监听 ConfigMap 来动态生成配置</li>
<li>sider: sidecar 容器实现了可配置的 DNS 探测，并采集对应的监控指标暴露出来供 prometheus 使用<br><img src="/images/k8s/kubedns.jpg" alt="kubedns"></li>
</ul>
<h3 id="对-Pod-的影响"><a href="#对-Pod-的影响" class="headerlink" title="对 Pod 的影响"></a>对 Pod 的影响</h3><p>DNS Pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 被分配后，kubelet 会将使用 <code>--cluster-dns = &lt;dns-service-ip&gt;</code>参数配置的 DNS 传递给每个容器。DNS 名称也需要域名，本地域可以使用参数<code>--cluster-domain = &lt;default-local-domain&gt;</code>在<code>kubelet</code>中配置。</p>
<p>我们说<code>dnsmasq</code>容器通过监听<code>ConfigMap</code>来动态生成配置，可以自定义存根域和上下游域名服务器。</p>
<p>例如，下面的<code>ConfigMap</code>建立了一个 DNS 配置，它具有一个单独的存根域和两个上游域名服务器：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kube-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  stubDomains:</span> <span class="string">|</span><br><span class="line">    &#123;"jiunile.local": ["192.168.3.108"]&#125;</span><br><span class="line"></span><span class="attr">  upstreamNameservers:</span> <span class="string">|</span><br><span class="line">    ["8.8.8.8", "8.8.4.4"]</span></span><br></pre></td></tr></table></figure></p>
<p>如何已使用coredns，则通过修改coredns配置信息来调整，如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  Corefile:</span> <span class="string">|</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           # 用于解析外部主机主机（外部服务）</span><br><span class="line">           upstream 114.114.114.114 223.5.5.5</span><br><span class="line">           fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        # 任何不在集群域内的查询将转发到预定义的解析器，默认：/etc/resolv.conf；</span><br><span class="line">        # 在coredns "Deployment"资源中"dnsPolicy"设置为"Default"，即提供dns服务的pod从所在节点继承/etc/resolv.conf，如果节点的上游解析地址与"upstream"一致，则设置任意一个参数即可</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        #proxy . 114.114.114.114 223.5.5.5</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">    &#125;</span><br><span class="line">    # 自定义dns记录，对应kube-dns中的stubdomains；</span><br><span class="line">    # 每条记录，单独设置1各zone</span><br><span class="line">    jiunile.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        proxy . 192.168.3.108</span><br><span class="line">    &#125;</span><br><span class="line"></span><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="number">2019</span><span class="bullet">-01</span><span class="bullet">-25</span>T10:<span class="number">19</span>:<span class="number">17</span>Z</span><br><span class="line"><span class="attr">  name:</span> coredns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"2637234"</span></span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/kube-system/configmaps/coredns</span><br><span class="line"><span class="attr">  uid:</span> ac30bd36<span class="bullet">-208</span>a<span class="bullet">-11e9</span><span class="bullet">-999</span>a<span class="bullet">-02390</span>c37278e</span><br></pre></td></tr></table></figure></p>
<p>按如上说明，具有.jiunile.local后缀的 DNS 请求被转发到 DNS 192.168.3.108。Google 公共 DNS 服务器 为上游查询提供服务。下表描述了具有特定域名的查询如何映射到它们的目标 DNS 服务器：<br>| 域名      | 响应查询的服务器  |<br>| :——– | :——– |<br>| kubernetes.default.svc.cluster.local  | kube-dns |<br>| jiunile.local | 自定义 DNS (192.168.3.108) |<br>| widget.com | 上游 DNS (114.114.114.114, 223.5.5.5其中之一) |</p>
<p>另外我们还可以为每个 Pod 设置 DNS 策略。 当前 Kubernetes 支持两种 Pod 特定的 DNS 策略：“Default” 和 “ClusterFirst”。 可以通过<code>dnsPolicy</code>标志来指定这些策略。</p>
<blockquote>
<p><strong><code>注意</code></strong>：Default 不是默认的 DNS 策略。如果没有显式地指定dnsPolicy，将会使用 ClusterFirst</p>
</blockquote>
<ul>
<li>如果<code>dnsPolicy</code>被设置为 “Default”，则名字解析配置会继承自 Pod 运行所在的节点。自定义上游域名服务器和存根域不能够与这个策略一起使用</li>
<li>如果<code>dnsPolicy</code>被设置为 “ClusterFirst”，这就要依赖于是否配置了存根域和上游 DNS 服务器<ul>
<li>未进行自定义配置：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游域名服务器。<ul>
<li>进行自定义配置：如果配置了存根域和上游 DNS 服务器（类似于 前面示例 配置的内容），DNS 查询将基于下面的流程对请求进行路由：<ul>
<li>查询首先被发送到<code>kube-dns</code>中的 DNS 缓存层。</li>
<li>从缓存层，检查请求的后缀，并根据下面的情况转发到对应的 DNS 上：<ul>
<li>具有集群后缀的名字（例如 “.cluster.local”）：请求被发送到 kubedns。</li>
<li>具有存根域后缀的名字（例如 “.jiunile.local”）：请求被发送到配置的自定义 DNS 解析器（例如：监听在 192.168.3.108）。</li>
<li>未能匹配上后缀的名字（例如 “widget.com”）：请求被转发到上游 DNS（例如：Google 公共 DNS 服务器，114.114.114.114 和 223.5.5.5）。<br><img src="/images/k8s/dns.png" alt="dns"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="域名格式"><a href="#域名格式" class="headerlink" title="域名格式"></a>域名格式</h3><p>我们前面说了如果我们建立的 Service 如果支持域名形式进行解析，就可以解决我们的服务发现的功能，那么利用 kubedns 可以将 Service 生成怎样的 DNS 记录呢？</p>
<ul>
<li>普通的 Service：会生成<code>servicename.namespace.svc.cluster.local</code>的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成 <code>servicename.namespace</code>，如果处于同一个命名空间下面，甚至可以只写成 servicename 即可访问</li>
<li>Headless Service：无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过<code>podname.servicename.namespace.svc.cluster.local</code>访问到具体的某一个 Pod。</li>
</ul>
<blockquote>
<p>CoreDNS 实现的功能和 KubeDNS 是一致的，不过<code>CoreDNS</code>的所有功能都集成在了同一个容器中，在最新版的1.11.0版本中官方已经推荐使用 <code>CoreDNS</code>了，大家也可以安装CoreDNS来代替 KubeDNS，其他使用方法都是一致的：<a href="https://coredns.io/" target="_blank" rel="external">https://coredns.io/</a></p>
</blockquote>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>现在我们来使用一个简单 Pod 来测试下 Service 的域名访问：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run --rm -i --tty <span class="built_in">test</span>-dns --image=busybox /bin/sh</span><br><span class="line">If you don<span class="string">'t see a command prompt, try pressing enter.</span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ #</span></span><br></pre></td></tr></table></figure></p>
<p>我们进入到 Pod 中，查看/etc/resolv.conf中的内容，可以看到 nameserver 的地址10.96.0.10，该 IP 地址即是在安装 kubedns 插件的时候集群分配的一个固定的静态 IP 地址，我们可以通过下面的命令进行查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc kube-dns -n kube-system</span><br><span class="line">NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP   62d</span><br></pre></td></tr></table></figure></p>
<p>也就是说我们这个 Pod 现在默认的 nameserver 就是 kubedns 的地址，现在我们来访问下前面我们创建的 nginx-service 服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># wget -q -O- nginx-service.default.svc.cluster.local</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到上面我们使用 wget 命令去访问 nginx-service 服务的域名的时候被 hang 住了，没有得到期望的结果，这是因为上面我们建立 Service 的时候暴露的端口是 5000：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># wget -q -O- nginx-service.default.svc.cluster.local:5000</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.org/"</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.com/"</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you <span class="keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>加上 5000 端口，就正常访问到服务，再试一试访问：nginx-service.default.svc、nginx-service.default、nginx-service，不出意外这些域名都可以正常访问到期望的结果。</p>
<p>到这里我们是不是就实现了在集群内部通过 Service 的域名形式进行互相通信了，大家下去试着看看访问不同 namespace 下面的服务呢？</p>
<p>来源：阳明的博客</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kuberenetes 操作实践]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-best-practices.html</url>
      <content type="html"><![CDATA[<h2 id="常用操作命令"><a href="#常用操作命令" class="headerlink" title="常用操作命令"></a>常用操作命令</h2><h3 id="常规命令"><a href="#常规命令" class="headerlink" title="常规命令"></a>常规命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置主机名</span></span><br><span class="line">hostnamectl --static <span class="built_in">set</span>-hostname  xxxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取集群加入信息</span></span><br><span class="line">kubeadm token create --print-join-command</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker仓库口令</span></span><br><span class="line">kubectl create secret docker-registry registry-secret --docker-server=registry.cn-shanghai.aliyuncs.com --docker-username=xupeng@patsnap --docker-password=patsnap2019! --docker-email=xupeng@patsnap -n ningbo</span><br><span class="line"></span><br><span class="line"><span class="comment"># etcd3.3 node状态查看</span></span><br><span class="line">ETCDCTL_API=3 etcdctl endpoint health --endpoints <span class="string">"https://10.40.20.41:2379,https://10.40.20.46:2379,https://10.40.20.233:2379"</span>   --cacert=/etc/etcd/ssl/etcd-ca.pem  --cert=/etc/etcd/ssl/etcd.pem   --key=/etc/etcd/ssl/etcd-key.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># etcd key 查看</span></span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">--endpoints=https://127.0.0.1:2379 \</span><br><span class="line">--cacert /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">get /registry/minions/ningbo-db --prefix</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="kubernetes-node-常用操作命令"><a href="#kubernetes-node-常用操作命令" class="headerlink" title="kubernetes node 常用操作命令"></a>kubernetes node 常用操作命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置node 不可调度</span></span><br><span class="line">kubectl cordon k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#驱逐node 上的pod</span></span><br><span class="line">kubectl drain k8s-node-1 --delete-local-data --force --ignore-daemonsets</span><br><span class="line"></span><br><span class="line"><span class="comment">#node 重新加入</span></span><br><span class="line">kubectl uncordon k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除node</span></span><br><span class="line">kubectl delete node k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置master不进行调度</span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置label</span></span><br><span class="line">kubectl label node k8s-master project=ipms-app</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置role标签</span></span><br><span class="line">kubectl label node k8s-node-01 node-role.kubernetes.io/node=</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除标签</span></span><br><span class="line">kubectl label nodes k8s-master mtype-</span><br></pre></td></tr></table></figure>
<h2 id="如何调整网络插件"><a href="#如何调整网络插件" class="headerlink" title="如何调整网络插件"></a>如何调整网络插件</h2><blockquote>
<p>calico网络插件调整，原本没使用IPIP模式，现在需要使用CrossSubnet</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先删除原本的calico插件</span></span><br><span class="line">kubectl delete <span class="_">-f</span> calico.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改calico.yaml，将CALICO_IPV4POOL_IPIP调整为CrossSubnet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#启用calico插件</span></span><br><span class="line">kubectl apply <span class="_">-f</span> calico.yaml</span><br></pre></td></tr></table></figure>
<h2 id="如何将kubeProxy的iptables修改为ipvs"><a href="#如何将kubeProxy的iptables修改为ipvs" class="headerlink" title="如何将kubeProxy的iptables修改为ipvs"></a>如何将kubeProxy的iptables修改为ipvs</h2><h3 id="1-加载内核模块"><a href="#1-加载内核模块" class="headerlink" title="1. 加载内核模块"></a>1. 加载内核模块</h3><p>查看内核模块是否加载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod|grep ip_vs</span><br></pre></td></tr></table></figure></p>
<p>如果没有加载，使用如下命令加载ipvs相关模块<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure></p>
<h3 id="2-更改kube-proxy配置"><a href="#2-更改kube-proxy配置" class="headerlink" title="2. 更改kube-proxy配置"></a>2. 更改kube-proxy配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit configmap kube-proxy -n kube-system</span><br></pre></td></tr></table></figure>
<p>找到并修改如下部分的内容<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enableProfiling:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">10256</span></span><br><span class="line"><span class="attr">hostnameOverride:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">iptables:</span></span><br><span class="line"><span class="attr">  masqueradeAll:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  masqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">ipvs:</span></span><br><span class="line"><span class="attr">  excludeCIDRs:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  scheduler:</span> <span class="string">""</span>  <span class="comment">#=========================&gt; 为空表示默认的负载均衡算法为轮询， rr, wrr, lc, wlc, sh, dh, lblc...</span></span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">metricsBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">10249</span></span><br><span class="line"><span class="attr">mode:</span> iptables   <span class="comment">#=========================&gt; 修改此处为ipvs</span></span><br><span class="line"><span class="attr">nodePortAddresses:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">portRange:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">resourceContainer:</span> /kube-proxy</span><br><span class="line"><span class="attr">udpIdleTimeout:</span> <span class="number">250</span>ms</span><br></pre></td></tr></table></figure></p>
<p>编辑完，保存退出</p>
<h3 id="3-删除所有kube-proxy的pod"><a href="#3-删除所有kube-proxy的pod" class="headerlink" title="3. 删除所有kube-proxy的pod"></a>3. 删除所有kube-proxy的pod</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod $(kubectl get pod -n kube-system | grep kube-proxy | awk -F <span class="string">' '</span> <span class="string">'&#123;print $1&#125;'</span>) -n kube-system</span><br></pre></td></tr></table></figure>
<h3 id="4-查看kube-proxy的pod日志"><a href="#4-查看kube-proxy的pod日志" class="headerlink" title="4. 查看kube-proxy的pod日志"></a>4. 查看kube-proxy的pod日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs kube-proxy-xxx -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">#I0308 02:16:02.980965       1 server_others.go:183] Using ipvs Proxier.</span></span><br><span class="line"><span class="comment">#W0308 02:16:02.991188       1 proxier.go:356] IPVS scheduler not specified, use rr by default</span></span><br><span class="line"><span class="comment">#I0308 02:16:02.991338       1 server_others.go:210] Tearing down inactive rules.</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.022123       1 server.go:448] Version: v1.11.6</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.028801       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_max' to 131072</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029030       1 conntrack.go:52] Setting nf_conntrack_max to 131072</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029208       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029296       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029639       1 config.go:102] Starting endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029682       1 controller_utils.go:1025] Waiting for caches to sync for endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029723       1 config.go:202] Starting service config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029777       1 controller_utils.go:1025] Waiting for caches to sync for service config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.129930       1 controller_utils.go:1032] Caches are synced for endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.129931       1 controller_utils.go:1032] Caches are synced for service config controller</span></span><br><span class="line"></span><br><span class="line">看到有 Using ipvs Proxier 即表明切换成功.</span><br></pre></td></tr></table></figure>
<h3 id="5-安装ipvsadm"><a href="#5-安装ipvsadm" class="headerlink" title="5. 安装ipvsadm"></a>5. 安装ipvsadm</h3><p>使用ipvsadm查看ipvs相关规则，如果没有这个命令可以直接yum安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ipvsadm</span><br></pre></td></tr></table></figure></p>
<h2 id="kubelet-GC-机制及对应配置"><a href="#kubelet-GC-机制及对应配置" class="headerlink" title="kubelet GC 机制及对应配置"></a>kubelet GC 机制及对应配置</h2><blockquote>
<p>可通过kubelet gc的能力来自动清理无用的container以及image，从而来释放磁盘空间。kubelet会启动两个GC，分别回收container和image。其中container的回收频率为1分钟一次，而image回收频率为2分钟一次。</p>
</blockquote>
<h3 id="容器GC"><a href="#容器GC" class="headerlink" title="容器GC"></a>容器GC</h3><blockquote>
<p>退出的容器也会继续占用系统资源，比如还会在文件系统存储很多数据、docker 应用也要占用 CPU 和内存去维护这些容器。docker 本身并不会自动删除已经退出的容器，因此 kubelet 就负起了这个责任。kubelet 容器的回收是为了删除已经退出的容器以节省节点的空间，提升性能。容器 GC 虽然有利于空间和性能，但是删除容器也会导致错误现场被清理，不利于 debug 和错误定位，因此不建议把所有退出的容器都删除。因此容器的清理需要一定的策略，主要是告诉 kubelet 你要保存多少已经退出的容器。和容器 GC 有关的可以配置的 kubelet 启动参数 <code>/var/lib/kubelet/config.yaml</code></p>
</blockquote>
<ul>
<li><code>MinimumGCAge</code>：container 结束多长时间之后才能够被回收，默认是一分钟</li>
<li><code>MaxPerPodContainerCount</code>：每个 container 最终可以保存多少个已经结束的容器，默认是 1，设置为负数表示不做限制</li>
<li><code>MaxContainerCount</code>：节点上最多能保留多少个结束的容器，默认是 -1，表示不做限制</li>
</ul>
<p>gc的步骤如下：</p>
<ol>
<li>获取可以清除的容器，这些容器都是非活动的，并且创建时间比 gcPolicy.MinAge 要早</li>
<li>通过强制执行 gcPolicy.MaxPerPodContainer，为每个pod删除最老的死亡容器</li>
<li>通过强制执行 gcPolicy.MaxContainers 来移除最老的死亡容器</li>
<li>获取未准备好且不包含容器的可清除沙箱</li>
<li>移除可移除的沙箱</li>
</ol>
<h3 id="镜像GC"><a href="#镜像GC" class="headerlink" title="镜像GC"></a>镜像GC</h3><blockquote>
<p>镜像主要占用磁盘空间，虽然 docker 使用镜像分层可以让多个镜像共享存储，但是长时间运行的节点如果下载了很多镜像也会导致占用的存储空间过多。如果镜像导致磁盘被占满，会造成应用无法正常工作。docker 默认也不会做镜像清理，镜像一旦下载就会永远留在本地，除非被手动删除。其实很多镜像并没有被实际使用，这些不用的镜像继续占用空间是非常大的浪费，也是巨大的隐患，因此 kubelet 也会周期性地去清理镜像。镜像的清理和容器不同，是以占用的空间作为标准的，用户可以配置当镜像占据多大比例的存储空间时才进行清理。清理的时候会优先清理最久没有被使用的镜像，镜像被 pull 下来或者被容器使用都会更新它的最近使用时间。启动 kubelet 的时候，可以配置这些参数控制镜像清理的策略 <code>/var/lib/kubelet/config.yaml</code></p>
</blockquote>
<ul>
<li><code>imageMinimumGCAge</code>：镜像最少多久没有被使用才会被清理</li>
<li><code>imageGCHighThresholdPercent</code>：磁盘使用率的上限，当达到这一使用率的时候会触发镜像清理。默认值为 90%</li>
<li><code>imageGCLowThresholdPercent</code>：磁盘使用率的下限，每次清理直到使用率低于这个值或者没有可以清理的镜像了才会停止.默认值为 80%</li>
</ul>
<p>也就是说，默认情况下，当镜像占满所在盘 90% 容量的时候，kubelet 就会进行清理，一直到镜像占用率低于 80% 为止。</p>
<h2 id="使用kubeadm进行cluster升级"><a href="#使用kubeadm进行cluster升级" class="headerlink" title="使用kubeadm进行cluster升级"></a>使用kubeadm进行cluster升级</h2><blockquote>
<p>使用kubeadm 安装好后kubernetes，后续如何进行升级，升级 kubernetes集群，只能逐版本升级。只能从 1.12 升级到 1.13 而不能从 1.1 直接升级到 1.13，升级步骤： 1.11—&gt;1.12—&gt;1.13—&gt;1.14</p>
</blockquote>
<p>需要注意的地方是，kubernetes从1.11版本开始变化比较大，CoreDNS已作为默认DNS。<br><code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>中的环境变量被分配为三个文件：<code>/var/lib/kubelet/config.yaml</code> (其中cgroup驱动默认cgroupfs)、<code>/var/lib/kubelet/kubeadm-flags.env</code> (cgroup驱动默认systemd，优先权)、<code>/etc/sysconfig/kubelet</code><br>全新安装的kubernetes集群是有网络CNI配置的，升级安装的是没有CNI配置的，配置文件<code>/var/lib/kubelet/kubeadm-flags.env</code>。依赖的镜像tag抬头从<code>gcr.io/google_containers</code>变成<code>k8s.gcr.io</code>，升级基本使用gcr.io/google<em>containers，全新安装则使用<code>k8s.gcr.io</code>&gt;</em>&lt;</p>
<blockquote>
<p>从1.8开始为kube-proxy组件引入了IPVS模式，1.11版本开始正式支持IPVS，默认不开启，1.12以上版本默认开启，不开启则使用iptables模式</p>
</blockquote>
<h3 id="准备升级镜像"><a href="#准备升级镜像" class="headerlink" title="准备升级镜像"></a>准备升级镜像</h3><p>提前准备好升级所需的镜像image，并打成官方标准tag。<br>master节点需要所有镜像，node节点仅需要proxy、pause镜像</p>
<h4 id="在所有Master节点下载各版本镜像"><a href="#在所有Master节点下载各版本镜像" class="headerlink" title="在所有Master节点下载各版本镜像"></a>在所有Master节点下载各版本镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-scheduler:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-controller-manager:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-apiserver:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/etcd:3.2.24</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/pause:3.1</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/coredns:1.2.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-scheduler:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-controller-manager:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-apiserver:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/etcd:3.2.24</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/pause:3.1</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/coredns:1.2.6</span></span><br></pre></td></tr></table></figure>
<h4 id="Node节点下载各版本镜像"><a href="#Node节点下载各版本镜像" class="headerlink" title="Node节点下载各版本镜像"></a>Node节点下载各版本镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/pause:3.1</span></span><br></pre></td></tr></table></figure>
<h3 id="1-11-6升级到1-12-7"><a href="#1-11-6升级到1-12-7" class="headerlink" title="1.11.6升级到1.12.7"></a>1.11.6升级到1.12.7</h3><h4 id="master节点（使用内部etcd）"><a href="#master节点（使用内部etcd）" class="headerlink" title="master节点（使用内部etcd）"></a>master节点（使用内部etcd）</h4><h5 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在所有master节点上执行以下命令</span></span><br><span class="line">kubectl get configmap -n kube-system kubeadm-config -o yaml &gt; kubeadm-config-cm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#编辑第一台master节点kubeadm-config-cm.yaml配置，修改以下参数</span></span><br><span class="line">api.advertiseAddress <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.advertise-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.initial-advertise-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.initial-cluster <span class="comment">#---&gt; 当前master ip地址和master主机名，例如："ip-172-31-92-42=https://172.31.92.42:2380,ip-172-31-89-186=https://172.31.89.186:2380,ip-172-31-90-42=https://172.31.90.42:2380"</span></span><br><span class="line"></span><br><span class="line">You must also pass an additional argument (initial-cluster-state: existing) to etcd.local.extraArgs.</span><br><span class="line"></span><br><span class="line"><span class="comment">#编辑其余master主机上的kubeadm-config-cm.yaml 修改ClusterConfiguration以下参数：</span></span><br><span class="line">etcd.local.extraArgs.advertise-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.initial-advertise-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line"></span><br><span class="line">You must also modify the ClusterStatus to add a mapping <span class="keyword">for</span> the current host under apiEndpoints.</span><br></pre></td></tr></table></figure>
<h5 id="开始master升级"><a href="#开始master升级" class="headerlink" title="开始master升级"></a>开始master升级</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.12.7 kubelet-1.12.7 kubectl-1.12.7 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line">执行升级，确保上面已经准备好镜像image</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级第一台master节点</span></span><br><span class="line">kubectl apply <span class="_">-f</span> kubeadm-config-cm.yaml --force</span><br><span class="line">kubeadm upgrade apply v1.12.7</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级其余master节点</span></span><br><span class="line">kubectl annotate node &lt;nodename&gt; kubeadm.alpha.kubernetes.io/cri-socket=/var/run/dockershim.sock</span><br><span class="line">kubectl apply <span class="_">-f</span> kubeadm-config-cm.yaml --force</span><br><span class="line">kubeadm upgrade apply v1.12.7</span><br><span class="line"></span><br><span class="line"><span class="comment">#看到如下，说明升级成功</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.12.7"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="master节点（使用外部etcd）"><a href="#master节点（使用外部etcd）" class="headerlink" title="master节点（使用外部etcd）"></a>master节点（使用外部etcd）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在所有master节点上执行以下命令</span></span><br><span class="line">kubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.MasterConfiguration&#125; &gt; kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line">编辑kubeadm-config.yaml, 修改api.advertiseAddress值为当前master的ip地址</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.12.3 kubelet-1.12.7 kubectl-1.12.7 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line">执行升级，确保上面已经准备好镜像image</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#正式升级命令</span></span><br><span class="line">kubeadm upgrade apply v1.12.7 --config kubeadm-config.yaml</span><br><span class="line"><span class="comment">#看到如下，说明升级成功</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.12.3"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="node节点"><a href="#node节点" class="headerlink" title="node节点"></a>node节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级kubernetes的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon node其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain node其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#节点上执行，升级kubelet kubeadm kubectl</span></span><br><span class="line">yum install -y kubeadm-1.12.7 kubelet-1.12.7 kubectl-1.12.7 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级node节点的配置，配置文件/var/lib/kubelet/config.yaml中的cgroupDriver需要保持与docker的Cgroup Driver一致</span></span><br><span class="line">kubeadm upgrade node config --kubelet-version $(kubelet --version | cut <span class="_">-d</span> <span class="string">' '</span> <span class="_">-f</span> 2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon node节点名称</span><br></pre></td></tr></table></figure>
<h3 id="1-12-7升级到1-13-5"><a href="#1-12-7升级到1-13-5" class="headerlink" title="1.12.7升级到1.13.5"></a>1.12.7升级到1.13.5</h3><h4 id="master节点（使用内部etcd）-1"><a href="#master节点（使用内部etcd）-1" class="headerlink" title="master节点（使用内部etcd）"></a>master节点（使用内部etcd）</h4><h5 id="前置准备-1"><a href="#前置准备-1" class="headerlink" title="前置准备"></a>前置准备</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br><span class="line"></span><br><span class="line">1. 移除etcd相关部分</span><br><span class="line">2. 修改apiEndpoints值 Add an entry <span class="keyword">for</span> each of the additional control plane hosts，如下例子：</span><br><span class="line"><span class="comment">#      ip-10-40-40-14.cn-northwest-1.compute.internal:</span></span><br><span class="line"><span class="comment">#        advertiseAddress: 10.40.40.14</span></span><br><span class="line"><span class="comment">#        bindPort: 6443</span></span><br><span class="line"><span class="comment">#      ip-10-40-40-190.cn-northwest-1.compute.internal:</span></span><br><span class="line"><span class="comment">#        advertiseAddress: 10.40.40.190</span></span><br><span class="line"><span class="comment">#        bindPort: 6443</span></span><br><span class="line"><span class="comment">#      ip-10-40-40-195.cn-northwest-1.compute.internal:</span></span><br><span class="line"><span class="comment">#        advertiseAddress: 10.40.40.195</span></span><br><span class="line"><span class="comment">#        bindPort: 6443</span></span><br></pre></td></tr></table></figure>
<h5 id="开始升级master"><a href="#开始升级master" class="headerlink" title="开始升级master"></a>开始升级master</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.13.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级，确保上面已经准备好镜像image</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master 使用以下命令执行</span></span><br><span class="line">kubeadm upgrade apply v1.13.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#其余master使用以下命令执行</span></span><br><span class="line">kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master升级成功信息</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.13.5"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"><span class="comment">#其余master升级成功信息</span></span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.13.5 kubectl-1.13.5</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="master节点（使用外部etcd）-1"><a href="#master节点（使用外部etcd）-1" class="headerlink" title="master节点（使用外部etcd）"></a>master节点（使用外部etcd）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.13.5</span><br><span class="line"></span><br><span class="line">执行升级，确保上面已经准备好镜像image</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master使用以下命令执行：</span></span><br><span class="line">kubeadm upgrade apply v1.13.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#剩余master使用以下命令执行：</span></span><br><span class="line">kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master升级成功信息</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.13.5"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"><span class="comment">#其余master升级成功信息</span></span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.13.5 kubectl-1.13.5</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="node节点-1"><a href="#node节点-1" class="headerlink" title="node节点"></a>node节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级kubernetes的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon node其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain node其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#节点上执行，升级kubelet kubeadm kubectl</span></span><br><span class="line">yum install -y kubeadm-1.13.5 kubelet-1.13.5 kubectl-1.13.5 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级node节点的配置，配置文件/var/lib/kubelet/config.yaml中的cgroupDriver需要保持与docker的Cgroup Driver一致</span></span><br><span class="line">kubeadm upgrade node config --kubelet-version $(kubelet --version | cut <span class="_">-d</span> <span class="string">' '</span> <span class="_">-f</span> 2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon node节点名称</span><br></pre></td></tr></table></figure>
<h3 id="1-13-5升级到1-14-0"><a href="#1-13-5升级到1-14-0" class="headerlink" title="1.13.5升级到1.14.0"></a>1.13.5升级到1.14.0</h3><h4 id="如果使用的外部的etcd则进行如下操作"><a href="#如果使用的外部的etcd则进行如下操作" class="headerlink" title="如果使用的外部的etcd则进行如下操作"></a>如果使用的外部的etcd则进行如下操作</h4><p>Modify <code>configmap/kubeadm-config</code> for this control plane node by removing the <strong>etcd</strong> section completely</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  ClusterConfiguration:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiServer:</span></span><br><span class="line"><span class="attr">      certSANs:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span></span><br><span class="line"><span class="attr">      extraArgs:</span></span><br><span class="line"><span class="attr">        authorization-mode:</span> Node,RBAC</span><br><span class="line"><span class="attr">      timeoutForControlPlane:</span> <span class="number">4</span>m0s</span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">    certificatesDir:</span> /etc/kubernetes/pki</span><br><span class="line"><span class="attr">    clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">    controlPlaneEndpoint:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span>:<span class="number">6443</span></span><br><span class="line"><span class="attr">    controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">    dns:</span></span><br><span class="line"><span class="attr">      type:</span> CoreDNS</span><br><span class="line"><span class="attr">    etcd:</span></span><br><span class="line"><span class="attr">      local:</span></span><br><span class="line"><span class="attr">        dataDir:</span> /var/lib/etcd</span><br><span class="line"><span class="attr">    imageRepository:</span> k8s.gcr.io</span><br><span class="line"><span class="attr">    kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">    kubernetesVersion:</span> v1<span class="number">.14</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    networking:</span></span><br><span class="line"><span class="attr">      dnsDomain:</span> cluster.local</span><br><span class="line"><span class="attr">      podSubnet:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">      serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span></span><br><span class="line"><span class="attr">    scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="attr">  ClusterStatus:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiEndpoints:</span></span><br><span class="line"><span class="attr">      k8s-master1:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.161</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master2:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.162</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master3:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.163</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">    kind:</span> ClusterStatus</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="string">"2019-05-21T10:08:03Z"</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"209870"</span></span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/kube-system/configmaps/kubeadm-config</span><br><span class="line"><span class="attr">  uid:</span> <span class="number">52419642</span><span class="bullet">-7</span>bb0<span class="bullet">-11e9</span><span class="bullet">-8</span>a89<span class="bullet">-0800270</span>fde1d</span><br></pre></td></tr></table></figure>
<h4 id="master节点"><a href="#master节点" class="headerlink" title="master节点"></a>master节点</h4><h5 id="升级第一个控制平面-mater1"><a href="#升级第一个控制平面-mater1" class="headerlink" title="升级第一个控制平面(mater1)"></a>升级第一个控制平面(mater1)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade apply v1.14.0</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.14.0-0 kubectl-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h5 id="升级其他控制平面-mater2、master3"><a href="#升级其他控制平面-mater2、master3" class="headerlink" title="升级其他控制平面(mater2、master3)"></a>升级其他控制平面(mater2、master3)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.14.0-0 kubectl-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#master2日志</span></span><br><span class="line">[upgrade] Reading configuration from the cluster...</span><br><span class="line">[upgrade] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[upgrade] Upgrading your Static Pod-hosted control plane instance to version <span class="string">"v1.14.0"</span>...</span><br><span class="line">Static pod: kube-apiserver-k8s-master2 <span class="built_in">hash</span>: ba03afd84d454d318c2cc6e3a6e23f53</span><br><span class="line">Static pod: kube-controller-manager-k8s-master2 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-scheduler-k8s-master2 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">[upgrade/etcd] Upgrading to TLS <span class="keyword">for</span> etcd</span><br><span class="line">Static pod: etcd-k8s-master2 <span class="built_in">hash</span>: d990ad5b88743835159168644453f90b</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/etcd.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: etcd-k8s-master2 <span class="built_in">hash</span>: d990ad5b88743835159168644453f90b</span><br><span class="line">Static pod: etcd-k8s-master2 <span class="built_in">hash</span>: e56ee6ac7c0de512a17ef30c3a44e01c</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=etcd</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"etcd"</span> upgraded successfully!</span><br><span class="line">[upgrade/etcd] Waiting <span class="keyword">for</span> etcd to become available</span><br><span class="line">[upgrade/staticpods] Writing new Static Pod manifests to <span class="string">"/etc/kubernetes/tmp/kubeadm-upgraded-manifests998233672"</span></span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-apiserver.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/kube-apiserver.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-apiserver-k8s-master2 <span class="built_in">hash</span>: ba03afd84d454d318c2cc6e3a6e23f53</span><br><span class="line">Static pod: kube-apiserver-k8s-master2 <span class="built_in">hash</span>: 94e207e0d84e092ae98dc64af5b870ba</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-apiserver</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-apiserver"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-controller-manager.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/kube-controller-manager.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-controller-manager-k8s-master2 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-controller-manager-k8s-master2 <span class="built_in">hash</span>: e45f10af1ae684722cbd74cb11807900</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-controller-manager</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-controller-manager"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-scheduler.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/kube-scheduler.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-scheduler-k8s-master2 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">Static pod: kube-scheduler-k8s-master2 <span class="built_in">hash</span>: 58272442e226c838b193bbba4c44091e</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-scheduler</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br><span class="line"></span><br><span class="line"><span class="comment">#master3 升级日志</span></span><br><span class="line">[upgrade] Reading configuration from the cluster...</span><br><span class="line">[upgrade] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[upgrade] Upgrading your Static Pod-hosted control plane instance to version <span class="string">"v1.14.0"</span>...</span><br><span class="line">Static pod: kube-apiserver-k8s-master3 <span class="built_in">hash</span>: 556e7d43da7a389c6b0b116ae5a46d97</span><br><span class="line">Static pod: kube-controller-manager-k8s-master3 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-scheduler-k8s-master3 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">[upgrade/etcd] Upgrading to TLS <span class="keyword">for</span> etcd</span><br><span class="line">[upgrade/staticpods] Writing new Static Pod manifests to <span class="string">"/etc/kubernetes/tmp/kubeadm-upgraded-manifests859456185"</span></span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-apiserver.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-48-13/kube-apiserver.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-apiserver-k8s-master3 <span class="built_in">hash</span>: 556e7d43da7a389c6b0b116ae5a46d97</span><br><span class="line">Static pod: kube-apiserver-k8s-master3 <span class="built_in">hash</span>: 1a94c94ecfa9f698cfc902<span class="built_in">fc</span>37c15be9</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-apiserver</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-apiserver"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-controller-manager.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-48-13/kube-controller-manager.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-controller-manager-k8s-master3 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-controller-manager-k8s-master3 <span class="built_in">hash</span>: e45f10af1ae684722cbd74cb11807900</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-controller-manager</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-controller-manager"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-scheduler.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-48-13/kube-scheduler.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-scheduler-k8s-master3 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">Static pod: kube-scheduler-k8s-master3 <span class="built_in">hash</span>: 58272442e226c838b193bbba4c44091e</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-scheduler</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br></pre></td></tr></table></figure>
<h4 id="node节点-node1、node2、node3-…"><a href="#node节点-node1、node2、node3-…" class="headerlink" title="node节点(node1、node2、node3 …)"></a>node节点(node1、node2、node3 …)</h4><h5 id="升级kubeadm"><a href="#升级kubeadm" class="headerlink" title="升级kubeadm"></a>升级kubeadm</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.14.x-0 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<h5 id="驱除节点上的容器并让节点不可调度"><a href="#驱除节点上的容器并让节点不可调度" class="headerlink" title="驱除节点上的容器并让节点不可调度"></a>驱除节点上的容器并让节点不可调度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$WORKERNODE</span> --ignore-daemonsets</span><br><span class="line"><span class="comment">#$WORKERNODE: node1、node2、node3 ...</span></span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-配置"><a href="#升级-kubelet-配置" class="headerlink" title="升级 kubelet 配置"></a>升级 kubelet 配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm upgrade node config --kubelet-version v1.14.0</span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-与-kubectl"><a href="#升级-kubelet-与-kubectl" class="headerlink" title="升级 kubelet 与 kubectl"></a>升级 kubelet 与 kubectl</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.14.x-0 kubectl-1.14.x-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启kubelet</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#让节点重新在线</span></span><br><span class="line">kubectl uncordon <span class="variable">$WORKERNODE</span></span><br></pre></td></tr></table></figure>
<h5 id="验证集群的状态"><a href="#验证集群的状态" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure>
<h3 id="1-14-0升级到1-15-0"><a href="#1-14-0升级到1-15-0" class="headerlink" title="1.14.0升级到1.15.0"></a>1.14.0升级到1.15.0</h3><h4 id="如果使用的外部的etcd则进行如下操作-1"><a href="#如果使用的外部的etcd则进行如下操作-1" class="headerlink" title="如果使用的外部的etcd则进行如下操作"></a>如果使用的外部的etcd则进行如下操作</h4><p>Modify <code>configmap/kubeadm-config</code> for this control plane node by removing the <strong>etcd</strong> section completely</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  ClusterConfiguration:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiServer:</span></span><br><span class="line"><span class="attr">      certSANs:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span></span><br><span class="line"><span class="attr">      extraArgs:</span></span><br><span class="line"><span class="attr">        authorization-mode:</span> Node,RBAC</span><br><span class="line"><span class="attr">      timeoutForControlPlane:</span> <span class="number">4</span>m0s</span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta2</span><br><span class="line"><span class="attr">    certificatesDir:</span> /etc/kubernetes/pki</span><br><span class="line"><span class="attr">    clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">    controlPlaneEndpoint:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span>:<span class="number">6443</span></span><br><span class="line"><span class="attr">    controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">    dns:</span></span><br><span class="line"><span class="attr">      type:</span> CoreDNS</span><br><span class="line"><span class="attr">    etcd:</span></span><br><span class="line"><span class="attr">      local:</span></span><br><span class="line"><span class="attr">        dataDir:</span> /var/lib/etcd</span><br><span class="line"><span class="attr">    imageRepository:</span> k8s.gcr.io</span><br><span class="line"><span class="attr">    kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">    kubernetesVersion:</span> v1<span class="number">.15</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    networking:</span></span><br><span class="line"><span class="attr">      dnsDomain:</span> cluster.local</span><br><span class="line"><span class="attr">      serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span></span><br><span class="line"><span class="attr">    scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="attr">  ClusterStatus:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiEndpoints:</span></span><br><span class="line"><span class="attr">      k8s-master1:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.161</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master2:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.162</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master3:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.163</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta2</span><br><span class="line"><span class="attr">    kind:</span> ClusterStatus</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="string">"2019-06-25T04:12:53Z"</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"1337635"</span></span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/kube-system/configmaps/kubeadm-config</span><br><span class="line"><span class="attr">  uid:</span> <span class="number">81334</span>a15<span class="bullet">-96</span>ff<span class="bullet">-11e9</span>-b14f<span class="bullet">-0800270</span>fde1d</span><br></pre></td></tr></table></figure>
<h4 id="master节点-1"><a href="#master节点-1" class="headerlink" title="master节点"></a>master节点</h4><h5 id="升级第一个控制平面-mater1-1"><a href="#升级第一个控制平面-mater1-1" class="headerlink" title="升级第一个控制平面(mater1)"></a>升级第一个控制平面(mater1)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade apply v1.15.0</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.15.0-0 kubectl-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h5 id="升级其他控制平面-mater2、master3-1"><a href="#升级其他控制平面-mater2、master3-1" class="headerlink" title="升级其他控制平面(mater2、master3)"></a>升级其他控制平面(mater2、master3)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade node</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.15.0-0 kubectl-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h4 id="node节点-node1、node2、node3-…-1"><a href="#node节点-node1、node2、node3-…-1" class="headerlink" title="node节点(node1、node2、node3 …)"></a>node节点(node1、node2、node3 …)</h4><h5 id="升级kubeadm-1"><a href="#升级kubeadm-1" class="headerlink" title="升级kubeadm"></a>升级kubeadm</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.15.x-0 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<h5 id="驱除节点上的容器并让节点不可调度-1"><a href="#驱除节点上的容器并让节点不可调度-1" class="headerlink" title="驱除节点上的容器并让节点不可调度"></a>驱除节点上的容器并让节点不可调度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$WORKERNODE</span> --ignore-daemonsets</span><br><span class="line"><span class="comment">#$WORKERNODE: node1、node2、node3 ...</span></span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-配置-1"><a href="#升级-kubelet-配置-1" class="headerlink" title="升级 kubelet 配置"></a>升级 kubelet 配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm upgrade node</span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-与-kubectl-1"><a href="#升级-kubelet-与-kubectl-1" class="headerlink" title="升级 kubelet 与 kubectl"></a>升级 kubelet 与 kubectl</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.15.x-0 kubectl-1.15.x-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启kubelet</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#让节点重新在线</span></span><br><span class="line">kubectl uncordon <span class="variable">$WORKERNODE</span></span><br></pre></td></tr></table></figure>
<h5 id="验证集群的状态-1"><a href="#验证集群的状态-1" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure>
<h3 id="1-15-0升级到1-16-0"><a href="#1-15-0升级到1-16-0" class="headerlink" title="1.15.0升级到1.16.0"></a>1.15.0升级到1.16.0</h3><h4 id="如果使用的外部的etcd则进行如下操作-2"><a href="#如果使用的外部的etcd则进行如下操作-2" class="headerlink" title="如果使用的外部的etcd则进行如下操作"></a>如果使用的外部的etcd则进行如下操作</h4><p>Modify <code>configmap/kubeadm-config</code> for this control plane node by removing the <strong>etcd</strong> section completely</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br></pre></td></tr></table></figure>
<h4 id="master节点-2"><a href="#master节点-2" class="headerlink" title="master节点"></a>master节点</h4><h5 id="升级第一个控制平面-mater1-2"><a href="#升级第一个控制平面-mater1-2" class="headerlink" title="升级第一个控制平面(mater1)"></a>升级第一个控制平面(mater1)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.16.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line"><span class="comment">#让master1下线</span></span><br><span class="line">kubectl drain <span class="variable">$MASTER</span> --ignore-daemonsets</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade apply v1.16.0</span><br><span class="line"></span><br><span class="line"><span class="comment">#master 上线</span></span><br><span class="line">kubectl uncordon <span class="variable">$MASTER</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.15.0-0 kubectl-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h5 id="升级其他控制平面-mater2、master3-2"><a href="#升级其他控制平面-mater2、master3-2" class="headerlink" title="升级其他控制平面(mater2、master3)"></a>升级其他控制平面(mater2、master3)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.16.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade node</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.16.0-0 kubectl-1.16.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h4 id="node节点-node1、node2、node3-…-2"><a href="#node节点-node1、node2、node3-…-2" class="headerlink" title="node节点(node1、node2、node3 …)"></a>node节点(node1、node2、node3 …)</h4><h5 id="升级kubeadm-2"><a href="#升级kubeadm-2" class="headerlink" title="升级kubeadm"></a>升级kubeadm</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.16.x-0 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<h5 id="驱除节点上的容器并让节点不可调度-2"><a href="#驱除节点上的容器并让节点不可调度-2" class="headerlink" title="驱除节点上的容器并让节点不可调度"></a>驱除节点上的容器并让节点不可调度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$WORKERNODE</span> --ignore-daemonsets</span><br><span class="line"><span class="comment">#$WORKERNODE: node1、node2、node3 ...</span></span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-配置-2"><a href="#升级-kubelet-配置-2" class="headerlink" title="升级 kubelet 配置"></a>升级 kubelet 配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm upgrade node</span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-与-kubectl-2"><a href="#升级-kubelet-与-kubectl-2" class="headerlink" title="升级 kubelet 与 kubectl"></a>升级 kubelet 与 kubectl</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.16.x-0 kubectl-1.16.x-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启kubelet</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#让节点重新在线</span></span><br><span class="line">kubectl uncordon <span class="variable">$WORKERNODE</span></span><br></pre></td></tr></table></figure>
<h5 id="验证集群的状态-2"><a href="#验证集群的状态-2" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure>
<h3 id="1-16-0升级到1-17-0"><a href="#1-16-0升级到1-17-0" class="headerlink" title="1.16.0升级到1.17.0"></a>1.16.0升级到1.17.0</h3><blockquote>
<p>同1.15.0到1.16.0的升级</p>
</blockquote>
<h3 id="1-17-0升级到1-18-0"><a href="#1-17-0升级到1-18-0" class="headerlink" title="1.17.0升级到1.18.0"></a>1.17.0升级到1.18.0</h3><blockquote>
<p>同1.15.0到1.16.0的升级</p>
</blockquote>
<p>参考：<br><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-12/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-12/</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-13/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-13/</a><br><a href="https://github.com/truongnh1992/upgrade-kubeadm-cluster" target="_blank" rel="external">https://github.com/truongnh1992/upgrade-kubeadm-cluster</a></p>
<h2 id="kubeadm-init-config-配置"><a href="#kubeadm-init-config-配置" class="headerlink" title="kubeadm init config 配置"></a>kubeadm init config 配置</h2><h3 id="kubeadm-v1-12"><a href="#kubeadm-v1-12" class="headerlink" title="kubeadm v1.12"></a>kubeadm v1.12</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3</span><br><span class="line"><span class="attr">kind:</span> InitConfiguration</span><br><span class="line"><span class="attr">apiEndpoint:</span></span><br><span class="line"><span class="attr">  advertiseAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">  bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="attr">- groups:</span></span><br><span class="line"><span class="attr">  - system:</span>bootstrappers:kubeadm:default-node-token</span><br><span class="line"><span class="attr">  token:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"><span class="attr">  ttl:</span> <span class="number">24</span>h0m0s</span><br><span class="line"><span class="attr">  usages:</span></span><br><span class="line"><span class="bullet">  -</span> signing</span><br><span class="line"><span class="bullet">  -</span> authentication</span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">  criSocket:</span> /var/run/containerd/containerd.sock</span><br><span class="line"><span class="attr">  name:</span> master</span><br><span class="line"><span class="attr">  taints:</span></span><br><span class="line"><span class="attr">  - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">    key:</span> node-role.kubernetes.io/master</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/apis/kubeadm/v1alpha3/types.go</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">auditPolicy:</span></span><br><span class="line"><span class="attr">  logDir:</span> /var/log/kubernetes/audit</span><br><span class="line"><span class="attr">  logMaxAge:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">certificatesDir:</span> /etc/kubernetes/pki</span><br><span class="line"><span class="attr">clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">controllerManagerExtraArgs:</span></span><br><span class="line"><span class="attr">  address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">schedulerExtraArgs:</span></span><br><span class="line"><span class="attr">  address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line"><span class="attr">    dataDir:</span> /var/lib/etcd</span><br><span class="line"><span class="attr">    image:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">imageRepository:</span> k8s.gcr.io</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.12</span><span class="number">.0</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line"><span class="attr">  dnsDomain:</span> cluster.local</span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span></span><br><span class="line"><span class="attr">unifiedControlPlaneImage:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3</span><br><span class="line"><span class="attr">kind:</span> JoinConfiguration</span><br><span class="line"><span class="attr">apiEndpoint:</span></span><br><span class="line"><span class="attr">  advertiseAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">  bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">caCertPath:</span> /etc/kubernetes/pki/ca.crt</span><br><span class="line"><span class="attr">clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">discoveryFile:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">discoveryTimeout:</span> <span class="number">5</span>m0s</span><br><span class="line"><span class="attr">discoveryToken:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"><span class="attr">discoveryTokenAPIServers:</span></span><br><span class="line"><span class="attr">- kube-apiserver:</span><span class="number">6443</span></span><br><span class="line"><span class="attr">discoveryTokenUnsafeSkipCAVerification:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">  criSocket:</span> /var/run/containerd/containerd.sock</span><br><span class="line"><span class="attr">  name:</span> master</span><br><span class="line"><span class="attr">tlsBootstrapToken:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"><span class="attr">token:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">bindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">clientConnection:</span></span><br><span class="line"><span class="attr">  acceptContentTypes:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  burst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  contentType:</span> application/vnd.kubernetes.protobuf</span><br><span class="line"><span class="attr">  kubeconfig:</span> /var/lib/kube-proxy/kubeconfig.conf</span><br><span class="line"><span class="attr">  qps:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">clusterCIDR:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">configSyncPeriod:</span> <span class="number">15</span>m0s</span><br><span class="line"><span class="attr">conntrack:</span></span><br><span class="line"><span class="attr">  max:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  maxPerCore:</span> <span class="number">32768</span></span><br><span class="line"><span class="attr">  min:</span> <span class="number">131072</span></span><br><span class="line"><span class="attr">  tcpCloseWaitTimeout:</span> <span class="number">1</span>h0m0s</span><br><span class="line"><span class="attr">  tcpEstablishedTimeout:</span> <span class="number">24</span>h0m0s</span><br><span class="line"><span class="attr">enableProfiling:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">10256</span></span><br><span class="line"><span class="attr">hostnameOverride:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">iptables:</span></span><br><span class="line"><span class="attr">  masqueradeAll:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  masqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">ipvs:</span></span><br><span class="line"><span class="attr">  excludeCIDRs:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  scheduler:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">metricsBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">10249</span></span><br><span class="line"><span class="attr">mode:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">nodePortAddresses:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">portRange:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">resourceContainer:</span> /kube-proxy</span><br><span class="line"><span class="attr">udpIdleTimeout:</span> <span class="number">250</span>ms</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> kubelet.config.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> KubeletConfiguration</span><br><span class="line"><span class="attr">address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line"><span class="attr">  anonymous:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  webhook:</span></span><br><span class="line"><span class="attr">    cacheTTL:</span> <span class="number">2</span>m0s</span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  x509:</span></span><br><span class="line"><span class="attr">    clientCAFile:</span> /etc/kubernetes/pki/ca.crt</span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line"><span class="attr">  mode:</span> Webhook</span><br><span class="line"><span class="attr">  webhook:</span></span><br><span class="line"><span class="attr">    cacheAuthorizedTTL:</span> <span class="number">5</span>m0s</span><br><span class="line"><span class="attr">    cacheUnauthorizedTTL:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">cgroupDriver:</span> cgroupfs</span><br><span class="line"><span class="attr">cgroupsPerQOS:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">clusterDNS:</span></span><br><span class="line"><span class="bullet">-</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line"><span class="attr">clusterDomain:</span> cluster.local</span><br><span class="line"><span class="attr">configMapAndSecretChangeDetectionStrategy:</span> Watch</span><br><span class="line"><span class="attr">containerLogMaxFiles:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">containerLogMaxSize:</span> <span class="number">10</span>Mi</span><br><span class="line"><span class="attr">contentType:</span> application/vnd.kubernetes.protobuf</span><br><span class="line"><span class="attr">cpuCFSQuota:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">cpuCFSQuotaPeriod:</span> <span class="number">100</span>ms</span><br><span class="line"><span class="attr">cpuManagerPolicy:</span> none</span><br><span class="line"><span class="attr">cpuManagerReconcilePeriod:</span> <span class="number">10</span>s</span><br><span class="line"><span class="attr">enableControllerAttachDetach:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">enableDebuggingHandlers:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line"><span class="bullet">-</span> pods</span><br><span class="line"><span class="attr">eventBurst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">eventRecordQPS:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  imagefs.available: <span class="number">15</span>%</span><br><span class="line">  memory.available: <span class="number">100</span>Mi</span><br><span class="line">  nodefs.available: <span class="number">10</span>%</span><br><span class="line">  nodefs.inodesFree: <span class="number">5</span>%</span><br><span class="line"><span class="attr">evictionPressureTransitionPeriod:</span> <span class="number">5</span>m0s</span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">fileCheckFrequency:</span> <span class="number">20</span>s</span><br><span class="line"><span class="attr">hairpinMode:</span> promiscuous-bridge</span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">healthzPort:</span> <span class="number">10248</span></span><br><span class="line"><span class="attr">httpCheckFrequency:</span> <span class="number">20</span>s</span><br><span class="line"><span class="attr">imageGCHighThresholdPercent:</span> <span class="number">85</span></span><br><span class="line"><span class="attr">imageGCLowThresholdPercent:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">imageMinimumGCAge:</span> <span class="number">2</span>m0s</span><br><span class="line"><span class="attr">iptablesDropBit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">iptablesMasqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">kubeAPIBurst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">kubeAPIQPS:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">makeIPTablesUtilChains:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">maxOpenFiles:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">110</span></span><br><span class="line"><span class="attr">nodeLeaseDurationSeconds:</span> <span class="number">40</span></span><br><span class="line"><span class="attr">nodeStatusUpdateFrequency:</span> <span class="number">10</span>s</span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">podPidsLimit:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">port:</span> <span class="number">10250</span></span><br><span class="line"><span class="attr">registryBurst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">registryPullQPS:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">resolvConf:</span> /etc/resolv.conf</span><br><span class="line"><span class="attr">rotateCertificates:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">runtimeRequestTimeout:</span> <span class="number">2</span>m0s</span><br><span class="line"><span class="attr">serializeImagePulls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">staticPodPath:</span> /etc/kubernetes/manifests</span><br><span class="line"><span class="attr">streamingConnectionIdleTimeout:</span> <span class="number">4</span>h0m0s</span><br><span class="line"><span class="attr">syncFrequency:</span> <span class="number">1</span>m0s</span><br><span class="line"><span class="attr">volumeStatsAggPeriod:</span> <span class="number">1</span>m0s</span><br></pre></td></tr></table></figure>
<h2 id="traefik如何设置路由"><a href="#traefik如何设置路由" class="headerlink" title="traefik如何设置路由"></a>traefik如何设置路由</h2><p><a href="http://yunke.science/2018/03/28/Ingress-traefik/" target="_blank" rel="external">http://yunke.science/2018/03/28/Ingress-traefik/</a></p>
<h2 id="etcd-3-2-x-gt-3-3-x"><a href="#etcd-3-2-x-gt-3-3-x" class="headerlink" title="etcd 3.2.x -&gt; 3.3.x"></a>etcd 3.2.x -&gt; 3.3.x</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先进行备份</span></span><br><span class="line">ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert=/etc/etcd/ssl/etcd-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem snapshot save snapshot.db</span><br><span class="line"></span><br><span class="line"><span class="comment">#停止etcd 状态</span></span><br><span class="line">systemctl stop etcd</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级</span></span><br><span class="line">wget http://mirror.centos.org/centos/7/extras/x86_64/Packages/etcd-3.3.11-2.el7.centos.x86_64.rpm</span><br><span class="line">yum localinstall -y etcd-3.2.22-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动</span></span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://www.jianshu.com/p/aa528c57f3be" target="_blank" rel="external">https://www.jianshu.com/p/aa528c57f3be</a></p>
<h2 id="Coredns-相关操作"><a href="#Coredns-相关操作" class="headerlink" title="Coredns 相关操作"></a>Coredns 相关操作</h2><h3 id="如何使用外部dns解析"><a href="#如何使用外部dns解析" class="headerlink" title="如何使用外部dns解析"></a>如何使用外部dns解析</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> coredns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">      addonmanager.kubernetes.io/mode: EnsureExists</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  Corefile:</span> <span class="string">|</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">            pods insecure</span><br><span class="line">            # 默认</span><br><span class="line">            upstream</span><br><span class="line">            # 用于解析外部主机主机（外部服务）</span><br><span class="line">            # upstream 114.114.114.114 223.5.5.5</span><br><span class="line">            fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        # 默认使用主机的</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        # 任何不在集群域内的查询将转发到预定义的解析器，默认：/etc/resolv.conf；</span><br><span class="line">        # 在coredns “Deployment”资源中“dnsPolicy“设置为”Default”，即提供dns服务的pod从所在节点继承/etc/resolv.conf，如果节点的上游解析地址与”upstream”一致，则设置任意一个参数即可</span><br><span class="line">        #proxy . 114.114.114.114 223.5.5.5</span><br><span class="line">        cache 30</span><br><span class="line">        loop</span><br><span class="line">        reload</span><br><span class="line">        loadbalance</span><br><span class="line">&#125;</span><br><span class="line">#自定义dns记录，对应kube-dns中的stubdomains；</span><br><span class="line">#每条记录，单独设置1各zone</span><br><span class="line">    patsnap.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        proxy . 192.168.3.108</span><br><span class="line">    &#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="如何给kubernetes-service做cname解析"><a href="#如何给kubernetes-service做cname解析" class="headerlink" title="如何给kubernetes service做cname解析"></a>如何给kubernetes service做cname解析</h3><h4 id="coredns-配置文件"><a href="#coredns-配置文件" class="headerlink" title="coredns 配置文件"></a>coredns 配置文件</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">  "kind":</span> <span class="string">"ConfigMap"</span>,</span><br><span class="line"><span class="attr">  "apiVersion":</span> <span class="string">"v1"</span>,</span><br><span class="line"><span class="attr">  "metadata":</span> &#123;</span><br><span class="line"><span class="attr">    "name":</span> <span class="string">"coredns"</span>,</span><br><span class="line"><span class="attr">    "namespace":</span> <span class="string">"kube-system"</span>,</span><br><span class="line"><span class="attr">    "selfLink":</span> <span class="string">"/api/v1/namespaces/kube-system/configmaps/coredns"</span>,</span><br><span class="line"><span class="attr">    "uid":</span> <span class="string">"aa45aaab-4c79-11e9-9629-00163e022859"</span>,</span><br><span class="line"><span class="attr">    "resourceVersion":</span> <span class="string">"118616"</span>,</span><br><span class="line"><span class="attr">    "creationTimestamp":</span> <span class="string">"2019-03-22T08:08:24Z"</span></span><br><span class="line">  &#125;,</span><br><span class="line"><span class="attr">  "data":</span> &#123;</span><br><span class="line">    //格式化内容如下</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#格式化后的效果：</span></span><br><span class="line"><span class="attr">Corefile:</span></span><br><span class="line">.:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    health</span><br><span class="line">    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">       pods insecure</span><br><span class="line">       upstream</span><br><span class="line">       fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9153</span></span><br><span class="line">    proxy . /etc/resolv.conf</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    autopath @kubernetes</span><br><span class="line">    reload</span><br><span class="line">    file /etc/coredns/patsnap.io.zone  nexus.patsnap.io &#123;</span><br><span class="line">        upstream</span><br><span class="line">    &#125;</span><br><span class="line">    file /etc/coredns/patsnap.local.zone  patsnap.local &#123;</span><br><span class="line">        upstream</span><br><span class="line">    &#125;</span><br><span class="line">    file /etc/coredns/patsnap.com.zone  nexus.patsnap.com &#123;</span><br><span class="line">        upstream</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">patsnap.com.zone:</span><br><span class="line">;@ 当前域名 nexus.patsnap.com</span><br><span class="line">;<span class="number">900</span> 表示ttl</span><br><span class="line">;ns<span class="bullet">-1304.</span>awsdns<span class="bullet">-35.</span>org. 表示根</span><br><span class="line">;icyboy.jiunile.com. 表示邮箱 icyboy@jiunile.com</span><br><span class="line">@                   <span class="number">900</span>     IN      SOA         ns<span class="bullet">-1304.</span>awsdns<span class="bullet">-35.</span>org. icyboy.jiunile.com. (</span><br><span class="line">    <span class="number">2017033001</span> ; serial</span><br><span class="line">    <span class="number">7200</span>       ; refresh (<span class="number">2</span> hour)</span><br><span class="line">    <span class="number">900</span>        ; retry (<span class="number">15</span> min)</span><br><span class="line">    <span class="number">1209600</span>    ; expire</span><br><span class="line">    <span class="number">86400</span>      ; min TTL (<span class="number">1</span> day)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">;@ 等价于nexus.patsnap.com CNAME 后面的域名必须写全</span><br><span class="line">@                            IN        CNAME       s-ops-maven-nexus.ops-qa.svc.cluster.local.</span><br><span class="line"></span><br><span class="line">patsnap.io.zone:</span><br><span class="line">@                   <span class="number">900</span>      IN      SOA         ns<span class="bullet">-196.</span>awsdns<span class="bullet">-24.</span>com. icyboy.jiunile.com. (</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">    <span class="number">7200</span></span><br><span class="line">    <span class="number">900</span></span><br><span class="line">    <span class="number">1209600</span></span><br><span class="line">    <span class="number">86400</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">;npm 等价于npm.nexus.patsnap.io.</span><br><span class="line">npm                         IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br><span class="line">pypi                        IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br><span class="line"></span><br><span class="line">patsnap.local.zone:</span><br><span class="line">;@ 当前域名 patsnap.local</span><br><span class="line">;<span class="number">900</span> 表示ttl</span><br><span class="line">;<span class="number">192.168</span><span class="number">.3</span><span class="number">.108</span>. 表示根</span><br><span class="line">;icyboy.jiunile.com. 表示邮箱 icyboy@jiunile.com</span><br><span class="line">@                   <span class="number">900</span>     IN      SOA         <span class="number">192.168</span><span class="number">.3</span><span class="number">.108</span>. icyboy.jiunile.com. (</span><br><span class="line">    <span class="number">2019092202</span></span><br><span class="line">    <span class="number">21600</span></span><br><span class="line">    <span class="number">3600</span></span><br><span class="line">    <span class="number">604800</span></span><br><span class="line">    <span class="number">86400</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">；npm.nexus 等价于 npm.nexus.patsnap.local.</span><br><span class="line">npm.nexus                   IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br><span class="line">pypi.nexus                  IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br></pre></td></tr></table></figure>
<h4 id="coredns-deployment"><a href="#coredns-deployment" class="headerlink" title="coredns deployment"></a>coredns deployment</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"volumes":</span> [</span><br><span class="line">          &#123;</span><br><span class="line"><span class="attr">            "name":</span> <span class="string">"config-volume"</span>,</span><br><span class="line"><span class="attr">            "configMap":</span> &#123;</span><br><span class="line"><span class="attr">              "name":</span> <span class="string">"coredns"</span>,</span><br><span class="line"><span class="attr">              "items":</span> [</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"Corefile"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"Corefile"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"patsnap.io.zone"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"patsnap.io.zone"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"patsnap.local.zone"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"patsnap.local.zone"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"patsnap.com.zone"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"patsnap.com.zone"</span></span><br><span class="line">                &#125;</span><br><span class="line">              ],</span><br><span class="line"><span class="attr">              "defaultMode":</span> <span class="number">420</span></span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>
<p>参考链接：<br><a href="https://www.cnblogs.com/netonline/p/9935228.html" target="_blank" rel="external">https://www.cnblogs.com/netonline/p/9935228.html</a><br><a href="https://yuerblog.cc/2018/12/29/k8s-dns/#post-4008-_Toc533670192" target="_blank" rel="external">https://yuerblog.cc/2018/12/29/k8s-dns/#post-4008-_Toc533670192</a><br><a href="https://github.com/coredns/coredns.io/blob/master/content/blog/custom-dns-and-kubernetes.md" target="_blank" rel="external">https://github.com/coredns/coredns.io/blob/master/content/blog/custom-dns-and-kubernetes.md</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 服务质量 Qos 解析]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-pod-qos.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><code>QoS</code>是 Quality of Service 的缩写，即服务质量。为了实现资源被有效调度和分配的同时提高资源利用率，<code>kubernetes</code>针对不同服务质量的预期，通过 QoS（Quality of Service）来对 pod 进行服务质量管理。对于一个 pod 来说，服务质量体现在两个具体的指标：<code>CPU 和内存</code>。当节点上内存资源紧张时，kubernetes 会根据预先设置的不同 QoS 类别进行相应处理。</p>
<p>QoS 主要分为<code>Guaranteed</code>、<code>Burstable</code> 和 <code>Best-Effort</code>三类，优先级从高到低。</p>
<h2 id="Guaranteed-有保证的"><a href="#Guaranteed-有保证的" class="headerlink" title="Guaranteed(有保证的)"></a>Guaranteed(有保证的)</h2><p>对于绑定 CPU 和具有相对可预测性的工作负载（例如，用来处理请求的 Web 服务）来说，这是一个很好的 QoS 等级。属于该级别的pod有以下两种：</p>
<ul>
<li><strong>Pod中的所有容器都且仅设置了 CPU 和内存的 limits</strong></li>
<li><strong>pod中的所有容器都设置了 CPU 和内存的 requests 和 limits ，且单个容器内的requests==limits（requests不等于0）</strong><a id="more"></a>
<strong>示例1：pod中的所有容器都且仅设置了limits</strong><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>示例2： pod 中的所有容器都设置了 requests 和 limits，且单个容器内的<code>requests==limits</code></strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br></pre></td></tr></table></figure></p>
<p>容器foo和bar内resources的requests和limits均相等，该pod的QoS级别属于<code>Guaranteed</code>。</p>
<h2 id="Burstable-不稳定的"><a href="#Burstable-不稳定的" class="headerlink" title="Burstable(不稳定的)"></a>Burstable(不稳定的)</h2><p>这对短时间内需要消耗大量资源或者初始化过程很密集的工作负载非常有用，例如：用来构建 Docker 容器的 Worker 和运行未优化的 JVM 进程的容器都可以使用该 QoS 等级。<strong>pod中只要有一个容器的requests和limits的设置不相同</strong>，该pod的QoS即为<code>Burstable</code>。</p>
<p><strong>示例1：容器foo指定了resource，而容器bar未指定</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br></pre></td></tr></table></figure></p>
<p><strong>示例2：容器foo设置了内存limits，而容器bar设置了CPU limits</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong><code>注意：若容器指定了requests而未指定limits，则limits的值等于节点resource的最大值；若容器指定了limits而未指定requests，则requests的值等于limits。</code></strong></p>
</blockquote>
<h2 id="Best-Effort-尽最大努力"><a href="#Best-Effort-尽最大努力" class="headerlink" title="Best-Effort(尽最大努力)"></a>Best-Effort(尽最大努力)</h2><p>这对于可中断和低优先级的工作负载非常有用，例如：迭代运行的幂等优化过程。<strong>如果Pod中所有容器的resources均未设置requests与limits</strong>，该pod的QoS即为<code>Best-Effort</code>。</p>
<p><strong>示例1：容器foo和容器bar均未设置requests和limits</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br></pre></td></tr></table></figure></p>
<h2 id="根据QoS进行资源回收策略"><a href="#根据QoS进行资源回收策略" class="headerlink" title="根据QoS进行资源回收策略"></a>根据QoS进行资源回收策略</h2><p>Kubernetes 通过<code>cgroup</code>给pod设置QoS级别，当资源不足时先<code>kill</code>优先级低的 pod，在实际使用过程中，通过<code>OOM</code>分数值来实现，<code>OOM</code>分数值范围为0-1000。OOM 分数值根据<code>OOM_ADJ</code>参数计算得出。</p>
<p>对于<code>Guaranteed</code>级别的 Pod，OOM_ADJ参数设置成了-998，对于<code>Best-Effort</code>级别的 Pod，OOM_ADJ参数设置成了1000，对于<code>Burstable</code>级别的 Pod，OOM_ADJ参数取值从2到999。</p>
<p>对于 kuberntes 保留资源，比如kubelet，docker，OOM_ADJ参数设置成了-999，表示不会被OOM kill掉。<strong>OOM_ADJ参数设置的越大，计算出来的OOM分数越高，表明该pod优先级就越低，当出现资源竞争时会越早被kill掉</strong>，对于OOM_ADJ参数是-999的表示kubernetes永远不会因为OOM将其kill掉。</p>
<h2 id="QoS-pods被kill掉场景与顺序"><a href="#QoS-pods被kill掉场景与顺序" class="headerlink" title="QoS pods被kill掉场景与顺序"></a>QoS pods被kill掉场景与顺序</h2><ul>
<li><code>Best-Effort pods</code>：系统用完了全部内存时，该类型 pods 会最先被kill掉。</li>
<li><code>Burstable pods</code>：系统用完了全部内存，且没有 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。</li>
<li><code>Guaranteed pods</code>：系统用完了全部内存，且没有 Burstable 与 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。</li>
</ul>
<h2 id="QoS使用建议"><a href="#QoS使用建议" class="headerlink" title="QoS使用建议"></a>QoS使用建议</h2><p>如果资源充足，可将 QoS pods 类型均设置为<code>Guaranteed</code>。用计算资源换业务性能和稳定性，减少排查问题时间和成本。如果想更好的提高资源利用率，<strong>业务服务</strong>可以设置为<code>Guaranteed</code>，而其他服务根据重要程度可分别设置为<code>Burstable</code>或<code>Best-Effort</code>。</p>
<p>在搞清楚服务什么时候会出现故障以及为什么会出现故障之前，都不应该将其部署到生产环境中。可通过一些技术手段（<strong>负载压测等</strong>）来设置应用的资源 limits 和 requests。这将会为你的系统增加弹性能力和可预测性。</p>
<p>在测试过程中，记录服务失败时做了哪些操作是至关重要的。可以将发现的故障模式添加到相关的书籍和文档中，这对分类生产环境中出现的问题很有用。下面是我们在测试过程中发现的一些故障模式：</p>
<ul>
<li>内存缓慢增加</li>
<li>CPU 使用率达到 100%</li>
<li>响应时间太长</li>
<li>请求被丢弃</li>
<li>不同请求的响应时间差异很大</li>
</ul>
<p>你最好将这些发现都收集起来，以备不时之需，因为有一天它们可能会为你或团队节省一整天的时间。</p>
<h2 id="一些有用的工具"><a href="#一些有用的工具" class="headerlink" title="一些有用的工具"></a>一些有用的工具</h2><h3 id="Loader-io"><a href="#Loader-io" class="headerlink" title="Loader.io"></a>Loader.io</h3><p><a href="http://loader.io/" target="_blank" rel="external">Loader.io</a> 是一个在线负载测试工具，它允许你配置负载增加测试和负载不变测试，在测试过程中可视化应用程序的性能和负载，并能快速启动和停止测试。它也会保存测试结果的历史记录，因此在资源限制发生变化时很容易对结果进行比较。<br><img src="/images/k8s/loader.jpg" alt="Loader"></p>
<h3 id="Kubescope-cli"><a href="#Kubescope-cli" class="headerlink" title="Kubescope cli"></a>Kubescope cli</h3><p><a href="https://github.com/hharnisc/kubescope-cli" target="_blank" rel="external">Kubescope cli</a> 是一个可以运行在本地或 Kubernetes 中的工具，可直接从 Docker Daemon 中收集容器指标并可视化。和 <code>cAdvisor</code> 等其他集群指标收集服务一样， <code>kubescope cli</code> 收集指标的周期是 1 秒（而不是 10-15 秒）。如果周期是 10-15 秒，你可能会在测试期间错过一些引发性能瓶颈的问题。如果你使用 cAdvisor 进行测试，每次都要使用新的 Pod 作为测试对象，因为 Kubernetes 在超过资源限制时就会将 Pod 杀死，然后重新启动一个全新的 Pod。而 <code>kubescope cli</code> 就没有这方面的忧虑，它直接从 Docker Daemon 中收集容器指标（你可以自定义收集指标的时间间隔），并使用正则表达式来选择和过滤你想要显示的容器。<br><img src="/images/k8s/kubescope-cli.gif" alt="kubescope-cli"></p>
<p>来源：</p>
<ul>
<li>阳明的博客</li>
<li>Ryan Yang</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[理解 kubernetes 亲和性调度]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-affinity.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一般情况下我们部署的 Pod 是通过集群的自动调度策略来选择节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 Pod 的调度，比如我们内部的一些服务 gitlab 之类的也是跑在<code>Kubernetes</code>集群上的，我们就不希望对外的一些服务和内部的服务跑在同一个节点上了，害怕内部服务对外部的服务产生影响；但是有的时候我们的服务之间交流比较频繁，又希望能够将这两个服务的 Pod 调度到同一个的节点上。这就需要用到 Kubernetes 里面的一个概念：亲和性和反亲和性。</p>
<p>亲和性有分成节点亲和性(<code>nodeAffinity</code>)和 Pod 亲和性(<code>podAffinity</code>)。</p>
<h2 id="nodeSelector"><a href="#nodeSelector" class="headerlink" title="nodeSelector"></a>nodeSelector</h2><p>在了解亲和性之前，我们先来了解一个非常常用的调度方式：nodeSelector。我们知道label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的一个就是 service 通过匹配 label 去匹配 Pod 资源，而 Pod 的调度也可以根据节点的 label 来进行调度。</p>
<p>我们可以通过下面的命令查看我们的 node 的 label：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    147d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    67d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    127d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>现在我们先给节点node02增加一个com=youdianzhishi的标签，命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label nodes node02 com=youdianzhishi</span><br><span class="line">node <span class="string">"node02"</span> labeled</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>我们可以通过上面的<code>--show-labels</code>参数可以查看上述标签是否生效。当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 Pod 的<code>spec</code>字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label 即可。比如，下面的 Pod 我们要强制调度到 node02 这个节点上去，我们就可以使用 nodeSelector 来表示了：(node-selector-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> busybox-pod</span><br><span class="line"><span class="attr">  name:</span> test-busybox</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> sleep</span><br><span class="line"><span class="bullet">    -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    imagePullPolicy:</span> Always</span><br><span class="line"><span class="attr">    name:</span> test-busybox</span><br><span class="line"><span class="attr">  nodeSelector:</span></span><br><span class="line"><span class="attr">    com:</span> youdianzhishi</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以通过 describe 命令查看调度结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> node-selector-demo.yaml</span><br><span class="line">pod <span class="string">"test-busybox"</span> created</span><br><span class="line">$ kubectl describe pod <span class="built_in">test</span>-busybox</span><br><span class="line">Name:         <span class="built_in">test</span>-busybox</span><br><span class="line">Namespace:    default</span><br><span class="line">Node:         node02/10.151.30.63</span><br><span class="line">......</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  com=youdianzhishi</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason                 Age   From               Message</span><br><span class="line">  ----    ------                 ----  ----               -------</span><br><span class="line">  Normal  SuccessfulMountVolume  55s   kubelet, node02    MountVolume.SetUp succeeded <span class="keyword">for</span> volume <span class="string">"default-token-n9w2d"</span></span><br><span class="line">  Normal  Scheduled              54s   default-scheduler  Successfully assigned <span class="built_in">test</span>-busybox to node02</span><br><span class="line">  Normal  Pulling                54s   kubelet, node02    pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Pulled                 40s   kubelet, node02    Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Created                40s   kubelet, node02    Created container</span><br><span class="line">  Normal  Started                40s   kubelet, node02    Started container</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 Events 下面的信息，我们的 Pod 通过默认的 default-scheduler 调度器被绑定到了node02节点。不过需要注意的是<code>nodeSelector</code>属于强制性的，如果我们的目标节点没有可用的资源，我们的 Pod 就会一直处于 Pending 状态，这就是<code>nodeSelector</code>的用法。</p>
<p>通过上面的例子我们可以感受到<code>nodeSelector</code>的方式比较直观，但是还够灵活，控制粒度偏大，接下来我们再和大家了解下更加灵活的方式：节点亲和性(<code>nodeAffinity</code>)。</p>
<h2 id="亲和性和反亲和性调度"><a href="#亲和性和反亲和性调度" class="headerlink" title="亲和性和反亲和性调度"></a>亲和性和反亲和性调度</h2><p>我们了解了 kubernetes 调度器的一个调度流程，我们知道默认的调度器在使用的时候，经过了 predicates 和 priorities 两个阶段，但是在实际的生产环境中，往往我们需要根据自己的一些实际需求来控制 pod 的调度，这就需要用到 nodeAffinity(节点亲和性)、podAffinity(pod 亲和性) 以及 podAntiAffinity(pod 反亲和性)。</p>
<p>亲和性调度可以分成软策略和硬策略两种方式:</p>
<ul>
<li><code>软策略</code>就是如果你没有满足调度要求的节点的话，pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓了的策略</li>
<li><code>硬策略</code>就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不干的策略。</li>
</ul>
<p>对于亲和性和反亲和性都有这两种规则可以设置： <code>preferredDuringSchedulingIgnoredDuringExecution</code>和<code>requiredDuringSchedulingIgnoredDuringExecution</code>，前面的就是软策略，后面的就是硬策略。</p>
<blockquote>
<p>这命名不觉得有点反人类吗？有点无语……</p>
</blockquote>
<h2 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h2><p>节点亲和性主要是用来控制 pod 要部署在哪些主机上，以及不能部署在哪些主机上的。它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。</p>
<p>比如现在我们用一个 Deployment 来管理3个 pod 副本，现在我们来控制下这些 pod 的调度，如下例子：（node-affinity-demo.yaml）<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">            nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">            - matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> kubernetes.io/hostname</span><br><span class="line"><span class="attr">                operator:</span> NotIn</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> node03</span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 软策略</span></span><br><span class="line"><span class="attr">          - weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            preference:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> com</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> youdianzhishi</span><br></pre></td></tr></table></figure></p>
<p>上面这个 pod 首先是要求不能运行在 node03 这个节点上，如果有个节点满足<code>com=youdianzhishi</code>的话就优先调度到这个节点上。</p>
<p>下面是我们测试的节点列表信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>可以看到 node02 节点有<code>com=youdianzhishi</code>这样的 label，按要求会优先调度到这个节点来的，现在我们来创建这个 pod，然后使用descirbe命令查看具体的调度情况是否满足我们的要求。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> node-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods <span class="_">-l</span> app=affinity -o wide</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7b4c946854-5gfln   1/1       Running   0          47s       10.244.4.214   node02</span><br><span class="line">affinity-7b4c946854<span class="_">-l</span>8b47   1/1       Running   0          47s       10.244.4.215   node02</span><br><span class="line">affinity-7b4c946854-r86p5   1/1       Running   0          47s       10.244.4.213   node02</span><br></pre></td></tr></table></figure></p>
<p>从结果可以看出 pod 都被部署到了 node02，其他节点上没有部署 pod，这里的匹配逻辑是 label 的值在某个列表中，现在<code>Kubernetes</code>提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<blockquote>
<p>如果<code>nodeSelectorTerms</code>下面有多个选项的话，满足任何一个条件就可以了；如果<code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 POD。</p>
</blockquote>
<h2 id="podAffinity"><a href="#podAffinity" class="headerlink" title="podAffinity"></a>podAffinity</h2><p>pod 亲和性主要解决 pod 可以和哪些 pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 pod 反亲和性主要是解决 pod 不能和哪些 pod 部署在同一个拓扑域中的问题，它们都是处理的 pod 与 pod 之间的关系，比如一个 pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 pod 在节点上了，那么我就不想和你待在同一个节点上。</p>
<p>由于我们这里只有一个集群，并没有区域或者机房的概念，所以我们这里直接使用主机名来作为拓扑域，把 pod 创建在同一个主机上面。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>同样，还是针对上面的资源对象，我们来测试下 pod 的亲和性：（pod-affinity-demo.yaml）<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">          - labelSelector:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> app</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> busybox-pod</span><br><span class="line"><span class="attr">            topologyKey:</span> kubernetes.io/hostname</span><br></pre></td></tr></table></figure></p>
<p>上面这个例子中的 pod 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 pod：这个 pod 有一个<code>app=busybox-pod</code>的 label。</p>
<p>我们查看有标签<code>app=busybox-pod</code>的 pod 列表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=busybox-pod</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line"><span class="built_in">test</span>-busybox   1/1       Running   164        7d        10.244.4.205   node02</span><br></pre></td></tr></table></figure></p>
<p>我们看到这个 pod 运行在了 node02 的节点上面，所以按照上面的亲和性来说，上面我们部署的3个 pod 副本也应该运行在 node02 节点上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=affinity</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-564f9d7db9-lzzvq   1/1       Running   0          3m        10.244.4.216   node02</span><br><span class="line">affinity-564f9d7db9-p79cq   1/1       Running   0          3m        10.244.4.217   node02</span><br><span class="line">affinity-564f9d7db9-spfzs   1/1       Running   0          3m        10.244.4.218   node02</span><br></pre></td></tr></table></figure></p>
<p>如果我们把上面的 test-busybox 和 affinity 这个 Deployment 都删除，然后重新创建 affinity 这个资源，看看能不能正常调度呢：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete <span class="_">-f</span> node-selector-demo.yaml</span><br><span class="line">pod <span class="string">"test-busybox"</span> deleted</span><br><span class="line">$ kubectl delete <span class="_">-f</span> pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> deleted</span><br><span class="line">$ kubectl create <span class="_">-f</span> pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=affinity</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP        NODE</span><br><span class="line">affinity-564f9d7db9-fbc8w   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-n8gcf   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-qc7x6   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到处于<code>Pending</code>状态了，这是因为现在没有一个节点上面拥有<code>busybox-pod</code>这个 label 的 pod，而上面我们的调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 test-busybox 这个 pod 调度到 node03 这个节点上，看看上面的 affinity 的3个副本会不会也被调度到 node03 这个节点上去？</p>
<p>我们这个地方使用的是<code>kubernetes.io/hostname</code>这个拓扑域，意思就是我们当前调度的 pod 要和目标的 pod 处于同一个主机上面，因为要处于同一个拓扑域下面，为了说明这个问题，我们把拓扑域改成<code>beta.kubernetes.io/os</code>，同样的我们当前调度的 pod 要和目标的 pod 处于同一个拓扑域中，目标的 pod 是不是拥有<code>beta.kubernetes.io/os=linux</code>的标签，而我们这里3个节点都有这样的标签，这也就意味着我们3个节点都在同一个拓扑域中，所以我们这里的 pod 可能会被调度到任何一个节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7d86749984-glkhz                 1/1       Running     0          3m        10.244.2.16    node03</span><br><span class="line">affinity-7d86749984-h4fb9                 1/1       Running     0          3m        10.244.4.219   node02</span><br><span class="line">affinity-7d86749984-tj7k2                 1/1       Running     0          3m        10.244.2.14    node03</span><br></pre></td></tr></table></figure></p>
<h2 id="podAntiAffinity"><a href="#podAntiAffinity" class="headerlink" title="podAntiAffinity"></a>podAntiAffinity</h2><p>这就是 pod 亲和性的用法，而 pod 反亲和性则是反着来的，比如一个节点上运行了某个 pod，那么我们的 pod 则希望被调度到其他节点上去，同样我们把上面的 podAffinity 直接改成 podAntiAffinity，(pod-antiaffinity-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">          - labelSelector:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> app</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> busybox-pod</span><br><span class="line"><span class="attr">            topologyKey:</span> kubernetes.io/hostname</span><br></pre></td></tr></table></figure></p>
<p>这里的意思就是如果一个节点上面有一个<code>app=busybox-pod</code>这样的 pod 的话，那么我们的 pod 就别调度到这个节点上面来，上面我们把<code>app=busybox-pod</code>这个 pod 固定到了 node03 这个节点上面来，所以正常来说我们这里的 pod 不会出现在 node03 节点上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> pod-antiaffinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-bcbd8854f-br8z8                  1/1       Running     0          5s        10.244.4.222   node02</span><br><span class="line">affinity-bcbd8854f-cdffh                  1/1       Running     0          5s        10.244.4.223   node02</span><br><span class="line">affinity-bcbd8854f-htb52                  1/1       Running     0          5s        10.244.4.224   node02</span><br><span class="line"><span class="built_in">test</span>-busybox                              1/1       Running     0          23m       10.244.2.10    node03</span><br></pre></td></tr></table></figure></p>
<p>这就是 pod 反亲和性的用法。</p>
<h2 id="污点（taints）与容忍（tolerations）"><a href="#污点（taints）与容忍（tolerations）" class="headerlink" title="污点（taints）与容忍（tolerations）"></a>污点（taints）与容忍（tolerations）</h2><p>对于<code>nodeAffinity</code>无论是硬策略还是软策略方式，都是调度 pod 到预期节点上，而<code>Taints</code>恰好与之相反，如果一个节点标记为 Taints ，除非 pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 pod。</p>
<p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 pod，则污点就很有用了，pod 不会再被调度到 taint 标记过的节点。我们使用<code>kubeadm</code>搭建的集群默认就给 master 节点添加了一个污点标记，所以我们看到我们平时的 pod 都没有被调度到 master 上去：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node master</span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/hostname=master</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">......</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      <span class="literal">false</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以使用上面的命令查看 master 节点的信息，其中有一条关于 Taints 的信息：node-role.kubernetes.io/master:NoSchedule，就表示给 master 节点打了一个污点的标记，其中影响的参数是<code>NoSchedule</code>，表示 pod 不会被调度到标记为 taints 的节点，除了 NoSchedule 外，还有另外两个选项：</p>
<ul>
<li><code>PreferNoSchedule</code>：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去</li>
<li><code>NoExecute</code>：该选项意味着一旦 Taint 生效，如该节点内正在运行的 pod 没有对应 Tolerate 设置，会直接被逐出</li>
</ul>
<p>污点 taint 标记节点的命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes node02 <span class="built_in">test</span>=node02:NoSchedule</span><br><span class="line">node <span class="string">"node02"</span> tainted</span><br></pre></td></tr></table></figure></p>
<p>上面的命名将 node02 节点标记为了污点，影响策略是 NoSchedule，只会影响新的 pod 调度，如果仍然希望某个 pod 调度到 taint 节点上，则必须在 Spec 中做出<code>Toleration</code>定义，才能调度到该节点，比如现在我们想要将一个 pod 调度到 master 节点：(taint-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> taint</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> taint</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> taint</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> http</span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node-role.kubernetes.io/master"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>由于 master 节点被标记为了污点节点，所以我们这里要想 pod 能够调度到 master 节点去，就需要增加容忍的声明：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"node-role.kubernetes.io/master"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>然后创建上面的资源，查看结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> taint-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"taint"</span> created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS             RESTARTS   AGE       IP             NODE</span><br><span class="line">......</span><br><span class="line">taint-845d8bb4fb-57mhm                    1/1       Running            0          1m        10.244.4.247   node02</span><br><span class="line">taint-845d8bb4fb-bbvmp                    1/1       Running            0          1m        10.244.0.33    master</span><br><span class="line">taint-845d8bb4fb-zb78x                    1/1       Running            0          1m        10.244.4.246   node02</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到有一个 pod 副本被调度到了 master 节点，这就是容忍的使用方法。</p>
<p>对于 tolerations 属性的写法，其中的 key、value、effect 与 Node 的 Taint 设置需保持一致， 还有以下几点说明：</p>
<ul>
<li>1.如果 operator 的值是 Exists，则 value 属性可省略</li>
<li>2.如果 operator 的值是 Equal，则表示其 key 与 value 之间的关系是 equal(等于)</li>
<li>3.如果不指定 operator 属性，则默认值为 Equal</li>
</ul>
<p>另外，还有两个特殊值：</p>
<ul>
<li>1.空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也是是能容忍所有 node 的所有 Taints</li>
<li>2.空的 effect 匹配所有的 effect</li>
</ul>
<p>最后，如果我们要取消节点的污点标记，可以使用下面的命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes node02 <span class="built_in">test</span>-</span><br><span class="line">node <span class="string">"node02"</span> untainted</span><br></pre></td></tr></table></figure></p>
<p>这就是污点和容忍的使用方法。                                                                                             </p>
<p>来源：www.qikqiak.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在阿里云使用Kubeadm 1.13.x 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2019/02/k8s-kubeadm-ha-1-13-x.html</url>
      <content type="html"><![CDATA[<h2 id="前言（坑）"><a href="#前言（坑）" class="headerlink" title="前言（坑）"></a>前言（坑）</h2><p><strong>负载均衡问题</strong></p>
<ul>
<li>阿里不支持LVS，没有vip可用，必须通过申请SLB来固定VIP</li>
<li>因Kubernetes apiserver为https协议，阿里SLB中能负载均衡HTTPS的只有TCP方式，但TCP协议不能转发到发起主机（<code>apiserver 需要有回环路访问，简单说就是自己给自己发请求</code>）</li>
</ul>
<p>为了解决kubernets apiserver高可用问题，故用以下方式来解决：</p>
<ul>
<li>申请一个内网的SLB（获取VIP），8443为监听端口，6443为apiserver的后端端口</li>
<li>在每台master机器上搭建keepalived+haproxy，VIP 用SLB的VIP</li>
</ul>
<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">申请阿里云的SLB获取到</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.13.3-0 kubelet-1.13.3-0 kubectl-1.13.3-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy"><a href="#搭建keepalived-haproxy" class="headerlink" title="搭建keepalived + haproxy"></a>搭建keepalived + haproxy</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.183']"</span> \ <span class="comment">#master-1地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.184']"</span> \ <span class="comment">#master-2地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.185']"</span> \ <span class="comment">#master-3地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="配置阿里云-SLB"><a href="#配置阿里云-SLB" class="headerlink" title="配置阿里云 SLB"></a>配置阿里云 SLB</h3><blockquote>
<p>在阿里云SLB 管理界面添加8443监听端口，使用<code>TCP</code>协议，后端服务器选择一台你即将初始化的master机器，后端服务器端口为6443，健康检查默认配置即可，保存配置。此时你的SLB是不进行工作的，因为后端服务器的6443端口还未监听。</p>
</blockquote>
<h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.13</span><span class="number">.3</span></span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.100</span>:<span class="number">8443</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line"><span class="attr">  certSANs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.183</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.184</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.185</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.100</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">16</span></span><br><span class="line"><span class="attr">imageRepository:</span> registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">mode:</span> iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:<span class="number">3.1</span>  k8s.gcr.io/pause:<span class="number">3.1</span></span><br><span class="line">kubeadm init --config $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -f /etc/kubernetes/admin.conf $&#123;HOME&#125;/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="初始化其它master节点"><a href="#初始化其它master节点" class="headerlink" title="初始化其它master节点"></a>初始化其它master节点</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化master-2</span></span><br><span class="line">ssh master-2 <span class="string">"kubeadm reset -f </span><br><span class="line">rm -rf /etc/kubernetes/pki/</span><br><span class="line">mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化master-3</span></span><br><span class="line">ssh master-3 <span class="string">"kubeadm reset -f </span><br><span class="line">rm -rf /etc/kubernetes/pki/ </span><br><span class="line">mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件到master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件到master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取加入口令</span></span><br><span class="line">JOIN_CMD=`kubeadm token create --print-join-command`</span><br><span class="line"></span><br><span class="line"><span class="comment">#将master-2 加入</span></span><br><span class="line">ssh master-2 <span class="string">"<span class="variable">$&#123;JOIN_CMD&#125;</span> --experimental-control-plane"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将master-3 加入</span></span><br><span class="line">ssh master-3 <span class="string">"<span class="variable">$&#123;JOIN_CMD&#125;</span> --experimental-control-plane"</span></span><br></pre></td></tr></table></figure>
<h3 id="再次配置阿里云-SLB"><a href="#再次配置阿里云-SLB" class="headerlink" title="再次配置阿里云 SLB"></a>再次配置阿里云 SLB</h3><blockquote>
<p>同样进入阿里云SLB管理界面，将其余的两台master机器加入到后端服务器中。这样你的apiserver 就高可用了</p>
</blockquote>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.3.2</span><br><span class="line">docker pull quay.io/calico/node:v3.3.2</span><br><span class="line">docker pull quay.io/calico/cni:v3.3.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.13.3</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.13.3</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.13.3</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.13.3</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.13.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 调度器介绍]]></title>
      <url>http://team.jiunile.com/blog/2019/02/k8s-scheduler.html</url>
      <content type="html"><![CDATA[<h3 id="kube-scheduler-简介"><a href="#kube-scheduler-简介" class="headerlink" title="kube-scheduler 简介"></a>kube-scheduler 简介</h3><blockquote>
<p><code>kube-scheduler</code>是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。</p>
</blockquote>
<h3 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h3><p>默认情况下，kube-scheduler 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。</p>
<p><code>kube-scheduler</code> 是 kubernetes 的调度器，它的主要作用就是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上去，是一个独立的二进制程序，启动之后会一直监听 API Server，获取到 PodSpec.NodeName 为空的 Pod，对每个 Pod 都会创建一个 binding。<br><img src="/images/k8s/kube-scheduler-structrue.jpg" alt="K8s scheduler structure"><br><a id="more"></a><br>这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：</p>
<ul>
<li>如何保证全部的节点调度的公平性？要知道并不是说有节点资源配置都是一样的</li>
<li>如何保证每个节点都能被分配资源？</li>
<li>集群资源如何能够被高效利用？</li>
<li>集群资源如何才能被最大化使用？</li>
<li>如何保证 Pod 调度的性能和效率？</li>
<li>用户是否可以根据自己的实际需求定制自己的调度策略？</li>
</ul>
<p>考虑到实际环境中的各种复杂情况，kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</p>
<p>kubernetes 调度器的源码位于 kubernetes/pkg/scheduler 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/scheduler</span><br><span class="line">-- scheduler.go         <span class="comment">#调度相关的具体实现</span></span><br><span class="line">|-- algorithm</span><br><span class="line">|   |-- predicates      <span class="comment">#节点筛选策略</span></span><br><span class="line">|   |-- priorities      <span class="comment">#节点打分策略</span></span><br><span class="line">|-- algorithmprovider</span><br><span class="line">|   |-- defaults        <span class="comment">#定义默认的调度器</span></span><br></pre></td></tr></table></figure></p>
<p>其中 Scheduler 创建和运行的核心程序，对应的代码在 pkg/scheduler/scheduler.go，如果要查看<code>kube-scheduler</code>的入口程序，对应的代码在 cmd/kube-scheduler/scheduler.go。</p>
<p>调度主要分为以下几个部分：</p>
<ul>
<li>首先是预选过程，过滤掉不满足条件的节点，这个过程称为<code>Predicates</code></li>
<li>然后是优选过程，对通过的节点按照优先级排序，称之为<code>Priorities</code></li>
<li>最后从中选择优先级最高的节点，如果中间任何一步骤有错误，就直接返回错误</li>
</ul>
<p><code>Predicates</code>阶段首先遍历全部节点，过滤掉不满足条件的节点，属于强制性规则，这一阶段输出的所有满足要求的 Node 将被记录并作为第二阶段的输入，如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试。</p>
<blockquote>
<p>所以我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。</p>
</blockquote>
<p><code>Priorities</code>阶段即再次对节点进行筛选，如果有多个节点都满足条件的话，那么系统会按照节点的优先级(priorites)大小对节点进行排序，最后选择优先级最高的节点来部署 Pod 应用。</p>
<p>下面是调度过程的简单示意图：<br><img src="/images/k8s/kube-scheduler-filter.jpg" alt="K8s scheduler filter"></p>
<p>更详细的流程是这样的：</p>
<ul>
<li>首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源</li>
<li>API Server 收到用户请求后，存储相关数据到 etcd 数据库中</li>
<li>调度器监听 API Server 查看为调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点，这个分配过程就是我们上面提到的两个阶段：<ul>
<li>预选阶段(Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉</li>
<li>优选阶段(Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本分布到不同的主机上，使用最低负载的主机等等策略</li>
</ul>
</li>
<li>经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 binding 操作，然后将结果存储到 etcd 中</li>
<li>最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作</li>
</ul>
<p>其中<code>Predicates</code>过滤有一系列的算法可以使用，我们这里简单列举几个：</p>
<ul>
<li>PodFitsResources：节点上剩余的资源是否大于 Pod 请求的资源</li>
<li>PodFitsHost：如果 Pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配</li>
<li>PodFitsHostPorts：节点上已经使用的 port 是否和 Pod 申请的 port 冲突</li>
<li>PodSelectorMatches：过滤掉和 Pod 指定的 label 不匹配的节点</li>
<li>NoDiskConflict：已经 mount 的 volume 和 Pod 指定的 volume 不冲突，除非它们都是只读的</li>
<li>CheckNodeDiskPressure：检查节点磁盘空间是否符合要求</li>
<li>CheckNodeMemoryPressure：检查节点内存是否够用</li>
</ul>
<p>除了这些过滤算法之外，还有一些其他的算法，更多更详细的我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/predicates.go。</p>
<p>而<code>Priorities</code>优先级是由一系列键值对组成的，键是该优先级的名称，值是它的权重值，同样，我们这里给大家列举几个具有代表性的选项：</p>
<ul>
<li>LeastRequestedPriority：通过计算 CPU 和内存的使用率来决定权重，使用率越低权重越高，当然正常肯定也是资源是使用率越低权重越高，能给别的 Pod 运行的可能性就越大</li>
<li>SelectorSpreadPriority：为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高</li>
<li>ImageLocalityPriority：就是如果在某个节点上已经有要使用的镜像节点了，镜像总大小值越大，权重就越高</li>
<li>NodeAffinityPriority：这个就是根据节点的亲和性来计算一个权重值，后面我们会详细讲解亲和性的使用方法</li>
</ul>
<p>除了这些策略之外，还有很多其他的策略，同样我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/ 了解更多信息。每一个优先级函数会返回一个0-10的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn)</span><br></pre></td></tr></table></figure></p>
<h3 id="自定义调度"><a href="#自定义调度" class="headerlink" title="自定义调度"></a>自定义调度</h3><p>上面就是 kube-scheduler 默认调度的基本流程，除了使用默认的调度器之外，我们也可以自定义调度策略。</p>
<h4 id="调度器扩展"><a href="#调度器扩展" class="headerlink" title="调度器扩展"></a>调度器扩展</h4><p><code>kube-scheduler</code>在启动的时候可以通过 <code>--policy-config-file</code>参数来指定调度策略文件，我们可以根据我们自己的需要来组装<code>Predicates</code>和<code>Priority</code>函数。选择不同的过滤函数和优先级函数、控制优先级函数的权重、调整过滤函数的顺序都会影响调度过程。</p>
<p>下面是官方的 Policy 文件示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"kind"</span> : <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"apiVersion"</span> : <span class="string">"v1"</span>,</span><br><span class="line">    <span class="string">"predicates"</span> : [</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"PodFitsHostPorts"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"PodFitsResources"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"NoDiskConflict"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"NoVolumeZoneConflict"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"MatchNodeSelector"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"HostName"</span>&#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"priorities"</span> : [</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"LeastRequestedPriority"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"BalancedResourceAllocation"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"ServiceSpreadingPriority"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"EqualPriority"</span>, <span class="string">"weight"</span> : 1&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="多调度器"><a href="#多调度器" class="headerlink" title="多调度器"></a>多调度器</h4><p>如果默认的调度器不满足要求，还可以部署自定义的调度器。并且，在整个集群中还可以同时运行多个调度器实例，通过<code>podSpec.schedulerName</code> 来选择使用哪一个调度器（默认使用内置的调度器）。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  schedulerName:</span> my-scheduler  <span class="comment"># 选择使用自定义调度器 my-scheduler</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:<span class="number">1.10</span></span><br></pre></td></tr></table></figure></p>
<p>要开发我们自己的调度器也是比较容易的，比如我们这里的 my-scheduler:</p>
<ul>
<li>首先需要通过指定的 API 获取节点和 Pod</li>
<li>然后选择<code>phase=Pending</code>和<code>schedulerName=my-scheduler</code>的pod</li>
<li>计算每个 Pod 需要放置的位置之后，调度程序将创建一个<code>Binding</code>对象</li>
<li>然后根据我们自定义的调度器的算法计算出最适合的目标节点</li>
</ul>
<h4 id="优先级调度"><a href="#优先级调度" class="headerlink" title="优先级调度"></a>优先级调度</h4><p>与前面所讲的调度优选策略中的优先级（Priorities）不同，前面所讲的优先级指的是节点优先级，而我们这里所说的优先级 pod priority 指的是 Pod 的优先级，高优先级的 Pod 会优先被调度，或者在资源不足低情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。</p>
<p>要定义 Pod 优先级，就需要先定义<code>PriorityClass</code>对象，该对象没有 Namespace 的限制：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> PriorityClass</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> high-priority</span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"This priority class should be used for XYZ service pods only."</span></span><br></pre></td></tr></table></figure></p>
<p>其中：</p>
<ul>
<li><code>value</code>为 32 位整数的优先级，该值越大，优先级越高</li>
<li><code>globalDefault</code>用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个<code>PriorityClass</code>将其设置为 true</li>
</ul>
<p>然后通过在 Pod 的<code>spec.priorityClassName</code>中指定已定义的<code>PriorityClass</code>名称即可：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx</span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">  priorityClassName:</span> high-priority</span><br></pre></td></tr></table></figure></p>
<p>另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 pending 时，抢占（preemption）逻辑就会被触发。<code>Preemption</code>会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。</p>
<p>现在我们通过下面的图再去回顾下 kubernetes 的调度过程是不是就清晰很多了：<br><img src="/images/k8s/kube-scheduler-detail.png" alt="K8s scheduler detail"></p>
<p>来源: k8s技术圈</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.11.x + etcd集群 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-etcd-ha-1-11-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.11.5-0 kubelet-1.11.5-0 kubectl-1.11.5-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CADVISOR_ARGS=--cadvisor-port=0"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CADVISOR_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建etcd"><a href="#搭建etcd" class="headerlink" title="搭建etcd"></a>搭建etcd</h2><h3 id="在master-1上执行"><a href="#在master-1上执行" class="headerlink" title="在master-1上执行"></a>在master-1上执行</h3><h4 id="安装cfssl"><a href="#安装cfssl" class="headerlink" title="安装cfssl"></a>安装cfssl</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">wget -O /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">wget -O /bin/cfssl-certinfo  https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line"><span class="keyword">for</span> cfssl <span class="keyword">in</span> `ls /bin/cfssl*`;<span class="keyword">do</span> chmod +x <span class="variable">$cfssl</span>;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<h4 id="配置生成etcd-证书"><a href="#配置生成etcd-证书" class="headerlink" title="配置生成etcd 证书"></a>配置生成etcd 证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置证书</span></span><br><span class="line">mkdir -pv <span class="variable">$HOME</span>/ssl &amp;&amp; <span class="built_in">cd</span> <span class="variable">$HOME</span>/ssl</span><br><span class="line"></span><br><span class="line">cat &gt; ca-config.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"signing"</span>: &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">      <span class="string">"expiry"</span>: <span class="string">"87600h"</span> //10年</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"profiles"</span>: &#123;</span><br><span class="line">      <span class="string">"kubernetes"</span>: &#123;</span><br><span class="line">        <span class="string">"usages"</span>: [</span><br><span class="line">            <span class="string">"signing"</span>,</span><br><span class="line">            <span class="string">"key encipherment"</span>,</span><br><span class="line">            <span class="string">"server auth"</span>,</span><br><span class="line">            <span class="string">"client auth"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; etcd-ca-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"JiangSu"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"SuZhou"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"Etcd Security"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; etcd-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.19.170.183"</span>,</span><br><span class="line">      <span class="string">"172.19.170.184"</span>,</span><br><span class="line">      <span class="string">"172.19.170.185"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"JiangSu"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"SuZhou"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"Etcd Security"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成证书并复制证书至其他etcd节点</span></span><br><span class="line">cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca</span><br><span class="line">cfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动证书</span></span><br><span class="line">mkdir -pv /etc/etcd/ssl</span><br><span class="line">cp etcd*.pem /etc/etcd/ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将证书拷贝到其它节点</span></span><br><span class="line">scp -r /etc/etcd master-2:/etc/</span><br><span class="line">scp -r /etc/etcd master-3:/etc/</span><br></pre></td></tr></table></figure>
<h4 id="安装配置etcd"><a href="#安装配置etcd" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">yum install -y etcd </span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.183:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.183:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd1"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.183:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.183:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span> <span class="comment">#这里要设置为new</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line"><span class="comment">#启动会卡住，等待其它节点的加入</span></span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="在master-2-上执行"><a href="#在master-2-上执行" class="headerlink" title="在master-2 上执行"></a>在master-2 上执行</h3><h4 id="安装配置etcd-1"><a href="#安装配置etcd-1" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">yum install -y etcd </span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.184:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.184:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd2"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.184:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.184:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"existing"</span> <span class="comment">#如果etcd 启动报错，可先把这行注释掉，然后在启动</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="在master-3-上执行"><a href="#在master-3-上执行" class="headerlink" title="在master-3 上执行"></a>在master-3 上执行</h3><h4 id="安装配置etcd-2"><a href="#安装配置etcd-2" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.185:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd3"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.185:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"existing"</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="etcd-集群验证"><a href="#etcd-集群验证" class="headerlink" title="etcd 集群验证"></a>etcd 集群验证</h3><blockquote>
<p>随意在任何一台etcd (master) 节点上进行验证</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">etcdctl --endpoints <span class="string">"https://172.19.170.183:2379,https://172.19.170.184:2379,https://172.19.170.185:2379"</span>   --ca-file=/etc/etcd/ssl/etcd-ca.pem  \</span><br><span class="line">--cert-file=/etc/etcd/ssl/etcd.pem   --key-file=/etc/etcd/ssl/etcd-key.pem   cluster-health</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">token=$(kubeadm token generate)</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$token</span></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.2.4</span><br><span class="line">docker pull quay.io/calico/node:v3.2.4</span><br><span class="line">docker pull quay.io/calico/cni:v3.2.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.11.5</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[chrome 访问k8s dashboard 出现ssl证书错误]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-dashboard-chrome-err.html</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>最近上了k8s后，给开发提供dashboard访问，发现有不少开发无法使用chrome访问，出现问题的大部分都是使用的windows系统，主要有以下两类问题，这里记录下如何解决</p>
</blockquote>
<h3 id="ERR-SSL-SERVER-CERT-BAD-FORMAT"><a href="#ERR-SSL-SERVER-CERT-BAD-FORMAT" class="headerlink" title="ERR_SSL_SERVER_CERT_BAD_FORMAT"></a>ERR_SSL_SERVER_CERT_BAD_FORMAT</h3><blockquote>
<p>解决方法：重新安装chrome最新版本解决</p>
</blockquote>
<h3 id="NET-ERR-CERT-INVALID"><a href="#NET-ERR-CERT-INVALID" class="headerlink" title="NET::ERR_CERT_INVALID"></a>NET::ERR_CERT_INVALID</h3><p><img src="/images/k8s/chrome_err.png" alt="NET::ERR_CERT_INVALID"></p>
<blockquote>
<p>解决方法：创建chrome桌面快捷方式，然后到桌面：右键chrome–&gt;属性–&gt;在目标后面添加如下：<code>--disable-infobars --ignore-certificate-errors</code></p>
<p>示例：<code>&quot;C:\Program Files (x86)\Google\Chrome\Application\chrome.exe&quot; --disable-infobars --ignore-certificate-errors</code></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.12.x 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ha-1-12-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.12.3-0 kubelet-1.12.3-0 kubectl-1.12.3-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.183:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.183:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380</span><br><span class="line">      initial-cluster-state: new</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源 </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.184:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.184:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源 </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-2 https://172.19.170.184:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-2执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.185:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.185:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380,master-3=https://172.19.170.185:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-3 https://172.19.170.185:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-3执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="调整配置，使用vip来进行调度"><a href="#调整配置，使用vip来进行调度" class="headerlink" title="调整配置，使用vip来进行调度"></a>调整配置，使用vip来进行调度</h3><blockquote>
<p>在所有master机器上都执行以下命令</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在master-1 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-2 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-3 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.3.2</span><br><span class="line">docker pull quay.io/calico/node:v3.3.2</span><br><span class="line">docker pull quay.io/calico/cni:v3.3.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.12.3</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.12.3</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.12.3</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.12.3</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.12.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.11.x 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ha-1-11-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.11.5-0 kubelet-1.11.5-0 kubectl-1.11.5-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CADVISOR_ARGS=--cadvisor-port=0"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;300Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CADVISOR_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.183:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.183:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380</span><br><span class="line">      initial-cluster-state: new</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.184:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.184:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-2 https://172.19.170.184:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-2执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.185:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.185:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380,master-3=https://172.19.170.185:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-3 https://172.19.170.185:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-3执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="调整配置，使用vip来进行调度"><a href="#调整配置，使用vip来进行调度" class="headerlink" title="调整配置，使用vip来进行调度"></a>调整配置，使用vip来进行调度</h3><blockquote>
<p>在所有master机器上都执行以下命令</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在master-1 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-2 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-3 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.2.4</span><br><span class="line">docker pull quay.io/calico/node:v3.2.4</span><br><span class="line">docker pull quay.io/calico/cni:v3.2.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.11.5</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[认识Kubernetes Descheduler]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-descheduler.html</url>
      <content type="html"><![CDATA[<p><code>kube-scheduler</code> 是 Kubernetes 中负责调度的组件，它本身的调度功能已经很强大了。但由于 Kubernetes 集群非常活跃，它的状态会随时间而改变，由于各种原因，你可能需要将已经运行的 Pod 移动到其他节点：</p>
<ul>
<li>某些节点负载过高</li>
<li>某些资源对象被添加了 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature" target="_blank" rel="external">node 亲和性</a> 或 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature" target="_blank" rel="external">pod （反）亲和性</a></li>
<li>集群中加入了新节点</li>
</ul>
<p>一旦 Pod 启动之后 <code>kube-scheduler</code> 便不会再尝试重新调度它。根据环境的不同，你可能会有很多需要手动调整 Pod 的分布，例如：如果集群中新加入了一个节点，那么已经运行的 Pod 并不会被分摊到这台节点上，这台节点可能只运行了少量的几个 Pod，这并不理想，对吧？</p>
<h3 id="Descheduler-如何工作？"><a href="#Descheduler-如何工作？" class="headerlink" title="Descheduler 如何工作？"></a>Descheduler 如何工作？</h3><p><a href="https://github.com/kubernetes-incubator/descheduler" target="_blank" rel="external">Descheduler</a> 会检查 Pod 的状态，并根据自定义的策略将不满足要求的 Pod 从该节点上驱逐出去。Descheduler 并不是 <code>kube-scheduler</code> 的替代品，而是要依赖于它。该项目目前放在 Kubernetes 的孵化项目中，还没准备投入生产，但经过我实验发现它的运行效果很好，而且非常稳定。那么该如何安装呢？<br><a id="more"></a></p>
<h3 id="部署方法"><a href="#部署方法" class="headerlink" title="部署方法"></a>部署方法</h3><p>你可以通过 <code>Job</code> 或 <code>CronJob</code> 来运行 descheduler。我已经创建了一个镜像 <code>komljen/descheduler:v0.5.0-4-ga7ceb671</code>（包含在下面的 yaml 文件中），但由于这个项目的更新速度很快，你可以通过以下的命令创建你自己的镜像：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">⚡ git <span class="built_in">clone</span> https://github.com/kubernetes-incubator/descheduler</span><br><span class="line">⚡ <span class="built_in">cd</span> descheduler &amp;&amp; make image</span><br></pre></td></tr></table></figure></p>
<p>然后打好标签 push 到自己的镜像仓库中。</p>
<p>通过我创建的 chart 模板，你可以用 <code>Helm</code> 来部署 descheduler，该模板支持 RBAC 并且已经在 Kubernetes v1.9 上测试通过。</p>
<p>添加我的 helm 私有仓库，然后部署 descheduler：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">⚡ helm repo add akomljen-charts \</span><br><span class="line">    https://raw.githubusercontent.com/komljen/helm-charts/master/charts/</span><br><span class="line"></span><br><span class="line">⚡ helm install --name ds \</span><br><span class="line">    --namespace kube-system \</span><br><span class="line">    akomljen-charts/descheduler</span><br></pre></td></tr></table></figure></p>
<p>你也可以不使用 helm，通过手动部署。首先创建 serviceaccount 和 clusterrolebinding：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a cluster role</span></span><br><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"nodes"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>]</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>, <span class="string">"delete"</span>]</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods/eviction"</span>]</span><br><span class="line">  verbs: [<span class="string">"create"</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a service account</span></span><br><span class="line">⚡ kubectl create sa descheduler -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the cluster role to the service account</span></span><br><span class="line">⚡ kubectl create clusterrolebinding descheduler \</span><br><span class="line">    -n kube-system \</span><br><span class="line">    --clusterrole=descheduler \</span><br><span class="line">    --serviceaccount=kube-system:descheduler</span><br></pre></td></tr></table></figure></p>
<p>然后通过 <code>configmap</code> 创建 descheduler 策略。目前只支持四种策略：</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removeduplicates" target="_blank" rel="external">RemoveDuplicates</a><br>RS、deployment 中的 pod 不能同时出现在一台机器上</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#lownodeutilization" target="_blank" rel="external">LowNodeUtilization</a><br>找到资源使用率比较低的 node，然后驱逐其他资源使用率比较高节点上的 pod，期望调度器能够重新调度让资源更均衡</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removepodsviolatinginterpodantiaffinity" target="_blank" rel="external">RemovePodsViolatingInterPodAntiAffinity</a><br>找到已经违反 Pod Anti Affinity 规则的 pods 进行驱逐，可能是因为反亲和是后面加上去的</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removepodsviolatingnodeaffinity" target="_blank" rel="external">RemovePodsViolatingNodeAffinity</a><br>找到违反 Node Affinity 规则的 pods 进行驱逐，可能是因为 node 后面修改了 label</li>
</ul>
<p>默认这四种策略全部开启，你可以根据需要关闭它们。下面在 <code>kube-system</code> 命名空间中创建一个 configmap：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">data:</span><br><span class="line">  policy.yaml: |-  </span><br><span class="line">    apiVersion: descheduler/v1alpha1</span><br><span class="line">    kind: DeschedulerPolicy</span><br><span class="line">    strategies:</span><br><span class="line">      RemoveDuplicates:</span><br><span class="line">         enabled: <span class="literal">false</span></span><br><span class="line">      LowNodeUtilization:</span><br><span class="line">         enabled: <span class="literal">true</span></span><br><span class="line">         params:</span><br><span class="line">           nodeResourceUtilizationThresholds:</span><br><span class="line">             thresholds:</span><br><span class="line">               cpu: 20</span><br><span class="line">               memory: 20</span><br><span class="line">               pods: 20</span><br><span class="line">             targetThresholds:</span><br><span class="line">               cpu: 50</span><br><span class="line">               memory: 50</span><br><span class="line">               pods: 50</span><br><span class="line">      RemovePodsViolatingInterPodAntiAffinity:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">      RemovePodsViolatingNodeAffinity:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">        params:</span><br><span class="line">          nodeAffinityType:</span><br><span class="line">          - requiredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>在 <code>kube-system</code> 命名空间中创建一个 CronJob，该 CroJob 每 30 分钟运行一次：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">spec:</span><br><span class="line">  schedule: <span class="string">"*/30 * * * *"</span></span><br><span class="line">  jobTemplate:</span><br><span class="line">    metadata:</span><br><span class="line">      name: descheduler</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">"true"</span></span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          serviceAccountName: descheduler</span><br><span class="line">          containers:</span><br><span class="line">          - name: descheduler</span><br><span class="line">            image: komljen/descheduler:v0.6.0</span><br><span class="line">            volumeMounts:</span><br><span class="line">            - mountPath: /policy-dir</span><br><span class="line">              name: policy-volume</span><br><span class="line">            <span class="built_in">command</span>:</span><br><span class="line">            - /bin/descheduler</span><br><span class="line">            - --v=4</span><br><span class="line">            - --max-pods-to-evict-per-node=10</span><br><span class="line">            - --policy-config-file=/policy-dir/policy.yaml</span><br><span class="line">          restartPolicy: <span class="string">"OnFailure"</span></span><br><span class="line">          volumes:</span><br><span class="line">          - name: policy-volume</span><br><span class="line">            configMap:</span><br><span class="line">              name: descheduler</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">⚡ kubectl get cronjobs -n kube-system</span><br><span class="line">NAME             SCHEDULE       SUSPEND   ACTIVE    LAST SCHEDULE   AGE</span><br><span class="line">descheduler      */30 * * * *   False     0         2m              32m</span><br></pre></td></tr></table></figure></p>
<p>当 CronJob 开始工作后，可以通过以下命令查看已经成功结束的 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">⚡ kubectl get pods -n kube-system <span class="_">-a</span> | grep Completed</span><br><span class="line">descheduler-1525520700-297pq          0/1       Completed   0          1h</span><br><span class="line">descheduler-1525521000-tz2ch          0/1       Completed   0          32m</span><br><span class="line">descheduler-1525521300-mrw4t          0/1       Completed   0          2m</span><br></pre></td></tr></table></figure></p>
<p>也可以查看这些 Pod 的日志，然后根据需要调整 descheduler 策略：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">⚡ kubectl logs descheduler-1525521300-mrw4t -n kube-system</span><br><span class="line">I0505 11:55:07.554195       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84</span><br><span class="line">I0505 11:55:07.554255       1 reflector.go:240] Listing and watching *v1.Node from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84</span><br><span class="line">I0505 11:55:07.767903       1 lownodeutilization.go:147] Node <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span> is appropriately utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:41.5, <span class="string">"memory"</span>:1.3635487207675927, <span class="string">"pods"</span>:8.181818181818182&#125;</span><br><span class="line">I0505 11:55:07.767942       1 lownodeutilization.go:149] allPods:9, nonRemovablePods:9, bePods:0, bPods:0, gPods:0</span><br><span class="line">I0505 11:55:07.768141       1 lownodeutilization.go:144] Node <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span> is over utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:48.75, <span class="string">"memory"</span>:61.05259502942694, <span class="string">"pods"</span>:30&#125;</span><br><span class="line">I0505 11:55:07.768156       1 lownodeutilization.go:149] allPods:33, nonRemovablePods:12, bePods:1, bPods:19, gPods:1</span><br><span class="line">I0505 11:55:07.768376       1 lownodeutilization.go:144] Node <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span> is over utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:39.125, <span class="string">"memory"</span>:98.19259268881142, <span class="string">"pods"</span>:33.63636363636363&#125;</span><br><span class="line">I0505 11:55:07.768390       1 lownodeutilization.go:149] allPods:37, nonRemovablePods:8, bePods:0, bPods:29, gPods:0</span><br><span class="line">I0505 11:55:07.768538       1 lownodeutilization.go:147] Node <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span> is appropriately utilized with usage: api.ResourceThresholds&#123;<span class="string">"memory"</span>:43.19826999287199, <span class="string">"pods"</span>:30.90909090909091, <span class="string">"cpu"</span>:35.25&#125;</span><br><span class="line">I0505 11:55:07.768552       1 lownodeutilization.go:149] allPods:34, nonRemovablePods:11, bePods:8, bPods:15, gPods:0</span><br><span class="line">I0505 11:55:07.768556       1 lownodeutilization.go:65] Criteria <span class="keyword">for</span> a node under utilization: CPU: 20, Mem: 20, Pods: 20</span><br><span class="line">I0505 11:55:07.768571       1 lownodeutilization.go:69] No node is underutilized, nothing to <span class="keyword">do</span> here, you might tune your thersholds further</span><br><span class="line">I0505 11:55:07.768576       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.779313       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.796766       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.813303       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.829109       1 node_affinity.go:40] Executing <span class="keyword">for</span> nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">I0505 11:55:07.829133       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.840416       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.856735       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.945566       1 request.go:480] Throttling request took 88.738917ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-41-14.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded</span><br><span class="line">I0505 11:55:07.972702       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:08.145559       1 request.go:480] Throttling request took 172.751657ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-34-29.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded</span><br><span class="line">I0505 11:55:08.160964       1 node_affinity.go:72] Evicted 0 pods</span><br></pre></td></tr></table></figure></p>
<p>哇哦，现在你的集群中已经运行了一个 descheduler！</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Kubernetes 的默认调度器已经做的很好，但由于集群处于不断变化的状态中，某些 Pod 可能运行在错误的节点上，或者你想要均衡集群资源的分配，这时候就需要 descheduler 来帮助你将某些节点上的 Pod 驱逐到正确的节点上去。我很期待正式版的发布！</p>
<p>参考文档:</p>
<ul>
<li>Meet a Kubernetes Descheduler</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubernetes正确的处理用户请求]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-handling-client-requests-properly-with-kubernetes.html</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>毫无疑问，我们希望正确处理客户端请求。当pod正在启动或关闭时，我们显然不希望看到断开的连接。Kubernetes本身并不能确保这种情况永远不会发生。您的应用需要遵循一些规则以防止断开连接。本文讨论这些规则。</p>
</blockquote>
<h3 id="确保正确处理所有客户端请求"><a href="#确保正确处理所有客户端请求" class="headerlink" title="确保正确处理所有客户端请求"></a>确保正确处理所有客户端请求</h3><p>我们先从访问Pod的客户端的视角来看看Pod的生命周期（客户端使用pod提供的服务）。我们希望确保客户的请求得到妥善处理，因为如果当pod启动或关闭时连接开始中断，我们就会遇到麻烦。Kubernetes本身并不保证不会发生这种情况，所以让我们看看我们需要做些什么来防止它发生。</p>
<h3 id="当Pod启动时阻止连接中断"><a href="#当Pod启动时阻止连接中断" class="headerlink" title="当Pod启动时阻止连接中断"></a>当Pod启动时阻止连接中断</h3><p>如果你理解services和service endpoints的工作原理，确保在pod启动时正确处理每个连接都非常简单。当pod启动时，它会作为一个endpoints被添加到所有匹配该Pod标签的Services里。Pod也会发出信号告诉Kubernetes它已就绪。只有它变成一个service endpoints时才可以接收来自客户端的请求。</p>
<p>如果你没有在Pod Spec里指定readiness探针，则会始终认为该pod已准备就绪。这意味着它将立即开始接收请求 - 只要第一个Kube-Proxy在其节点上更新iptables规则并且第一个客户端pod尝试连接到该服务。如果那个时候你的应用并没有做好接收请求的准备，那么客户端将会见到“connection refused”类型的错误。</p>
<p>你所需要做的就是保证你的readiness探针当且仅当你的应用可以正确处理收到的请求时才返回成功结果。所以添加一个HTTP GET readiness探针并让它指向你应用的基础URL会是一个很好的开始。在很多情况下，这可以让你省去实现一个特定的readiness端点的工作量。<br><a id="more"></a></p>
<h3 id="在pod关闭期间防止断开连接"><a href="#在pod关闭期间防止断开连接" class="headerlink" title="在pod关闭期间防止断开连接"></a>在pod关闭期间防止断开连接</h3><p>现在让我们看看当一个Pod生命周期结束时发生了什么——当Pod被删除和它里面的容器被停止时。一旦Pod的容器接收到SIGTERM后它就会开始关闭（或者是在那之前先执行prestop钩子），但这是否能保证所有的客户端请求可以被正确地处理？</p>
<p>我们的应用在收到结束信号时应该如何响应？它是否应该继续接收请求？那么那些已经收到的请求但是还未完成的请求呢？那么那些正在发送请求的间隔中且仍然处理打开状态的持久HTTP连接（连接上没有活跃的请求）呢？在回答这些问题之前，我们需要深入了解一下当Pod结束时集群里发生的一系列事件。</p>
<h3 id="了解Pod删除时发生的一系列事件"><a href="#了解Pod删除时发生的一系列事件" class="headerlink" title="了解Pod删除时发生的一系列事件"></a>了解Pod删除时发生的一系列事件</h3><p>您需要始终牢记Kubernetes的各个组件是独立运行在集群的节点上的。它们之间并不是一个巨大的单体应用。这些组件间同步状态会花费一点时间。让我们一起来看看当Pod删除时发生了什么。</p>
<p>当APIserver收到一个停止Pod的请求时，它首先修改了etcd里Pod的状态，并通知关注这个删除事件所有的watcher。这些watcher里包括Kubelet和Endpoint Controller。这两个序列的事件是并行发生的（标记为A和B），如图1所示。<br><img src="/images/k8s/k8s_pod_03.png" alt="pod停止时的事件"></p>
<p>在A系列事件里，你会看到在Kubelet收到该Pod要停止的通知以后会尽快开始停止Pod的一系列操作（执行prestop钩子，发送SIGTERM信号，等待一段时间然后如果这个容器没有自动退出的话就强行杀掉这个容器）。如果应用响应了SIGTERM并迅速停止接收请求，那么任何尝试连接它的客户端都会收到一个Connection Refusd的错误。因为APIserver是直接向Kubelet发送的请求，那么从Pod被删除开始计算，Pod用于执行这段逻辑的时间相当短。</p>
<p>现在，让我们一起看看另外一系列事件都发生了什么——移除这个Pod相关的iptables规则（图中所示事件系列B）。当Endpoints Controller（运行在在Kubernetes控制平面里的Controller Manager里）收到这个Pod被删除的通知，然后它会把这个Pod从所有关联到这个Pod的Service里剔除。它通过向APIserver发送一个REST请求对Endpoint对象进行修改来实现。APIserver然后通知每个监视Endpoint对象的组件。在这些组件里包含了每个工作节点上运行的Kubeproxy。这些代理负责更新它所在节点上的iptables规则，这些规则可以用来阻止外面的请求被转发到要停止的Pod上。这里有个非常重要的细节，移除这些iptables规则对于已有的连接不会有任何影响——连接到这个Pod的客户端仍然可以通过已有连接向它发送请求。</p>
<p>这些请求都是并行发生的。更确切地，关停应用所需要的时间要比iptables更新花费所需的时间稍短一些。这是因为iptables修改的事件链看起来稍微长一些（见图2），因为这些事件需要先到达Endpoints Controller，然后它向APIServer发送一个新请求，接着在Proxy最终修改iptables规则之前，APIserver必须通知到每个KubeProxy。这意味着SIGTERM可能会在所有节点iptables规则更新前发送。<br><img src="/images/k8s/k8s_pod_04.png" alt="删除pod时的事件时间线"></p>
<p>结果是pod在收到终止信号后仍然可以接收客户端请求。如果应用程序立即停止接受连接，则会导致客户端收到“Connection Refused”类型的错误（就像在没有定义Readiness探针时，Pod启动但无法对外提供服务时一样） 。</p>
<h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>谷歌搜索此问题的解决方案，似乎给Pod添加一个Readiness探针就可以解决这个问题。按照推测，你所需要做的就是让你的Readiness探针在应用收到SIGTERM信号后尽快失败。这可以让Pod从Service上移除。只有在Readiness探针连续失败几次后才会从Service上移除（这可以在Readiness探针里进行配置）。并且显然这个移除事件会在iptables规则更新前先到达Kubeproxy。</p>
<p>实际上，Readiness探针在整个过程中并未起到关键作用。一旦Endpoints Controller收到Pod删除事件后（当Pod配置里的deletionTimestamp域不再为空时），它会尽快从Service上移除这些Pod。从那时起，这已经就与Readiness探测结果不相关了。</p>
<p>那么什么是问题的正确解决方案？我们如何保证所有的请求都可以被正确处理？</p>
<p>好吧，很明显，即使收到终止信号后，pod也需要继续接受连接，直到所有Kube代理完成更新iptables规则。那么，不止是Kubeproxy。可能有一些IngressController或者其他直接向Pod转发请求的负载均衡设备等不需要经过Service的。这还包括使用客户端负载平衡的客户端。<br>为了确保没有任何客户端遇到断开的连接，您必须等到所有客户端以某种方式通知您他们将不再转发到该Pod的连接。</p>
<p>但这是不可能的，因为这些组件都分布在不同的计算机上。即使你知道它们每个所在的位置或者等着它们每个都满足停止Pod的需求，如果它们其中之一没有响应你会该怎么办？你要等待多久？记着，在那段时间里，你需要暂停关闭进程。</p>
<p>你唯一可能的做事情是你可以等待足够长的时间直到所有的代理都完成了它们的工作。但是多长时间才算够？在大多数情况中，短暂的几秒就足够了，但是显然这并不能满足全部的情况。当APIserver或者Endpoints Controller过载时，它们可能需要更长时间来通知到每个代理。重要的是要了解您无法完美地解决问题，但是即使是5秒或者10秒都可以显著提升用户体验。你可以设置更长的延迟，但是别太过分，因为这些延迟推迟了容器的停止时间，并且会导致容器在被关停后仍然被显示在列表里，这会对用户带来一定的困扰。</p>
<p>正确关闭应用程序包括以下步骤：</p>
<ul>
<li>等待几秒，然后停止接收新连接</li>
<li>关闭所有未在请求中的keep-alived连接</li>
<li>等到所有活跃的请求关闭，然后</li>
<li>彻底关闭</li>
</ul>
<p>要了解在此过程中连接和请求发生的情况，请仔细检查图3。<br><img src="/images/k8s/k8s_pod_05.png" alt="在收到终止信号后正确处理现有和新连接"></p>
<p>没有像收到终止信号后立即退出过程一样简单，对吧？这一切是值得的吗？这要靠你自己来决定。但是至少你可以添加一个prestop钩子并等待几秒，就像下面所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span>  </span><br><span class="line"><span class="attr">    preStop:</span>    </span><br><span class="line"><span class="attr">        exec:</span>      </span><br><span class="line"><span class="attr">            command:</span></span><br><span class="line"><span class="bullet">             -</span> sh</span><br><span class="line"><span class="bullet">             -</span> -c</span><br><span class="line"><span class="bullet">             -</span> <span class="string">"sleep 5"</span></span><br></pre></td></tr></table></figure></p>
<p>这样，您根本不需要修改应用程序的代码。如果您的应用要确保完全处理所有正在进行的请求，这个preStop钩子就可以满足所有你的需要。</p>
<p>参考文档:</p>
<ul>
<li>handling-client-requests-properly-with-kubernetes</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm 证书说明]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ca-desc.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>如果你使用过kubeadm部署过Kubernetes的环境, master主机节点上就一定会在相应的目录创建了一大批证书文件, 本篇文章就来说说kubeadm到底为我们生成了哪些证书</p>
</blockquote>
<p>在Kubernetes的部署中, 创建证书, 配置证书是一道绕不过去坎儿, 好在有kubeadm这样的自动化工具, 帮我们去生成, 配置这些证书. 对于只是想体验Kubernetes或只是想测试的亲来说, 这已经够了, 但是作为Kubernetes的集群维护者来说, kubeadm更像是一个黑盒, 本篇文章就来说说黑盒中关于证书的事儿~</p>
<p>使用kubeadm创建完Kubernetes集群后, 默认会在<code>/etc/kubernetes/pki</code>目录下存放集群中需要用到的证书文件, 整体结构如下图所示:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:/etc/kubernetes/pki<span class="comment"># tree</span></span><br><span class="line">.</span><br><span class="line">|-- apiserver.crt</span><br><span class="line">|-- apiserver-etcd-client.crt</span><br><span class="line">|-- apiserver-etcd-client.key</span><br><span class="line">|-- apiserver.key</span><br><span class="line">|-- apiserver-kubelet-client.crt</span><br><span class="line">|-- apiserver-kubelet-client.key</span><br><span class="line">|-- ca.crt</span><br><span class="line">|-- ca.key</span><br><span class="line">|-- etcd</span><br><span class="line">|   |-- ca.crt</span><br><span class="line">|   |-- ca.key</span><br><span class="line">|   |-- healthcheck-client.crt</span><br><span class="line">|   |-- healthcheck-client.key</span><br><span class="line">|   |-- peer.crt</span><br><span class="line">|   |-- peer.key</span><br><span class="line">|   |-- server.crt</span><br><span class="line">|   `-- server.key</span><br><span class="line">|-- front-proxy-ca.crt</span><br><span class="line">|-- front-proxy-ca.key</span><br><span class="line">|-- front-proxy-client.crt</span><br><span class="line">|-- front-proxy-client.key</span><br><span class="line">|-- sa.key</span><br><span class="line">`-- sa.pub</span><br><span class="line"></span><br><span class="line">1 directory, 22 files</span><br></pre></td></tr></table></figure></p>
<p>以上22个文件就是kubeadm为我们创建的所有证书相关的文件, 下面我们来一一解析</p>
<h3 id="证书分组"><a href="#证书分组" class="headerlink" title="证书分组"></a>证书分组</h3><p>Kubernetes把证书放在了两个文件夹中</p>
<ul>
<li>/etc/kubernetes/pki</li>
<li>/etc/kubernetes/pki/etcd</li>
</ul>
<p>我们再将这22个文件按照更细的粒度去分组<br><a id="more"></a></p>
<h3 id="Kubernetes-集群根证书"><a href="#Kubernetes-集群根证书" class="headerlink" title="Kubernetes 集群根证书"></a>Kubernetes 集群根证书</h3><p>Kubernetes 集群根证书CA(Kubernetes集群组件的证书签发机构)</p>
<ul>
<li>/etc/kubernetes/pki/ca.crt</li>
<li>/etc/kubernetes/pki/ca.key</li>
</ul>
<p>以上这组证书为签发其他Kubernetes组件证书使用的根证书, 可以认为是Kubernetes集群中证书签发机构之一</p>
<p>由此根证书签发的证书有:</p>
<ol>
<li>kube-apiserver 组件持有的服务端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver.crt</li>
<li>/etc/kubernetes/pki/apiserver.key</li>
</ul>
<ol>
<li>kubelet 组件持有的客户端证书, 用作 kube-apiserver 主动向 kubelet 发起请求时的客户端认证</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver-kubelet-client.crt</li>
<li>/etc/kubernetes/pki/apiserver-kubelet-client.key</li>
</ul>
<blockquote>
<p>注意: Kubernetes集群组件之间的交互是双向的, kubelet 既需要主动访问 kube-apiserver, kube-apiserver 也需要主动向 kubelet 发起请求, 所以双方都需要有自己的根证书以及使用该根证书签发的服务端证书和客户端证书. 在 kube-apiserver 中, 一般明确指定用于 https 访问的服务端证书和带有<code>CN 用户名</code>信息的客户端证书. 而在 kubelet 的启动配置中, 一般只指定了 ca 根证书, 而没有明确指定用于 https 访问的服务端证书, 这是因为, 在生成服务端证书时, 一般会指定服务端地址或主机名, kube-apiserver 相对变化不是很频繁, 所以在创建集群之初就可以预先分配好用作 kube-apiserver 的 IP 或主机名/域名, 但是由于部署在 node 节点上的 kubelet 会因为集群规模的变化而频繁变化, 而无法预知 node 的所有 IP 信息, 所以 kubelet 上一般不会明确指定服务端证书, 而是只指定 ca 根证书, 让 kubelet 根据本地主机信息自动生成服务端证书并保存到配置的<code>cert-dir</code>文件夹中.</p>
</blockquote>
<p>好了, 至此, Kubernetes集群根证书所签发的证书都在上面了, 算上根证书一共涉及到6个文件, 22-6=16, 我们还剩下16个文件</p>
<h3 id="汇聚层证书"><a href="#汇聚层证书" class="headerlink" title="汇聚层证书"></a>汇聚层证书</h3><p>kube-apiserver 的另一种访问方式就是使用 <code>kubectl proxy</code> 来代理访问, 而该证书就是用来支持SSL代理访问的. 在该种访问模式下, 我们是以http的方式发起请求到代理服务的, 此时, 代理服务会将该请求发送给 kube-apiserver, 在此之前, 代理会将发送给 kube-apiserver 的请求头里加入证书信息, 以下两个配置</p>
<p>API Aggregation允许在不修改Kubernetes核心代码的同时扩展Kubernetes API. 开启 API Aggregation 需要在 kube-apiserver 中添加如下配置:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-client-ca-file=&lt;path to aggregator CA cert&gt;</span><br><span class="line">--requestheader-allowed-names=front-proxy-client</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class="line">--requestheader-group-headers=X-Remote-Group</span><br><span class="line">--requestheader-username-headers=X-Remote-User</span><br><span class="line">--proxy-client-cert-file=&lt;path to aggregator proxy cert&gt;</span><br><span class="line">--proxy-client-key-file=&lt;path to aggregator proxy key&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>官方警告: 除非你了解保护 CA 使用的风险和机制, 否则不要在不通上下文中重用已经使用过的 CA</strong></p>
<p>如果 kube-proxy 没有和 API server 运行在同一台主机上，那么需要确保启用了如下 apiserver 标记：<code>--enable-aggregator-routing=true</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">客户端 ---发起请求---&gt; 代理 ---Add Header:发起请求---&gt; kube-apiserver</span><br><span class="line">                   (客户端证书)                        (服务端证书)</span><br></pre></td></tr></table></figure>
<p>kube-apiserver 代理根证书(客户端证书)</p>
<p>用在<code>requestheader-client-ca-file</code>配置选项中, kube-apiserver 使用该证书来验证客户端证书是否为自己所签发</p>
<ul>
<li>/etc/kubernetes/pki/front-proxy-ca.crt</li>
<li>/etc/kubernetes/pki/front-proxy-ca.key</li>
</ul>
<p>由此根证书签发的证书只有一组:</p>
<p>代理层(如汇聚层aggregator)使用此套代理证书来向 kube-apiserver 请求认证</p>
<ol>
<li>代理端使用的客户端证书, 用作代用户与 kube-apiserver 认证</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/front-proxy-client.crt</li>
<li>/etc/kubernetes/pki/front-proxy-client.key</li>
</ul>
<p>参考文档:</p>
<ul>
<li>kube-apiserver 配置参数: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver" target="_blank" rel="external">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver</a></li>
<li>使用汇聚层扩展 Kubernetes API: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/" target="_blank" rel="external">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation</a></li>
<li>配置汇聚层: <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer</a></li>
</ul>
<p>至此, 刨除代理专用的证书外, 还剩下 16-4=12 个文件</p>
<h3 id="etcd-集群根证书"><a href="#etcd-集群根证书" class="headerlink" title="etcd 集群根证书"></a>etcd 集群根证书</h3><p>etcd集群所用到的证书都保存在<code>/etc/kubernetes/pki/etcd</code>这路径下, 很明显, 这一套证书是用来专门给etcd集群服务使用的, 设计以下证书文件</p>
<p>etcd 集群根证书CA(etcd 所用到的所有证书的签发机构)</p>
<ul>
<li>/etc/kubernetes/pki/etcd/ca.crt</li>
<li>/etc/kubernetes/pki/etcd/ca.key</li>
</ul>
<p>由此根证书签发机构签发的证书有:</p>
<ol>
<li>etcd server 持有的服务端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/server.crt</li>
<li>/etc/kubernetes/pki/etcd/server.key</li>
</ul>
<ol>
<li>peer 集群中节点互相通信使用的客户端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/peer.crt</li>
<li>/etc/kubernetes/pki/etcd/peer.key</li>
</ul>
<p>注: Peer：对同一个etcd集群中另外一个Member的称呼</p>
<ol>
<li>pod 中定义 Liveness 探针使用的客户端证书<br> kubeadm 部署的 Kubernetes 集群是以 pod 的方式运行 etcd 服务的, 在该 pod 的定义中, 配置了 Liveness 探活探针</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/healthcheck-client.crt</li>
<li>/etc/kubernetes/pki/etcd/healthcheck-client.key<br>当你 describe etcd 的 pod 时, 会看到如下一行配置:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Liveness:       <span class="built_in">exec</span> [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] delay=15s timeout=15s period=10s <span class="comment">#success=1 #failure=8</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li>配置在 kube-apiserver 中用来与 etcd server 做双向认证的客户端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver-etcd-client.crt</li>
<li>/etc/kubernetes/pki/apiserver-etcd-client.key</li>
</ul>
<p>至此, 介绍了涉及到 etcd 服务的10个证书文件, 12-10=2, 仅剩两个没有介绍到的文件啦, 胜利在望, 坚持一下~</p>
<h3 id="Serveice-Account秘钥"><a href="#Serveice-Account秘钥" class="headerlink" title="Serveice Account秘钥"></a>Serveice Account秘钥</h3><p>最后介绍的这组”证书”其实不是证书, 而是一组秘钥. 看着后缀名是不是有点眼熟呢, 没错, 这组秘钥对儿其实跟我们在Linux上创建, 用于免密登录的密钥对儿原理是一样的~</p>
<blockquote>
<p>这组的密钥对儿仅提供给 kube-controller-manager 使用. kube-controller-manager 通过 sa.key 对 token 进行签名, master 节点通过公钥 sa.pub 进行签名的验证</p>
</blockquote>
<ul>
<li>/etc/kubernetes/pki/sa.key</li>
<li>/etc/kubernetes/pki/sa.pub</li>
</ul>
<p>至此, kubeadm 工具帮我们创建的所有证书文件都已经介绍完了, 整个 Kubernetes&amp;etcd 集群中所涉及到的绝大部分证书都差不多在这里了. 有的行家可能会看出来, 至少还少了一组证书呀, 就是 kube-proxy 持有的证书怎么没有自动生成呀. 因为 kubeadm 创建的集群, kube-proxy 是以 pod 形式运行的, 在 pod 中, 直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书了. 如果你的 kube-proxy 是以守护进程的方式直接运行在宿主机的, 那么你就需要为它创建一套证书了. 创建的方式也很简单, 直接使用上面第一条提到的 <code>Kubernetes 集群根证书</code> 进行签发就可以了(注意CN和O的设置)</p>
<p>参考文档:</p>
<ul>
<li><a href="https://kubernetes.io/docs/setup/certificates/" target="_blank" rel="external">https://kubernetes.io/docs/setup/certificates/</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/</a></li>
<li>docs.lvrui.io</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm证书过期时间调整]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ca-upgdate.html</url>
      <content type="html"><![CDATA[<blockquote>
<p> kubeadm 默认证书为一年，一年过期后，会导致api service不可用，使用过程中会出现：x509: certificate has expired or is not yet valid</p>
</blockquote>
<p><strong><code>如何进行调整，下面给了两个方案，供大家选择</code></strong></p>
<h3 id="方案一-通过修改kubeadm-调整证书过期时间"><a href="#方案一-通过修改kubeadm-调整证书过期时间" class="headerlink" title="方案一 通过修改kubeadm 调整证书过期时间"></a>方案一 通过修改kubeadm 调整证书过期时间</h3><a id="more"></a>
<h4 id="修改代码，调整过期时间"><a href="#修改代码，调整过期时间" class="headerlink" title="修改代码，调整过期时间"></a>修改代码，调整过期时间</h4><p> 克隆代码：<code>git clone https://github.com/kubernetes/kubernetes.git</code>, 切换到指定的tag或者版本修改<code>vendor/k8s.io/client-go/util/cert/cert.go</code>文件，<code>git diff</code> 对比如下：<br><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/staging/src/k8s.io/client-go/util/cert/cert.go b/staging/src/k8s.io/client-go/util/cert/cert.go</span><br><span class="line">index fb7f5fa..e800962 100644</span><br><span class="line"><span class="comment">--- a/staging/src/k8s.io/client-go/util/cert/cert.go</span></span><br><span class="line"><span class="comment">+++ b/staging/src/k8s.io/client-go/util/cert/cert.go</span></span><br><span class="line">@@ -104,7 +104,7 @@ func NewSignedCert(cfg Config, key *rsa.PrivateKey, caCert *x509.Certificate, ca</span><br><span class="line">                IPAddresses:  cfg.AltNames.IPs,</span><br><span class="line">                SerialNumber: serial,</span><br><span class="line">                NotBefore:    caCert.NotBefore,</span><br><span class="line"><span class="deletion">-               NotAfter:     time.Now().Add(duration365d).UTC(),</span></span><br><span class="line"><span class="addition">+               NotAfter:     time.Now().Add(duration365d * 10).UTC(),</span></span><br><span class="line">                KeyUsage:     x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature,</span><br><span class="line">                ExtKeyUsage:  cfg.Usages,</span><br><span class="line">        &#125;</span><br><span class="line">@@ -149,7 +149,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS</span><br><span class="line">                        CommonName: fmt.Sprintf("%s-ca@%d", host, time.Now().Unix()),</span><br><span class="line">                &#125;,</span><br><span class="line">                NotBefore: time.Now(),</span><br><span class="line"><span class="deletion">-               NotAfter:  time.Now().Add(time.Hour * 24 * 365),</span></span><br><span class="line"><span class="addition">+               NotAfter:  time.Now().Add(time.Hour * 24 * 3650),</span></span><br><span class="line"></span><br><span class="line">                KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,</span><br><span class="line">                BasicConstraintsValid: true,</span><br></pre></td></tr></table></figure></p>
<h4 id="编译代码"><a href="#编译代码" class="headerlink" title="编译代码"></a>编译代码</h4><p>编译环境我已经做了对应的1.11.5、1.12.3、1.13.0、1.13.2、1.13.4、1.14.1、1.15.3，已上传到docker hub 上，大家可下载使用，地址如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker pull icyboy/k8s_build:v1.11.5  <span class="comment"># 基于 golang:1.10.3</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.12.3  <span class="comment"># 基于 golang:1.10.4 </span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.0  <span class="comment"># 基于 golang:1.11.2</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.2  <span class="comment"># 基于 golang:1.11.4</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.4  <span class="comment"># 基于 golang:1.11.5</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.14.1  <span class="comment"># 基于 golang:1.12.2</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.15.3  <span class="comment"># 基于 golang:1.12.9</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.16.0  <span class="comment"># 基于 golang:1.12.9</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.16.3  <span class="comment"># 基于 golang:1.12.12</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.17.0  <span class="comment"># 基于 golang:1.13.4</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.17.1  <span class="comment"># 基于 golang:1.13.5</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.17.3  <span class="comment"># 基于 golang:1.13.6</span></span><br></pre></td></tr></table></figure></p>
<p>编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -v 你修改后的代码目录:/go/src/k8s.io/kubernetes -it icyboy/k8s_build:v1.11.5 bash</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /go/src/k8s.io/kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubeadm, 这里主要编译kubeadm 即可</span></span><br><span class="line">make all WHAT=cmd/kubeadm GOFLAGS=-v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubelet</span></span><br><span class="line"><span class="comment"># make all WHAT=cmd/kubelet GOFLAGS=-v</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubectl</span></span><br><span class="line"><span class="comment"># make all WHAT=cmd/kubectl GOFLAGS=-v</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译完产物在 _output/bin/kubeadm 目录下</span></span><br><span class="line"><span class="comment">#将kubeadm 文件拷贝出来，替换系统中的kubeadm</span></span><br></pre></td></tr></table></figure></p>
<p>对应的kubeadm 文件我也编译好后放到百度云中，大家可放心下载使用，可通过<code>kubeadm version</code> 查看对应的版本信息和官方的进行比对<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译过后的</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"11+"</span>, GitVersion:<span class="string">"v1.11.5-dirty"</span>, GitCommit:<span class="string">"753b2dbc622f5cc417845f0ff8a77f539a4213ea"</span>, GitTreeState:<span class="string">"dirty"</span>, BuildDate:<span class="string">"2018-12-07T05:58:18Z"</span>, GoVersion:<span class="string">"go1.10.3"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#官方的</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"11"</span>, GitVersion:<span class="string">"v1.11.5"</span>, GitCommit:<span class="string">"753b2dbc622f5cc417845f0ff8a77f539a4213ea"</span>, GitTreeState:<span class="string">"clean"</span>, BuildDate:<span class="string">"2018-11-26T14:38:30Z"</span>, GoVersion:<span class="string">"go1.10.3"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>kubeadm 下载地址：<a href="https://pan.baidu.com/s/1PplHyDkYDTusx46j9uHwDA" target="_blank" rel="external">https://pan.baidu.com/s/1PplHyDkYDTusx46j9uHwDA</a><br>提取码：dy6f</p>
<h4 id="替换证书"><a href="#替换证书" class="headerlink" title="替换证书"></a>替换证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用新的kubeadm 替换官方的kubeadm</span></span><br><span class="line">chmod +x kubeadm &amp;&amp; \cp <span class="_">-f</span> kubeadm /usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#备份原有的证书</span></span><br><span class="line">mv /etc/kubernetes/pki /etc/kubernetes/pki.old</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成新的证书，kubeadm.yaml 指定你自己服务器上的</span></span><br><span class="line">kubeadm alpha phase certs all --config  ~/kubeadm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#备份原有的conf文件</span></span><br><span class="line">mv /etc/kubernetes/*conf /etc/kubernetes/*conf-old</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据新证书重新生成新的配置文件</span></span><br><span class="line">kubeadm alpha phase kubeconfig all --config ~/kubeadm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#替换老的config文件</span></span><br><span class="line">\cp <span class="_">-f</span> /etc/kubernetes/admin.conf ~/.kube/config</span><br></pre></td></tr></table></figure>
<p><strong>验证</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/pki</span><br><span class="line">openssl x509 -in apiserver-etcd-client.crt -text -noout</span><br><span class="line"><span class="comment">#Certificate:</span></span><br><span class="line"><span class="comment">#    Data:</span></span><br><span class="line"><span class="comment">#        Version: 3 (0x2)</span></span><br><span class="line"><span class="comment">#        Serial Number: 2755977466456048186 (0x263f32e76918023a)</span></span><br><span class="line"><span class="comment">#    Signature Algorithm: sha256WithRSAEncryption</span></span><br><span class="line"><span class="comment">#        Issuer: CN=kubernetes</span></span><br><span class="line"><span class="comment">#        Validity</span></span><br><span class="line"><span class="comment">#            Not Before: Dec  7 09:33:32 2018 GMT</span></span><br><span class="line">             Not After : Dec  4 09:33:32 2028 GMT  <span class="comment">#这里变成10年了</span></span><br><span class="line"><span class="comment">#        Subject: O=system:masters, CN=kube-apiserver-etcd-client</span></span><br><span class="line"><span class="comment">#        Subject Public Key Info:</span></span><br><span class="line"><span class="comment">#        ....</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量验证证书</span></span><br><span class="line"><span class="keyword">for</span> crt <span class="keyword">in</span> $(find /etc/kubernetes/pki/ -name <span class="string">"*.crt"</span>); <span class="keyword">do</span> openssl x509 -in <span class="variable">$crt</span> -noout -dates; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<h3 id="方案二-启用自动轮换kubelet-证书"><a href="#方案二-启用自动轮换kubelet-证书" class="headerlink" title="方案二 启用自动轮换kubelet 证书"></a>方案二 启用自动轮换kubelet 证书</h3><blockquote>
<p>kubelet证书分为server和client两种， <code>k8s 1.9</code>默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启</p>
</blockquote>
<h4 id="增加-kubelet-参数"><a href="#增加-kubelet-参数" class="headerlink" title="增加 kubelet 参数"></a>增加 kubelet 参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true"</span></span><br></pre></td></tr></table></figure>
<h4 id="增加-controller-manager-参数"><a href="#增加-controller-manager-参数" class="headerlink" title="增加 controller-manager 参数"></a>增加 controller-manager 参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数</span></span><br><span class="line">  - <span class="built_in">command</span>:</span><br><span class="line">    - kube-controller-manager</span><br><span class="line">    - --experimental-cluster-signing-duration=87600h0m0s</span><br><span class="line">    - --feature-gates=RotateKubeletServerCertificate=<span class="literal">true</span></span><br><span class="line">    - ....</span><br></pre></td></tr></table></figure>
<h4 id="创建-rbac-对象"><a href="#创建-rbac-对象" class="headerlink" title="创建 rbac 对象"></a>创建 rbac 对象</h4><p>创建rbac对象，允许节点轮换kubelet server证书：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; ca-update.yaml &lt;&lt; EOF</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line"><span class="attr">  name:</span> system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> certificates.k8s.io</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> certificatesigningrequests/selfnodeserver</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> create</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm:node-autoapprove-certificate-server</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> Group</span><br><span class="line"><span class="attr">  name:</span> system:nodes</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create –f ca-update.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="如果证书已经过期，如何进行重新签发证书"><a href="#如果证书已经过期，如何进行重新签发证书" class="headerlink" title="如果证书已经过期，如何进行重新签发证书"></a>如果证书已经过期，如何进行重新签发证书</h3><h4 id="针对kubeadm-1-13-x-及以上处理"><a href="#针对kubeadm-1-13-x-及以上处理" class="headerlink" title="针对kubeadm 1.13.x 及以上处理"></a>针对kubeadm 1.13.x 及以上处理</h4><h5 id="准备kubeadm-conf-配置文件一份"><a href="#准备kubeadm-conf-配置文件一份" class="headerlink" title="准备kubeadm.conf 配置文件一份"></a>准备kubeadm.conf 配置文件一份</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.14</span><span class="number">.1</span> <span class="comment">#--&gt;这里改成你集群对应的版本</span></span><br><span class="line"><span class="attr">imageRepository:</span> registry.cn-hangzhou.aliyuncs.com/google_containers </span><br><span class="line"><span class="comment">#这里使用国内的镜像仓库，否则在重新签发的时候会报错：could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt"</span></span><br></pre></td></tr></table></figure>
<h5 id="重新签发命令"><a href="#重新签发命令" class="headerlink" title="重新签发命令"></a>重新签发命令</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha certs renew all --config=/root/kubeadm.conf</span><br><span class="line"></span><br><span class="line">运行如上命令会重新生成以下证书</span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.crt</span></span><br></pre></td></tr></table></figure>
<h5 id="更新-etc-kubernetes-conf文件"><a href="#更新-etc-kubernetes-conf文件" class="headerlink" title="更新/etc/kubernetes/*.conf文件"></a>更新/etc/kubernetes/*.conf文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#备份删除旧的/etc/kubernetes/*.conf文件</span></span><br><span class="line">mkdir /etc/kubernetes/old-conf  </span><br><span class="line">mv /etc/kubernetes/*.conf /etc/kubernetes/old-conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成新的conf文件</span></span><br><span class="line">kubeadm init phase kubeconfig all --config=/root/kubeadm.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#运行如上命令会重新生成以下conf文件</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/admin.conf</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/controller-manager.conf</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/kubelet.conf</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/scheduler.conf</span></span><br></pre></td></tr></table></figure>
<h5 id="完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器-最后覆盖config文件"><a href="#完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器-最后覆盖config文件" class="headerlink" title="完成后重启kube-apiserver,kube-controller,kube-scheduler,etcd这4个容器,最后覆盖config文件"></a>完成后重启<code>kube-apiserver</code>,<code>kube-controller</code>,<code>kube-scheduler</code>,<code>etcd</code>这4个容器,最后覆盖config文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure>
<h4 id="针对kubeadm-1-13-0（不包含1-13-0）-以下处理"><a href="#针对kubeadm-1-13-0（不包含1-13-0）-以下处理" class="headerlink" title="针对kubeadm 1.13.0（不包含1.13.0） 以下处理"></a>针对kubeadm 1.13.0（不包含1.13.0） 以下处理</h4><h5 id="移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】"><a href="#移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】" class="headerlink" title="移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】"></a>移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes</span><br><span class="line">mkdir ./pki_bak</span><br><span class="line">mkdir ./pki_bak/etcd</span><br><span class="line">mkdir ./conf_bak</span><br><span class="line">mv pki/apiserver* ./pki_bak/</span><br><span class="line">mv pki/front-proxy-client.* ./pki_bak/</span><br><span class="line">mv pki/etcd/healthcheck-client.* ./pki_bak/etcd/</span><br><span class="line">mv pki/etcd/peer.* ./pki_bak/etcd/</span><br><span class="line">mv pki/etcd/server.* ./pki_bak/etcd/</span><br><span class="line">mv ./admin.conf ./conf_bak/</span><br><span class="line">mv ./kubelet.conf ./conf_bak/</span><br><span class="line">mv ./controller-manager.conf ./conf_bak/</span><br><span class="line">mv ./scheduler.conf ./conf_bak/</span><br></pre></td></tr></table></figure>
<h5 id="创建证书"><a href="#创建证书" class="headerlink" title="创建证书"></a>创建证书</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs all --apiserver-advertise-address=<span class="variable">$&#123;MASTER_API_SERVER_IP&#125;</span> --apiserver-cert-extra-sans=主机内网ip,主机公网ip</span><br><span class="line"></span><br><span class="line">运行如上命令会重新生成以下证书</span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.crt</span></span><br><span class="line"></span><br><span class="line">不移动证书会有如下提示</span><br><span class="line"><span class="comment">#[certificates] Using the existing apiserver certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing apiserver-kubelet-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing front-proxy-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing etcd/server certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing etcd/peer certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing etcd/healthcheck-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing apiserver-etcd-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing sa key.</span></span><br></pre></td></tr></table></figure>
<h5 id="生成新配置文件"><a href="#生成新配置文件" class="headerlink" title="生成新配置文件"></a>生成新配置文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --apiserver-advertise-address=<span class="variable">$&#123;MASTER_API_SERVER_IP&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="将新生成的admin配置文件覆盖掉原本的admin文件"><a href="#将新生成的admin配置文件覆盖掉原本的admin文件" class="headerlink" title="将新生成的admin配置文件覆盖掉原本的admin文件"></a>将新生成的admin配置文件覆盖掉原本的admin文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="variable">$HOME</span>/.kube/config <span class="variable">$HOME</span>/.kube/config.old</span><br><span class="line">cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chmod 644 <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure>
<h5 id="完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器"><a href="#完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器" class="headerlink" title="完成后重启kube-apiserver,kube-controller,kube-scheduler,etcd这4个容器"></a>完成后重启kube-apiserver,kube-controller,kube-scheduler,etcd这4个容器</h5><h5 id="如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。"><a href="#如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。" class="headerlink" title="如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。"></a>如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。</h5><h3 id="离线一键安装包"><a href="#离线一键安装包" class="headerlink" title="离线一键安装包"></a>离线一键安装包</h3><blockquote>
<p>k8s 离线一键安装包教程&amp;&amp;地址：<a href="http://team.jiunile.com/pro/k8s/">一键安装</a></p>
</blockquote>
<h3 id="kubeadm-1-14-证书调整教程"><a href="#kubeadm-1-14-证书调整教程" class="headerlink" title="kubeadm 1.14 证书调整教程"></a>kubeadm 1.14 证书调整教程</h3><blockquote>
<p><a href="http://team.jiunile.com/blog/2019/05/k8s-kubeadm14-ca-upgrade.html">教程地址</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes Pod 的生命周期管理]]></title>
      <url>http://team.jiunile.com/blog/2018/11/k8s-k8s-pod-life-cycle.html</url>
      <content type="html"><![CDATA[<h2 id="Pod的生命周期"><a href="#Pod的生命周期" class="headerlink" title="Pod的生命周期"></a>Pod的生命周期</h2><hr>
<h3 id="Pod-phase"><a href="#Pod-phase" class="headerlink" title="Pod phase"></a>Pod phase</h3><p>Pod 的 status 在信息保存在 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471" target="_blank" rel="external">PodStatus</a> 中定义，其中有一个 phase 字段。</p>
<p>Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。</p>
<p>Pod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。</p>
<p>无论你是手动创建 Pod，还是通过 deployment、daemonset 或 statefulset来创建，Pod 的 phase 都有以下几个可能的值：</p>
<ul>
<li><strong>挂起（Pending）</strong>：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。</li>
<li><strong>运行中（Running）</strong>：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。</li>
<li><strong>成功（Successed）</strong>：Pod 中的所有容器都被成功终止，并且不会再重启。</li>
<li><strong>失败（Failed）</strong>：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。</li>
<li><strong>未知（Unkonwn）</strong>：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。</li>
</ul>
<p>下图是 Pod 的生命周期示意图，从图中可以看到 Pod 状态的变化。<br><img src="/images/k8s/k8s_pod_01.jpg" alt="K8s pod 生命周期"></p>
<a id="more"></a>
<h3 id="Pod-状态"><a href="#Pod-状态" class="headerlink" title="Pod 状态"></a>Pod 状态</h3><p>Pod 有一个 PodStatus 对象，其中包含一个 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L1964" target="_blank" rel="external">PodCondition</a> 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 <code>PodScheduled</code>、<code>Ready</code>、<code>Initialized</code> 和 <code>Unschedulable</code>。status 字段是一个字符串，可能的值有 <code>True</code>、<code>False</code> 和 <code>Unknown</code>。</p>
<p>当你通过 <code>kubectl get pod</code> 查看 Pod 时，<code>STATUS</code> 这一列可能会显示与上述5个状态不同的值，例如 <code>Init:0/1</code> 和 <code>CrashLoopBackOff</code>。这是因为 Pod 状态的定义除了包含 phase 之外，还有 <code>InitContainerStatuses</code> 和 <code>containerStatuses</code> 等其他字段，具体代码参考 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471" target="_blank" rel="external">overall status of a pod</a> .</p>
<p>如果想知道究竟发生了什么，可以通过命令 <code>kubectl describe pod/$PODNAME</code> 查看输出信息的 <code>Events</code> 条目。通过 Events 条目可以看到一些具体的信息，比如正在拉取容器镜像，Pod 已经被调度，或者某个 container 处于 unhealthy 状态。</p>
<h2 id="Pod-的启动关闭流程"><a href="#Pod-的启动关闭流程" class="headerlink" title="Pod 的启动关闭流程"></a>Pod 的启动关闭流程</h2><hr>
<p>下面通过一个具体的示例来探究一下 Pod 的整个生命周期流程。为了确定事情发生的顺序，通过下面的 manifest 来部署一个 deployment。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span>                   Deployment</span><br><span class="line"><span class="attr">apiVersion:</span>             apps/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span>                 loap</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span>             <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span>            loap</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span>           init</span><br><span class="line"><span class="attr">        image:</span>          busybox</span><br><span class="line"><span class="attr">        command:</span>       [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): INIT &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span>    /loap</span><br><span class="line"><span class="attr">          name:</span>         timing</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span>           main</span><br><span class="line"><span class="attr">        image:</span>          busybox</span><br><span class="line"><span class="attr">        command:</span>       [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): START &gt;&gt; /loap/timing;</span><br><span class="line">sleep 10; echo $(date +%s): END &gt;&gt; /loap/timing;'</span>]</span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span>    /loap</span><br><span class="line"><span class="attr">          name:</span>         timing</span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): LIVENESS &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): READINESS &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        lifecycle:</span></span><br><span class="line"><span class="attr">          postStart:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): POST-START &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">          preStop:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span>  [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): PRE-HOOK &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span>           timing</span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span>         /tmp/loap</span><br></pre></td></tr></table></figure></p>
<p>等待 Pod 状态变为 <code>Running</code> 之后，通过以下命令来强制停止 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale deployment loap --replicas=0</span><br></pre></td></tr></table></figure></p>
<p>查看 <code>/tmp/loap/timing</code> 文件的内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /tmp/loap/timing</span><br><span class="line"></span><br><span class="line">1525334577: INIT</span><br><span class="line">1525334581: START</span><br><span class="line">1525334581: POST-START</span><br><span class="line">1525334584: READINESS</span><br><span class="line">1525334584: LIVENESS</span><br><span class="line">1525334588: PRE-HOOK</span><br><span class="line">1525334589: END</span><br></pre></td></tr></table></figure></p>
<p><code>/tmp/loap/timing</code> 文件的内容很好地体现了 Pod 的启动和关闭流程，具体过程如下：<br><img src="/images/k8s/k8s_pod_02.jpg" alt="Pod 的启动和关闭流程"></p>
<ol>
<li>首先启动一个 Infra 容器（又叫 Pause 容器），用来和 Pod 中的其他容器共享 linux 命名空间，并开启 init 进程。（上图中忽略了这一步）</li>
<li>然后启动 Init 容器，它是一种专用的容器，在应用程序容器启动之前运行，用来对 Pod 进行一些初始化操作，并包括一些应用镜像中不存在的实用工具和安装脚本。</li>
<li>4 秒之后，应用程序容器和 <code>post-start hook</code> 同时启动。</li>
<li>7 秒之后开始启动 <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank" rel="external">liveness 和 readiness 探针</a>。</li>
<li>11 秒之后，通过手动杀掉 Pod，<code>pre-stop hook</code> 执行，优雅删除期限过期后（默认是 30 秒），应用程序容器停止。实际的 Pod 终止过程要更复杂，具体参考 <a href="https://jimmysong.io/kubernetes-handbook/concepts/pod.html" target="_blank" rel="external">Pod 的终止</a>。</li>
</ol>
<blockquote>
<p>必须主动杀掉 Pod 才会触发 <code>pre-stop hook</code>，如果是 Pod 自己 Down 掉，则不会执行 <code>pre-stop hook</code>。</p>
</blockquote>
<h2 id="如何快速-DEBUG"><a href="#如何快速-DEBUG" class="headerlink" title="如何快速 DEBUG"></a>如何快速 DEBUG</h2><hr>
<p>当 Pod 出现致命的错误时，如果能够快速 DEBUG，将会帮助我们快速定位问题。为了实现这个目的，可以把把致命事件的信息通过 <code>.spec.terminationMessagePath</code> 配置写入指定位置的文件，就像打印错误、异常和堆栈信息一样。该位置的内容可以很方便的通过 dashboards、监控软件等工具检索和展示，默认路径为 <code>/dev/termination-log</code>。</p>
<p>以下是一个小例子：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># termination-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> termination-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> termination-demo-container</span><br><span class="line"><span class="attr">    image:</span> alpine</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>]</span><br><span class="line"><span class="attr">    args:</span> [<span class="string">"-c"</span>, <span class="string">"sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log"</span>]</span><br></pre></td></tr></table></figure></p>
<p>这些消息的最后部分会使用其他的规定来单独存储：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> termination-demo.yaml</span><br><span class="line"></span><br><span class="line">$ sleep 20</span><br><span class="line"></span><br><span class="line">$ kubectl get pod termination-demo -o go-template=<span class="string">'&#123;&#123;range .status.containerStatuses&#125;&#125;&#123;&#123;.lastState.terminated.message&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line">Sleep expired</span><br><span class="line"></span><br><span class="line">$ kubectl get pod termination-demo -o go-template=<span class="string">'&#123;&#123;range .status.containerStatuses&#125;&#125;&#123;&#123;.lastState.terminated.exitCode&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line">0</span><br></pre></td></tr></table></figure></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><hr>
<ul>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/pod-hook.html" target="_blank" rel="external">Pod hook</a></li>
<li><a href="https://blog.openshift.com/kubernetes-pods-life/" target="_blank" rel="external">Kubernetes: A Pod’s Life</a></li>
<li><a href="https://k8smeetup.github.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/" target="_blank" rel="external">确定 Pod 失败的原因</a></li>
<li>Ryan Yang </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java和Docker限制的那些事儿]]></title>
      <url>http://team.jiunile.com/blog/2018/07/docker-java%E4%B8%8Edocker%E7%9A%84%E9%82%A3%E7%82%B9%E4%BA%8B.html</url>
      <content type="html"><![CDATA[<h2 id="揭秘"><a href="#揭秘" class="headerlink" title="揭秘"></a>揭秘</h2><p>Java和Docker不是天然的朋友。 Docker可以设置内存和CPU限制，而Java不能自动检测到。使用Java的Xmx标识（繁琐/重复）或新的实验性JVM标识，我们可以解决这个问题。</p>
<h2 id="虚拟化中的不匹配"><a href="#虚拟化中的不匹配" class="headerlink" title="虚拟化中的不匹配"></a>虚拟化中的不匹配</h2><p>Java和Docker的结合并不是完美匹配的，最初的时候离完美匹配有相当大的距离。对于初学者来说，JVM的全部设想就是，虚拟机可以让程序与底层硬件无关。</p>
<p>那么，把我们的Java应用打包到JVM中，然后整个再塞进Docker容器中，能给我们带来什么好处呢？大多数情况下，你只是在复制JVMs和Linux容器，除了浪费更多的内存，没任何好处。感觉这样子挺傻的。</p>
<p>不过，Docker可以把你的程序，设置，特定的JDK，Linux设置和应用服务器，还有其他工具打包在一起，当做一个东西。站在DevOps/Cloud的角度来看，这样一个完整的容器有着更高层次的封装。</p>
<h3 id="问题一：内存"><a href="#问题一：内存" class="headerlink" title="问题一：内存"></a>问题一：内存</h3><p>时至今日，绝大多数产品级应用仍然在使用Java 8（或者更旧的版本），而这可能会带来问题。Java 8（update 131之前的版本）跟Docker无法很好地一起工作。问题是在你的机器上，JVM的可用内存和CPU数量并不是Docker允许你使用的可用内存和CPU数量。</p>
<p>比如，如果你限制了你的Docker容器只能使用100MB内存，但是呢，旧版本的Java并不能识别这个限制。Java看不到这个限制。JVM会要求更多内存，而且远超这个限制。如果使用太多内存，Docker将采取行动并杀死容器内的进程！JAVA进程被干掉了，很明显，这并不是我们想要的。</p>
<p>为了解决这个问题，你需要给Java指定一个最大内存限制。在旧版本的Java（8u131之前），你需要在容器中通过设置-Xmx来限制堆大小。这感觉不太对，你可不想定义这些限制两次，也不太想在你的容器中来定义。</p>
<p>幸运的是我们现在有了更好的方式来解决这个问题。从Java 9之后（8u131+），JVM增加了如下标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap</span><br></pre></td></tr></table></figure></p>
<p>这些标志强制JVM检查Linux的cgroup配置，Docker是通过cgroup来实现最大内存设置的。现在，如果你的应用到达了Docker设置的限制（比如500MB），JVM是可以看到这个限制的。JVM将会尝试GC操作。如果仍然超过内存限制，JVM就会做它该做的事情，抛出OutOfMemoryException。也就是说，JVM能够看到Docker的这些设置。</p>
<p>从Java 10之后（参考下面的测试），这些体验标志位是默认开启的，也可以使用-XX:+UseContainerSupport来使能（你可以通过设置-XX:-UseContainerSupport来禁止这些行为）。</p>
<h3 id="问题二：CPU"><a href="#问题二：CPU" class="headerlink" title="问题二：CPU"></a>问题二：CPU</h3><p>第二个问题是类似的，但它与CPU有关。简而言之，JVM将查看硬件并检测CPU的数量。它会优化你的runtime以使用这些CPUs。但是同样的情况，这里还有另一个不匹配，Docker可能不允许你使用所有这些CPUs。可惜的是，这在Java 8或Java 9中并没有修复，但是在Java 10中得到了解决。</p>
<p>从Java 10开始，可用的CPUs的计算将采用以不同的方式（默认情况下）解决此问题（同样是通过UseContainerSupport）。<br><a id="more"></a></p>
<h2 id="Java和Docker的内存处理测试"><a href="#Java和Docker的内存处理测试" class="headerlink" title="Java和Docker的内存处理测试"></a>Java和Docker的内存处理测试</h2><p>作为一个有趣的练习，让我们验证并测试Docker如何使用几个不同的JVM版本/标志甚至不同的JVM来处理内存不足。</p>
<p>首先，我们创建一个测试应用程序，它只是简单地“吃”内存并且不释放它。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MemEat</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        List l = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">byte</span> b[] = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1048576</span>];</span><br><span class="line">            l.add(b);</span><br><span class="line">            Runtime rt = Runtime.getRuntime();</span><br><span class="line">            System.out.println( <span class="string">"free memory: "</span> + rt.freeMemory() );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们可以启动Docker容器并运行这个应用程序来查看会发生什么。</p>
<h3 id="测试一：Java-8u111"><a href="#测试一：Java-8u111" class="headerlink" title="测试一：Java 8u111"></a>测试一：Java 8u111</h3><p>首先，我们将从具有旧版本Java 8的容器开始（update 111）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it java:openjdk-8u111 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>我们编译并运行MemEat.java文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 67194416</span><br><span class="line">free memory: 66145824</span><br><span class="line">free memory: 65097232</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>正如所料，Docker已经杀死了我们的Java进程。不是我们想要的（！）。你也可以看到输出，Java认为它仍然有大量的内存需要分配。</p>
<p>我们可以通过使用-Xmx标志为Java提供最大内存来解决此问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java -Xmx100m MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 1155664</span><br><span class="line">free memory: 1679936</span><br><span class="line">free memory: 2204208</span><br><span class="line">free memory: 1315752</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>在提供了我们自己的内存限制之后，进程正常停止，JVM理解它正在运行的限制。然而，问题在于你现在将这些内存限制设置了两次，Docker一次，JVM一次。</p>
<h3 id="测试二：Java-8u144"><a href="#测试二：Java-8u144" class="headerlink" title="测试二：Java 8u144"></a>测试二：Java 8u144</h3><p>如前所述，随着增加新标志来修复问题，JVM现在可以遵循Docker所提供的设置。我们可以使用版本新一点的JVM来测试它。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk8 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>（在撰写本文时，此OpenJDK Java镜像的版本是Java 8u144）</p>
<p>接下来，我们再次编译并运行MemEat.java文件，不带任何标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 67194416</span><br><span class="line">free memory: 66145824</span><br><span class="line">free memory: 65097232</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>依然存在同样的问题。但是我们现在可以提供上面提到的实验性标志来试试看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 1679936</span><br><span class="line">free memory: 2204208</span><br><span class="line">free memory: 1155616</span><br><span class="line">free memory: 1155600</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">   at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>这一次我们没有告诉JVM限制的是什么，我们只是告诉JVM去检查正确的限制设置！现在感觉好多了。</p>
<h3 id="测试三：Java-10u23"><a href="#测试三：Java-10u23" class="headerlink" title="测试三：Java 10u23"></a>测试三：Java 10u23</h3><p>有些人在评论和Reddit上提到Java 10通过使实验标志成为新的默认值来解决所有问题。这种行为可以通过禁用此标志来关闭：-XX：-UseContainerSupport。</p>
<p>当我测试它时，它最初不起作用。在撰写本文时，AdoptAJDK OpenJDK10镜像与jdk-10+23一起打包。这个JVM显然还是不理解UseContainerSupport标志，该进程仍然被Docker杀死。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk10 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>测试了代码（甚至手动提供需要的标志）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 96262112</span><br><span class="line">free memory: 94164960</span><br><span class="line">free memory: 92067808</span><br><span class="line">free memory: 89970656</span><br><span class="line">Killed</span><br><span class="line">java -XX:+UseContainerSupport MemEat</span><br><span class="line">Unrecognized VM option <span class="string">'UseContainerSupport'</span></span><br><span class="line">Error: Could not create the Java Virtual Machine.</span><br><span class="line">Error: A fatal exception has occurred. Program will exit.</span><br></pre></td></tr></table></figure></p>
<h3 id="测试四：Java-10u46（Nightly）"><a href="#测试四：Java-10u46（Nightly）" class="headerlink" title="测试四：Java 10u46（Nightly）"></a>测试四：Java 10u46（Nightly）</h3><p>我决定尝试AdoptAJDK OpenJDK 10的最新nightly构建。它包含的版本是Java 10+46，而不是Java 10+23。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk10:nightly /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>然而，在这个ngithly构建中有一个问题，导出的PATH指向旧的Java 10+23目录，而不是10+46，我们需要修复这个问题。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/opt/java/openjdk/jdk-10+46/bin/</span><br><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 3566824</span><br><span class="line">free memory: 2796008</span><br><span class="line">free memory: 1480320</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">  at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>成功！不提供任何标志，Java 10依然可以正确检测到Dockers内存限制。</p>
<h3 id="测试五：OpenJ9"><a href="#测试五：OpenJ9" class="headerlink" title="测试五：OpenJ9"></a>测试五：OpenJ9</h3><p>我最近也在试用OpenJ9，这个免费的替代JVM已经从IBM J9开源，现在由Eclipse维护。</p>
<p>请在我的下一篇博文<a href="http://royvanrijn.com/blog/2018/05/openj9-jvm-shootout/" target="_blank" rel="external">http://royvanrijn.com/blog/2018/05/openj9-jvm-shootout/</a>阅读关于OpenJ9的更多信息。</p>
<p>它运行速度快，内存管理非常好，性能卓越，经常可以为我们的微服务节省多达30-50％的内存。这几乎可以将Spring Boot应用程序定义为’micro’了，其运行时间只有100-200mb，而不是300mb+。我打算尽快就此写一篇关于这方面的文章。</p>
<p>但令我惊讶的是，OpenJ9还没有类似于Java 8/9/10+中针对cgroup内存限制的标志（backported）的选项。如果我们将以前的测试用例应用到最新的AdoptAJDK OpenJDK 9 + OpenJ9 build：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk9-openj9 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>我们添加OpenJDK标志（OpenJ9会忽略的标志）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 83988984</span><br><span class="line">free memory: 82940400</span><br><span class="line">free memory: 81891816</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>Oops，JVM再次被Docker杀死。</p>
<p>我真的希望类似的选项将很快添加到OpenJ9中，因为我希望在生产环境中运行这个选项，而不必指定最大内存两次。 Eclipse/IBM正在努力修复这个问题，已经提了issues，甚至已经针对issues提交了PR。</p>
<h3 id="更新：（不推荐Hack）"><a href="#更新：（不推荐Hack）" class="headerlink" title="更新：（不推荐Hack）"></a>更新：（不推荐Hack）</h3><p>一个稍微丑陋/hacky的方式来解决这个问题是使用下面的组合标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">java -Xmx`cat /sys/fs/cgroup/memory/memory.limit_<span class="keyword">in</span>_bytes` MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 3171536</span><br><span class="line">free memory: 2127048</span><br><span class="line">free memory: 2397632</span><br><span class="line">free memory: 1344952</span><br><span class="line">JVMDUMP039I Processing dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span> at 2018/05/15 14:04:26 - please wait.</span><br><span class="line">JVMDUMP032I JVM requested System dump using <span class="string">'//core.20180515.140426.125.0001.dmp'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I System dump written to //core.20180515.140426.125.0001.dmp</span><br><span class="line">JVMDUMP032I JVM requested Heap dump using <span class="string">'//heapdump.20180515.140426.125.0002.phd'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Heap dump written to //heapdump.20180515.140426.125.0002.phd</span><br><span class="line">JVMDUMP032I JVM requested Java dump using <span class="string">'//javacore.20180515.140426.125.0003.txt'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Java dump written to //javacore.20180515.140426.125.0003.txt</span><br><span class="line">JVMDUMP032I JVM requested Snap dump using <span class="string">'//Snap.20180515.140426.125.0004.trc'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Snap dump written to //Snap.20180515.140426.125.0004.trc</span><br><span class="line">JVMDUMP013I Processed dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span>.</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">  at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>在这种情况下，堆大小受限于分配给Docker实例的内存，这适用于较旧的JVM和OpenJ9。这当然是错误的，因为容器本身和堆外的JVM的其他部分也使用内存。但它似乎工作，显然Docker在这种情况下是宽松的。也许某些bash大神会做出更好的版本，从其他进程的字节中减去一部分。</p>
<p>无论如何，不要这样做，它可能无法正常工作。</p>
<h3 id="测试六：OpenJ9（Nightly）"><a href="#测试六：OpenJ9（Nightly）" class="headerlink" title="测试六：OpenJ9（Nightly）"></a>测试六：OpenJ9（Nightly）</h3><p>有人建议使用OpenJ9的最新nightly版本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk9-openj9:nightly /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>最新的OpenJ9夜间版本，它有两个东西：</p>
<ol>
<li>另一个有问题的PATH参数，需要先解决这个问题</li>
<li>JVM支持新标志UseContainerSupport（就像Java 10一样）</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/opt/java/openjdk/jdk-9.0.4+12/bin/</span><br><span class="line">javac MemEat.java</span><br><span class="line">java -XX:+UseContainerSupport MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 5864464</span><br><span class="line">free memory: 4815880</span><br><span class="line">free memory: 3443712</span><br><span class="line">free memory: 2391032</span><br><span class="line">JVMDUMP039I Processing dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span> at 2018/05/15 21:32:07 - please wait.</span><br><span class="line">JVMDUMP032I JVM requested System dump using <span class="string">'//core.20180515.213207.62.0001.dmp'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I System dump written to //core.20180515.213207.62.0001.dmp</span><br><span class="line">JVMDUMP032I JVM requested Heap dump using <span class="string">'//heapdump.20180515.213207.62.0002.phd'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Heap dump written to //heapdump.20180515.213207.62.0002.phd</span><br><span class="line">JVMDUMP032I JVM requested Java dump using <span class="string">'//javacore.20180515.213207.62.0003.txt'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Java dump written to //javacore.20180515.213207.62.0003.txt</span><br><span class="line">JVMDUMP032I JVM requested Snap dump using <span class="string">'//Snap.20180515.213207.62.0004.trc'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Snap dump written to //Snap.20180515.213207.62.0004.trc</span><br><span class="line">JVMDUMP013I Processed dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span>.</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br></pre></td></tr></table></figure>
<p>TADAAA，正在修复中！</p>
<p>奇怪的是，这个标志在OpenJ9中默认没有启用，就像它在Java 10中一样。再说一次：确保你测试了这是你想在一个Docker容器中运行Java。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>简言之：注意资源限制的不匹配。测试你的内存设置和JVM标志，不要假设任何东西。</p>
<p>如果您在Docker容器中运行Java，请确保你设置了Docker内存限制和在JVM中也做了限制，或者你的JVM能够理解这些限制。</p>
<p>如果您无法升级您的Java版本，请使用-Xmx设置您自己的限制。</p>
<p>对于Java 8和Java 9，请更新到最新版本并使用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX：+UnlockExperimentalVMOptions -XX：+UseCGroupMemoryLimitForHeap</span><br></pre></td></tr></table></figure></p>
<p>对于Java 10，确保它支持’UseContainerSupport’（更新到最新版本）。</p>
<p>对于OpenJ9（我强烈建议使用，可以在生产环境中有效减少内存占用量），现在使用-Xmx设置限制，但很快会出现一个支持UseContainerSupport标志的版本。</p>
<p>原文链接：<a href="http://royvanrijn.com/blog/2018/05/java-and-docker-memory-limits/" target="_blank" rel="external">http://royvanrijn.com/blog/2018/05/java-and-docker-memory-limits/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Centos 6.x 上升级go 1.10.3]]></title>
      <url>http://team.jiunile.com/blog/2018/07/go-%E5%8D%87%E7%BA%A7go1-10-x%E9%97%AE%E9%A2%98.html</url>
      <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><blockquote>
<p>目前本地Linux (Centos 6.x) 编译环境还滞留在1.8.x上，为了提升go性能，想将go升级到最新版，但在升级过程中遇到如下问题，故此记录下！忘后续的go友能跳过此坑！</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在Centos 6.x 升级go 1.10.x过程中遇到如下问题：</p>
<ul>
<li>step1.  下载go 1.10.x 源码</li>
<li>step2.  解压，进入go/src</li>
<li>step3.  执行./all.bash</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Building Go cmd/dist using /root/go1.4.</span><br><span class="line">Building Go toolchain1 using /root/go1.4.</span><br><span class="line">Building Go bootstrap cmd/go (go_bootstrap) using Go toolchain1.</span><br><span class="line">Building Go toolchain2 using go_bootstrap and Go toolchain1.</span><br><span class="line">Building Go toolchain3 using go_bootstrap and Go toolchain2.</span><br><span class="line">Building packages and commands <span class="keyword">for</span> linux/amd64.</span><br><span class="line"></span><br><span class="line"><span class="comment">##### Testing packages.</span></span><br><span class="line">.... 过程略长，特此省略</span><br><span class="line">ok      cmd/internal/src    0.001s</span><br><span class="line">ok      cmd/internal/<span class="built_in">test</span>2json    0.097s</span><br><span class="line">ok      cmd/link    1.988s</span><br><span class="line">ok      cmd/link/internal/ld    43.529s</span><br><span class="line">ok      cmd/nm    3.417s</span><br><span class="line">ok      cmd/objdump    1.588s</span><br><span class="line">ok      cmd/pack    1.217s</span><br><span class="line">ok      cmd/trace    0.007s</span><br><span class="line">--- FAIL: TestObjFile (0.01s)</span><br><span class="line">    binutils_test.go:231: SourceLine: unexpected error write |1: broken pipe</span><br><span class="line">FAIL</span><br><span class="line">FAIL    cmd/vendor/github.com/google/pprof/internal/binutils    0.018s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/driver    12.194s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/elfexec    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/graph    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/measurement    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/report    0.048s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/symbolizer    0.004s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/symbolz    0.004s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/profile    0.045s</span><br><span class="line">ok      cmd/vendor/github.com/ianlancetaylor/demangle    0.012s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/arm/armasm    0.007s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/arm64/arm64asm    0.043s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/ppc64/ppc64asm    0.003s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/x86/x86asm    0.064s</span><br><span class="line">ok      cmd/vet    1.205s</span><br><span class="line">ok      cmd/vet/internal/cfg    0.002s</span><br><span class="line">2018/07/18 17:59:22 Failed: <span class="built_in">exit</span> status 1</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>为什么在执行binutils_test.go 会Failed，最终查看代码原因是因为如下命令引起：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">120807 3650  execve(<span class="string">"/usr/bin/addr2line"</span>, [<span class="string">"/usr/bin/addr2line"</span>, <span class="string">"-aif"</span>, <span class="string">"-e"</span>, <span class="string">"testdata/exe_linux_64"</span>], [/* 15 vars */] &lt;unfinished ...&gt;</span><br></pre></td></tr></table></figure></p>
<p>有关生成此命令的源代码可查看如下地址：<a href="https://github.com/google/pprof/blob/a74ae6fb3cd7047c79272e3ea0814b08154a2d3c/internal/binutils/addr2liner.go#L92" target="_blank" rel="external">addr2line</a></p>
<p>add2line文件来自于包：binutils</p>
<p>执行命令失败，是因为在CentOS 6.x上，binutils的版本是2.20，<a href="https://sourceware.org/git/gitweb.cgi?p=binutils-gdb.git;a=blob_plain;f=binutils/NEWS;hb=refs/tags/binutils-2_27" target="_blank" rel="external">参考文献</a> ，然后addr2line命令中的-a参数在binutils 2.21版中才添加</p>
<p>因此，为了解决这个问题，我从源码重新进行编译binutils并将其构建的二进制文件添加到PATH中，然后运行测试，并成功通过。</p>
<p>编译安装binutils过程如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://ftp.gnu.org/gnu/binutils/binutils-2.27.tar.gz</span><br><span class="line">tar zxvf binutils-2.27.tar.gz </span><br><span class="line"><span class="built_in">cd</span> binutils-2.27</span><br><span class="line">./configure --prefix=/usr</span><br><span class="line">make </span><br><span class="line">make install</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong><code>特此说明，在Centos 7.x 上能成功避免此坑！！</code></strong></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Logstash filter插件开发]]></title>
      <url>http://team.jiunile.com/blog/2017/08/log-logstash-filter.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Logstash是一个具有实时管线能力的开源数据收集引擎。在ELK Stack中，通常选择更轻量级的Filebeat收集日志，然后将日志输出到Logstash进行加工处理，再将处理后的日志输出到指定的目标（ElasticSearch，Kafka等）当中。</p>
<p>Logstash事件的处理管线是inputs → filters → outputs，三个阶段都可以自定义插件，本文主要介绍如何开发自定义需求最多的filter插件。</p>
<p>Logstash的安装就不详细介绍了，下载传送门：<a href="https://www.elastic.co/downloads/logstash" target="_blank" rel="external">https://www.elastic.co/downloads/logstash</a>。</p>
<h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><p>我们使用docker来讲解如何给logstash定制一个filter插件，首先，我们下载logstash官方最新版本的镜像。下载方式: <code>docker pull logstash</code>，下载好后我们就来Run这个镜像，命令：<code>docker run -it logstash bash</code>，这样我们就进入到logstash容器中了。接下来我们就安装下两个基本的软件包，一个vim，一个rsyslog，命令如下：<code>apt-get update &amp;&amp; apt-get -y install vim rsyslog &amp;&amp; /etc/init.d/rsysylog start</code>。到此我们的准备工作就完毕了。</p>
<a id="more"></a>
<h2 id="生成filter插件"><a href="#生成filter插件" class="headerlink" title="生成filter插件"></a>生成filter插件</h2><p>cd到Logstash的跟目录，使用<code>bin/logstash-plugin</code>生成filter插件模板，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/share/logstash</span><br><span class="line">mkdir -p vendor/localgems</span><br><span class="line">bin/logstash-plugin generate --type filter --name <span class="built_in">test</span> --path vendor/localgems</span><br><span class="line"><span class="comment">#vendor/localgems 可修改为你自己的路径</span></span><br></pre></td></tr></table></figure></p>
<p>查看filter插件的目录结构，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@c11787c4cef3:/usr/share/logstash/vendor/localgems<span class="comment"># tree .</span></span><br><span class="line">.</span><br><span class="line">└── logstash-filter-test</span><br><span class="line">    ├── CHANGELOG.md</span><br><span class="line">    ├── CONTRIBUTORS</span><br><span class="line">    ├── DEVELOPER.md</span><br><span class="line">    ├── Gemfile</span><br><span class="line">    ├── LICENSE</span><br><span class="line">    ├── README.md</span><br><span class="line">    ├── Rakefile</span><br><span class="line">    ├── lib</span><br><span class="line">    │   └── logstash</span><br><span class="line">    │       └── filters</span><br><span class="line">    │           └── test.rb</span><br><span class="line">    ├── logstash-filter-test.gemspec</span><br><span class="line">    ├── spec</span><br><span class="line">    │   ├── filters</span><br><span class="line">    │   │   └── <span class="built_in">test</span>_spec.rb</span><br><span class="line">    │   └── spec_helper.rb</span><br><span class="line">    └── test.conf</span><br><span class="line"></span><br><span class="line">6 directories, 12 files</span><br></pre></td></tr></table></figure></p>
<h2 id="filter插件初探"><a href="#filter插件初探" class="headerlink" title="filter插件初探"></a>filter插件初探</h2><p>Logstash插件是用ruby写的，查看<code>logstash-filter-test/lib/logstash/filters/test.rb</code>文件，如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># logstash依赖于UTF-8编码，需要在插件代码开始处添加</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#引入了插件必备的包</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">"logstash/filters/base"</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">"logstash/namespace"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插件继承自Base基类，并配置插件的使用名称</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogStash::Filters::Test</span> &lt; LogStash::Filters::<span class="title">Base</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插件名称，在Logstash配置的filter块中使用</span></span><br><span class="line">  <span class="comment">#filter&#123;</span></span><br><span class="line">  <span class="comment">#  test&#123;</span></span><br><span class="line">  <span class="comment">#      source =&gt; "message"</span></span><br><span class="line">  <span class="comment">#  &#125;</span></span><br><span class="line">  <span class="comment">#&#125;</span></span><br><span class="line">  config_name <span class="string">"test"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插件参数配置</span></span><br><span class="line">  <span class="comment"># source是插件test的可选参数，默认值是"Hello World!"。</span></span><br><span class="line">  config <span class="symbol">:source</span>, <span class="symbol">:validate</span> =&gt; <span class="symbol">:string</span>, <span class="symbol">:default</span> =&gt; <span class="string">"Hello World!"</span></span><br><span class="line">  <span class="comment"># 下面是参数的通用配置代码</span></span><br><span class="line">  <span class="comment"># config :variable_name, :validate =&gt; :variable_type, :default =&gt; "Default value", :required =&gt; boolean, :deprecated =&gt; boolean, :obsolete =&gt; string</span></span><br><span class="line">  <span class="comment"># :variable_name：参数名称</span></span><br><span class="line">  <span class="comment"># :validate：验证参数类型，如:string, :password, :boolean, :number, :array, :hash, :path等</span></span><br><span class="line">  <span class="comment"># :required：是否必须配置</span></span><br><span class="line">  <span class="comment"># :default：默认值</span></span><br><span class="line">  <span class="comment"># :deprecated：是否废弃</span></span><br><span class="line">  <span class="comment"># :obsolete：声明该配置不再使用，通常提供升级方案</span></span><br><span class="line"></span><br><span class="line">  public</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">register</span></span></span><br><span class="line">    <span class="comment"># 方法相当于初始化方法，不需要手动调用，可以在这个方法里面调用配置变量，如<span class="doctag">@message</span>，也可以初始化自己的实例变量。</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  public</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span><span class="params">(event)</span></span></span><br><span class="line">    <span class="comment"># 方法是插件的数据处理逻辑，其中event变量封装了数据流，可以通过接口访问event中的内容，具体参见 https://www.elastic.co/guide/en/logstash/5.1/event-api.html。</span></span><br><span class="line">    <span class="keyword">if</span> (source = event.get(@source))      </span><br><span class="line">      datas = source.split(<span class="string">"|"</span>)</span><br><span class="line">      event.set(<span class="string">"data"</span>, datas[<span class="number">0</span>])</span><br><span class="line">      event.set(<span class="string">"data2"</span>, datas[<span class="number">1</span>])</span><br><span class="line">      event.set(<span class="string">"user"</span>, <span class="string">"xp"</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#这个方法用于保证Logstash的配置`add_field`, `remove_field`, `add_tag`和`remove_tag`会被正确执行。</span></span><br><span class="line">    filter_matched(event)</span><br><span class="line">  <span class="keyword">end</span> <span class="comment"># def filter</span></span><br><span class="line"><span class="keyword">end</span> <span class="comment"># class LogStash::Filters::Test</span></span><br></pre></td></tr></table></figure></p>
<p>查看<code>logstash-filter-test/logstash-filter-test.gemspec</code>文件，如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Gem::Specification.new <span class="keyword">do</span> <span class="params">|s|</span></span><br><span class="line">  s.name          = <span class="string">'logstash-filter-test'</span>  </span><br><span class="line">  s.version       = <span class="string">'0.1.0'</span></span><br><span class="line">  s.licenses      = [<span class="string">'Apache License (2.0)'</span>]</span><br><span class="line">  s.summary       = <span class="string">'描述这个插件的概要'</span></span><br><span class="line">  s.description   = <span class="string">'这个插件的详细说明'</span></span><br><span class="line">  s.homepage      = <span class="string">'http://icyxp.github.io'</span></span><br><span class="line">  s.authors       = [<span class="string">'icyboy'</span>]</span><br><span class="line">  s.email         = <span class="string">'icyboy@me.com'</span></span><br><span class="line">  s.require_paths = [<span class="string">'lib'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Files</span></span><br><span class="line">  s.files = Dir[<span class="string">'lib/**/*'</span>,<span class="string">'spec/**/*'</span>,<span class="string">'vendor/**/*'</span>,<span class="string">'*.gemspec'</span>,<span class="string">'*.md'</span>,<span class="string">'CONTRIBUTORS'</span>,<span class="string">'Gemfile'</span>,<span class="string">'LICENSE'</span>,<span class="string">'NOTICE.TXT'</span>]</span><br><span class="line">   <span class="comment"># Tests</span></span><br><span class="line">  s.test_files = s.files.grep(<span class="regexp">%r&#123;^(test|spec|features)/&#125;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Special flag to let us know this is actually a logstash plugin</span></span><br><span class="line">  s.metadata = &#123; <span class="string">"logstash_plugin"</span> =&gt; <span class="string">"true"</span>, <span class="string">"logstash_group"</span> =&gt; <span class="string">"filter"</span> &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Gem dependencies</span></span><br><span class="line">  s.add_runtime_dependency <span class="string">"logstash-core-plugin-api"</span>, <span class="string">"~&gt; 2.0"</span></span><br><span class="line">  s.add_development_dependency <span class="string">'logstash-devutils'</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<h2 id="在Logstash中配置定制的插件"><a href="#在Logstash中配置定制的插件" class="headerlink" title="在Logstash中配置定制的插件"></a>在Logstash中配置定制的插件</h2><p>cd到Logstash根目录下<code>cd /usr/shar/logstash</code>，在<code>Gemfile</code>末尾添加以下配置：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="string">"logstash-filter-test"</span>, <span class="symbol">:path</span> =&gt; <span class="string">"vendor/localgems/logstash-filter-test"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="启动Logstash"><a href="#启动Logstash" class="headerlink" title="启动Logstash"></a>启动Logstash</h2><p>先编写一个配置文件<code>test.conf</code>，这里我使用了rsyslog<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#来源rsyslog</span></span><br><span class="line">input&#123;</span><br><span class="line">    file&#123;</span><br><span class="line">	    path =&gt; <span class="string">"/var/log/messages"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#或者可以设置为终端输入</span></span><br><span class="line"><span class="comment">#input&#123;</span></span><br><span class="line"><span class="comment">#    stdin&#123;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    &#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line">filter&#123;</span><br><span class="line">    <span class="comment"># 定制的filter插件</span></span><br><span class="line">    <span class="built_in">test</span>&#123;</span><br><span class="line">	    <span class="built_in">source</span> =&gt; <span class="string">"message"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#输出到终端</span></span><br><span class="line">output&#123;</span><br><span class="line">    stdout&#123;</span><br><span class="line">        codec =&gt; rubydebug</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行起来<code>logstash -f test.conf</code>，然后往rsyslog里写日志你就可以看下如下情况就说明ok了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#logger "测试一下"</span></span><br><span class="line">&#123;</span><br><span class="line">          <span class="string">"path"</span> =&gt; <span class="string">"/var/log/messages"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2017-08-16T15:27:46.606Z,</span><br><span class="line">          <span class="string">"data"</span> =&gt; <span class="string">"Aug 16 15:27:45 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">         <span class="string">"data2"</span> =&gt; nil,</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"c11787c4cef3"</span>,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"Aug 16 15:27:45 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">          <span class="string">"user"</span> =&gt; <span class="string">"xp"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#logger "测试一下|from by icyboy"</span></span><br><span class="line">&#123;</span><br><span class="line">          <span class="string">"path"</span> =&gt; <span class="string">"/var/log/messages"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2017-08-16T15:28:17.661Z,</span><br><span class="line">          <span class="string">"data"</span> =&gt; <span class="string">"Aug 16 15:28:16 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">         <span class="string">"data2"</span> =&gt; <span class="string">"from by icyboy"</span>,</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"c11787c4cef3"</span>,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"Aug 16 15:28:16 c11787c4cef3 root: 测试一下|from by icyboy"</span>,</span><br><span class="line">          <span class="string">"user"</span> =&gt; <span class="string">"xp"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一语点醒技术人：你不是Google]]></title>
      <url>http://team.jiunile.com/blog/2017/07/gossip-%E4%BD%A0%E4%B8%8D%E6%98%AFGoogle.html</url>
      <content type="html"><![CDATA[<p><img src="/images/gossip/01/cover.png" alt="01"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在为问题寻找解决方案时要先充分了解问题本身，而不是一味地盲目崇拜那些巨头公司。Ozan Onay以Amazon、LinkedIn和Google为例，为执迷不悟的人敲响警钟。以下内容已获得作者翻译授权，查看原文：<a href="https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb" target="_blank" rel="external">You Are Not Google</a>。</p>
<p>软件工程师总是着迷于荒唐古怪的事。我们看起来似乎很理性，但在面对技术选型时，总是陷入抓狂——从Hacker News到各种博客，像一只飞蛾一样，来回折腾，最后精疲力尽，无助地飞向一团亮光，跪倒在它的前面——那就是我们一直在寻找的东西。</p>
<p>真正理性的人不是这样做决定的。不过工程师一贯如此，比如决定是否使用MapReduce。</p>
<p>Joe Hellerstein在他的大学数据库教程视频中说道：</p>
<blockquote>
<p>世界上只有差不多5个公司需要运行这么大规模的作业。至于其他公司……他们使用了所有的IO来实现不必要的容错。在2000年代，人们狂热地追随着Google：“我们要做Google做过的每一件事，因为我们也运行着世界上最大的互联网数据服务。”</p>
</blockquote>
<p>超出实际需求的容错没有什么问题，但我们却为此付出了的惨重的代价：不仅增加了IO，还有可能让原先成熟的系统——包含了事务、索引和查询优化器——变得破碎不堪。这是一个多么严重的历史倒退！有多少个Hadoop用户是有意识地做出这种决定的？有多少人知道他们的决定到底是不是一个明智之举？</p>
<p>MapReduce已经成为一个众矢之的，那些盲目崇拜者也意识到事情不对劲。但这种情况却普遍存在：虽然你使用了大公司的技术，但你的情况却与他们大不一样，而且你的决定并没有经过深思熟虑，你只是习以为常地认为，模仿巨头公司就一定也能给你带来同样的财富。</p>
<p>是的，这又是一篇劝大家“不要盲目崇拜”的文章。不过这次我列出了一长串有用的清单，或许能够帮助你们做出更好的决定。</p>
<a id="more"></a>
<h2 id="很酷的技术？UNPHAT"><a href="#很酷的技术？UNPHAT" class="headerlink" title="很酷的技术？UNPHAT"></a>很酷的技术？UNPHAT</h2><p>如果你还在使用Google搜索新技术来重建你的软件架构，那么我建议你不要再这么做了。相反，你可以考虑应用UNPHAT原则。</p>
<ol>
<li>在彻底了解（Understand）你的问题之前，不要急着去寻找解决方案。你的目标应该是在问题领域内“解决”问题，而不是在方案领域内解决问题。</li>
<li>列出（eNumerate）多种方案，不要只把眼睛盯在你最喜欢的方案上。</li>
<li>选择一个候选方案，并阅读相关论文（Paper）。</li>
<li>了解候选方案的产生背景（Historical context）。</li>
<li>比较优点（Advantages）和缺点，扬长避短。</li>
<li>思考（Think）！冷静地思考候选方案是否适合用于解决你的问题。要出现怎样异常的情况才会让你改变注意？例如，数据要少到什么程度才会让你打消使用Hadoop的念头？</li>
</ol>
<h2 id="你不是Amazon"><a href="#你不是Amazon" class="headerlink" title="你不是Amazon"></a>你不是Amazon</h2><p>UNPHAT原则十分直截了当。最近我与一个公司有过一次对话，这个公司打算在一个读密集的系统里使用Cassandra，他们的数据是在夜间加载到系统里的。</p>
<p>他们阅读了Dynamo的相关论文，并且知道Cassandra是最接近Dynamo的一个产品。我们知道，这些分布式数据库优先保证写可用性（Amazon是不会让“添加到购物车”这种操作出现失败的）。为了达到这个目的，他们在一致性以及几乎所有在传统RDBMS中出现过的特性上做出了妥协。但这家公司其实没有必要优先考虑写可用性，因为他们每天只有一次写入操作，只是数据量比较大。</p>
<p>他们之所以考虑使用Cassandra，是因为PostgreSQL查询需要耗费几分钟的时间。他们认为是硬件的问题，经过排查，我们发现数据表里有5000万条数据，每条数据最多80个字节。如果从SSD上整块地读取所有数据大概需要5秒钟，这个不算快，但比起实际的查询，它要快上两个数量级。</p>
<p>我真的很想多问他们几个问题（了解问题！），在问题变得愈加严重时，我为他们准备了5个方案（列出多个候选方案！），不过很显然，Cassandra对于他们来说完全是一个错误的方案。他们只需要耐心地做一些调优，比如对部分数据重新建模，或许可以考虑使用（当然也有可能没有）其他技术……但一定不是这种写高可用的键值存储系统，Amazon当初创建Cassandra是用来解决他们的购物车问题的！</p>
<h2 id="你不是LinkedIn"><a href="#你不是LinkedIn" class="headerlink" title="你不是LinkedIn"></a>你不是LinkedIn</h2><p>我发现一个学生创办的小公司居然在他们的系统里使用Kafka，这让我感到很惊讶。因为据我所知，他们每天只有很少的事务需要处理——最好的情况下，一天最多只有几百个。这样的吞吐量几乎可以直接记在记事本上。</p>
<p>Kafka被设计用于处理LinkedIn内部的吞吐量，那可是一个天文数字。即使是在几年前，这个数字已经达到了每天数万亿，在高峰时段每秒钟需要处理1000万个消息。不过Kafka也可以用于处理低吞吐量的负载，或许再低10个数量级？</p>
<p>或许工程师们在做决定时确实是基于他们的预期需求，并且也很了解Kafka的适用场景。但我猜测他们是抵挡不住社区对Kafka的追捧，并没有仔细想过Kafka是否适合他们。要知道，那可是10个数量级的差距！</p>
<h2 id="再一次，你不是Amazon"><a href="#再一次，你不是Amazon" class="headerlink" title="再一次，你不是Amazon"></a>再一次，你不是Amazon</h2><p>比Amazon的分布式数据库更为著名的是它的可伸缩架构模式，也就是面向服务架构。Werner Vogels在2006年的一次访谈中指出，Amazon在2001年时就意识到他们的前端需要横向伸缩，而面向服务架构有助于他们实现前端伸缩。工程师们面面相觑，最后只有少数几个工程师着手去做这件事情，而几乎没有人愿意将他们的静态网页拆分成小型的服务。</p>
<p>不过Amazon还是决定向SOA转型，他们当时有7800个员工和30亿美元的销售规模。</p>
<p>当然，并不是说你也要等到有7800个员工的时候才能转向SOA……只是你要多想想，它真的能解决你的问题吗？你的问题的根源是什么？可以通过其他的方式解决它们吗？</p>
<p>如果你告诉我说，你那50个人的公司打算转向SOA，那么我不禁感到疑惑：为什么很多大型的公司仍然在乐此不彼地使用具有模块化的大型单体应用？</p>
<h2 id="甚至Google也不是Google"><a href="#甚至Google也不是Google" class="headerlink" title="甚至Google也不是Google"></a>甚至Google也不是Google</h2><p>使用Hadoop和Spark这样的大规模数据流引擎会非常有趣，但在很多情况下，传统的DBMS更适合当前的负载，有时候数据量小到可以直接放进内存。你是否愿意花10,000美金去购买1TB的内存？如果你有十亿个用户，每个用户仅能使用1KB的内存，所以你的投入远远不够。</p>
<p>或许你的负载大到需要把数据写回磁盘。那么你需要多少磁盘？你到底有多少数据量？Google之所以要创建GFS和MapReduce，是要解决整个Web的计算问题，比如重建整个Web的搜索索引。</p>
<p>或许你已经阅读过GFS和MapReduce的论文，Google的部分问题在于吞吐量，而不是容量，他们之所以需要分布式的存储，是因为从磁盘读取字节流要花费太多的时间。那么你在2017年需要使用多少设备吞吐量？你一定不需要像Google那么大的吞吐量，所以你可能会考虑使用更好的设备。如果都用上SSD会给你增加多少成本？</p>
<p>或许你还想要伸缩性。但你有仔细算过吗，你的数据增长速度会快过SSD降价的速度吗？在你的数据撑爆所有的机器之前，你的业务会有多少增长？截止2016年，Stack Exchange每天要处理2亿个请求，但是他们只用了4个SQL Server，一个用于Stack Overflow，一个用于其他用途，另外两个作为备份复本。</p>
<p>或许你在应用UNPHAT原则之后，仍然决定要使用Hadoop或Spark。或许你的决定是对的，但关键的是你要用对工具。Google非常明白这个道理，当他们意识到MapReduce不再适合用于构建索引之后，他们就不再使用它。</p>
<h2 id="先了解你的问题"><a href="#先了解你的问题" class="headerlink" title="先了解你的问题"></a>先了解你的问题</h2><p>我所说的也不是什么新观点，不过或许UNPHAT对于你们来说已经足够了。如果你觉得还不够，可以听听Rich Hickey的演讲“<a href="https://www.youtube.com/watch?v=f84n5oFoZBc" target="_blank" rel="external">吊床驱动开发</a>”，或者看看Polya的书《<a href="https://www.amazon.com/How-Solve-Mathematical-Princeton-Science/dp/069111966X?ie=UTF8&amp;%2aVersion%2a=1&amp;%2aentries%2a=0" target="_blank" rel="external">How to Solve It</a>》， 或者学习一下Hamming的课程“<a href="https://www.youtube.com/playlist?list=PL2FF649D0C4407B30" target="_blank" rel="external">The Art of Doing Science and Engineering</a>”。我恳请你们一定要多思考！在尝试解决问题之前先对它们有充分的了解。最后送上Polya的一个金句名言：</p>
<blockquote>
<p>回答一个你不了解的问题是愚蠢的，到达一个你不期望的终点是悲哀的。</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[高负载微服务系统的诞生过程]]></title>
      <url>http://team.jiunile.com/blog/2017/07/microservice-%E9%AB%98%E8%B4%9F%E8%BD%BD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%9E%E7%94%9F%E8%BF%87%E7%A8%8B.html</url>
      <content type="html"><![CDATA[<p><img src="/images/ms/02/cover.png" alt="封面"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在2016 <a href="http://highload.co/" target="_blank" rel="external">LighLoad++</a>大会上，“M-Tex”的开发经理Vadim Madison讲述了从一个由数百个微服务组成的系统到包含数千个微服务的高负载项目的发展历程。本文已获得翻译授权，查看英文原文：<a href="https://kukuruku.co/post/microservices-in-a-high-load-project/" target="_blank" rel="external">Microservices in a High-Load Project</a> 。</p>
<p>我将告诉大家我们是如何开始一个高负载微服务项目的。在讲述我们的经历之前，先让我们简单地自我介绍一下。</p>
<p>简单地说，我们从事视频输出方面的工作——我们提供实时的视频。我们负责“NTV-Plus”和“Match TV”频道的视频平台。该平台有30万的并发用户，每小时输出300TB的内容。这是一个很有意思的任务。那么我们是如何做到的呢？</p>
<p>这背后都有哪些故事？这些故事都是关于项目的开发和成长，关于我们对项目的思考。总而言之，是关于如何提升项目的伸缩能力，承受更大的负载，在不宕机和不丢失关键特性的情况下为客户提供更多的功能。我们总是希望能够满足客户的需求。当然，这也涉及到我们是如何实现这一切，以及这一切是如何开始的。</p>
<a id="more"></a>
<p><strong>在最开始，我们有两台运行在Docker集群里的服务器，数据库运行在相同机器的容器里。没有专用的存储，基础设施非常简单。</strong></p>
<p>我们就是这样开始的，只有两台运行在Docker集群里的服务器。那个时候，数据库也运行在同一个集群里。我们的基础设施里没有什么专用的组件，十分简单。<br><img src="/images/ms/02/00.jpg" alt="00"></p>
<p>我们的基础设施最主要的组件就是Docker和TeamCity，我们用它们来交付和构建代码。</p>
<p>在接下来的时期——我称其为我们的发展中期——是我们项目发展的关键时期。我们拥有了80台服务器，并在一组特殊的机器上为数据库搭建了一个单独的专用集群。我们开始使用基于CEPH的分布式存储，并开始思考服务之间的交互问题，同时要更新我们的监控系统。</p>
<p>现在，让我们来看看我们在这一时期都做了哪些事情。Docker集群里已经有数百台服务器，微服务就运行在它们上面。这个时候，我们开始根据数据总线和逻辑分离原则将我们的系统拆分成服务子系统。当微服务越来越多时，我们决定拆分我们的系统，这样维护起来就容易得多（也更容易理解）。<br><img src="/images/ms/02/01.jpg" alt="01"></p>
<p>这张图展示的是我们系统其中的一小部分。这部分系统负责视频剪切。半年前，我在“RIT++”也展示过类似的图片。那个时候只有17个绿色的微服务，而现在有28个绿色的微服务。这些服务只占我们整个系统的二十分之一，所以可以想象我们系统大致的规模有多大。</p>
<h2 id="深入细节"><a href="#深入细节" class="headerlink" title="深入细节"></a>深入细节</h2><p>服务间的通信是一件很有趣的事情。一般来说，我们应该尽可能提升服务间通信效率。我们使用了<a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="external">protobuf</a>，我们认为它就是我们需要的东西。</p>
<p>它看起来是这样的：<br><img src="/images/ms/02/02.jpg" alt="02"></p>
<p>微服务的前面有一个负载均衡器。请求到达前端，或者直接发送给提供了JSON API的服务。protobuf被用于内部服务之间的交互。</p>
<p>protobuf真是一个好东西。它为消息提供了很好的压缩率。现如今有很多框架，只要使用很小的开销就能实现序列化和反序列化。我们可以将其视为有条件的请求类型。</p>
<p>但如果从微服务角度来看，我们会发现，微服务之间也存在某种私有的协议。如果只有一两个或者五个微服务，我们可以为每个微服务打开一个控制台，通过它们来访问微服务，并获得响应结果。如果出现了问题，我们可以对其进行诊断。不过这在一定程度上让微服务的支持工作变得复杂。</p>
<p>在一定时期内，这倒不是什么问题，因为并没有太多的微服务。另外，Google发布了<a href="https://grpc.io/" target="_blank" rel="external">gRPC</a>。在那个时候，gRPC满足了所有我们想做的事情。于是我们逐渐迁移到gRPC。于是我们的技术栈里出现了另一个组件。<br><img src="/images/ms/02/03.jpg" alt="03"></p>
<p>实现的细节也是很有趣的。gRPC默认是基于HTTP/2的。如果你的环境相对稳定，应用程序不怎么发生变更，也不需要在机器间迁移，那么gRCP对于你来说就是个不错的东西。另外，gRPC支持很多客户端和服务器端的编程语言。</p>
<p>现在，我们从微服务角度来看待这个问题。从一方面来看，gRPC是一个好东西，但从另一方面来看，它也有不足之处。当我们开始对日志进行标准化（这样就可以将它们聚合到一个独立的系统里）时，我们发现，从gRPC中抽取日志非常麻烦。</p>
<p>于是，我们决定开发自己的日志系统。它解析消息，并将它们转成我们需要的格式。这样我们才可以获得我们想要的日志。还有一个问题，添加新的微服务会让服务间的依赖变得更加复杂。这是微服务一直存在的问题，这也是除版本问题之外的另一个具有一定复杂性的问题。</p>
<p>于是，我们开始考虑使用JSON。在很长的一段时间里，我们无法相信，在使用了紧凑的二进制协议之后会转回使用JSON。有一天，我们看到一篇文章，来自<a href="http://engineering.dailymotion.com/" target="_blank" rel="external">DailyMotion</a>的一个家伙在文章里提到了同样的事情：“我们知道该如何使用JSON，每个人都可以使用JSON。既然如此，为什么还要自寻烦恼呢？”<br><img src="/images/ms/02/04.jpg" alt="04"></p>
<p>于是，我们逐渐从gRPC转向我们自己实现的JSON。我们保留了HTTP/2，它与JSON组合起来可以带来更快的速度。</p>
<p>现在，我们具备了所有必要的特性。我们可以通过cURL访问我们的服务。我们的QA团队使用<a href="https://www.getpostman.com/" target="_blank" rel="external">Postman</a>，所以他们也感觉很满意。一切都变得简单起来。这是一个有争议性的决定，但却为我们带来了很多好处。</p>
<p>JSON唯一的缺点就是它的紧凑性不足。根据我们的测试结果，它与MessagePack之间有30%的差距。不过对于一个支持系统来说，这不算是个大问题。</p>
<p>况且，我们在转到JSON之后还获得了更多的特性，比如协议版本。有时候，当我们在新版本的协议上使用protobuf时，客户端也必须改用protobuf。如果你有数百个服务，就算只有10%的服务进行了迁移，这也会引起很大的连锁反应。你在一个服务上做了一些变更，就会有十多个服务也需要跟着改动。</p>
<p>因此，我们就会面临这样的一种情况，一个服务的开发人员已经发布了第五个、第六个，甚至第七个版本，但生产环境里仍然在运行第四个版本，就因为其他相关服务的开发人员有他们自己的优先级和截止日期。他们无法持续地更新他们的服务，并使用新版本的协议。所以，新版本的服务虽然发布了，但还派不上用场。然后，我们却要以一种很奇怪的方式来修复旧版本的bug，这让支持工作变得更加复杂。</p>
<p>最后，我们决定停止发布新版本的协议。我们提供协议的基础版本，可以往里面添加少量的属性。服务的消费者开始使用JSON schema。</p>
<p>标准看起来是这样的：<br><img src="/images/ms/02/05.jpg" alt="05"></p>
<p>我们没有使用版本1、2和3，而是只使用版本1和指向它的schema。<br><img src="/images/ms/02/06.jpg" alt="06"></p>
<p>这是从我们服务返回的一个典型的响应结果。它是一个内容管理器，返回有关广播的信息。这里有一个消费者schema的例子。<br><img src="/images/ms/02/07.jpg" alt="07"></p>
<p>最底下的字符串最有意思，也就是”required”那块。我们可以看到，这个服务只需要4个字段——id、content、date和status。如果我们使用了这个schema，那么消费者就只会得到这样的数据。<br><img src="/images/ms/02/08.jpg" alt="08"></p>
<p>它们可以被用在每一个协议版本里，从第一个版本到后来的每一个变更版本。这样，在版本之间迁移就容易很多。在我们发布新版本之后，客户端的迁移就会简单很多。</p>
<p>下一个重要的议题是系统的稳定性问题。这是微服务和其他任何一个系统都需要面临的问题（在微服务架构里，我们可以更强烈地感觉到它的重要性）。系统总会在某个时候变得不稳定。</p>
<p>如果服务间的调用链只包含了一两个服务，那么就没有什么问题。在这种情况下，你看不出单体和分布式系统之间有多大区别。但当调用链里包含了5到7个调用，那么问题就会接踵而至。你根本不知道为什么会这样，也不知道能做些什么。在这种情况下，调试会变得很困难。在单体系统里，你可以通过逐步调试来找出错误。但对于微服务来说，网络不稳定性或高负载下的性能不稳定性也会对微服务造成影响。特别是对于拥有大量节点的分布式系统来说，这些情况就更加显而易见了。<br><img src="/images/ms/02/09.jpg" alt="09"></p>
<p>在一开始，我们采用了传统的办法。我们监控所有的东西，查看问题和问题的发生点，然后尝试尽快修复它们。我们将微服务的度量指标收集到一个独立的数据库里。我们使用<a href="https://github.com/python-diamond/Diamond" target="_blank" rel="external">Diamond</a>来收集系统度量指标。我们使用<a href="https://github.com/google/cadvisor" target="_blank" rel="external">cAdvisor</a>来分析容器的资源使用情况和性能特征。所有的结果都被保存到<a href="https://github.com/influxdata/influxdb" target="_blank" rel="external">InfluxDB</a>，然后我们在Grafana里创建仪表盘。<br><img src="/images/ms/02/10.jpg" alt="10"></p>
<p>于是，我们现在的基础设施里又多了三个组件。</p>
<p>我们比以往更加关注所发生的一切。我们对问题的反应速度更快了。不过，这并没有阻止问题的出现。</p>
<p>奇怪的是，微服务架构的主要问题出在那些不稳定的服务上。它们有的今天运行正常，明天就不行，而且有各种各样的原因。如果服务出现超载，而你继续向它发送负载，它就会宕机一段时间。如果它在一段时间不提供服务，负载就会下降，然后它就又活过来了。这类系统很难维护，也很难知道到底出了什么问题。</p>
<p>最后，我们决定把这些服务停掉，而不是让它们来回折腾。我们因此需要改变服务的实现方式。</p>
<p>我们做了一件很重要的事情。我们对每个服务接收的请求数量设定了一个上限。每个服务知道自己可以处理多少个来自客户端的请求（我们稍后会详细说明）。如果请求数量达到上限，服务将抛出503 Service Unavailable异常。客户端知道这个节点无法提供服务，就会选择另一个节点。</p>
<p>当系统出现问题时，我们就可以通过这种方式来减少请求时间。另外，我们也提升了服务的稳定性。</p>
<p>我们引入了第二种模式——<strong>回路断路器</strong>（Circuit Breaker）。我们在客户端实现了这种模式。</p>
<p>假设有一个服务A，它有4个可以访问的服务B的实例。它向注册中心索要服务B的地址：“给我这些服务的地址”。它得到了服务B的4个地址。服务A向第一个服务B的实例发起了请求。第一个服务B实例正常返回响应。服务A将其标记为可访问：“是的，我可以访问它”。然后，服务A向第二个服务B实例发起请求，不过它没有在期望的时间内得到响应。我们禁用了这个实例，然后向下一个实例发起请求。下一个实例因为某些原因返回了不正确的协议版本。于是我们也将其禁用，然后转向第四个实例。</p>
<p>总得来说，只有一半的服务能够为客户端提供服务。于是服务A将会向能够正常返回响应的两个服务发起请求。而另外两个无法满足要求的实例被禁用了一段时间。</p>
<p>我们通过这种方式来提升性能的稳定性。如果服务出现了问题，我们就将其关闭，并发出告警，然后尝试找出问题所在。</p>
<p>因为引入了回路断路器模式，我们的基础设施里又多了一个组件——<a href="https://github.com/Netflix/Hystrix" target="_blank" rel="external">Hystrix</a>。<br><img src="/images/ms/02/11.jpg" alt="11"></p>
<p>Hystrix不仅实现了回路断路器模式，它也有助于我们了解系统里出现了哪些问题：<br><img src="/images/ms/02/12.jpg" alt="12"></p>
<p>圆环的大小表示服务与其他组件之间的流量大小。颜色表示系统的健康状况。如果圆环是绿色的，那么说明一切正常。如果圆环是红色的，那么就有问题了。</p>
<p>如果一个服务应该被停掉，那么它看起来是这个样子的。圆环是打开的。<br><img src="/images/ms/02/13.jpg" alt="13"></p>
<p>我们的系统变得相对稳定。每个服务至少都有两个可用的实例，这样我们就可以选择停掉其中的一个。不过，尽管是这样，我们仍然不知道我们的系统究竟发生了什么问题。在处理请求期间如果出现了问题，我们应该怎样才能知道问题的根源是什么呢？</p>
<p>这是一个标准的请求：<br><img src="/images/ms/02/14.jpg" alt="14"></p>
<p>这是一个处理链条。用户发送请求到第一个服务，然后是第二个。从第二个服务开始，链条将请求发送到第三个和第四个服务。</p>
<p>然后一个分支不明原因地消失了。在经历了这类场景之后，我们尝试着提升这种场景的可见性，于是我们找到了Appdash。Appdash是一个跟踪服务。<br><img src="/images/ms/02/16.jpg" alt="16"></p>
<p>它看起来是这个样子的：<br><img src="/images/ms/02/17.jpg" alt="17"></p>
<p>可以这么说，我们只是想尝试一下，看看它是否适合我们。将它用在我们的系统里是一件很容易的事情，因为我们那个时候使用的是Go语言。Appdash提供了一个开箱即用的包。我们认为Appdash是一个好东西，只是它的实现并不是很适合我们。<br><img src="/images/ms/02/18.jpg" alt="18"></p>
<p>于是，我们决定使用<a href="http://zipkin.io/" target="_blank" rel="external">Zipkin</a>来代替Appdash。Zipkin是由Twitter开源的。它看起来是这个样子的：<br><img src="/images/ms/02/19.jpg" alt="19"></p>
<p>我认为这样会更清楚一些。我们可以从中看到一些服务，也可以看到我们的请求是如何通过请求链的，还可以看到请求在每个服务里都做了哪些事情。一方面，我们可以看到服务的总时长和每个分段的时长，另一方面，我们完全可以添加描述服务内容的信息。</p>
<p>我们可以在这里添加一些与数据库的调用、文件系统的读取、缓存的访问有关的信息，这样就可以知道请求里哪一部分使用了最多的时间。TraceID可以帮助我们做到这一点。稍后我会介绍更多细节。<br><img src="/images/ms/02/20.jpg" alt="20"></p>
<p>我们就是通过这种方式知道请求在处理过程中发生了什么问题，以及为什么有时候无法被正常处理。刚开始一切都正常，然后突然间，其中的一个出现了问题。我们稍作排查，就知道出问题的服务发生了什么。<br><img src="/images/ms/02/21.jpg" alt="21"></p>
<p>不久前，一些厂商推出了一个跟踪系统的标准。为了简化系统的实现，主要的几个跟踪系统厂商在如何设计客户端API和客户端类库上达成了一致。现在已经有了<a href="http://opentracing.io/" target="_blank" rel="external">OpenTracing</a>的实现，支持几乎所有的主流开发语言。现在就可以使用它了。</p>
<p>我们已经有办法知道那些突然间崩溃的服务。我们可以看到其中的某部分在垂死挣扎，但是不知道为什么。光有环境信息是不够的，</p>
<p>我们还需要日志。是的，这应该成为标准的一部分，它就是Elasticsearch、Logstash和Kibana（ELK）。不过我们对它们做了一些改动。<br><img src="/images/ms/02/22.jpg" alt="22"></p>
<p>我们并没有将大量的日志直接通过forward传给Logstash，而是先传给syslog，让它把日志聚合到构建机器上，然后再通过forward导入到<strong>Elasticsearch</strong>和<strong>Kibana</strong>。这是一个很标准的流程，那么巧妙的地方在哪里呢？<br><img src="/images/ms/02/23.jpg" alt="23"></p>
<p>巧妙的是，我们可以在任何可能的地方往日志里加入Zipkin的TraceID。</p>
<p>这样一来，我们就可以在Kibana仪表盘上看到完整的用户请求执行情况。也就是说，一旦服务进入生产环境，就为运营做好了准备。它已经通过了自动化测试，如果有必要，QA可以再进行手动检查。它应该没有什么问题。如果它出现了问题，那说明有一些先决条件没有得到满足。日志里详细地记录了这些先决条件，通过过滤，我们可以看到某个请求的跟踪信息。我们因此可以快速地查出问题的根源，为我们节省了很多时间。</p>
<p>我们后来引入了动态调试模式。现在的日志数量还不是很大，大概只有100 GB到150 GB，我记不太清楚具体数字了。不过，这些日志是在正常的日志模式下生成的。如果我们添加更多的细节，那么日志就可能变成TB级别的，处理起来就很耗费资源。</p>
<p>当我们发现某些服务出现问题，就打开调试模式（通过一个API），看看发生了什么事情。有时候，我们找到出现问题的服务，在不将它关闭的情况下打开调试模式，尝试找出问题所在。</p>
<p>最后，我们在ELK端查找问题。我们还对关键服务的错误进行聚合。服务知道哪些错误是关键性的，哪些不是关键性的，然后将它们传给<a href="https://sentry.io/" target="_blank" rel="external">Sentry</a>。<br><img src="/images/ms/02/24.jpg" alt="24"></p>
<p>Sentry能够智能地收集错误日志，并形成度量指标，还会进行一些基本的过滤。我们在很多服务上使用了Sentry。我们从单体应用时期就开始使用它了。</p>
<p>那么最有趣的问题是，我们是如何进行伸缩的？这里需要先介绍一些概念。我们把每个机器看成一个黑盒。<br><img src="/images/ms/02/25.jpg" alt="25"></p>
<p>我们有一个编排系统，最开始使用<a href="https://www.nomadproject.io/" target="_blank" rel="external">Nomad</a>。确切地说，应该是<a href="https://www.ansible.com/" target="_blank" rel="external">Ansible</a>。我们自己编写脚本，但光是这些还不能满足要求。那个时候，Nomad的某些版本可以简化我们的工作，于是我们决定迁移到Nomad。<br><img src="/images/ms/02/26.jpg" alt="26"></p>
<p>同时还使用了<a href="https://www.consul.io/" target="_blank" rel="external">Consul</a>，将它作为服务发现的注册中心。还有Vault，用于存储敏感数据，比如密码、秘钥和其他所有不能保存在Git上的东西。</p>
<p>这样，所有的机器几乎都变得一模一样。每个机器上都安装了<strong>Docker</strong>，还有<strong>Consul</strong>和<strong>Nomad</strong>代理。总的来说，每一个机器都处于备用状态，可以在任何时候投入使用。如果不用了，我们就让它们下线。如果你构建了云平台，你就可以先准备好机器，在高峰期时将它们打开，在负载下降时将它们关闭。这会节省大量的成本。<br><img src="/images/ms/02/27.jpg" alt="27"></p>
<p>后来，我们决定从<strong>Nomad</strong>迁移到<a href="https://github.com/kubernetes/kubernetes" target="_blank" rel="external">Kubernetes</a>，<strong>Consul</strong>也因此成为了集中式的配置系统。</p>
<p>这样一来，部分栈可以进行自动伸缩。那么我们是怎么做的呢？</p>
<p>第一步，我们对内存、CPU和网络进行限制。<br><img src="/images/ms/02/28.jpg" alt="28"></p>
<p>我们分别将这三个元素分成三个等级，砍掉其中的一部分。例如，<br><img src="/images/ms/02/29.jpg" alt="29"></p>
<p>R3-C2-N1，我们已经限定只给某个服务一小部分网络流量、多一点点的CPU和更多的内存。这个服务真的很耗费资源。</p>
<p>我们在这里使用了助记符，我们的决策服务可以设置很多的组合值，这些值看起来是这样的：<br><img src="/images/ms/02/30.jpg" alt="30"></p>
<p>事实上，我们还有C4和R4，不过它们已经超出了这些标准的限制。标准看起来是这样的：<br><img src="/images/ms/02/31.jpg" alt="31"></p>
<p>下一步开始做一些预备工作。我们先确定服务的伸缩类型。</p>
<p>独立的服务最容易伸缩，它可以进行线性地伸缩。如果用户增长了两倍，我们就运行两倍的服务实例。这就万事大吉了。</p>
<p>第二种伸缩类型：服务依赖了外部的资源，比如那些使用了数据库的服务。数据库有它自己的容量上限，这个一定要注意。你还要知道，如果系统性能出现衰退，就不应该再增加更多的实例，而且你要知道这种情况会在什么时候发生。</p>
<p>第三种情况是，服务受到外部系统的牵制。例如，外部的账单系统。就算运行了100个服务实例，它也没办法处理超过500个请求。我们要考虑到这些限制。在确定了服务类型并设置了相应的标记之后，是时候看看它们是如何通过我们的构建管道的。<br><img src="/images/ms/02/32.jpg" alt="32"></p>
<p>我们在CI服务器上运行了一些单元测试，然后在测试环境运行集成测试，我们的QA团队会对它们做一些检查。在这之后，我们就进入了预生产环境的负载测试。<br><img src="/images/ms/02/33.jpg" alt="33"></p>
<p>如果是第一种类型的服务，我们使用一个实例，并在这个环境里运行它，给它最大的负载。在运行了几轮之后，我们取其中的最小值，将它存入<strong>InfluxDB</strong>，将它作为该服务的负载上限。</p>
<p>如果是第二种类型的服务，我们逐渐加大负载，直到出现了性能衰退。我们对这个过程进行评估，如果我们知道该系统的负载，那么就比较当前负载是否已经足够，否则，我们就会设置告警，不会把这个服务发布到生产环境。我们会告诉开发人员：“你们需要分离出一些东西，或者加进去另一个工具，让这个服务可以更好地伸缩。”<br><img src="/images/ms/02/34.jpg" alt="34"></p>
<p>因为我们知道第三种类型服务的上限，所以我们只运行一个实例。我们也会给它一些负载，看看它可以服务多少个用户。如果我们知道账单系统的上限是1000个请求，并且每个服务实例可以处理200个请求，那么就需要5个实例。</p>
<p>我们把这些信息都保存到了InfluxDB。我们的决策服务开始派上用场了。它会检查两个边界：上限和下限。如果超出了上限，那么就应该增加服务实例。如果超出下限，那么就减少实例。如果负载下降（比如晚上的时候），我们就不需要这么多机器，可以减少它们的数量，并关掉一部分机器，省下一些费用。</p>
<p>整体看起来是这样的：<br><img src="/images/ms/02/35.jpg" alt="35"></p>
<p>每个服务的度量指标表明了它们当前的负载。负载信息被保存到InfluxDB，如果决策服务发现服务实例达到了上限，它会向Nomad和Kubernetes发送命令，要求增加服务实例。有可能在云端已经有可用的实例，或者开始做一些准备工作。不管怎样，发出要求增加新服务实例的告警才是关键所在。</p>
<p>一些受限的服务如果达到上限，也会发出相关的告警。对于这类情况，我们除了加大等待队列，也做不了其他什么事情。不过最起码我们知道我们很快就会面临这样的问题，并开始做好应对措施。</p>
<p>这就是我想告诉大家有关伸缩性方面的事情。除了这些，还有另外一个东西——<a href="https://about.gitlab.com/gitlab-ci/" target="_blank" rel="external">Gitlab CI</a>。<br><img src="/images/ms/02/36.jpg" alt="36"></p>
<p>我们一般是通过TeamCity来开发服务的。后来，我们意识到，所有的服务都有一个共性，这些服务都是不一样的，并且知道自己该如何部署到容器里。要生成这么的项目真的很困难，不过如果使用yml文件来描述它们，并把这个文件与服务放在一起，就会方便很多。虽然我们只做了一些小的改变，不过却为我们带来了非常多的可能性。</p>
<p>现在，我想说一些一直想对自己说的话。</p>
<p>关于微服务开发，我建议在一开始就使用<strong>编排系统</strong>。可以使用最简单的编排系统，比如Nomad，通过nomad agent -dev命令启动一个编排系统，包括Consul和其他东西。</p>
<p>我们仿佛是在一个黑盒子工作。你试图避免被绑定到某台特定的机器上，或者被附加到某台特定机器的文件系统上。这些事情会让你开始重新思考。</p>
<p>在开发阶段，<strong>每个服务至少需要两个实例</strong>，如果其中一个出现问题，就可以关掉它，由另一个接管继续服务。</p>
<p>还有一些有关架构的问题。在微服务架构里，<strong>消息总线</strong>是一个非常重要的组件。</p>
<p>假设你有一个用户注册系统，那么如何以最简单的方式实现它呢？对于注册系统来说，需要创建账户，然后在账单系统里创建一个用户，并为他创建头像和其他东西。你有一组服务，其中的超级服务收到了一个请求，它将请求分发给其他服务。经过几次之后，它就知道该触发哪些服务来完成注册。</p>
<p>不过，我们可以使用一种更简单、更可靠、更高效的方式来实现。我们使用一个服务来处理注册，它注册了一个用户，然后发送一个事件到消息总线，比如“我已经注册了一个yoghurt，ID是……”。相关的服务会收到这个事件，其中的一个服务会在账单系统里创建一个账户，另一个服务会发送一封欢迎邮件。</p>
<p>不过，系统会因此失去强一致性。这个时候你没有超级服务，也不知道每个服务的状态。不过，这样的系统很容易维护。</p>
<p>现在，我再说一些之前提到过的问题。<strong>不要试图修复</strong>出问题的服务。如果某些服务实例出现了问题，将它找出来，然后把流量定向到其他服务实例（可能是新增的实例）上，然后再诊断问题。这样可以显著提升系统的可用性。</p>
<p>通过<strong>收集度量指标</strong>来了解系统的状态自然不在话下。</p>
<p>不过要注意，如果你对某个度量指标不了解，不知道怎么使用它，或者它对你来说没有什么意义，就不要收集它。因为有时候，这样的度量指标会有数百万个。你在这些无用的度量指标上面浪费了很多资源和时间。这些是无效的负载。</p>
<p>如果你认为你需要某些度量指标，那么就收集它们。如果不需要，就不要收集。</p>
<p>如果你发现了一个问题，不要急着去修复。在很多情况下，<strong>系统会对此作出反应</strong>。当系统需要你采取行动的时候，它会给你发出告警。如果它不要求你在半夜跑去修复问题，那么它就不算是一个告警。它只不过是一种警告，你可以在把它当成一般的问题来处理。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[微服务，够了]]></title>
      <url>http://team.jiunile.com/blog/2017/07/microservice-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%A4%9F%E4%BA%86.html</url>
      <content type="html"><![CDATA[<p><img src="/images/ms/01/cover.png" alt="01"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  资深架构师<a href="https://twitter.com/aadrake" target="_blank" rel="external">Adam Drake</a>在他的博客上分享了他对微服务的看法,他 从自己的经验出发,结合Martin Fowler对微服务的见解,帮助想要采用 微服务的公司重新审视微服务。以下内容已获得作者翻译授权,查看英文 原文 <a href="https://aadrake.com/posts/2017-05-20-enough-with-the-microservices.html" target="_blank" rel="external">Enough with the microservices</a>。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  关于微服务的优势和劣势已经有过太多的讨论,不过我仍然看到很多 成长型初创公司对它进行着盲目崇拜。冒着“重复发明轮子”的风险(Martin Fowler已经写过“Microservice Premium”的文章),我想把我的一些 想法写下来,在必要的时候可以发给客户,也希望能够帮助人们避免犯下我之前见过的那些错误。在进行架构或技术选型时,将网络上找到的一些所谓的最佳实践文章作为指南,一旦做出了错误的决定,就要付出惨重的代价。如果能够帮助哪怕一个公司避免犯下这种错误,那么写这篇文章都是值得的。</p>
<p>  如今微服务是个热门技术,微服务架构一直以来都存在(面向服务架构也算是吧?),但对于我所见过的大部分公司来说,微服务不仅浪费了他们的时间,分散了他们的注意力,而且让事情变得更糟糕。</p>
<p>  这听起来似乎很奇怪,因为大部分关于微服务的文章都会肯定微服务 的各种好处,比如解耦系统、更好的伸缩性、消除开发团队之间的依赖, 等等。如果你的公司有 Uber、Airbnb、Facebook 或 Twitter 那样的规模, 那么就没有什么问题。我曾经帮助一些大型组织转型到微服务架构,包括 搭建消息系统和采用一些能够提升伸缩性的技术。不过,对于成长型初创 公司来说,很少需要这些技术和服务。</p>
<p>  Russ Miles在他的《<a href="http://www.russmiles.com/essais/8-ways-to-lose-at-microservices-adoption" target="_blank" rel="external">让微服务失效的八种方式</a>》这篇文章中表达了 他的首要观点,而在我看来,这些场景却到处可见。成长型初创公司总是 想模仿那些大公司的最佳实践,用它们来弥补自身的不足。但是,最佳实 践是要视情况而定的。有些东西对于 Facebook 来说是最佳实践,但对于 只有不到百人的初创公司来说,它们就不一定也是最佳实践。</p>
<p>  如果你的公司比那些大公司小一些,在一定程度上你仍然能够从微服务架构中获益。但是,对于成长型初创公司来说,大规模地迁移到微服务是一种过错,而且对技术人来说是不公平的。</p>
<a id="more"></a>
<h2 id="为什么选择微服务"><a href="#为什么选择微服务" class="headerlink" title="为什么选择微服务?"></a>为什么选择微服务?</h2><p>  一般来说,成长型初创公司采用微服务架构最主要的目的为了减少或消除开发团队之间的依赖,或者提升系统处理大流量负载的能力(比如伸缩性)。开发人员经常抱怨的问题和常见的症状包括合并冲突、由未完整实现的功能引起的部署错误以及伸缩性问题。接下来让我们逐个说明这些问题。</p>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>  在初创公司的早期阶段,开发团队规模不大,使用的技术也很简单。人们在一起工作,不会出现混乱,要实现一些功能也比较快。一切看起来都很美好。</p>
<p>  随着公司的不断发展,开发团队也在壮大,代码库也在增长,然后就出现了多个团队在同一个代码库上工作的情况。这些团队的大部分成员都是公司早期的员工。因为初创公司的早期员工一般都是初级开发人员,他们并没有意识到一个问题,那就是在团队规模增长和代码库增长的同时,沟通效率也需要随之提升。对于缺乏经验的技术人员来说,他们倾向于通过技术问题来解决人的问题,并希望通过微服务来减少开发团队之间的依赖和耦合。</p>
<p>  实际上,他们真正需要做的是通过有效的沟通来解决人的问题。当一个初创公司有多个开发团队时,团队之间需要协调,团队成员需要知道每个人都在做什么,他们需要协作。在这样规模的企业里,软件开发其实具备了社交的性质。如果团队之间缺乏沟通或者缺乏信息分享,不管用不用微服务,一样存在依赖问题,而就算使用了微服务,也仍然存在负面的技术问题。</p>
<p>  将代码模块化作为解决这个问题的技术方案,确实能够缓解软件开发固有的团队依赖问题,但团队间的沟通仍然要随着团队规模的增长而不断改进。</p>
<p>  不要混淆了解耦和分布式二者的含义。由模块和接口组成的单体可以帮助你达到解耦的目的,而且你也应该这么做。你没有必要把应用程序拆分成分布式的多个独立服务,在模块间定义清晰的接口同样能达到解耦的目的。</p>
<h2 id="部分功能实现"><a href="#部分功能实现" class="headerlink" title="部分功能实现"></a>部分功能实现</h2><p>  微服务里需要用到<a href="https://en.wikipedia.org/wiki/Feature_toggle" target="_blank" rel="external">功能标志</a> (feature flag),微服务开发人员需要熟悉这种技术。特别是在进行快速开发(下面会深入讨论)的时候,你可能需要部署一些功能,这些功能在某些平台上还没有实现,或者前端已经完全实现,但后端还没有。随着公司的发展,部署和运维系统变得越来越自动化和复杂,功能标志也变得越来越重要。</p>
<h2 id="水平伸缩"><a href="#水平伸缩" class="headerlink" title="水平伸缩"></a>水平伸缩</h2><p>  通过部署同一个微服务的多个实例来获得伸缩性,这是微服务的优点 之一。不过,大多数过早采用微服务的公司在这些微服务背后使用了同一 个存储系统。也就是说,这些服务具备了伸缩性,但整个应用并不具备伸 缩性。如果你正打算使用这样的伸缩方式,那为什么不直接在负载均衡器 后面部署多个单体实例呢?你可以用更简单的方式达到相同的目的。再者, 水平伸缩应该被作为杀手锏来使用。你首先要关注的应该是如何提升应用 程序的性能。一些简单的优化常常能带来数百倍的性能提升,这里也包括 如何正确地使用其他服务。例如,我在一篇博文里提到的<a href="https://aadrake.com/posts/2017-05-15-redis-performance-triage-handbook.html" target="_blank" rel="external">Redis性能诊断</a>。</p>
<h2 id="我们为微服务做好准备了吗"><a href="#我们为微服务做好准备了吗" class="headerlink" title="我们为微服务做好准备了吗?"></a>我们为微服务做好准备了吗?</h2><p>  在讨论架构选型时,人们经常会忽略这个问题,但其实却是最重要的。 高级技术人员在了解了开发人员或业务人员的抱怨或痛点之后,在网上找 寻找解决方案,他们总是宣称能解决这些问题。但在这些信誓旦旦的观点 背后,有很多需要注意的地方。微服务有利也有弊。如果你的企业足够成 熟,并且具有一定的技术积累,那么采用微服务所面临的挑战会小很多, 并且能够带来更多正面好处。那么怎样才算已经为微服务做好准备了呢? Martin Fowler在多年前表达了他对<a href="https://martinfowler.com/bliki/MicroservicePrerequisites.html" target="_blank" rel="external">微服务先决条件</a>的看法,但是从我的 经验来看,大多数成长型初创公司完全忽略了他的观点。Martin 的观点 是一个很好的切入点,让我们来逐个说明。</p>
<p>  我敢说,大部分成长型初创公司几乎连一个先决条件都无法满足,更不用说满足所有的条件了。如果你的技术团队不具备快速配置、部署和监控能力,那么在迁移到微服务前必须先获得这些能力。接下来让我们更详细地讨论这些先决条件。</p>
<h2 id="快速配置"><a href="#快速配置" class="headerlink" title="快速配置"></a>快速配置</h2><p>  如果你的开发团队里只有少数几个人可以配置新服务、虚拟环境或其 他配套设施,那说明你们还没有为微服务做好准备。你的每个团队里都应 该要有几个这样的人,他们具备了配置基础设施和部署服务的能力,而且 不需要求助于外部。要注意,光是有一个 DevOps 团队并不意味着你在实 施 DevOps。开发人员应该参与管理与应用程序相关的组件,包括基础设施。</p>
<p>  类似的,如果你没有灵活的基础设施(易于伸缩并且可以由团队里的不同人员来管理)来支撑当前的架构,那么在迁移到微服务前必须先解决这个问题。你当然可以在裸机上运行微服务,以更低的成本获得出众的性能,但在服务的运维和部署方面也必须具备灵活性。</p>
<h2 id="基本的监控"><a href="#基本的监控" class="headerlink" title="基本的监控"></a>基本的监控</h2><p>  如果你不曾对你的单体应用进行过性能监控,那么在迁移到微服务时, 你的日子会很难过。你需要熟悉系统级别的度量指标(比如 CPU 和内存)、 应用级别的度量指标(比如端点的请求延迟或端点的错误)和业务级别的 度量指标(比如每秒事务数或每秒收益),这样才可以更好地理解系统的 性能。在性能方面,微服务生态系统比单体系统要复杂得多,就更不用提 诊断问题的复杂性了。你可以搭建一个监控系统(如 Prometheus),在 将单体应用拆分成微服务之前对应用做一些增强,以便进行监控。</p>
<h2 id="快速部署"><a href="#快速部署" class="headerlink" title="快速部署"></a>快速部署</h2><p>  如果你的单体系统没有一个很好的持续集成流程和部署系统,那么要 集成和部署好你的微服务几乎是件不可能的事。想象一下这样的场景:10 个团队和 100 个服务,它们都需要进行手动测试和部署,然后再将这些工 作与测试和部署一个单体所需要的工作进行对比。100 个服务会出现多少 种问题?而单体系统呢?这些先决条件很好地说明了微服务的复杂性。</p>
<p>  Phil Calcado在Fowler的先决条件清单里添加了一些东西,不过我 认为它们更像是重要的扩展,而不是真正的先决条件。</p>
<h2 id="如果我们具备了这些先决条件呢"><a href="#如果我们具备了这些先决条件呢" class="headerlink" title="如果我们具备了这些先决条件呢?"></a>如果我们具备了这些先决条件呢?</h2><p>  就算具备了这些条件,仍然需要注意微服务的负面因素,确保微服务能够为你的业务带来真正的价值。事实上,很多技术人员对微服务中存在的<a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="external">分布式计算谬论</a>视而不见,但为了确保能够成功,这些问题是必须要考虑到的。对于大部分成长型初创公司来说,基于各种原因,他们应该避免使用微服务。</p>
<h2 id="运营成本的增加"><a href="#运营成本的增加" class="headerlink" title="运营成本的增加"></a>运营成本的增加</h2><p>  快速部署这一先决条件已经涵盖了一部分成本,除此之外,对 微服务进行容器化(可能使用 Docker)和使用容器编排系统(比如 Kubernetes)也需要耗费很多成本。Docker 和 Kubernetes 都是很优秀的 技术,但是对于大部分成长型初创公司来说,它们都是一种负担。我见过 初创公司使用 rsync 作为部署和编排工具,我也见过很多的初创公司陷入 运维工具的复杂性泥潭里,他们因此浪费了很多时间,而这些时间本来可 以用于为用户开发更多的功能。</p>
<h2 id="你的应用会被拖慢"><a href="#你的应用会被拖慢" class="headerlink" title="你的应用会被拖慢"></a>你的应用会被拖慢</h2><p>  如果你的单体系统里包含了多个模块,并且在模块间定义了良好的 API,那么 API 之间的交互就几乎没有什么额外开销。但对于微服务来说 就不是这么一回事了,因为它们一般运行在不同的机器上,它们之间需要 通过网络进行交互。这样会在一定程度上拖慢整个系统。如果一个请求需 要多个服务进行同步交互,那么情况会变得更加糟糕。我曾经工作过的一 个公司,他们需要调用将近 10 个服务才能处理完某些请求。处理请求的 每一个步骤都需要额外的网络开销和延迟,但实际上,他们可以把这些服 务放在单个软件包里,按照不同的模块来区分,或者把它们设计成异步的。这样可以为他们节省大量的基础设施成本。</p>
<h2 id="本地开发变得更加困难"><a href="#本地开发变得更加困难" class="headerlink" title="本地开发变得更加困难"></a>本地开发变得更加困难</h2><p>  如果你有一个单体应用,后端只有一个数据库,那么在开发过程中, 在本地运行这个应用是很容易的。如果你有 100 个服务,并使用了多个数 据存储系统,而且它们之间互相依赖,那么本地开发就会变成一个噩梦。即使是 Docker 也无法把你从这种复杂性泥潭中拯救出来。虽然事情原本 可以简单一些,不过仍然需要处理依赖问题。理论上说,微服务不存在这 些问题,因为微服务被认为是相互独立的。不过,对于成长型初创公司来说,就不是这么一回事了。技术人员一般需要在本地运行所有(或者几乎 所有)的服务才能进行新功能的开发和测试。这种复杂性是对资源的巨大浪费。</p>
<h2 id="难以伸缩"><a href="#难以伸缩" class="headerlink" title="难以伸缩"></a>难以伸缩</h2><p>  对单体系统进行伸缩的最简单方式是在负载均衡器后面部署单体系 统的多个实例。在流量增长的情况下,这是一种非常简单的伸缩方式, 而且从运维角度来讲,它的复杂性是最低的。你的系统在编排平台(如 <a href="https://aws.amazon.com/cn/elasticbeanstalk/" target="_blank" rel="external">Elastic Beanstalk</a>)上运行的时间越长越好,你和你的团队就可以集中 精力开发客户需要的东西,而不是忙于解决部署管道问题。使用合适的 CI/CD 系统可以缓解这个问题,但在微服务生态系统里,事情要复杂得多, 而且这些复杂性所造成的麻烦已经超过了它们所能带来的好处。</p>
<h2 id="然后呢"><a href="#然后呢" class="headerlink" title="然后呢?"></a>然后呢?</h2><p>  如果你刚好身处一个成长型初创公司里,需要对架构做一些调整,而微服务似乎不能解决你的问题,这个时候应该怎么办?</p>
<p>  Fowler 提出的先决条件可以说是技术领域的<a href="https://en.wikipedia.org/wiki/Capability_Maturity_Model" target="_blank" rel="external">能力成熟度模型</a>, Fowler 在他的文章里对成熟度模型进行过介绍。如果这种成熟度模型对于公司来说是说得通的,那么我们可以按照 Fowler 提出的先决条件,并使用其他的一些中间步骤为向微服务迁移做好准备。下面的内容引用自 Fowler 的文章。</p>
<p>  关键是你要认识到,成熟度模型的评估结果并不代表你的当前水平,它们只是在告诉你需要做哪些工作才能朝着改进的目标前进。你当前的水平只是一种中间工作,用于确定下一步该获得什么样的技能。</p>
<p>  那么,我们该做出怎样的改进,以及如何达成这些目标?我们需要经过一些简单的步骤,其中前面两步就可以解决很多在向微服务迁移过程中会出现的问题,而且不会带来相关的复杂性。</p>
<ul>
<li>清理应用程序。确保应用程序具有良好的自动化测试套件,并使 用了最新版本的软件包、框架和编程语言。</li>
<li>重构应用程序,把它拆分成多个模块,为模块定义清晰的API。不 要让外部代码直接触及模块内部,所有的交互都应该通过模块提 供的API来进行。</li>
<li>从应用程序中选择一个模块,并把它拆分成独立的应用程序,部 署在相同的主机上。你可以从中获得一些好处,而不会带来太多 的运维麻烦。不过,你仍然需要解决这两个应用之间的交互问 题,虽然它们都部署在同一个主机上。不过你可以无视微服务架 构里固有的网络分区问题和分布式系统的可用性问题。</li>
<li>把独立出来的模块移动到不同的主机上。现在,你需要处理跨网 络交互问题,不过这样可以让这两个系统之间的耦合降得更低。</li>
<li>如果有可能,可以重构数据存储系统,让另一个主机上的模块负 责自己的数据存储。</li>
</ul>
<p>在我所见过的公司里,如果他们能够完成前面两个步骤就算万事大吉了。如果他们能够完成前面两个步骤,那么剩下的步骤一般不会像他们最初想象的那么重要了。如果你决定在这个过程的某个点上停下来,而系统仍然具有可维护性和比刚开始时更好的状态,那么就再好不过了。</p>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>  我不能说这些想法都是独一无二的,也不能说是我所独有的。我只是 从其他遭遇了相同问题的人那里收集想法,并连同观察到的现象在这里作 了一次总结。还有其他很多比我更有经验的人也写过这方面的文章,他们 剖析地更加深入,比如Sander Mak写的有关模块和<a href="https://www.oreilly.com/ideas/modules-vs-microservices" target="_blank" rel="external">微服务的文章</a>。不管 怎样,对于正在考虑对他们的未来架构做出调整的公司来说,这些经验都 是非常重要的。认真地思考每一个问题,确保微服务对你们的组织来说是 一个正确的选择。</p>
<p>  最起码在完成了上述的前面两个步骤之后,再慎重考虑一下微服务对于你的组织来说是否是正确的方向。你之前的很多问题可能会迎刃而解。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL性能监控慢日志分析利器]]></title>
      <url>http://team.jiunile.com/blog/2017/07/mysql-mysql-performance-monitoring.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>入题之前先讲讲为什么写这篇文章，这就不得不提起MySQL与percona，阿里基于mysql开发了AliSQL，写这篇文章的时候阿里已经将其开源，percona是一家领先的MySQL咨询公司，该公司基于mysql开发了Percona Server，Percona Server是一款独立的数据库产品，为用户提供了换出其MySQL安装并换入Percona Server产品的能力。percona除了开发了多款数据库产品，还开发了数据库监控程序：pmm（Percona Monitoring and Management）服务器，我们都知道mysql自身缺乏实时的监控功能，而此时pmm-server就恰好解决了我们这一难题，好了废话不多说，先看一张pmm server的监控图。<br><img src="/images/mysql_pmm_1.png" alt="PMM"></p>
<p>常规的监测项目都有了，最吸引我的一点在于它的慢日志分析功能，如下图所示：<br><img src="/images/mysql_pmm_2.png" alt="PMM"></p>
<a id="more"></a>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="1-在vmware或者virtualbox上安装ubuntu14-04-Server镜像，可以选择清华大学的镜像，下载速度快"><a href="#1-在vmware或者virtualbox上安装ubuntu14-04-Server镜像，可以选择清华大学的镜像，下载速度快" class="headerlink" title="1. 在vmware或者virtualbox上安装ubuntu14.04 Server镜像，可以选择清华大学的镜像，下载速度快"></a>1. 在vmware或者virtualbox上安装ubuntu14.04 Server镜像，可以选择清华大学的镜像，下载速度快</h3><h3 id="2-系统装完后接下来就要在ubuntu上安装docker了，执行命令："><a href="#2-系统装完后接下来就要在ubuntu上安装docker了，执行命令：" class="headerlink" title="2. 系统装完后接下来就要在ubuntu上安装docker了，执行命令："></a>2. 系统装完后接下来就要在ubuntu上安装docker了，执行命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="_">-s</span>SL https://get.daocloud.io/docker | sh</span><br></pre></td></tr></table></figure>
<p>等待完成即可，这是一种安装docker比较快的方式，而且安装的docker版本也比较高，安装完成后输入docker -v看到下面信息说明安装完成：<br><code>Docker version 17.04.0-ce, build 4845c56</code></p>
<h3 id="3-安装完docker，接下来就需要下载pmm-server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像："><a href="#3-安装完docker，接下来就需要下载pmm-server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像：" class="headerlink" title="3. 安装完docker，接下来就需要下载pmm server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像："></a>3. 安装完docker，接下来就需要下载pmm server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像：</h3><p>在 Docker 的启动参数中加入:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--registry-mirror=https://docker.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure></p>
<p>Ubuntu 用户（包括使用 systemd 的 Ubuntu 15.04）可以修改 /etc/default/docker 文件，加入如下参数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_OPTS=<span class="string">"--registry-mirror=https://docker.mirrors.ustc.edu.cn"</span></span><br></pre></td></tr></table></figure></p>
<p>其他 systemd 用户可以通过执行 sudo systemctl edit docker.service 来修改设置, 覆盖默认的启动参数:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/docker <span class="_">-d</span> -H fd:// --registry-mirror=https://docker.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure></p>
<h3 id="4-接下来下载pmm镜像的速度就会大大提升，执行下面命令："><a href="#4-接下来下载pmm镜像的速度就会大大提升，执行下面命令：" class="headerlink" title="4. 接下来下载pmm镜像的速度就会大大提升，执行下面命令："></a>4. 接下来下载pmm镜像的速度就会大大提升，执行下面命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull percona/pmm-server:1.2.0</span><br></pre></td></tr></table></figure>
<p>然后等待完成即可。</p>
<h3 id="5-创建PMM-数据容器："><a href="#5-创建PMM-数据容器：" class="headerlink" title="5. 创建PMM 数据容器："></a>5. 创建PMM 数据容器：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker create \</span><br><span class="line">   -v /opt/prometheus/data \</span><br><span class="line">   -v /opt/consul-data \</span><br><span class="line">   -v /var/lib/mysql \</span><br><span class="line">   -v /var/lib/grafana \</span><br><span class="line">   --name pmm-data \</span><br><span class="line">   percona/pmm-server:1.2.0 /bin/<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="6-运行PMM-server容器"><a href="#6-运行PMM-server容器" class="headerlink" title="6. 运行PMM server容器:"></a>6. 运行PMM server容器:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="_">-d</span> \</span><br><span class="line">   -p 80:80 \</span><br><span class="line">   --volumes-from pmm-data \</span><br><span class="line">   --name pmm-server \</span><br><span class="line">   --restart always \</span><br><span class="line">   percona/pmm-server:1.2.0</span><br></pre></td></tr></table></figure>
<h3 id="7-安装PMM客户端："><a href="#7-安装PMM客户端：" class="headerlink" title="7. 安装PMM客户端："></a>7. 安装PMM客户端：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh https://www.percona.com/downloads/pmm-client/pmm-client-1.2.0/binary/redhat/7/x86_64/pmm-client-1.2.0-1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<h3 id="8-连接PMM服务器："><a href="#8-连接PMM服务器：" class="headerlink" title="8. 连接PMM服务器："></a>8. 连接PMM服务器：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmm-admin config --server pmm服务器主机地址</span><br></pre></td></tr></table></figure>
<h3 id="9-配置mysql监控："><a href="#9-配置mysql监控：" class="headerlink" title="9. 配置mysql监控："></a>9. 配置mysql监控：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmm-admin add mysql --user root --password 123456 --host mysql ip地址 --port 3306 instace3306</span><br></pre></td></tr></table></figure>
<p><code>注：pmm-client收的监控数据来源有这么几方面</code></p>
<ul>
<li>MySQL所在机器的系统指标</li>
<li>MySQL的performance_schema库</li>
<li>slow-log(慢查询日志–mysql要开启慢日志功能)</li>
</ul>
<p><code>如果我们想收集a和c中的指标的话，最好还是将pmm-client部署在MySQL所在机器</code></p>
<h3 id="10-至此访问pmm服务器ip地址即可查看接口"><a href="#10-至此访问pmm服务器ip地址即可查看接口" class="headerlink" title="10. 至此访问pmm服务器ip地址即可查看接口"></a>10. 至此访问pmm服务器ip地址即可查看接口</h3>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL too many connection 问题分析]]></title>
      <url>http://team.jiunile.com/blog/2016/09/mysql-mysql-sleep.html</url>
      <content type="html"><![CDATA[<h2 id="问题缘由"><a href="#问题缘由" class="headerlink" title="问题缘由"></a>问题缘由</h2><p>线上一个网站在运行一段时间后，页面打开速度变慢随之出现<code>502 bad gateway</code>的错误。</p>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>由<code>502 bad gateway</code>的错误，我们知道就是nginx链接后端的php，而php程序没有及时返回，造成了超时。<br>那么造成php超时的原因也有多种，比如：</p>
<ol>
<li>php-fpm资源消耗光</li>
<li>调用外部资源超时，比如外部的web service、数据库等等</li>
<li>….</li>
</ol>
<p>出现问题，我们先登录服务器查看相关日志。</p>
<p>结合我们的业务，首先想到了mysql数据库，先查看了一下mysql数据库的状态，通过show full processlist命令发现有大量的链接处于sleep状态。</p>
<p><code>sleep</code>状态的意思就是说，某个客户端一直占着这个链接，但是什么事也不干，或者是客户端压根儿就已经断开了，而服务端却不知道。</p>
<p>我们知道，mysql的连接数是有限制的，比如默认是151个，那么当大量的链接处于sleep状态时，php程序就无法同mysql建立链接，就会发生超时现象。</p>
<a id="more"></a>
<p>那么造成sleep的原因，有三个，下面是mysql手册给出的解释：</p>
<ol>
<li>客户端程序在退出之前没有调用<code>mysql_close()</code>。[写程序的疏忽，或者数据库的db类库没有自动关闭每次的连接。。。]</li>
<li>客户端sleep的时间在<code>wait_timeout</code>或<code>interactive_timeout</code>规定的秒内没有发出任何请求到服务器. [类似常连，类似于不完整的tcp ip协议构造，服务端一直认为客户端仍然存在（有可能客户端已经断掉了）]</li>
<li>客户端程序在结束之前向服务器发送了请求还没得到返回结果就结束掉了。 [参看：tcp ip协议的三次握手]</li>
</ol>
<p>那么知道了问题所在，就要找到是什么原因导致的sleep线程的存在，</p>
<p>通过上面的信息，我们知道是 192.168.1.2这个IP的20318端口和mysql建立的链接，而192.168.1.2正是我们的web服务器，</p>
<p>于是ssh登录服务器，通过<code>netstat -tunp</code>找到端口20318所对应的进程和pid，一看就是php-fpm引起的。</p>
<p>下面就是要看一下这个php-fpm是调用的哪一个php文件，找到了具体的php文件就好办了。</p>
<p>具体可以通过<code>lsof</code>列出这个<code>pid</code>打开的文件，也可以通过<code>strace</code>跟踪进程的系统调用。</p>
<p>下面是lsof的部分输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lsof -p 23018</span></span><br><span class="line">COMMAND   PID   USER   FD   TYPE     DEVICE     SIZE       NODE NAME</span><br><span class="line">php-fpm 15687 daemon  cwd    DIR      104,2     4096   69437193 /xxx/daemon.php</span><br><span class="line">php-fpm 15687 daemon  rtd    DIR      104,6     4096          2 /</span><br><span class="line">php-fpm 15687 daemon  txt    REG      104,5 27714205    3466635 /app/php/sbin/php-fpm</span><br></pre></td></tr></table></figure></p>
<p>从中可以看到，是我们的<code>daemon.php</code>引起的，这个程序是我们向ios设备推送通知的程序，其中要跟苹果（Apple）的服务器建立链接，可能是苹果服务器不稳定,超时引起的。</p>
<p>程序大致流程：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql_connect();  <span class="comment">//建立链接</span></span><br><span class="line"></span><br><span class="line">$queryBid = mysql_query(<span class="string">"select bundleId from apps"</span>);</span><br><span class="line"><span class="keyword">while</span> ($row = mysql_fetch_assoc($queryBid)) &#123;</span><br><span class="line">	<span class="comment">//取出通知内容，连接苹果服务器，进行推送</span></span><br><span class="line">	<span class="comment">//这里有时候会花一二十分钟，此时mysql服务器那里的连接一直是sleep状态</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mysql_close();</span><br></pre></td></tr></table></figure></p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>方案一：修改的思路也很简单，我们先通过mysql把数据取出来，之后马上关掉mysql连接，释放mysql资源，剩下的就慢慢干好了。 修改后的程序是这样的：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql_connect();  <span class="comment">//建立链接</span></span><br><span class="line"></span><br><span class="line">$arr_bid=<span class="keyword">array</span>();</span><br><span class="line">$queryBid = mysql_query(<span class="string">"select bundleId from apps"</span>);</span><br><span class="line"><span class="keyword">while</span>($row = mysql_fetch_assoc($queryBid)) &#123;</span><br><span class="line">	$arr_bid[] = $row;</span><br><span class="line">&#125;</span><br><span class="line">mysql_close(); <span class="comment">//从mysql中取完数据就马上关闭连接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">foreach</span>($arr_bid <span class="keyword">as</span> $row)&#123;</span><br><span class="line">	<span class="comment">//取出通知内容，连接苹果服务器，进行推送</span></span><br><span class="line">	<span class="comment">//这里有时候会花一二十分钟</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>方案二：[不治本，有弊端]<br>写一个定时脚本，每分钟检查下mysql连接数，超过sleep时间的自动kill掉<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span>  </span><br><span class="line">define(<span class="string">'MAX_SLEEP_TIME'</span>， <span class="number">120</span>);  </span><br><span class="line">  </span><br><span class="line">$hostname = <span class="string">"localhost"</span>;  </span><br><span class="line">$username = <span class="string">"root"</span>;  </span><br><span class="line">$password = <span class="string">"password"</span>;  </span><br><span class="line">  </span><br><span class="line">$connect = mysql_connect($hostname， $username， $password);  </span><br><span class="line">$result = mysql_query(<span class="string">"SHOW PROCESSLIST"</span>， $connect);  </span><br><span class="line"><span class="keyword">while</span> ($proc = mysql_fetch_assoc($result)) &#123;  </span><br><span class="line">	<span class="keyword">if</span> ($proc[<span class="string">"Command"</span>] == <span class="string">"Sleep"</span> &amp;&amp; $proc[<span class="string">"Time"</span>] &gt; MAX_SLEEP_TIME) &#123;  </span><br><span class="line">	@mysql_query(<span class="string">"KILL "</span> . $proc[<span class="string">"Id"</span>]， $connect);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br><span class="line">mysql_close($connect);</span><br></pre></td></tr></table></figure></p>
<p>加入到crontab定时计划里<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * php /usr/<span class="built_in">local</span>/sbin/<span class="built_in">kill</span>-mysql-sleep-proc.php</span><br></pre></td></tr></table></figure></p>
<p>方案三：[不推荐]<br>修改mysql配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="built_in">wait</span>_timeout=10</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">mysql&gt; <span class="built_in">set</span> global <span class="built_in">wait</span>_timeout=10;</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MaxScale 在 slave 有故障后如何处理？]]></title>
      <url>http://team.jiunile.com/blog/2016/08/mysql-maxscale-02.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前介绍了 <a href="/blog/2016/08/maxscale-01.html">MaxScale</a> 可以实现 Mysql 的读写分离和读负载均衡，那么当 slave 出现故障后，MaxScale 会如何处理呢？</p>
<p>例如有 3 台数据库服务器，一主二从的结构，数据库名称分别为 master, slave1, slave2</p>
<p>现在我们实验以下两种情况：</p>
<ol>
<li>当一台从服务器（ slave1 或者 slave2 ）出现故障后，查看 MaxScale 如何应对，及故障服务器重新上线后的情况</li>
<li>当两台从服务器（ slave1 和 slave2 ）都出现故障后，查看 MaxScale 如何应对，及故障服务器重新上线后的情况</li>
</ol>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>为了更深入的查看 MaxScale 的状态，需要把 MaxScale 的日志打开</p>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [maxscale] 部分，这里用来进行全局设置，在其中添加日志配置<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log_info</span>=<span class="number">1</span></span><br><span class="line"><span class="attr">logdir</span>=/tmp/</span><br></pre></td></tr></table></figure></p>
<p>通过开启 log_info 级别，可以看到 MaxScale 的路由日志</p>
<p>修改配置后，重启 MaxScale </p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><a id="more"></a>
<h3 id="单个-slave-故障的情况"><a href="#单个-slave-故障的情况" class="headerlink" title="单个 slave 故障的情况"></a>单个 slave 故障的情况</h3><p>初始状态是一切正常<br><img src="/images/maxscale_08.png" alt="Maxscale"></p>
<p>停掉 slave2 的复制，登录 slave2 的 mysql 执行<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; stop slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_09.png" alt="Maxscale"></p>
<p>slave2 已经失效了</p>
<p>查看日志信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/maxscale1.log</span><br><span class="line"><span class="comment">#尾部显示：</span></span><br><span class="line">2016-08-15 12:26:02   notice : Server changed state: slave2[172.17.0.4:3306]: lost_slave</span><br></pre></td></tr></table></figure></p>
<p>提示 slave2 已经丢失</p>
<p>查看客户端查询结果<br><img src="/images/maxscale_10.png" alt="Maxscale"></p>
<p>查询操作全都转到了 <code>slave1</code></p>
<p>可以看到， 在有 slave 故障后，MaxScale 会自动进行排除，不再向其转发请求</p>
<p>下面看下 slave2 再次<strong>上线后的情况</strong></p>
<p>登录 slave2 的 mysql 执行<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_11.png" alt="Maxscale"></p>
<p>恢复了正常状态，重新识别到了 slave2</p>
<p>查看日志信息，显示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-08-15 12:32:36   notice : Server changed state: slave2[172.17.0.4:3306]: new_slave</span><br></pre></td></tr></table></figure></p>
<p>查看客户端查询结果<br><img src="/images/maxscale_12.png" alt="Maxscale"></p>
<p>slave2 又可以正常接受查询请求</p>
<p>通过实验可以看到，在部分 slave 发生故障时，MaxScale 可以自动识别出来，并移除路由列表，当故障恢复重新上线后，MaxScale 也能自动将其加入路由，过程透明</p>
<h3 id="全部-slave-故障的情况"><a href="#全部-slave-故障的情况" class="headerlink" title="全部 slave 故障的情况"></a>全部 slave 故障的情况</h3><p>分别登陆 slave1 和 slave2 的 mysql，执行停止复制的命令<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; stop slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_13.png" alt="Maxscale"></p>
<p>发现各个服务器的角色都识别不出来了</p>
<p>查看日志<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2016-08-15 12:44:11   notice : Server changed state: master[172.17.0.2:3306]: lost_master</span><br><span class="line">2016-08-15 12:44:11   notice : Server changed state: slave1[172.17.0.3:3306]: lost_slave</span><br><span class="line">2016-08-15 12:44:11   notice : Server changed state: slave2[172.17.0.4:3306]: lost_slave</span><br><span class="line">2016-08-15 12:44:11   error  : No Master can be determined. Last known was 172.17.0.2:3306</span><br></pre></td></tr></table></figure></p>
<p>从日志中看到，MaxScale 发现2个slave 和 master 都丢了，然后报错：没有 master 了</p>
<p>客户端连接 MaxScale 时也失败了<br><img src="/images/maxscale_14.png" alt="Maxscale"></p>
<p>说明从服务器全部失效后，会导致 master 也无法识别，使整个数据库服务都失效了</p>
<p>对于 slave 全部失效的情况，能否让 master 还可用？这样至少可以正常提供数据库服务</p>
<p>这需要修改 MaxScale 的配置，告诉 MaxScale 我们需要一个稳定的 master</p>
<h4 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h4><p>先恢复两个 slave，让集群回到正常状态，登陆两个 slave 的mysql<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure></p>
<p>修改 MaxScale 配置文件，添加新的配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [MySQL Monitor] 部分，添加：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">detect_stale_master</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>保存退出，然后重启 MaxScale</p>
<h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>停掉两台 slave ，查看 MaxScale 服务器状态<br><img src="/images/maxscale_15.png" alt="Maxscale"></p>
<p>可以看到，虽然 slave 都无法识别了，但 master 还在，并提示处于稳定状态</p>
<p>客户端执行请求<br><img src="/images/maxscale_16.png" alt="Maxscale"></p>
<p>客户端可以连接 MaxScale，而且请求都转到了 master 上，说明 slave 全部失效时，由 master 支撑了全部请求</p>
<p>当恢复两个 slave 后，整体状态自动恢复正常，从客户端执行请求时，又可以转到 slave 上<br><img src="/images/maxscale_17.png" alt="Maxscale"></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>通过测试发现，在部分 slave 故障情况下，对于客户端是完全透明的，当全部 slave 故障时，经过简单的配置，MaxScale 也可以很好的处理</p>
<hr>
<p>来源：杜亦舒</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Mysql 读写分离中间件 MaxScale]]></title>
      <url>http://team.jiunile.com/blog/2016/08/mysql-maxscale-01.html</url>
      <content type="html"><![CDATA[<h2 id="MaxScale-是干什么的？"><a href="#MaxScale-是干什么的？" class="headerlink" title="MaxScale 是干什么的？"></a>MaxScale 是干什么的？</h2><p>配置好了 Mysql 的主从复制结构后，我们希望实现读写分离，把读操作分散到从服务器中，并且对多个从服务器能实现负载均衡</p>
<p><strong>读写分离</strong>和<strong>负载均衡</strong>是 Mysql 集群的基础需求，<strong>MaxScale</strong> 就可以帮着我们方便的实现这些功能<br><img src="/images/maxscale_01.png" alt="Maxscale"></p>
<h2 id="MaxScale-的基础构成"><a href="#MaxScale-的基础构成" class="headerlink" title="MaxScale 的基础构成"></a>MaxScale 的基础构成</h2><ul>
<li>MaxScale 是 Mysql 的兄弟公司 MariaDB 开发的，现在已经发展得非常成熟</li>
<li>MaxScale 是插件式结构，允许用户开发适合自己的插件</li>
<li>MaxScale 目前提供的插件功能分为<strong>5类</strong></li>
</ul>
<h3 id="认证插件"><a href="#认证插件" class="headerlink" title="认证插件"></a>认证插件</h3><p>提供了登录认证功能，MaxScale 会读取并缓存数据库中 user 表中的信息，当有连接进来时，先从缓存信息中进行验证，如果没有此用户，会从后端数据库中更新信息，再次进行验证</p>
<h3 id="协议插件"><a href="#协议插件" class="headerlink" title="协议插件"></a>协议插件</h3><p>包括客户端连接协议，和连接数据库的协议</p>
<h3 id="路由插件"><a href="#路由插件" class="headerlink" title="路由插件"></a>路由插件</h3><p>决定如何把客户端的请求转发给后端数据库服务器，读写分离和负载均衡的功能就是由这个模块实现的</p>
<h3 id="监控插件"><a href="#监控插件" class="headerlink" title="监控插件"></a>监控插件</h3><p>对各个数据库服务器进行监控，例如发现某个数据库服务器响应很慢，那么就不向其转发请求了</p>
<h3 id="日志和过滤插件"><a href="#日志和过滤插件" class="headerlink" title="日志和过滤插件"></a>日志和过滤插件</h3><p>提供简单的数据库防火墙功能，可以对SQL进行过滤和容错</p>
<a id="more"></a>
<h2 id="MaxScale-的安装使用"><a href="#MaxScale-的安装使用" class="headerlink" title="MaxScale 的安装使用"></a>MaxScale 的安装使用</h2><p>例如有 3 台数据库服务器，是一主二从的结构</p>
<h3 id="过程概述"><a href="#过程概述" class="headerlink" title="过程概述"></a>过程概述</h3><ol>
<li>配置好集群环境</li>
<li>下载安装 MaxScale</li>
<li>配置 MaxScale，添加各数据库信息</li>
<li>启动 MaxScale，查看是否正确连接数据库</li>
<li>客户端连接 MaxScale，进行测试</li>
</ol>
<h3 id="详细过程"><a href="#详细过程" class="headerlink" title="详细过程"></a>详细过程</h3><h4 id="配置一主二从的集群环境"><a href="#配置一主二从的集群环境" class="headerlink" title="配置一主二从的集群环境"></a>配置一主二从的集群环境</h4><p>准备3台服务器，安装 Mysql，配置一主二从的复制结构<br>主从复制的配置过程略过</p>
<h4 id="安装-MaxScale"><a href="#安装-MaxScale" class="headerlink" title="安装 MaxScale"></a>安装 MaxScale</h4><p>最好在另一台服务器上安装，如果资源不足，可以和某个 Mysql 放在一起</p>
<p>MaxScale 的下载地址<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://downloads.mariadb.com/files/MaxScale</span><br></pre></td></tr></table></figure></p>
<p>根据自己的服务器选择合适的安装包</p>
<p>以 centos 7 为例 安装步骤如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install libaio.x86_64 libaio-devel.x86_64 novacom-server.x86_64 libedit -y</span><br><span class="line">rpm -ivh maxscale-1.4.3-1.centos.7.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<h4 id="配置-MaxScale"><a href="#配置-MaxScale" class="headerlink" title="配置 MaxScale"></a>配置 MaxScale</h4><p>在开始配置之前，需要在 master 中为 MaxScale 创建两个用户，用于监控模块和路由模块</p>
<p>创建监控用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user scalemon@'%' identified by "111111";</span><br><span class="line">mysql&gt; grant replication slave, replication client on *.* to scalemon@'%';</span><br></pre></td></tr></table></figure></p>
<p>创建路由用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user maxscale@'%' identified by "111111";</span><br><span class="line">mysql&gt; grant select on mysql.* to maxscale@'%';</span><br></pre></td></tr></table></figure></p>
<p>用户创建完成后，开始配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [server1] 部分，修改其中的 address 和 port，指向 master 的 IP 和端口</p>
<p>复制2次 [server1] 的整块儿内容，改为 [server2] 与 [server3]，同样修改其中的 address 和 port，分别指向 slave1 和 slave2<br><img src="/images/maxscale_02.png" alt="Maxscale"></p>
<p>找到 <code>[MySQL Monitor]</code> 部分，修改 servers 为 server1,server2,server3，修改 user 和 passwd 为之前创建的监控用户的信息（scalemon,111111）<br><img src="/images/maxscale_03.png" alt="Maxscale"></p>
<p>找到 <code>[Read-Write Service]</code> 部分，修改 servers 为 server1,server2,server3，修改 user 和 passwd 为之前创建的路由用户的信息（maxscale,111111）<br><img src="/images/maxscale_04.png" alt="Maxscale"></p>
<p>由于我们使用了 <code>[Read-Write Service]</code>，需要删除另一个服务 <code>[Read-Only Service]</code>，删除其整块儿内容即可</p>
<p>配置完成，保存并退出编辑器</p>
<h4 id="启动-MaxScale"><a href="#启动-MaxScale" class="headerlink" title="启动 MaxScale"></a>启动 MaxScale</h4><p>执行启动命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maxscale --config=/etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 的响应端口是否已经就绪<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ntelp</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/maxscale_05.png" alt="Maxscale"></p>
<ul>
<li>4006 是连接 MaxScale 时使用的端口</li>
<li>6603 是 MaxScale 管理器的端口</li>
</ul>
<p>登录 MaxScale 管理器，查看一下数据库连接状态，默认的用户名和密码是 admin/mariadb<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maxadmin --user=admin --password=mariadb</span><br><span class="line">MaxScale&gt; list servers</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/maxscale_06.png" alt="Maxscale"></p>
<p>可以看到，MaxScale 已经连接到了 master 和 slave</p>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>先在 master 上创建一个测试用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant ALL PRIVILEGES on *.* to rtest@"%" Identified by "111111";</span><br></pre></td></tr></table></figure></p>
<p>使用 Mysql 客户端到连接 MaxScale<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h MaxScale所在的IP -P 4006 -u rtest -p111111</span><br></pre></td></tr></table></figure></p>
<p>执行查看数据库服务器名的操作来知道当前实际所在的数据库<br><img src="/images/maxscale_07.png" alt="Maxscale"></p>
<p>开启事务后，就自动路由到了 master，普通的查询操作，是在 slave上</p>
<p>MaxScale 的配置完成了</p>
<hr>
<p>来源：杜亦舒</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[双机高可用-负载均衡-MySQL(读写分离、主从自动切换)架构设计]]></title>
      <url>http://team.jiunile.com/blog/2016/08/ha-load-balance-mysql-master-slave-architecture.html</url>
      <content type="html"><![CDATA[<h2 id="架构简介"><a href="#架构简介" class="headerlink" title="架构简介"></a>架构简介</h2><p>只有两台机器，需要实现其中一台死机之后另一台能接管这台机器的服务，并且在两台机器正常服务时，两台机器都能用上。如何设计这样的架构场景。<br><img src="/images/high_availability.png" alt="High Availability"></p>
<p>此架构主要是由keepalived实现双机高可用，维护了一个外网VIP，一个内网VIP。正常情况时，外网VIP和内网VIP都绑定在server1服务器，web请求发送到server1的nginx，nginx对于静态资源请求就直接在本机检索并返回，对于php的动态请求，则负载均衡到server1和server2。对于SQL请求，会将此类请求发送到Atlas MySQL中间件，Atlas接收到请求之后，把涉及写操作的请求发送到内网VIP，读请求操作发送到mysql从，这样就实现了读写分离。</p>
<p>当主服务器server1宕机时，keepalived检测到后，立即把外网VIP和内网VIP绑定到server2，并把server2的mysql切换成主库。此时由于外网VIP已经转移到了server2，web请求将发送给server2的nginx。nginx检测到server1宕机，不再把请求转发到server1的php-fpm。之后的sql请求照常发送给本地的atlas，atlas把写操作发送给内网VIP，读操作发送给mysql从，由于内网VIP已经绑定到server2了，server2的mysql同时接受写操作和读操作。</p>
<p>当主服务器server1恢复后，server1的mysql自动设置为从，与server2的mysql主同步。keepalived不抢占server2的VIP，继续正常服务。</p>
<h2 id="架构要求"><a href="#架构要求" class="headerlink" title="架构要求"></a>架构要求</h2><p>要实现此架构，需要三个条件：</p>
<ol>
<li>服务器可以设置内网IP，并且设置的内网IP互通。</li>
<li>服务器可以随意绑定IDC分配给我们使用的外网IP，即外网IP没有绑定MAC地址。</li>
<li>MySQL服务器支持GTID，即MySQL-5.6.5以上版本。</li>
</ol>
<h2 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h2><blockquote>
<p><strong>对外VIP</strong>：10.96.153.239<br><strong>对内VIP</strong>：192.168.1.150</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">描述</th>
<th style="text-align:left">网卡1：eth0(公网IP)</th>
<th style="text-align:left">网卡2：eth1(内网IP)</th>
<th style="text-align:left">操作系统</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">server1</td>
<td style="text-align:left">10.96.153.110</td>
<td style="text-align:left">192.168.1.100</td>
<td style="text-align:left">Centos 6.X</td>
</tr>
<tr>
<td style="text-align:left">server2</td>
<td style="text-align:left">10.96.153.114</td>
<td style="text-align:left">192.168.1.101</td>
<td style="text-align:left">Centos 6.X</td>
</tr>
</tbody>
</table>
<h2 id="架构配置"><a href="#架构配置" class="headerlink" title="架构配置"></a>架构配置</h2><a id="more"></a>
<h3 id="hosts设置"><a href="#hosts设置" class="headerlink" title="hosts设置"></a>hosts设置</h3><p>Server1与Server2 hosts设置如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/hosts</span></span><br><span class="line">192.168.1.100 server1</span><br><span class="line">192.168.1.101 server2</span><br></pre></td></tr></table></figure></p>
<h3 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h3><p>Nginx、PHP、MySQL、Memcached安装略</p>
<h3 id="解决session共享问题"><a href="#解决session共享问题" class="headerlink" title="解决session共享问题"></a>解决session共享问题</h3><p>php默认的session存储是在/tmp目录下，现在我们是用两台服务器作php请求的负载，这样会造成session分布在两台服务器的/tmp目录下，导致依赖于session的功能不正常。我们可以使用memcached来解决此问题。</p>
<p>上一步我们已经安装好了memcached，现在只需要配置php.ini来使用memcached，配置如下，打开php.ini配置文件，修改为如下两行的值：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session.save_handler = memcache</span><br><span class="line">session.save_path = "tcp://192.168.1.100:11211,tcp://192.168.1.101:11211"</span><br></pre></td></tr></table></figure></p>
<p>之后重启php-fpm生效。</p>
<h3 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h3><h4 id="Server1配置"><a href="#Server1配置" class="headerlink" title="Server1配置"></a>Server1配置</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    [...]</span><br><span class="line">    upstream php-server &#123;</span><br><span class="line">        server 192.168.1.101:9000;</span><br><span class="line">        server 127.0.0.1:9000;</span><br><span class="line">        keepalive 100;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">    server &#123;</span><br><span class="line">        [...]</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            fastcgi_pass   php-server;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line">        [...]</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Server2配置"><a href="#Server2配置" class="headerlink" title="Server2配置"></a>Server2配置</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    [...]</span><br><span class="line">    upstream php-server &#123;</span><br><span class="line">        server 192.168.1.100:9000;</span><br><span class="line">        server 127.0.0.1:9000;</span><br><span class="line">        keepalive 100;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">    server &#123;</span><br><span class="line">        [...]</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            fastcgi_pass   php-server;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line">        [...]</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个配置主要的作用是设置php请求的负载均衡。</p>
<h3 id="MySQL配置"><a href="#MySQL配置" class="headerlink" title="MySQL配置"></a>MySQL配置</h3><h4 id="mysql-util安装"><a href="#mysql-util安装" class="headerlink" title="mysql util安装"></a>mysql util安装</h4><p>我们需要安装mysql util里的主从配置工具来实现主从切换。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-utilities-1.5.3.tar.gz</span><br><span class="line">tar xzf mysql-utilities-1.5.3.tar.gz</span><br><span class="line"><span class="built_in">cd</span> mysql-utilities-1.5.3</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p>
<h4 id="mysql-my-cnf配置"><a href="#mysql-my-cnf配置" class="headerlink" title="mysql my.cnf配置"></a>mysql my.cnf配置</h4><p><strong>server1：</strong><br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mysql]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="attr">protocol</span> = tcp</span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="comment"># BINARY LOGGING</span></span><br><span class="line"><span class="attr">log-bin</span> = /usr/local/mysql/data/mysql-bin</span><br><span class="line"><span class="attr">expire-logs-days</span> = <span class="number">14</span></span><br><span class="line"><span class="attr">binlog-format</span> = row</span><br><span class="line"><span class="attr">log-slave-updates</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">gtid-mode</span> = <span class="literal">on</span></span><br><span class="line"><span class="attr">enforce-gtid-consistency</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">master-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">relay-log-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">server-id</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">report-host</span> = server1</span><br><span class="line"><span class="attr">report-port</span> = <span class="number">3306</span></span><br><span class="line"><span class="section">[...]</span></span><br></pre></td></tr></table></figure></p>
<p><strong>server2：</strong><br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mysql]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="attr">protocol</span> = tcp</span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="comment"># BINARY LOGGING</span></span><br><span class="line"><span class="attr">log-bin</span> = /usr/local/mysql/data/mysql-bin</span><br><span class="line"><span class="attr">expire-logs-days</span> = <span class="number">14</span></span><br><span class="line"><span class="attr">binlog-format</span> = row</span><br><span class="line"><span class="attr">log-slave-updates</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">gtid-mode</span> = <span class="literal">on</span></span><br><span class="line"><span class="attr">enforce-gtid-consistency</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">master-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">relay-log-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">server-id</span> = <span class="number">2</span></span><br><span class="line"><span class="attr">report-host</span> = server2</span><br><span class="line"><span class="attr">report-port</span> = <span class="number">3306</span></span><br><span class="line"><span class="section">[...]</span></span><br></pre></td></tr></table></figure></p>
<p>这两个配置主要是设置了binlog和启用gtid-mode，并且需要设置不同的server-id和report-host。</p>
<h4 id="开放root帐号远程权限"><a href="#开放root帐号远程权限" class="headerlink" title="开放root帐号远程权限"></a>开放root帐号远程权限</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant all on *.* to 'root'@'192.168.1.%' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; grant all on *.* to 'root'@'server1' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; grant all on *.* to 'root'@'server2' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<h4 id="设置mysql主从"><a href="#设置mysql主从" class="headerlink" title="设置mysql主从"></a>设置mysql主从</h4><p>在任意一台执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysqlreplicate --master=root:Xp29at5F37@server1:3306 --slave=root:Xp29at5F37@server2:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># slave on server2: … connected.</span></span><br><span class="line"><span class="comment"># Checking for binary logging on master…</span></span><br><span class="line"><span class="comment"># Setting up replication…</span></span><br><span class="line"><span class="comment"># …done.</span></span><br></pre></td></tr></table></figure></p>
<h4 id="显示主从关系"><a href="#显示主从关系" class="headerlink" title="显示主从关系"></a>显示主从关系</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysqlrplshow --master=root:Xp29at5F37@server1 --discover-slaves-login=root:Xp29at5F37</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># Finding slaves for master: server1:3306</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Replication Topology Graph</span></span><br><span class="line">server1:3306 (MASTER)</span><br><span class="line">|</span><br><span class="line">+— server2:3306 – (SLAVE)</span><br></pre></td></tr></table></figure>
<h4 id="检查主从状态"><a href="#检查主从状态" class="headerlink" title="检查主从状态"></a>检查主从状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mysqlrplcheck --master=root:Xp29at5F37@server1 --slave=root:Xp29at5F37@server2</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># slave on server2: … connected.</span></span><br><span class="line">Test Description Status</span><br><span class="line">—————————————————————————</span><br><span class="line">Checking <span class="keyword">for</span> binary logging on master [pass]</span><br><span class="line">Are there binlog exceptions? [pass]</span><br><span class="line">Replication user exists? [pass]</span><br><span class="line">Checking server_id values [pass]</span><br><span class="line">Checking server_uuid values [pass]</span><br><span class="line">Is slave connected to master? [pass]</span><br><span class="line">Check master information file [pass]</span><br><span class="line">Checking InnoDB compatibility [pass]</span><br><span class="line">Checking storage engines compatibility [pass]</span><br><span class="line">Checking lower_<span class="keyword">case</span>_table_names settings [pass]</span><br><span class="line">Checking slave delay (seconds behind master) [pass]</span><br><span class="line"><span class="comment"># …done.</span></span><br></pre></td></tr></table></figure>
<h3 id="Keepalived配置"><a href="#Keepalived配置" class="headerlink" title="Keepalived配置"></a>Keepalived配置</h3><h4 id="安装-两台都装"><a href="#安装-两台都装" class="headerlink" title="安装(两台都装)"></a>安装(两台都装)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install keepalived</span><br><span class="line">chkconfig keepalived on</span><br></pre></td></tr></table></figure>
<h4 id="keepalived配置-server1"><a href="#keepalived配置-server1" class="headerlink" title="keepalived配置(server1)"></a>keepalived配置(server1)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">vrrp_sync_group VG_1 &#123;</span><br><span class="line">	group &#123;</span><br><span class="line">		inside_network</span><br><span class="line">		outside_network</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance inside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth1</span><br><span class="line">	virtual_router_id 51</span><br><span class="line">	priority 101</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		192.168.1.150/24</span><br><span class="line">	&#125;</span><br><span class="line">	nopreempt</span><br><span class="line">	notify /data/sh/mysqlfailover-server1.sh</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance outside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth0</span><br><span class="line">	virtual_router_id 50</span><br><span class="line">	priority 101</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">	auth_<span class="built_in">type</span> PASS</span><br><span class="line">	auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		10.96.153.239/24</span><br><span class="line">	&#125;</span><br><span class="line">	nopreempt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="keepalived配置-server2"><a href="#keepalived配置-server2" class="headerlink" title="keepalived配置(server2)"></a>keepalived配置(server2)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">vrrp_sync_group VG_1 &#123;</span><br><span class="line">	group &#123;</span><br><span class="line">		inside_network</span><br><span class="line">		outside_network</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance inside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth1</span><br><span class="line">	virtual_router_id 51</span><br><span class="line">	priority 100</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		192.168.1.150</span><br><span class="line">	&#125;</span><br><span class="line">	notify /data/sh/mysqlfailover-server2.sh</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance outside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth0</span><br><span class="line">	virtual_router_id 50</span><br><span class="line">	priority 100</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		10.96.153.239/24</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此keepalived配置需要注意的是：</p>
<ol>
<li>两台server的state都设置为backup，server1增加nopreempt配置，并且server1 priority比server2高，这样用来实现当server1从宕机恢复时，不抢占VIP;</li>
<li>server1设置notify /data/sh/mysqlfailover-server1.sh,server2设置notify /data/sh/mysqlfailover-server2.sh,作用是自动切换主从</li>
</ol>
<p>/data/sh/mysqlfailover-server1.sh脚本内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"> </span></span><br><span class="line">sleep 10</span><br><span class="line">state=<span class="variable">$3</span></span><br><span class="line">result=`mysql -h127.0.0.1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status;'</span>`</span><br><span class="line">[[ <span class="string">"<span class="variable">$result</span>"</span> == <span class="string">""</span> ]] &amp;&amp; mysqlState=<span class="string">"master"</span> || mysqlState=<span class="string">"slave"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"MASTER"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"slave"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlrpladmin --slave=root:Xp29at5F37@server1:3306 failover</span><br><span class="line">	<span class="keyword">fi</span> </span><br><span class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"BACKUP"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"master"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlreplicate --master=root:Xp29at5F37@server2:3306 --slave=root:Xp29at5F37@server1:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/proxy-read-only-backend-addresses.*/proxy-read-only-backend-addresses = 192.168.1.150:3306/'</span> /usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br><span class="line">mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"REMOVE BACKEND 2;"</span></span><br></pre></td></tr></table></figure></p>
<p>/data/sh/mysqlfailover-server2.sh脚本内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line">sleep 10</span><br><span class="line">state=<span class="variable">$3</span></span><br><span class="line">result=`mysql -h127.0.0.1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status;'</span>`</span><br><span class="line">[[ <span class="string">"<span class="variable">$result</span>"</span> == <span class="string">""</span> ]] &amp;&amp; mysqlState=<span class="string">"master"</span> || mysqlState=<span class="string">"slave"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"MASTER"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"slave"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlrpladmin --slave=root:Xp29at5F37@server2:3306 failover</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"BACKUP"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"master"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlreplicate --master=root:Xp29at5F37@server1:3306 --slave=root:Xp29at5F37@server2:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/proxy-read-only-backend-addresses.*/proxy-read-only-backend-addresses = 192.168.1.150:3306/'</span> /usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br><span class="line">mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"REMOVE BACKEND 2;"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Atlas设置"><a href="#Atlas设置" class="headerlink" title="Atlas设置"></a>Atlas设置</h3><h4 id="atlas安装（两台服务器）"><a href="#atlas安装（两台服务器）" class="headerlink" title="atlas安装（两台服务器）"></a>atlas安装（两台服务器）</h4><p>到这里下载最新版本，<a href="https://github.com/Qihoo360/Atlas/releases" target="_blank" rel="external">https://github.com/Qihoo360/Atlas/releases</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/Qihoo360/Atlas/releases/download/2.2.1/Atlas-2.2.1.el6.x86_64.rpm</span><br><span class="line">rpm -i Atlas-2.2.1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<h4 id="atlas配置（两台服务器）"><a href="#atlas配置（两台服务器）" class="headerlink" title="atlas配置（两台服务器）"></a>atlas配置（两台服务器）</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/mysql-proxy/conf</span><br><span class="line">cp test.cnf my.cnf</span><br><span class="line">vi my.cnf</span><br><span class="line">#调整如下参数</span><br><span class="line">proxy-backend-addresses = 192.168.1.150:3306</span><br><span class="line">proxy-read-only-backend-addresses = 192.168.1.101:3306</span><br><span class="line">pwds = root:qtyU1btXOo074Itvx0UR9Q==</span><br><span class="line">event-threads = 8</span><br></pre></td></tr></table></figure>
<p><strong><code>注意</code></strong>：</p>
<ul>
<li>proxy-backend-addresse设置为内网VIP</li>
<li>proxy-read-only-backend-addresses设置为server2的IP</li>
<li>root:qtyU1btXOo074Itvx0UR9Q==设置数据库的用户和密码，密码是通过/usr/local/mysql-proxy/bin/encrypt Xp29at5F37生成。</li>
<li>更详细参数解释请查看，<a href="https://github.com/Qihoo360/Atlas/wiki/Atlas%E9%83%A8%E5%88%86%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E5%8F%8A%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3" target="_blank" rel="external">Atlas配置详解</a>。</li>
</ul>
<h4 id="启动atlas（两台服务器）"><a href="#启动atlas（两台服务器）" class="headerlink" title="启动atlas（两台服务器）"></a>启动atlas（两台服务器）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/mysql-proxy/bin/mysql-proxy --defaults-file=/usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br></pre></td></tr></table></figure>
<p>之后程序里配置mysql就配置127.0.0.1:1234就好。</p>
<h4 id="部署atlas自动维护脚本（两台服务器）"><a href="#部署atlas自动维护脚本（两台服务器）" class="headerlink" title="部署atlas自动维护脚本（两台服务器）"></a>部署atlas自动维护脚本（两台服务器）</h4><p>添加定时任务（如每2分钟运行一次）我们把脚本放在/data/sh/auto_maintain_atlas.sh,脚本内容为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"> </span></span><br><span class="line">count=`mysql -N -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"select * from backends;"</span> | wc <span class="_">-l</span>`</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$count</span>"</span> == <span class="string">"1"</span> ]];<span class="keyword">then</span></span><br><span class="line">    result=`mysql -hserver1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status\G'</span>`</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | grep Slave_IO_State;<span class="keyword">then</span></span><br><span class="line">        slaveIP=192.168.1.100</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        result=`mysql -hserver2 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status\G'</span>`</span><br><span class="line">        slaveIP=192.168.1.101</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    slaveIORunning=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_IO_Running:/&#123;print $2&#125;'</span>`</span><br><span class="line">    slaveSQLRunning=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_SQL_Running:/&#123;print $2&#125;'</span>`</span><br><span class="line">    SlaveSQLRunning_State=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_SQL_Running_State:/&#123;print $2&#125;'</span>`</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$slaveIORunning</span>"</span> =~ <span class="string">"Yes"</span> &amp;&amp; <span class="string">"<span class="variable">$slaveSQLRunning</span>"</span> =~ <span class="string">"Yes"</span> &amp;&amp; <span class="string">"<span class="variable">$SlaveSQLRunning_State</span>"</span> =~ <span class="string">"Slave has read all relay log"</span> ]];<span class="keyword">then</span></span><br><span class="line">        mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"add slave <span class="variable">$&#123;slaveIP&#125;</span>:3306;"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></p>
<p>为什么需要这个脚本呢？假设目前mysql主服务器在s1，s1宕机后，s2接管VIP，接着删除atlas中设置的slave backend，将mysql提升为主。过一段时间后，s1从宕机中恢复，这时候s1的mysql自动切换为从，接着删除atlas中设置的slave backend，开始连接s2的mysql主同步数据。到这个时候我们发现，已经不存在读写分离了，所有的sql都发送给了s2的mysql。auto_maintain_atlas.sh脚本就派上用场了，此脚本会定时的检查主从是否已经同步完成，如果完成就自动增加slave backend，这样读写分离又恢复了，完全不需要人工干预。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="测试keepalived是否工作正常"><a href="#测试keepalived是否工作正常" class="headerlink" title="测试keepalived是否工作正常"></a>测试keepalived是否工作正常</h4><p>我们来模拟server1宕机。<br>在server1上执行shutdown关机命令。<br>此时我们登录server2，执行ip addr命令，输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1: lo: mtu 16436 qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet6 ::1/128 scope host</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:81:9d:42 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 10.96.153.114/24 brd 10.96.153.255 scope global eth0</span><br><span class="line">inet 10.96.153.239/24 scope global secondary eth0</span><br><span class="line">inet6 fe80::20c:29ff:fe81:9d42/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">3: eth1: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:81:9d:4c brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 192.168.1.101/24 brd 192.168.1.255 scope global eth1</span><br><span class="line">inet 192.168.1.150/32 scope global eth1</span><br><span class="line">inet6 fe80::20c:29ff:fe81:9d4c/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>我们看到对外VIP 10.96.153.239和对内IP 192.168.1.150已经转移到server2了，证明keepalived运行正常。</p>
<h4 id="测试是否自动切换了主从"><a href="#测试是否自动切换了主从" class="headerlink" title="测试是否自动切换了主从"></a>测试是否自动切换了主从</h4><p>登录server2的mysql服务器，执行show slave status;命令，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show slave status\G</span><br><span class="line">Empty <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>我们发现从状态已经为空，证明已经切换为主了。</p>
<h4 id="测试server1是否抢占VIP"><a href="#测试server1是否抢占VIP" class="headerlink" title="测试server1是否抢占VIP"></a>测试server1是否抢占VIP</h4><p>为什么要测试这个呢？如果server1恢复之后抢占了VIP，而我们的Atlas里后端设置的是VIP，这样server1启动之后，sql的写操作就会向server1的mysql发送，而server1的mysql数据是旧于server2的，所以这样会造成数据不一致，这个是非常重要的测试。<br>我们先来启动server1，之后执行ip addr，输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1: lo: mtu 16436 qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet6 ::1/128 scope host</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:f1:4f:4e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 10.96.153.110/24 brd 10.96.153.255 scope global eth0</span><br><span class="line">inet6 fe80::20c:29ff:fef1:4f4e/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">3: eth1: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:f1:4f:58 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 192.168.1.100/24 brd 192.168.1.255 scope global eth1</span><br><span class="line">inet6 fe80::20c:29ff:fef1:4f58/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>我们看到，server1并没有抢占VIP，测试正常。不过另人郁闷的是，在虚拟机的环境并没有测试成功，不知道为什么。</p>
<h4 id="测试server2的atlas是否已经删除slave-backend"><a href="#测试server2的atlas是否已经删除slave-backend" class="headerlink" title="测试server2的atlas是否已经删除slave backend"></a>测试server2的atlas是否已经删除slave backend</h4><p>我们测试这个是为了保证atlas已经没有slave backend，也就是没有从库的设置了，否则当server1恢复时，有可能会把读请求发送给server1的mysql，造成读取了旧数据的问题。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]<span class="comment"># mysql -h127.0.0.1 -P2345 -uuser -ppwd</span></span><br><span class="line">mysql&gt; select * from backends;</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| backend_ndx | address | state | <span class="built_in">type</span> |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| 1 | 192.168.1.150:3306 | up | rw |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">1 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>如果看到只有一个后端，证明运作正常。</p>
<h4 id="测试server1-mysql是否设置为从"><a href="#测试server1-mysql是否设置为从" class="headerlink" title="测试server1 mysql是否设置为从"></a>测试server1 mysql是否设置为从</h4><p>serve1恢复后，登录server1的mysql服务器，执行show slave status;命令，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">Slave_IO_State: Opening tables</span><br><span class="line">Master_Host: server1</span><br><span class="line">Master_User: rpl</span><br><span class="line">Master_Port: 3306</span><br><span class="line">Connect_Retry: 60</span><br><span class="line">Master_Log_File: mysql-bin.000015</span><br><span class="line">Read_Master_Log_Pos: 48405991</span><br><span class="line">Relay_Log_File: mysql-relay-bin.000002</span><br><span class="line">Relay_Log_Pos: 361</span><br><span class="line">Relay_Master_Log_File: mysql-bin.000015</span><br><span class="line">Slave_IO_Running: Yes</span><br><span class="line">Slave_SQL_Running: yes</span><br></pre></td></tr></table></figure></p>
<h4 id="测试是否自动恢复读写分离"><a href="#测试是否自动恢复读写分离" class="headerlink" title="测试是否自动恢复读写分离"></a>测试是否自动恢复读写分离</h4><p>server1恢复后一段时间，我们可以看是读写分离是否已经恢复。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]<span class="comment"># mysql -h127.0.0.1 -P2345 -uuser -ppwd</span></span><br><span class="line">Warning: Using a password on the <span class="built_in">command</span> line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor. Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1</span><br><span class="line">Server version: 5.0.99-agent-admin</span><br><span class="line">Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line">Type ‘<span class="built_in">help</span>;’ or ‘\h’ <span class="keyword">for</span> help. Type ‘\c’ to clear the current input statement.</span><br><span class="line">mysql&gt; select * from backends;</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| backend_ndx | address | state | <span class="built_in">type</span> |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| 1 | 192.168.1.150:3306 | up | rw |</span><br><span class="line">| 2 | 192.168.1.100:3306 | up | ro |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">2 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>我们看到server1已经被添加为slave backend了。这表示已经成功恢复读写分离。</p>
<hr>
<p>来自：<a href="https://www.centos.bz" target="_blank" rel="external">https://www.centos.bz</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用lsyncd实时同步文件]]></title>
      <url>http://team.jiunile.com/blog/2016/07/lsyncd.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Lysncd 实际上是lua语言封装了 inotify 和 rsync 工具，采用了 Linux 内核（2.6.13 及以后）里的 inotify 触发机制，然后通过rsync去差异同步，达到实时的效果。我认为它最令人称道的特性是，完美解决了 inotify + rsync海量文件同步带来的文件频繁发送文件列表的问题 —— 通过时间延迟或累计触发事件次数实现。另外，它的配置方式很简单，lua本身就是一种配置语言，可读性非常强。lsyncd也有多种工作模式可以选择，本地目录cp，本地目录rsync，远程目录rsyncssh。</p>
<p>实现简单高效的本地目录同步备份（网络存储挂载也当作本地目录），一个命令搞定。</p>
<p>github地址：<a href="https://github.com/axkibe/lsyncd" target="_blank" rel="external">https://github.com/axkibe/lsyncd</a> </p>
<h2 id="使用-lsyncd-本地目录实时备份"><a href="#使用-lsyncd-本地目录实时备份" class="headerlink" title="使用 lsyncd 本地目录实时备份"></a>使用 lsyncd 本地目录实时备份</h2><p>这一节实现的功能是，本地目录source实时同步到另一个目录target，而在source下有大量的文件，并且有部分目录和临时文件不需要同步。</p>
<h3 id="安装lsyncd"><a href="#安装lsyncd" class="headerlink" title="安装lsyncd"></a>安装lsyncd</h3><p>安装<code>lsyncd</code>极为简单，已经收录在ubuntu的官方镜像源里，直接通过<code>apt-get install lsyncd</code>就可以。<br>在Redhat系（我的环境是CentOS 6.2 x86_64 ），可以手动去下载 <a href="ftp://195.220.108.108/linux/fedora/linux/updates/21/x86_64/l/lsyncd-2.1.5-6.fc21.x86_64.rpm" target="_blank" rel="external">lsyncd-2.1.5-6.fc21.x86_64.rpm</a>，但首先你得安装两个依赖<code>yum install lua lua-devel</code>。也可以通过在线安装，需要<code>epel-release</code>扩展包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">yum install lsyncd</span><br></pre></td></tr></table></figure></p>
<p><strong>源码编译安装</strong><br>从源码编译安装可以使用最新版的lsyncd程序，但必须要相应的依赖库文件和编译工具：<code>yum install lua lua-devel asciidoc cmake</code>。</p>
<p>从 <a href="http://code.google.com/p/lsyncd/downloads/list" target="_blank" rel="external">googlecode lsyncd</a> 上下载的<code>lsyncd-2.1.5.tar.gz</code>，直接<code>./configure、make &amp;&amp; make install</code>就可以了。</p>
<p>从github上下载<a href="https://github.com/axkibe/lsyncd/archive/master.zip" target="_blank" rel="external">lsyncd-master.zip</a> 的2.1.5版本使用的是 cmake 编译工具，无法<code>./configure</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uzip lsyncd-master.zip</span><br><span class="line"><span class="built_in">cd</span> lsyncd-master</span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span>/lsyncd-2.1.5</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></p>
<p>我这个版本编译时有个小bug，如果按照<code>INSTALL</code>在    <code>build</code>目录中make，会提示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[100%] Generating doc/lsyncd.1</span><br><span class="line">Updating the manpage</span><br><span class="line">a2x: failed: <span class="built_in">source</span> file not found: doc/lsyncd.1.txt</span><br><span class="line">make[2]: *** [doc/lsyncd.1] Error 1</span><br><span class="line">make[1]: *** [CMakeFiles/manpage.dir/all] Error 2</span><br><span class="line">make: *** [all] Error 2</span><br></pre></td></tr></table></figure></p>
<p>解决办法是要么直接在解压目录下cmake，不要<code>mkdir build</code>，要么在<code>CMakeList.txt</code>中搜索doc字符串，在前面加上<code>${PROJECT_SOURCE_DIR}</code>。</p>
<a id="more"></a>
<h3 id="lsyncd-conf"><a href="#lsyncd-conf" class="headerlink" title="lsyncd.conf"></a>lsyncd.conf</h3><p>下面都是在编译安装的情况下操作。</p>
<h4 id="lsyncd同步配置"><a href="#lsyncd同步配置" class="headerlink" title="lsyncd同步配置"></a>lsyncd同步配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /usr/local/lsyncd-2.1.5</span></span><br><span class="line"><span class="comment"># mkdir etc var</span></span><br><span class="line"><span class="comment"># vi etc/lsyncd.conf</span></span><br><span class="line">settings &#123;</span><br><span class="line">    logfile      =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.log"</span>,</span><br><span class="line">    statusFile   =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.status"</span>,</span><br><span class="line">    inotifyMode  = <span class="string">"CloseWrite"</span>,</span><br><span class="line">    maxProcesses = 7,</span><br><span class="line">    -- nodaemon =<span class="literal">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest"</span>,</span><br><span class="line">    -- excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary    = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive   = <span class="literal">true</span>,</span><br><span class="line">        compress  = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>到这启动 lsycnd 就可以完成实时同步了，默认的许多参数可以满足绝大部分需求，非常简单。</p>
<h4 id="lsyncd-conf配置选项说明"><a href="#lsyncd-conf配置选项说明" class="headerlink" title="lsyncd.conf配置选项说明"></a>lsyncd.conf配置选项说明</h4><p><strong>settings</strong><br>里面是全局设置，<code>--</code>开头表示注释，下面是几个常用选项说明：</p>
<ul>
<li><code>logfile</code> 定义日志文件</li>
<li><code>stausFile</code> 定义状态文件</li>
<li><code>nodaemon=true</code> 表示不启用守护模式，默认</li>
<li><code>statusInterval</code> 将lsyncd的状态写入上面的statusFile的间隔，默认10秒</li>
<li><code>inotifyMode</code> 指定inotify监控的事件，默认是<code>CloseWrite</code>，还可以是<code>Modify</code>或<code>CloseWrite or Modify</code></li>
<li><code>maxProcesses</code> 同步进程的最大个数。假如同时有20个文件需要同步，而<code>maxProcesses = 8</code>，则最大能看到有8个rysnc进程</li>
<li><code>maxDelays</code> 累计到多少所监控的事件激活一次同步，即使后面的<code>delay</code>延迟时间还未到</li>
</ul>
<p><strong>sync</strong><br>里面是定义同步参数，可以继续使用<code>maxDelays</code>来重写settings的全局变量。一般第一个参数指定lsyncd以什么模式运行：<code>rsync</code>、<code>rsyncssh</code>、<code>direct</code>三种模式：</p>
<ul>
<li><p><code>default.rsync</code>：本地目录间同步，使用rsync，也可以达到使用ssh形式的远程rsync效果，或daemon方式连接远程rsyncd进程；<br><code>default.direct</code> ：本地目录间同步，使用<code>cp</code>、<code>rm</code>等命令完成差异文件备份；<br><code>default.rsyncssh</code> ：同步到远程主机目录，rsync的ssh模式，需要使用key来认证</p>
</li>
<li><p><code>source</code> 同步的源目录，使用绝对路径。</p>
</li>
<li><p><code>target</code> 定义目的地址.对应不同的模式有几种写法：<br><code>/tmp/dest</code> ：本地目录同步，可用于<code>direct</code>和<code>rsync</code>模式<br><code>172.29.88.223:/tmp/dest</code> ：同步到远程服务器目录，可用于<code>rsync</code>和<code>rsyncssh</code>模式，拼接的命令类似于<code>/usr/bin/rsync -ltsd --delete --include-from=- --exclude=* SOURCE TARGET</code>，剩下的就是rsync的内容了，比如指定username，免密码同步<br><code>172.29.88.223::module</code> ：同步到远程服务器目录，用于<code>rsync</code>模式<br>三种模式的示例会在后面给出。</p>
</li>
<li><p><code>init</code> 这是一个优化选项，当<code>init = false</code>，只同步进程启动以后发生改动事件的文件，原有的目录即使有差异也不会同步。默认是<code>true</code></p>
</li>
<li><p><code>delay</code> 累计事件，等待rsync同步延时时间，默认15秒（最大累计到1000个不可合并的事件）。也就是15s内监控目录下发生的改动，会累积到一次rsync同步，避免过于频繁的同步。（可合并的意思是，15s内两次修改了同一文件，最后只同步最新的文件）</p>
</li>
<li><p><code>excludeFrom</code> 排除选项，后面指定排除的列表文件，如<code>excludeFrom = &quot;/etc/lsyncd.exclude&quot;</code>，如果是简单的排除，可以使用<code>exclude = LIST</code>。<br>这里的排除规则写法与原生rsync有点不同，更为简单：</p>
<ul>
<li>监控路径里的任何部分匹配到一个文本，都会被排除，例如<code>/bin/foo/bar</code>可以匹配规则<code>foo</code></li>
<li>如果规则以斜线<code>/</code>开头，则从头开始要匹配全部</li>
<li>如果规则以<code>/</code>结尾，则要匹配监控路径的末尾</li>
<li><code>?</code>匹配任何字符，但不包括<code>/</code></li>
<li><code>*</code>匹配0或多个字符，但不包括<code>/</code></li>
<li><code>**</code>匹配0或多个字符，可以是<code>/</code></li>
</ul>
</li>
<li><p><code>delete</code> 为了保持target与souce完全同步，Lsyncd默认会<code>delete = true</code>来允许同步删除。它除了<code>false</code>，还有<code>startup</code>、<code>running</code>值，请参考 <a href="https://github.com/axkibe/lsyncd/wiki/Lsyncd%202.1.x%20%E2%80%96%20Layer%204%20Config%20%E2%80%96%20Default%20Behavior" target="_blank" rel="external">Lsyncd 2.1.x ‖ Layer 4 Config ‖ Default Behavior</a>。</p>
</li>
</ul>
<p><strong>rsync</strong><br>（提示一下，<code>delete</code>和<code>exclude</code>本来都是<strong>rsync</strong>的选项，上面是配置在sync中的，我想这样做的原因是为了减少rsync的开销）</p>
<ul>
<li><code>bwlimit</code> 限速，单位kb/s，与rsync相同（这么重要的选项在文档里竟然没有标出）</li>
<li><code>compress</code> 压缩传输默认为true。在带宽与cpu负载之间权衡，本地目录同步可以考虑把它设为<code>false</code></li>
<li><code>perms</code> 默认保留文件权限。</li>
<li>其它rsync的选项</li>
</ul>
<p>其它还有rsyncssh模式独有的配置项，如<code>host</code>、<code>targetdir</code>、<code>rsync_path</code>、<code>password_file</code>，见后文示例。<code>rsyncOps={&quot;-avz&quot;,&quot;--delete&quot;}</code>这样的写法在2.1.*版本已经不支持。</p>
<p><code>lsyncd.conf</code>可以有多个<code>sync</code>，各自的source，各自的target，各自的模式，互不影响。</p>
<h3 id="启动lsyncd"><a href="#启动lsyncd" class="headerlink" title="启动lsyncd"></a>启动lsyncd</h3><p>使用命令加载配置文件，启动守护进程，自动同步目录操作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsyncd -log Exec /usr/<span class="built_in">local</span>/lsyncd-2.1.5/etc/lsyncd.conf</span><br></pre></td></tr></table></figure></p>
<h3 id="lsyncd-conf其它模式示例"><a href="#lsyncd-conf其它模式示例" class="headerlink" title="lsyncd.conf其它模式示例"></a>lsyncd.conf其它模式示例</h3><p>以下配置本人都已经过验证可行，必须根据实际需要裁剪配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">settings &#123;</span><br><span class="line">    logfile =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.log"</span>,</span><br><span class="line">    statusFile =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.status"</span>,</span><br><span class="line">    inotifyMode = <span class="string">"CloseWrite"</span>,</span><br><span class="line">    maxProcesses = 8,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- I. 本地目录同步，direct：cp/rm/mv。 适用：500+万文件，变动不大</span><br><span class="line">sync &#123;</span><br><span class="line">    default.direct,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest"</span>,</span><br><span class="line">    delay = 1</span><br><span class="line">    maxProcesses = 1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- II. 本地目录同步，rsync模式：rsync</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest1"</span>,</span><br><span class="line">    excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        bwlimit   = 2000</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- III. 远程目录同步，rsync模式 + rsyncd daemon</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"syncuser@172.29.88.223::module1"</span>,</span><br><span class="line">    delete=<span class="string">"running"</span>,</span><br><span class="line">    exclude = &#123; <span class="string">".*"</span>, <span class="string">".tmp"</span> &#125;,</span><br><span class="line">    delay = 30,</span><br><span class="line">    init = <span class="literal">false</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span>,</span><br><span class="line">        password_file = <span class="string">"/etc/rsyncd.d/rsync.pwd"</span>,</span><br><span class="line">        _extra    = &#123;<span class="string">"--bwlimit=200"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- IV. 远程目录同步，rsync模式 + ssh shell</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"172.29.88.223:/tmp/dest"</span>,</span><br><span class="line">    -- target    = <span class="string">"root@172.29.88.223:/remote/dest"</span>,</span><br><span class="line">    -- 上面target，注意如果是普通用户，必须拥有写权限</span><br><span class="line">    maxDelays = 5,</span><br><span class="line">    delay = 30,</span><br><span class="line">    -- init = <span class="literal">true</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        bwlimit   = 2000</span><br><span class="line">        -- rsh = <span class="string">"/usr/bin/ssh -p 22 -o StrictHostKeyChecking=no"</span></span><br><span class="line">        -- 如果要指定其它端口，请用上面的rsh</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- V. 远程目录同步，rsync模式 + rsyncssh，效果与上面相同</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsyncssh,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src2"</span>,</span><br><span class="line">    host      = <span class="string">"172.29.88.223"</span>,</span><br><span class="line">    targetdir = <span class="string">"/remote/dir"</span>,</span><br><span class="line">    excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    -- maxDelays = 5,</span><br><span class="line">    delay = 0,</span><br><span class="line">    -- init = <span class="literal">false</span>,</span><br><span class="line">    rsync    = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span>,</span><br><span class="line">        _extra = &#123;<span class="string">"--bwlimit=2000"</span>&#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">    ssh      = &#123;</span><br><span class="line">        port  =  1234</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的内容几乎涵盖了所有同步的模式，其中第III个要求像rsync一样配置rsyncd服务端，见本文开头。第IV、V配置ssh方式同步，达到的效果相同，但实际同步时你会发现每次同步都会提示输入ssh的密码，可以通过以下方法解决：</p>
<p>在远端被同步的服务器上开启ssh无密码登录，请注意用户身份：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user$ ssh-keygen -t rsa</span><br><span class="line">一路回车...</span><br><span class="line">user$ <span class="built_in">cd</span> ~/.ssh</span><br><span class="line">user$ cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>把<code>id_rsa</code>私钥拷贝到执行lsyncd的机器上<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user$ chmod 600 ~/.ssh/id_rsa</span><br><span class="line">测试能否无密码登录</span><br><span class="line">user$ ssh user@172.29.88.223</span><br></pre></td></tr></table></figure></p>
<h2 id="lsyncd的其它功能"><a href="#lsyncd的其它功能" class="headerlink" title="lsyncd的其它功能"></a>lsyncd的其它功能</h2><p><code>lsyncd</code>的功能不仅仅是同步，官方手册 <a href="https://github.com/axkibe/lsyncd/wiki/Lsyncd%202.1.x%20%E2%80%96%20Layer%202%20Config%20%E2%80%96%20Advanced%20onAction" target="_blank" rel="external">Lsyncd 2.1.x ‖ Layer 2 Config ‖ Advanced onAction</a> 高级功能提到，还可以监控某个目录下的文件，根据触发的事件自己定义要执行的命令，example是监控某个某个目录，只要是有jpg、gif、png格式的文件参数，就把它们转成pdf，然后同步到另一个目录。正好在我运维的一个项目中有这个需求，现在都是在java代码里转换，还容易出现异常，通过lsyncd可以代替这样的功能。但，门槛在于要会一点点lua语言（根据官方example还是可以写出来）。</p>
<p>另外偶然想到个问题，同时设置了<code>maxDelays</code>和<code>delay</code>，当监控目录一直没有文件变化了，也会发生同步操作，虽然没有可rsync的文件。</p>
<hr>
<p>来源：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL数据库开发规范]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-develop-standard.html</url>
      <content type="html"><![CDATA[<h2 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h2><h3 id="库名、表名、字段名必须使用小写字母，并采用下划线分割"><a href="#库名、表名、字段名必须使用小写字母，并采用下划线分割" class="headerlink" title="库名、表名、字段名必须使用小写字母，并采用下划线分割"></a>库名、表名、字段名必须使用小写字母，并采用下划线分割</h3><ul>
<li>MySQL有配置参数lower_case_table_names=1，即库表名以小写存储，大小写不敏感。如果是0，则库表名以实际情况存储，大小写敏感；如果是2，以实际情况存储，但以小写比较。</li>
<li>如果大小写混合使用，可能存在abc，Abc，ABC等多个表共存，容易导致混乱。</li>
<li>字段名显示区分大小写，但实际使⽤时不区分，即不可以建立两个名字一样但大小写不一样的字段。</li>
<li>为了统一规范， 库名、表名、字段名使用小写字母。</li>
</ul>
<h3 id="库名以-d-开头，表名以-t-开头，字段名以-f-开头"><a href="#库名以-d-开头，表名以-t-开头，字段名以-f-开头" class="headerlink" title="库名以 d 开头，表名以 t 开头，字段名以 f_ 开头"></a>库名以 d 开头，表名以 t 开头，字段名以 f_ 开头</h3><ul>
<li>比如表 <code>t_crm_relation</code>，中间的 crm 代表业务模块名</li>
<li>视图以<code>view_</code>开头，事件以<code>event_</code>开头，触发器以<code>trig_</code>开头，存储过程以<code>proc_</code>开头，函数以<code>func_</code>开头</li>
<li>普通索引以<code>idx_col1_col2</code>命名，唯一索引以<code>uk_col1_col2</code>命名（可去掉f_公共部分）。如 <code>idx_companyid_corpid_contacttime</code>(f_company_id, f_corp_id, f_contact_time)</li>
</ul>
<h3 id="库名、表名、字段名禁止超过32个字符，需见名知意"><a href="#库名、表名、字段名禁止超过32个字符，需见名知意" class="headerlink" title="库名、表名、字段名禁止超过32个字符，需见名知意"></a>库名、表名、字段名禁止超过32个字符，需见名知意</h3><p>库名、表名、字段名支持最多64个字符，但为了统一规范、易于辨识以及减少传输量，禁止超过32个字符</p>
<h3 id="临时库、表名须以tmp加日期为后缀"><a href="#临时库、表名须以tmp加日期为后缀" class="headerlink" title="临时库、表名须以tmp加日期为后缀"></a>临时库、表名须以tmp加日期为后缀</h3><p>如 t_crm_relation_tmp0425。备份表也类似，形如 <code>_bak20160425</code> 。</p>
<h3 id="按日期时间分表须符合-YYYY-MM-DD-格式"><a href="#按日期时间分表须符合-YYYY-MM-DD-格式" class="headerlink" title="按日期时间分表须符合_YYYY[MM][DD]格式"></a>按日期时间分表须符合_YYYY[MM][DD]格式</h3><p>这也是为将来有可能分表做准备的，比如<code>t_crm_ec_record_201403</code>，但像 t_crm_contact_at201506就打破了这种规范。<br>不具有时间特性的，直接以 <code>t_tbname_001</code> 这样的方式命名。</p>
<h2 id="库表基础规范"><a href="#库表基础规范" class="headerlink" title="库表基础规范"></a>库表基础规范</h2><h3 id="使用Innodb存储引擎"><a href="#使用Innodb存储引擎" class="headerlink" title="使用Innodb存储引擎"></a>使用Innodb存储引擎</h3><p>5.5版本开始mysql默认存储引擎就是InnoDB，5.7版本开始，系统表都放弃MyISAM了。</p>
<a id="more"></a>
<h3 id="表字符集统一使用UTF8"><a href="#表字符集统一使用UTF8" class="headerlink" title="表字符集统一使用UTF8"></a>表字符集统一使用UTF8</h3><ul>
<li>UTF8字符集存储汉字占用3个字节，存储英文字符占用一个字节</li>
<li>校对字符集使用默认的 utf8_general_ci</li>
<li>连接的客户端也使用utf8，建立连接时指定charset或SET NAMES UTF8;。（对于已经在项目中长期使用latin1的，救不了了）</li>
<li>如果遇到EMOJ等表情符号的存储需求，可申请使用UTF8MB4字符集</li>
</ul>
<h3 id="所有表都要添加注释"><a href="#所有表都要添加注释" class="headerlink" title="所有表都要添加注释"></a>所有表都要添加注释</h3><ul>
<li>尽量给字段也添加注释</li>
<li>类status型需指明主要值的含义，如”0-离线，1-在线”</li>
</ul>
<h3 id="控制单表字段数量"><a href="#控制单表字段数量" class="headerlink" title="控制单表字段数量"></a>控制单表字段数量</h3><ul>
<li>单表字段数上限30左右，再多的话考虑垂直分表，一是冷热数据分离，二是大字段分离，三是常在一起做条件和返回列的不分离。</li>
<li>表字段控制少而精，可以提高IO效率，内存缓存更多有效数据，从而提高响应速度和并发能力，后续 alter table 也更快。</li>
</ul>
<h3 id="所有表都必须要显式指定主键"><a href="#所有表都必须要显式指定主键" class="headerlink" title="所有表都必须要显式指定主键"></a>所有表都必须要显式指定主键</h3><ul>
<li>主键尽量采用自增方式，InnoDB表实际是一棵索引组织表，顺序存储可以提高存取效率，充分利用磁盘空间。还有对一些复杂查询可能需要自连接来优化时需要用到。</li>
<li>需要全局唯一主键时，使用外部发号器ticket server（建设中）</li>
<li>如果没有主键或唯一索引，update/delete是通过所有字段来定位操作的行，相当于每行就是一次全表扫描</li>
<li>少数情况可以使用联合唯一主键，需与DBA协商</li>
</ul>
<h3 id="不强制使用外键参考"><a href="#不强制使用外键参考" class="headerlink" title="不强制使用外键参考"></a>不强制使用外键参考</h3><p>即使2个表的字段有明确的外键参考关系，也不使用 FOREIGN KEY ，因为新纪录会去主键表做校验，影响性能。</p>
<h3 id="适度使用存储过程、视图，禁止使用触发器、事件"><a href="#适度使用存储过程、视图，禁止使用触发器、事件" class="headerlink" title="适度使用存储过程、视图，禁止使用触发器、事件"></a>适度使用存储过程、视图，禁止使用触发器、事件</h3><ul>
<li>存储过程（procedure）虽然可以简化业务端代码，在传统企业写复杂逻辑时可能会用到，而在互联网企业变更是很频繁的，在分库分表的情况下要升级一个存储过程相当麻烦。又因为它是不记录log的，所以也不方便debug性能问题。如果使用过程，一定考虑如果执行失败的情况。</li>
<li>使用视图一定程度上也是为了降低代码里SQL的复杂度，但有时候为了视图的通用性会损失性能（比如返回不必要的字段）。</li>
<li>触发器（trigger）也是同样，但也不应该通过它去约束数据的强一致性，mysql只支持“基于行的触发”，也就是说，触发器始终是针对一条记录的，而不是针对整个sql语句的，如果变更的数据集非常大的话，效率会很低。掩盖一条sql背后的工作，一旦出现问题将是灾难性的，但又很难快速分析和定位。再者需要ddl时无法使用pt-osc工具。放在transaction执行。</li>
<li>事件（event）也是一种偷懒的表现，目前已经遇到数次由于定时任务执行失败影响业务的情况，而且mysql无法对它做失败预警。建立专门的 job scheduler 平台。</li>
</ul>
<h3 id="单表数据量控制在5000w以内"><a href="#单表数据量控制在5000w以内" class="headerlink" title="单表数据量控制在5000w以内"></a>单表数据量控制在5000w以内</h3><h3 id="数据库中不允许存储明文密码"><a href="#数据库中不允许存储明文密码" class="headerlink" title="数据库中不允许存储明文密码"></a>数据库中不允许存储明文密码</h3><h2 id="字段规范"><a href="#字段规范" class="headerlink" title="字段规范"></a>字段规范</h2><h3 id="char、varchar、text等字符串类型定义"><a href="#char、varchar、text等字符串类型定义" class="headerlink" title="char、varchar、text等字符串类型定义"></a>char、varchar、text等字符串类型定义</h3><ul>
<li>对于长度基本固定的列，如果该列恰好更新又特别频繁，适合char</li>
<li>varchar虽然存储变长字符串，但不可太小也不可太大。UTF8最多能存21844个汉字，或65532个英文</li>
<li>varbinary(M)保存的是二进制字符串，它保存的是字节而不是字符，所以没有字符集的概念，M长度0-255（字节）。只用于排序或比较时大小写敏感的类型，不包括密码存储</li>
<li>TEXT类型与VARCHAR都类似，存储可变长度，最大限制也是2^16，但是它20bytes以后的内容是在数据页以外的空间存储（row_format=dynamic），对它的使用需要多一次寻址，没有默认值。<br>一般用于存放容量平均都很大、操作没有其它字段那样频繁的值。<br>网上部分文章说要避免使用text和blob，要知道如果纯用varchar可能会导致行溢出，效果差不多，但因为每行占用字节数过多，会导致buffer_pool能缓存的数据行、页下降。另外text和blob上面一般不会去建索引，而是利用sphinx之类的第三方全文搜索引擎，如果确实要创建（前缀）索引，那就会影响性能。凡事看具体场景。<br>另外尽可能把text/blob拆到另一个表中</li>
<li>BLOB可以看出varbinary的扩展版本，内容以二进制字符串存储，无字符集，区分大小写，有一种经常提但不用的场景：不要在数据库里存储图片。</li>
</ul>
<h3 id="int、tinyint、decimal等数字类型定义"><a href="#int、tinyint、decimal等数字类型定义" class="headerlink" title="int、tinyint、decimal等数字类型定义"></a>int、tinyint、decimal等数字类型定义</h3><ul>
<li>使用tinyint来代替 enum和boolean<br>ENUM类型在需要修改或增加枚举值时，需要在线DDL，成本较高；ENUM列值如果含有数字类型，可能会引起默认值混淆<br>tinyint使用1个字节，一般用于status,type,flag的列</li>
<li>建议使用 UNSIGNED 存储非负数值<br>相比不使用 unsigned，可以扩大一倍使用数值范围</li>
<li>int使用固定4个字节存储，int(11)与int(4)只是显示宽度的区别</li>
<li>使用Decimal 代替float/double存储精确浮点数<br>对于货币、金额这样的类型，使用decimal，如 decimal(9,2)。float默认只能能精确到6位有效数字</li>
</ul>
<h3 id="timestamp与datetime选择"><a href="#timestamp与datetime选择" class="headerlink" title="timestamp与datetime选择"></a>timestamp与datetime选择</h3><ul>
<li>datetime 和 timestamp类型所占的存储空间不同，前者8个字节，后者4个字节，这样造成的后果是两者能表示的时间范围不同。前者范围为1000-01-01 00:00:00 ~ 9999-12-31 23:59:59，后者范围为 1970-01-01 08:00:01 到 2038-01-19 11:14:07 。所以 TIMESTAMP 支持的范围比 DATATIME 要小。</li>
<li>timestamp可以在insert/update行时，自动更新时间字段（如 f_set_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP），但一个表只能有一个这样的定义。</li>
<li>timestamp显示与时区有关，内部总是以 UTC 毫秒 来存的。还受到严格模式的限制</li>
<li>优先使用timestamp，datetime也没问题</li>
<li>where条件里不要对时间列上使用时间函数</li>
</ul>
<h3 id="建议字段都定义为NOT-NULL"><a href="#建议字段都定义为NOT-NULL" class="headerlink" title="建议字段都定义为NOT NULL"></a>建议字段都定义为NOT NULL</h3><ul>
<li>如果是索引字段，一定要定义为not null 。因为null值会影响cordinate统计，影响优化器对索引的选择</li>
<li>如果不能保证insert时一定有值过来，定义时使用default ‘’ ，或 0</li>
</ul>
<h3 id="同一意义的字段定义必须相同"><a href="#同一意义的字段定义必须相同" class="headerlink" title="同一意义的字段定义必须相同"></a>同一意义的字段定义必须相同</h3><p>比如不同表中都有 f_user_id 字段，那么它的类型、字段长度要设计成一样</p>
<h2 id="索引规范"><a href="#索引规范" class="headerlink" title="索引规范"></a>索引规范</h2><h3 id="任何新的select-update-delete上线，都要先explain，看索引使用情况"><a href="#任何新的select-update-delete上线，都要先explain，看索引使用情况" class="headerlink" title="任何新的select,update,delete上线，都要先explain，看索引使用情况"></a>任何新的select,update,delete上线，都要先explain，看索引使用情况</h3><p>尽量避免extra列出现：Using File Sort，Using Temporary，rows超过1000的要谨慎上线。<br><strong><code>explain解读</code></strong></p>
<ul>
<li><code>type</code>：ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）</li>
<li><code>possible_keys</code>：指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用</li>
<li><code>key</code>：表示MySQL实际决定使用的键（索引）<br>如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX</li>
<li><code>ref</code>：表示选择 key 列上的索引，哪些列或常量被用于查找索引列上的值</li>
<li><code>rows</code>：根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数</li>
<li><code>Extra</code><ul>
<li><code>Using temporary</code>：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询</li>
<li><code>Using filesort</code>：MySQL中无法利用索引完成的排序操作称为“文件排序”</li>
</ul>
</li>
</ul>
<h3 id="索引个数限制"><a href="#索引个数限制" class="headerlink" title="索引个数限制"></a>索引个数限制</h3><ul>
<li>索引是双刃剑，会增加维护负担，增大IO压力，索引占用空间是成倍增加的</li>
<li>单张表的索引数量控制在5个以内，或不超过表字段个数的20%。若单张表多个字段在查询需求上都要单独用到索引，需要经过DBA评估。</li>
</ul>
<h3 id="避免冗余索引"><a href="#避免冗余索引" class="headerlink" title="避免冗余索引"></a>避免冗余索引</h3><ul>
<li>InnoDB表是一棵索引组织表，主键是和数据放在一起的聚集索引，普通索引最终指向的是主键地址，所以把主键做最后一列是多余的。如f_crm_id作为主键，联合索引(f_user_id,f_crm_id)上的f_crm_id就完全多余</li>
<li>(a,b,c)、(a,b)，后者为冗余索引。可以利用前缀索引来达到加速目的，减轻维护负担</li>
</ul>
<h3 id="没有特殊要求，使用自增id作为主键"><a href="#没有特殊要求，使用自增id作为主键" class="headerlink" title="没有特殊要求，使用自增id作为主键"></a>没有特殊要求，使用自增id作为主键</h3><ul>
<li>主键是一种聚集索引，顺序写入。组合唯一索引作为主键的话，是随机写入，适合写少读多的表</li>
<li>主键不允许更新</li>
</ul>
<h3 id="索引尽量建在选择性高的列上"><a href="#索引尽量建在选择性高的列上" class="headerlink" title="索引尽量建在选择性高的列上"></a>索引尽量建在选择性高的列上</h3><ul>
<li>不在低基数列上建立索引，例如性别、类型。但有一种情况，idx_feedbackid_type (f_feedback_id,f_type)，如果经常用 f_type=1 比较，而且能过滤掉90%行，那这个组合索引就值得创建。有时候同样的查询语句，由于条件取值不同导致使用不同的索引，也是这个道理。</li>
<li>索引选择性计算方法（基数 ÷ 数据行数）<br>Selectivity = Cardinality / Total Rows = select count(distinct col1)/count(*) from tbname，越接近1说明col1上使用索引的过滤效果越好</li>
<li>走索引扫描行数超过30%时，改全表扫描</li>
</ul>
<h3 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h3><ul>
<li>mysql使用联合索引时，从左向右匹配，遇到断开或者范围查询时，无法用到后续的索引列<br>比如索引idx_c1_c2_c3 (c1,c2,c3)，相当于创建了(c1)、(c1,c2)、(c1,c2,c3)三个索引，where条件包含上面三种情况的字段比较则可以用到索引，但像 where c1=a and c3=c 只能用到c1列的索引，像 c2=b and c3=c等情况就完全用不到这个索引</li>
<li>遇到范围查询(&gt;、&lt;、between、like)也会停止索引匹配，比如 c1=a and c2 &gt; 2 and c3=c，只有c1,c2列上的比较能用到索引，(c1,c2,c3)排列的索引才可能会都用上</li>
<li>where条件里面字段的顺序与索引顺序无关，mysql优化器会自动调整顺序</li>
</ul>
<h3 id="前缀索引"><a href="#前缀索引" class="headerlink" title="前缀索引"></a>前缀索引</h3><ul>
<li>对超过30个字符长度的列创建索引时，考虑使用前缀索引，如 idx_cs_guid2 (f_cs_guid(26))表示截取前26个字符做索引，既可以提高查找效率，也可以节省空间</li>
<li>前缀索引也有它的缺点是，如果在该列上 ORDER BY 或 GROUP BY 时无法使用索引，也不能把它们用作覆盖索引(Covering Index)</li>
<li>如果在varbinary或blob这种以二进制存储的列上建立前缀索引，要考虑字符集，括号里表示的是字节数</li>
</ul>
<h3 id="合理使用覆盖索引减少IO"><a href="#合理使用覆盖索引减少IO" class="headerlink" title="合理使用覆盖索引减少IO"></a>合理使用覆盖索引减少IO</h3><p>INNODB存储引擎中，secondary index(非主键索引，又称为辅助索引、二级索引)没有直接存储行地址，而是存储主键值。<br>如果用户需要查询secondary index中所不包含的数据列，则需要先通过secondary index查找到主键值，然后再通过主键查询到其他数据列，因此需要查询两次。覆盖索引则可以在一个索引中获取所有需要的数据列，从而避免回表进行二次查找，节省IO因此效率较高。<br>例如SELECT email，uid FROM user_email WHERE uid=xx，如果uid不是主键，适当时候可以将索引添加为index(uid，email)，以获得性能提升。</p>
<h3 id="尽量不要在频繁更新的列上创建索引"><a href="#尽量不要在频繁更新的列上创建索引" class="headerlink" title="尽量不要在频繁更新的列上创建索引"></a>尽量不要在频繁更新的列上创建索引</h3><p>如不在定义了 ON UPDATE CURRENT_STAMP 的列上创建索引，维护成本太高（好在mysql有insert buffer，会合并索引的插入）</p>
<h2 id="SQL设计"><a href="#SQL设计" class="headerlink" title="SQL设计"></a>SQL设计</h2><h3 id="杜绝直接-SELECT-读取全部字段"><a href="#杜绝直接-SELECT-读取全部字段" class="headerlink" title="杜绝直接 SELECT * 读取全部字段"></a>杜绝直接 SELECT * 读取全部字段</h3><p>即使需要所有字段，减少网络带宽消耗，能有效利用覆盖索引，表结构变更对程序基本无影响</p>
<h3 id="能确定返回结果只有一条时，使用-limit-1"><a href="#能确定返回结果只有一条时，使用-limit-1" class="headerlink" title="能确定返回结果只有一条时，使用 limit 1"></a>能确定返回结果只有一条时，使用 limit 1</h3><p><strong>在保证数据不会有误的前提下</strong>，能确定结果集数量时，多使用limit，尽快的返回结果。</p>
<h3 id="小心隐式类型转换"><a href="#小心隐式类型转换" class="headerlink" title="小心隐式类型转换"></a>小心隐式类型转换</h3><ul>
<li><p>转换规则</p>
<blockquote>
<p>a. 两个参数至少有一个是 NULL 时，比较的结果也是 NULL，例外是使用 &lt;=&gt; 对两个 NULL 做比较时会返回 1，这两种情况都不需要做类型转换<br>b. 两个参数都是字符串，会按照字符串来比较，不做类型转换<br>c. 两个参数都是整数，按照整数来比较，不做类型转换<br>d. 十六进制的值和非数字做比较时，会被当做二进制串<br>e. 有一个参数是 TIMESTAMP 或 DATETIME，并且另外一个参数是常量，常量会被转换为 timestamp<br>f. 有一个参数是 decimal 类型，如果另外一个参数是 decimal 或者整数，会将整数转换为 decimal 后进行比较，如果另外一个参数是浮点数，则会把 decimal 转换为浮点数进行比较<br>g. 所有其他情况下，两个参数都会被转换为浮点数再进行比较。</p>
</blockquote>
</li>
<li><p>如果一个索引建立在string类型上，如果这个字段和一个int类型的值比较，符合第 g 条。如f_phone定义的类型是varchar，但where使用f_phone in (098890)，两个参数都会被当成成浮点型。发生这个隐式转换并不是最糟的，最糟的是string转换后的float，mysql无法使用索引，这才导致了性能问题。如果是 f_user_id = ‘1234567’ 的情况，符合第 b 条,直接把数字当字符串比较。</p>
</li>
</ul>
<h3 id="禁止在where条件列上使用函数"><a href="#禁止在where条件列上使用函数" class="headerlink" title="禁止在where条件列上使用函数"></a>禁止在where条件列上使用函数</h3><ul>
<li>会导致索引失效，如lower(email)，f_qq % 4。可放到右边的常量上计算</li>
<li>返回小结果集不是很大的情况下，可以对返回列使用函数，简化程序开发</li>
</ul>
<h3 id="使用like模糊匹配，-不要放首位"><a href="#使用like模糊匹配，-不要放首位" class="headerlink" title="使用like模糊匹配，%不要放首位"></a>使用like模糊匹配，%不要放首位</h3><p>会导致索引失效，有这种搜索需求是，考虑其它方案，如sphinx全文搜索</p>
<h3 id="涉及到复杂sql时，务必先参考已有索引设计，先explain"><a href="#涉及到复杂sql时，务必先参考已有索引设计，先explain" class="headerlink" title="涉及到复杂sql时，务必先参考已有索引设计，先explain"></a>涉及到复杂sql时，务必先参考已有索引设计，先explain</h3><ul>
<li>简单SQL拆分，不以代码处理复杂为由。</li>
<li>比如 OR 条件： f_phone=’10000’ or f_mobile=’10000’，两个字段各自有索引，但只能用到其中一个。可以拆分成2个sql，或者union all。</li>
<li>先explain的好处是可以为了利用索引，增加更多查询限制条件</li>
</ul>
<h3 id="使用join时，where条件尽量使用充分利用同一表上的索引"><a href="#使用join时，where条件尽量使用充分利用同一表上的索引" class="headerlink" title="使用join时，where条件尽量使用充分利用同一表上的索引"></a>使用join时，where条件尽量使用充分利用同一表上的索引</h3><ul>
<li>如 select t1.a,t2.b * from t1,t2 and t1.a=t2.a and t1.b=123 and t2.c= 4 ，如果t1.c与t2.c字段相同，那么t1上的索引(b,c)就只用到b了。此时如果把where条件中的t2.c=4改成t1.c=4，那么可以用到完整的索引</li>
<li>这种情况可能会在字段冗余设计（反范式）时出现</li>
<li>正确选取inner join和left join</li>
</ul>
<h3 id="少用子查询，改用join"><a href="#少用子查询，改用join" class="headerlink" title="少用子查询，改用join"></a>少用子查询，改用join</h3><p>小于5.6版本时，子查询效率很低，不像Oracle那样先计算子查询后外层查询。5.6版本开始得到优化</p>
<h3 id="考虑使用union-all，少使用union，注意考虑去重"><a href="#考虑使用union-all，少使用union，注意考虑去重" class="headerlink" title="考虑使用union all，少使用union，注意考虑去重"></a>考虑使用union all，少使用union，注意考虑去重</h3><ul>
<li>union all不去重，而少了排序操作，速度相对比union要快，如果没有去重的需求，优先使用union all</li>
<li>如果UNION结果中有使用limit，在2个子SQL可能有许多返回值的情况下，各自加上limit。如果还有order by，请找DBA。</li>
</ul>
<h3 id="IN的内容尽量不超过200个"><a href="#IN的内容尽量不超过200个" class="headerlink" title="IN的内容尽量不超过200个"></a>IN的内容尽量不超过200个</h3><p>超过500个值使用批量的方式，否则一次执行会影响数据库的并发能力，因为单SQL只能且一直占用单CPU，而且可能导致主从复制延迟</p>
<h3 id="拒绝大事务"><a href="#拒绝大事务" class="headerlink" title="拒绝大事务"></a>拒绝大事务</h3><p>比如在一个事务里进行多个select，多个update，如果是高频事务，会严重影响MySQL并发能力，因为事务持有的锁等资源只在事务rollback/commit时才能释放。但同时也要权衡数据写入的一致性。</p>
<h3 id="避免使用is-null-is-not-null这样的比较"><a href="#避免使用is-null-is-not-null这样的比较" class="headerlink" title="避免使用is null, is not null这样的比较"></a>避免使用is null, is not null这样的比较</h3><h3 id="order-by-limit"><a href="#order-by-limit" class="headerlink" title="order by .. limit"></a>order by .. limit</h3><p>这种查询更多的是通过索引去优化，但order by的字段有讲究，比如主键id与f_time都是顺序递增，那就可以考虑order by id而非 f_time 。</p>
<h3 id="c1-lt-a-order-by-c2"><a href="#c1-lt-a-order-by-c2" class="headerlink" title="c1 &lt; a order by c2"></a>c1 &lt; a order by c2</h3><p>与上面不同的是，order by之前有个范围查询，由前面的内容可知，用不到类似(c1,c2)的索引，但是可以利用(c2,c1)索引。另外还可以改写成join的方式实现。</p>
<h3 id="分页优化"><a href="#分页优化" class="headerlink" title="分页优化"></a>分页优化</h3><p>建议使用合理的分页方式以提高分页效率，大页情况下不使用跳跃式分页<br>假如有类似下面分页语句:<br>SELECT FROM table1 ORDER BY ftime DESC LIMIT 10000,10;<br>这种分页方式会导致大量的io，因为MySQL使用的是提前读取策略。<br>推荐分页方式：<br><code>SELECT FROM table1 WHERE ftime &lt; last_time ORDER BY ftime DESC LIMIT 10</code><br>即传入上一次分页的界值</p>
<p>SELECT * FROM table as t1 inner JOIN (SELECT id FROM table ORDER BY time LIMIT 10000，10) as t2 ON t1.id=t2.id</p>
<h3 id="count计数"><a href="#count计数" class="headerlink" title="count计数"></a>count计数</h3><ul>
<li>首先count()、count(1)、count(col1)是有区别的，count()表示整个结果集有多少条记录，count(1)表示结果集里以primary key统计数量，绝大多数情况下count()与count(1)效果一样的，但count(col1)表示的是结果集里 col1 列 NOT null 的记录数。优先采用count()</li>
<li>大数据量count是消耗资源的操作，甚至会拖慢整个库，查询性能问题无法解决的，应从产品设计上进行重构。例如当频繁需要count的查询，考虑使用汇总表</li>
<li>遇到distinct的情况，group by方式可能效率更高。</li>
</ul>
<h3 id="delete-update语句改成select再explain"><a href="#delete-update语句改成select再explain" class="headerlink" title="delete,update语句改成select再explain"></a>delete,update语句改成select再explain</h3><p>select最多导致数据库慢，写操作才是锁表的罪魁祸首</p>
<h3 id="减少与数据库交互的次数，尽量采用批量SQL语句"><a href="#减少与数据库交互的次数，尽量采用批量SQL语句" class="headerlink" title="减少与数据库交互的次数，尽量采用批量SQL语句"></a>减少与数据库交互的次数，尽量采用批量SQL语句</h3><ul>
<li><code>INSERT ... ON DUPLICATE KEY UPDATE ...</code>，插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行旧行UPDATE，如果不重复则直接插入，影响1行。</li>
<li><code>REPLACE INTO</code>类似，但它是冲突时删除旧行。<code>INSERT IGNORE</code>相反，保留旧行，丢弃要插入的新行。</li>
<li>INSERT INTO VALUES(),(),()，合并插入。</li>
</ul>
<h3 id="杜绝危险SQL"><a href="#杜绝危险SQL" class="headerlink" title="杜绝危险SQL"></a>杜绝危险SQL</h3><ul>
<li>去掉where 1=1 这样无意义或恒真的条件，如果遇到update/delete或遭到sql注入就恐怖了</li>
<li>SQL中不允许出现DDL语句。一般也不给予create/alter这类权限，但阿里云RDS只区分读写用户</li>
</ul>
<h2 id="行为规范"><a href="#行为规范" class="headerlink" title="行为规范"></a>行为规范</h2><ul>
<li>不允许在DBA不知情的情况下导现网数据</li>
<li>大批量更新，如修复数据，避开高峰期，并通知DBA。直接执行sql的由运维或DBA同事操作</li>
<li>及时处理已下线业务的SQL</li>
<li>复杂sql上线审核</li>
<li>重要项目的数据库方案选型和设计必须提前通知DBA参与</li>
</ul>
<hr>
<p>原文地址：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Advanced MySQL Query Tuning]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-query.html</url>
      <content type="html"><![CDATA[<iframe src="//www.slideshare.net/slideshow/embed_code/key/3HLJJcJmM9KLGT" width="100%" height="550" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" allowfullscreen> </iframe>

<p>Youtube: <a href="https://www.youtube.com/watch?v=TPFibi2G_oo" target="_blank" rel="external">https://www.youtube.com/watch?v=TPFibi2G_oo</a></p>
<p>Percona webinars上有许多类似的分享，传送门： <a href="https://www.percona.com/resources/webinars" target="_blank" rel="external">https://www.percona.com/resources/webinars</a> 。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[nginx配置location与rewrite规则教程]]></title>
      <url>http://team.jiunile.com/blog/2016/07/nginx-localtion-rewrite.html</url>
      <content type="html"><![CDATA[<h2 id="location教程"><a href="#location教程" class="headerlink" title="location教程"></a>location教程</h2><p><strong>示例：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">location  = / &#123;</span><br><span class="line">    # 精确匹配 / ，主机名后面不能带任何字符串</span><br><span class="line">    [ configuration A ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location  / &#123;</span><br><span class="line">    # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求</span><br><span class="line">    # 但是正则和最长字符串会优先匹配</span><br><span class="line">    [ configuration B ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /documents/ &#123;</span><br><span class="line">    # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">    # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">    [ configuration C ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ /documents/Abc &#123;</span><br><span class="line">    # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">    # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">    [ configuration CC ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ^~ /images/ &#123;</span><br><span class="line">    # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。</span><br><span class="line">    [ configuration D ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~* \.(gif|jpg|jpeg)$ &#123;</span><br><span class="line">    # 匹配所有以 gif,jpg或jpeg 结尾的请求</span><br><span class="line">    # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则</span><br><span class="line">    [ configuration E ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /images/ &#123;</span><br><span class="line">    # 字符匹配到 /images/，继续往下，会发现 ^~ 存在</span><br><span class="line">    [ configuration F ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /images/abc &#123;</span><br><span class="line">    # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在</span><br><span class="line">    # F与G的放置顺序是没有关系的</span><br><span class="line">    [ configuration G ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ /images/abc/ &#123;</span><br><span class="line">    # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用</span><br><span class="line">    [ configuration H ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~* /js/.*/\.js</span><br></pre></td></tr></table></figure></p>
<ul>
<li>已=开头表示精确匹配<br>如 A 中只匹配根目录结尾的请求，后面不能带任何字符串。</li>
<li>^~ 开头表示uri以某个常规字符串开头，不是正则匹配</li>
<li>~ 开头表示区分大小写的正则匹配</li>
<li>~* 开头表示不区分大小写的正则匹配</li>
<li>/ 通用匹配, 如果没有其它匹配,任何请求都会匹配到</li>
</ul>
<a id="more"></a>
<p><strong><code>顺序&amp;&amp;优先级</code></strong></p>
<blockquote>
<p>(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/)</p>
</blockquote>
<p>按照上面的location写法，以下的匹配示例成立：</p>
<ul>
<li><p>/ —&gt; config A</p>
<blockquote>
<p>精确完全匹配，即使/index.html也匹配不了</p>
</blockquote>
</li>
<li><p>/downloads/download.html —&gt; config B</p>
<blockquote>
<p>匹配B以后，往下没有任何匹配，采用B</p>
</blockquote>
</li>
<li><p>/images/1.gif —&gt; configuration D</p>
<blockquote>
<p>匹配到F，往下匹配到D，停止往下</p>
</blockquote>
</li>
<li><p>/images/abc/def —&gt; config D</p>
<blockquote>
<p>最长匹配到G，往下匹配D，停止往下<br>你可以看到 任何以/images/开头的都会匹配到D并停止，FG写在这里是没有任何意义的，H是永远轮不到的，这里只是为了说明匹配顺序</p>
</blockquote>
</li>
<li><p>/documents/document.html —&gt; config C</p>
<blockquote>
<p>匹配到C，往下没有任何匹配，采用C</p>
</blockquote>
</li>
<li><p>/documents/1.jpg —&gt; configuration E</p>
<blockquote>
<p>匹配到C，往下正则匹配到E</p>
</blockquote>
</li>
<li><p>/documents/Abc.jpg —&gt; config CC</p>
<blockquote>
<p>最长匹配到C，往下正则顺序匹配到CC，不会往下到E</p>
</blockquote>
</li>
</ul>
<h3 id="实际使用建议"><a href="#实际使用建议" class="headerlink" title="实际使用建议"></a>实际使用建议</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">所以实际使用中，个人觉得至少有三个匹配规则定义，如下：</span><br><span class="line">#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。</span><br><span class="line">#这里是直接转发给后端应用服务器了，也可以是一个静态首页</span><br><span class="line"># 第一个必选规则</span><br><span class="line">location = / &#123;</span><br><span class="line">    proxy_pass http://tomcat:8080/index</span><br><span class="line">&#125;</span><br><span class="line"># 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项</span><br><span class="line"># 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用</span><br><span class="line">location ^~ /static/ &#123;</span><br><span class="line">    root /webroot/static/;</span><br><span class="line">&#125;</span><br><span class="line">location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ &#123;</span><br><span class="line">    root /webroot/res/;</span><br><span class="line">&#125;</span><br><span class="line">#第三个规则就是通用规则，用来转发动态请求到后端应用服务器</span><br><span class="line">#非静态文件请求就默认是动态请求，自己根据实际把握</span><br><span class="line">#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了</span><br><span class="line">location / &#123;</span><br><span class="line">    proxy_pass http://tomcat:8080/</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Rewrite教程"><a href="#Rewrite教程" class="headerlink" title="Rewrite教程"></a>Rewrite教程</h2><p>rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在<code>server{},location{},if{}</code>中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 <code>http://seanlook.com/a/we/index.php?id=1&amp;u=str</code> 只对<code>/a/we/index.php</code>重写。语法<code>rewrite regex replacement [flag];</code></p>
<p>如果相对域名或参数字符串起作用，可以使用全局变量匹配，也可以使用proxy_pass反向代理。</p>
<p>表明看rewrite和location功能有点像，都能实现跳转，主要区别在于rewrite是在同一域名内更改获取资源的路径，而location是对一类路径做控制访问或反向代理，可以proxy_pass到其他机器。很多情况下rewrite也会写在location里，它们的执行顺序是：</p>
<ol>
<li>执行server块的rewrite指令</li>
<li>执行location匹配</li>
<li>执行选定的location中的rewrite指令</li>
</ol>
<p>如果其中某步URI被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 Internal Server Error错误。</p>
<h3 id="flag标志位"><a href="#flag标志位" class="headerlink" title="flag标志位"></a>flag标志位</h3><ul>
<li><code>last</code> : 相当于Apache的[L]标记，表示完成rewrite</li>
<li><code>break</code>: 停止执行当前虚拟主机的后续rewrite指令集</li>
<li><code>redirect</code> : 返回302临时重定向，地址栏会显示跳转后的地址</li>
<li><code>permanent</code> : 返回301永久重定向，地址栏会显示跳转后的地址</li>
</ul>
<p>因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解：</p>
<ol>
<li>last一般写在server和if中，而break一般使用在location中</li>
<li>last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配</li>
<li>break和last都能组织继续执行后面的rewrite指令</li>
</ol>
<h3 id="if指令与全局变量"><a href="#if指令与全局变量" class="headerlink" title="if指令与全局变量"></a>if指令与全局变量</h3><p><strong>if判断指令</strong><br>语法为<code>if(condition){...}</code>，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：</p>
<ul>
<li>当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false</li>
<li>直接比较变量和内容时，使用<code>=</code>或<code>!=</code></li>
<li><code>~</code>正则表达式匹配，<code>~*</code>不区分大小写的匹配，<code>!~</code>区分大小写的不匹配</li>
<li><code>-f</code>和<code>!-f</code>用来判断是否存在文件</li>
<li><code>-d</code>和<code>!-d</code>用来判断是否存在目录</li>
<li><code>-e</code>和<code>!-e</code>用来判断是否存在文件或目录</li>
<li><code>-x</code>和<code>!-x</code>用来判断文件是否可执行</li>
</ul>
<p><strong>例如：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">if ($http_user_agent ~ MSIE) &#123;</span><br><span class="line">    rewrite ^(.*)$ /msie/$1 break;</span><br><span class="line">&#125; #如果UA包含"MSIE"，rewrite请求到/msid/目录下</span><br><span class="line"></span><br><span class="line">if ($http_cookie ~* "id=([^;]+)(?:;|$)") &#123;</span><br><span class="line">    set $id $1;</span><br><span class="line"> &#125; #如果cookie匹配正则，设置变量$id等于正则引用部分</span><br><span class="line"></span><br><span class="line">if ($request_method = POST) &#123;</span><br><span class="line">    return 405;</span><br><span class="line">&#125; #如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302</span><br><span class="line"></span><br><span class="line">if ($slow) &#123;</span><br><span class="line">    limit_rate 10k;</span><br><span class="line">&#125; #限速，$slow可以通过 set 指令设置</span><br><span class="line"></span><br><span class="line">if (!-f $request_filename)&#123;</span><br><span class="line">    break;</span><br><span class="line">    proxy_pass  http://127.0.0.1; </span><br><span class="line">&#125; #如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查</span><br><span class="line"></span><br><span class="line">if ($args ~ post=140)&#123;</span><br><span class="line">    rewrite ^ http://example.com/ permanent;</span><br><span class="line">&#125; #如果query string中包含"post=140"，永久重定向到example.com</span><br><span class="line"></span><br><span class="line">location ~* \.(gif|jpg|png|swf|flv)$ &#123;</span><br><span class="line">    valid_referers none blocked www.jefflei.com www.leizhenfang.com;</span><br><span class="line">    if ($invalid_referer) &#123;</span><br><span class="line">        return 404;</span><br><span class="line">    &#125; #防盗链</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>全局变量</strong><br>下面是可以用作if判断的全局变量</p>
<ul>
<li><code>$args</code>： #这个变量等于请求行中的参数，同<code>$query_string</code></li>
<li><code>$content_length</code> ： 请求头中的Content-length字段。</li>
<li><code>$content_type</code> ： 请求头中的Content-Type字段。</li>
<li><code>$document_root</code> ： 当前请求在root指令中指定的值。</li>
<li><code>$host</code> ： 请求主机头字段，否则为服务器名称。</li>
<li><code>$http_user_agent</code> ： 客户端agent信息</li>
<li><code>$http_cookie</code> ： 客户端cookie信息</li>
<li><code>$limit_rate</code> ： 这个变量可以限制连接速率。</li>
<li><code>$request_method</code> ： 客户端请求的动作，通常为GET或POST。</li>
<li><code>$remote_addr</code> ： 客户端的IP地址。</li>
<li><code>$remote_port</code> ： 客户端的端口。</li>
<li><code>$remote_user</code> ： 已经经过Auth Basic Module验证的用户名。</li>
<li><code>$request_filename</code> ： 当前请求的文件路径，由root或alias指令与URI请求生成。</li>
<li><code>$scheme</code> ： HTTP方法（如http，https）。</li>
<li><code>$server_protocol</code> ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。</li>
<li><code>$server_addr</code> ： 服务器地址，在完成一次系统调用后可以确定这个值。</li>
<li><code>$server_name</code> ： 服务器名称。</li>
<li><code>$server_port</code> ： 请求到达服务器的端口号。</li>
<li><code>$request_uri</code> ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。</li>
<li><code>$uri</code> ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。</li>
<li><code>$document_uri</code> ： 与$uri相同。</li>
</ul>
<p>示例：<code>http://localhost:88/test1/test2/test.php</code></p>
<ul>
<li><code>$host</code>：localhost</li>
<li><code>$server_port</code>：88</li>
<li><code>$request_uri</code>：<a href="http://localhost:88/test1/test2/test.php" target="_blank" rel="external">http://localhost:88/test1/test2/test.php</a></li>
<li><code>$document_uri</code>：/test1/test2/test.php</li>
<li><code>$document_root</code>：/var/www/html</li>
<li><code>$request_filename</code>：/var/www/html/test1/test2/test.php</li>
</ul>
<h3 id="常用正则"><a href="#常用正则" class="headerlink" title="常用正则"></a>常用正则</h3><ul>
<li><code>.</code> ： 匹配除换行符以外的任意字符</li>
<li><code>?</code> ： 重复0次或1次</li>
<li><code>+</code> ： 重复1次或更多次</li>
<li><code>*</code> ： 重复0次或更多次</li>
<li><code>\d</code> ：匹配数字</li>
<li><code>^</code> ： 匹配字符串的开始</li>
<li><code>$</code> ： 匹配字符串的介绍</li>
<li><code>{n}</code> ： 重复n次</li>
<li><code>{n,}</code> ： 重复n次或更多次</li>
<li><code>[c]</code> ： 匹配单个字符c</li>
<li><code>[a-z]</code> ： 匹配a-z小写字母的任意一个</li>
</ul>
<p>小括号<code>()</code>之间匹配的内容，可以在后面通过<code>$1</code>来引用，<code>$2</code>表示的是前面第二个<code>()</code>里的内容。正则里面容易让人困惑的是<code>\</code>转义特殊字符。</p>
<h3 id="rewrite实例"><a href="#rewrite实例" class="headerlink" title="rewrite实例"></a>rewrite实例</h3><p><strong>例1：</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="comment"># 定义image日志格式</span></span><br><span class="line">    <span class="attribute">log_format</span> imagelog <span class="string">'[<span class="variable">$time_local</span>] '</span> <span class="variable">$image_file</span> <span class="string">' '</span> <span class="variable">$image_type</span> <span class="string">' '</span> <span class="variable">$body_bytes_sent</span> <span class="string">' '</span> <span class="variable">$status</span>;</span><br><span class="line">    <span class="comment"># 开启重写日志</span></span><br><span class="line">    <span class="attribute">rewrite_log</span> <span class="literal">on</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">root</span> /home/www;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">location</span> / &#123;</span><br><span class="line">                <span class="comment"># 重写规则信息</span></span><br><span class="line">                <span class="attribute">error_log</span> logs/rewrite.log <span class="literal">notice</span>; </span><br><span class="line">                <span class="comment"># 注意这里要用‘’单引号引起来，避免&#123;&#125;</span></span><br><span class="line">                <span class="attribute">rewrite</span> <span class="string">'^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$'</span> /data?file=<span class="variable">$3</span>.<span class="variable">$4</span>;</span><br><span class="line">                <span class="comment"># 注意不能在上面这条规则后面加上“last”参数，否则下面的set指令不会执行</span></span><br><span class="line">                <span class="attribute">set</span> <span class="variable">$image_file</span> <span class="variable">$3</span>;</span><br><span class="line">                <span class="attribute">set</span> <span class="variable">$image_type</span> <span class="variable">$4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">location</span> /data &#123;</span><br><span class="line">                <span class="comment"># 指定针对图片的日志格式，来分析图片类型和大小</span></span><br><span class="line">                <span class="attribute">access_log</span> logs/images.log mian;</span><br><span class="line">                <span class="attribute">root</span> /data/images;</span><br><span class="line">                <span class="comment"># 应用前面定义的变量。判断首先文件在不在，不在再判断目录在不在，如果还不在就跳转到最后一个url里</span></span><br><span class="line">                <span class="attribute">try_files</span> /<span class="variable">$arg_file</span> /image404.html;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="attribute">location</span> = /image404.html &#123;</span><br><span class="line">                <span class="comment"># 图片不存在返回特定的信息</span></span><br><span class="line">                <span class="attribute">return</span> <span class="number">404</span> <span class="string">"image not found\n"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对形如<code>/images/ef/uh7b3/test.png</code>的请求，重写到<code>/data?file=test.png</code>，于是匹配到<code>location /data</code>，先看<code>/data/images/test.png</code>文件存不存在，如果存在则正常响应，如果不存在则重写<code>tryfiles</code>到新的<code>image404 location</code>，直接返回404状态码。</p>
<p><strong>例2：</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rewrite</span><span class="regexp"> ^/images/(.*)_(\d+)x(\d+)\.(png|jpg|gif)$</span> /resizer/<span class="variable">$1</span>.<span class="variable">$4</span>?width=<span class="variable">$2</span>&amp;height=<span class="variable">$3</span>? <span class="literal">last</span>;</span><br></pre></td></tr></table></figure></p>
<p>对形如<code>/images/bla_500x400.jpg</code>的文件请求，重写到<code>/resizer/bla.jpg?width=500&amp;height=400</code>地址，并会继续尝试匹配location。</p>
<hr>
<p>来源：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL事务隔离级别]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-transaction-level.html</url>
      <content type="html"><![CDATA[<h2 id="四类隔离级别"><a href="#四类隔离级别" class="headerlink" title="四类隔离级别"></a>四类隔离级别</h2><p>SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。</p>
<ul>
<li>Read Uncommitted（读取未提交内容）</li>
</ul>
<p>在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 </p>
<ul>
<li>Read Committed（读取提交内容）</li>
</ul>
<p>这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。</p>
<ul>
<li>Repeatable Read（可重读）</li>
</ul>
<p>这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。</p>
<ul>
<li>Serializable（可串行化）</li>
</ul>
<p>这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。<br><a id="more"></a></p>
<h2 id="隔离级别与一致性"><a href="#隔离级别与一致性" class="headerlink" title="隔离级别与一致性"></a>隔离级别与一致性</h2><p>这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：</p>
<ul>
<li>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</li>
<li>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</li>
<li>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</li>
</ul>
<p>在MySQL中，实现了这四种隔离级别，分别有可能产生问题如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:left">隔离级别</th>
<th style="text-align:left">脏读</th>
<th style="text-align:left">不可重复读</th>
<th style="text-align:left">幻读</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读未提交(Read Uncommitted)</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">读已提交(Read Committed)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">可重复读(Repeatable Read)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">可串行化(Serializable)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
</tr>
</tbody>
</table>
<h2 id="设置当前隔离级别"><a href="#设置当前隔离级别" class="headerlink" title="设置当前隔离级别"></a>设置当前隔离级别</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 取消autocommit</span></span><br><span class="line"><span class="keyword">set</span> autocommit=<span class="number">0</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">"%autocommit%"</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 查看隔离级别</span></span><br><span class="line"><span class="keyword">SELECT</span> @@global.tx_isolation;</span><br><span class="line"><span class="keyword">SELECT</span> @@session.tx_isolation;</span><br><span class="line"><span class="keyword">SELECT</span> @@tx_isolation;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%iso%'</span>;</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| Variable_name | Value           |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| tx_isolation  | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> <span class="keyword">global</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%iso%'</span>;</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| Variable_name | Value           |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| tx_isolation  | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 设置隔离级别</span></span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">read</span> uncommitted;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">read</span> committed;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> repeatable <span class="keyword">read</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">serializable</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 事务操作</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> text.tx;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> text.tx;</span><br><span class="line"><span class="keyword">update</span> text.tx <span class="keyword">set</span> <span class="keyword">num</span> =<span class="number">10</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> text.tx(<span class="keyword">id</span>,<span class="keyword">num</span>) <span class="keyword">values</span>(<span class="number">9</span>,<span class="number">9</span>);</span><br><span class="line"><span class="keyword">rollback</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<h2 id="my-cnf设置"><a href="#my-cnf设置" class="headerlink" title="my.cnf设置"></a>my.cnf设置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MySQL支持4种事务隔离级别，他们分别是：</span></span><br><span class="line"><span class="comment"># READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE.</span></span><br><span class="line"><span class="comment"># 如没有指定，MySQL默认采用的是REPEATABLE-READ，ORACLE默认的是READ-COMMITTED</span></span><br><span class="line">transaction_isolation = REPEATABLE-READ</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL为什么需要一个自增主键]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-auto-increment-primary-key.html</url>
      <content type="html"><![CDATA[<h2 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h2><p>表中每一行都应该有可以唯一标识自己的一列（或一组列）。</p>
<p>一个顾客可以使用顾客编号列，而订单可以使用订单ID，雇员可以使用雇员ID 或 雇员社会保险号。</p>
<p>主键（primary key） 一列（或一组列），其值能够唯一区分表中的每个行。<br>唯一标识表中每行的这个列（或这组列）称为主键。<strong><code>没有主键，更新或删除表中特定行很困难，因为没有安全的方法保证只涉及相关的行。</code></strong></p>
<p>虽然并不总是都需要主键，但大多数数据库设计人员都应保证他们创建的每个表有一个主键，以便于以后数据操纵和管理</p>
<p>表中的任何列都可以作为主键，只要它满足一下条件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">任何两行都不具有相同的主键值</span><br><span class="line">每个行都必须具有一个主键值（主键列不允许NULL值）</span><br></pre></td></tr></table></figure></p>
<p>主键值规范：这里列出的规则是MySQL本身强制实施的。</p>
<p>主键的最好习惯：<br>除MySQL强制实施的规则外，应该坚持的几个普遍认为的最好习惯为:</p>
<pre><code class="plain">1、不更新主键列的值
2、不重用主键列的值
3、不在主键列中使用可能会更改的值（例如，如果使用一个名字作为主键以标识某个供应商，应该供应商合并和更改其名字时，必须更改这个主键）
</code></pre>
<p>总之：不应该使用一个具有意义的column（id 本身并不保存表 有意义信息） 作为主键，并且一个表必须要有一个主键，为方便扩展、松耦合，高可用的系统做铺垫。<br><a id="more"></a></p>
<h3 id="无特殊需求下Innodb建议使用与业务无关的自增ID作为主键"><a href="#无特殊需求下Innodb建议使用与业务无关的自增ID作为主键" class="headerlink" title="无特殊需求下Innodb建议使用与业务无关的自增ID作为主键"></a>无特殊需求下Innodb建议使用与业务无关的自增ID作为主键</h3><p>InnoDB引擎使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）</p>
<p>1、如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示：<br><img src="/images/mysql_aipk_1.jpg" alt="mysql_primary_key"><br>这样就会形成一个紧凑的索引结构，近似顺序填满。<strong><code>由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。</code></strong></p>
<p>2、 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置：<br><img src="/images/mysql_aipk_2.jpg" alt="mysql_primary_key"><br><strong><code>此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片。</code></strong>得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p>
<p>在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。</p>
<p><strong><code>mysql 在频繁的更新、删除操作，会产生碎片。而含碎片比较大的表，查询效率会降低。此时需对表进行优化，这样才会使查询变得更有效率。</code></strong></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zephir安装和初体验]]></title>
      <url>http://team.jiunile.com/blog/2016/06/zephir-zephir-02.html</url>
      <content type="html"><![CDATA[<h2 id="Zephir安装"><a href="#Zephir安装" class="headerlink" title="Zephir安装"></a>Zephir安装</h2><h3 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h3><p>Zephir主要依赖于下面环境</p>
<ul>
<li>gcc &gt;= 4.x/clang &gt;= 3.x</li>
<li>re2c 0.13或更高版本</li>
<li>gnu 3.81或更高版本</li>
<li>autoconf 2.31或更高版本</li>
<li>automake 1.14或更高版本</li>
<li>libpcre3</li>
<li>php开发工具-phpize</li>
</ul>
<p>如果你使用Ubuntu，你可以安装所需要的包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install git gcc make re2c php5 php5-json php5-dev libpcre3-dev</span><br></pre></td></tr></table></figure></p>
<p>由于Zephir是用PHP编写的，所以你需要安装php<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ php -v</span><br><span class="line">PHP 5.6.5 (cli) (built: Jan 24 2015 20:04:31)</span><br><span class="line">Copyright (c) 1997-2014 The PHP Group</span><br><span class="line">Zend Engine v2.6.0, Copyright (c) 1998-2014 Zend Technologies</span><br><span class="line">with Zend OPcache v7.0.4-dev, Copyright (c) 1999-2014, by Zend Technologies</span><br></pre></td></tr></table></figure></p>
<p>同时也必须确保安装了PHP开发库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ phpize -v</span><br><span class="line">Configuring <span class="keyword">for</span>:</span><br><span class="line">PHP Api Version:         20131106</span><br><span class="line">Zend Module Api No:      20131226</span><br><span class="line">Zend Extension Api No:   220131226</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="安装Zephir"><a href="#安装Zephir" class="headerlink" title="安装Zephir"></a>安装Zephir</h3><ol>
<li><p>下载最新稳定版</p>
</li>
<li><p>运行Zephir安装程序(编译/创建解析器)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> zephir</span><br><span class="line">$ ./install-json</span><br><span class="line">$ ./install -c</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zephir <span class="built_in">help</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>会得到如下返回</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> _____              __    _</span><br><span class="line">/__  /  ___  ____  / /_  (_)____</span><br><span class="line">  / /  / _ \/ __ \/ __ \/ / ___/</span><br><span class="line"> / /__/  __/ /_/ / / / / / /</span><br><span class="line">/____/\___/ .___/_/ /_/_/_/</span><br><span class="line">         /_/</span><br><span class="line"></span><br><span class="line">Zephir version 0.9.2a-dev</span><br><span class="line"></span><br><span class="line">Usage: </span><br><span class="line">    <span class="built_in">command</span> [options]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">    install             Installs the extension (requires root password)</span><br><span class="line">    builddev            Generate/Compile/Install a Zephir extension <span class="keyword">in</span> development mode</span><br><span class="line">    <span class="built_in">help</span>                Displays this <span class="built_in">help</span></span><br><span class="line">    build               Generate/Compile/Install a Zephir extension</span><br><span class="line">    compile             Compile a Zephir extension</span><br><span class="line">    stubs               Generates extension PHP stubs</span><br><span class="line">    version             Shows the Zephir version</span><br><span class="line">    init [namespace]    Initializes a Zephir extension</span><br><span class="line">    fullclean           Cleans the generated object files <span class="keyword">in</span> compilation</span><br><span class="line">    api [--theme-path=/path][--output-directory=/path][--theme-options=&#123;json&#125;|/path]Generates a HTML API</span><br><span class="line">    generate            Generates C code from the Zephir code</span><br><span class="line">    clean               Cleans the generated object files <span class="keyword">in</span> compilation</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">    <span class="_">-f</span>([a-z0-9\-]+)     Enables compiler optimizations</span><br><span class="line">    -fno-([a-z0-9\-]+)  Disables compiler optimizations</span><br><span class="line">    -w([a-z0-9\-]+)     Turns a warning on</span><br><span class="line">    -W([a-z0-9\-]+)     Turns a warning off</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Zephir初体验"><a href="#Zephir初体验" class="headerlink" title="Zephir初体验"></a>Zephir初体验</h2><p>还记得在开篇那个Helloword例子吗？我们先来简单介绍一下Zephir编译机制，在用例子介绍一下Zephir的语法。</p>
<h3 id="编译-解释"><a href="#编译-解释" class="headerlink" title="编译/解释"></a>编译/解释</h3><p>每一种语言都会有它们的”Hello World!”例子，对于Zehpir来说也不例外，下面的这个引导例子列举了许多它重要的特性。</p>
<p>Zephir的代码必须放置在类中。Zephir是基于面向对象类/框架打造的。所以代码放置在类的外面是不允许的。另外，一个命名空间也是必须的。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Test</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">say</span><span class="params">()</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"Hello World!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>一但这个类被编译完成，它会产生下面的一段C代码（gcc/clang/vc++编译）<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">EPHIR_INIT_CLASS(Test_Hello) &#123;</span><br><span class="line">    ZEPHIR_REGISTER_CLASS(Test, Hello, hello, test_hello_method_entry, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> SUCCESS;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PHP_METHOD(Test_Hello, say) &#123;</span><br><span class="line">    php_printf(<span class="string">"%s"</span>, <span class="string">"Hello World!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>事实上，使用Zephir的开发者无需懂得C语言，如果你有使用编译器，或者php内部的构造，或者C语言本身的经验， 在使用Zephir的时候你将会感到更加的清晰。</p>
<h3 id="Zephir初试"><a href="#Zephir初试" class="headerlink" title="Zephir初试"></a>Zephir初试</h3><p>在接下来的例子中，我们将会尽详细的描述，以便你知道是怎么回事。 我们的目标是让你感觉一下到底Zephir是怎么样的一个东西。 随便我们将会详细的探索Zephir的新特性。    </p>
<p>下面的例子很简单，它提供一个类和一个函数，检测一个数组的类型</p>
<p>让我们认真的检查下面的代码，开始认真的的学习Zephir. 这几行代码包括了很多详细的东西，我们将会慢慢的解释。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * MyTest (test/mytest.zep)</span><br><span class="line"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTest</span> </span>&#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">someMethod</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">/* 变量必须声明 */</span></span><br><span class="line">		 <span class="keyword">var</span> myArray;</span><br><span class="line">		 int i = <span class="number">0</span>, length;</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/*创建一个数组 */</span></span><br><span class="line">		 let myArray = [<span class="string">"hello"</span>, <span class="number">0</span>, <span class="number">100.25</span>, <span class="keyword">false</span>, <span class="keyword">null</span>];</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/* 数组有多少个元素*/</span></span><br><span class="line">		 let length = count(myArray);</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/* 打印值类型 */</span></span><br><span class="line">		 <span class="keyword">while</span> i &lt; length &#123;</span><br><span class="line">		     <span class="keyword">echo</span> typeof myArray[i], <span class="string">"\n"</span>;</span><br><span class="line">		     let i++;</span><br><span class="line">		 &#125;</span><br><span class="line">		 </span><br><span class="line">		 <span class="keyword">return</span> myArray;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在函数中，第一行使用了’var’ 和 ‘int’ 关键词来声明一个函数内的私有变量。 在函数中的每一个变量必须事先声明它们自己的类型。这些声明并不是随意的，它帮助编译器来报告给你关于 错误的变量，或者变量的使用是否超出的它的范围，通常它会在最后抛出错误。</p>
<p>动态的变量必须以关键词’var’来声明。这些变量可以被指定或再指定成不同的变量类型。另一方面，’i’ and ‘length’使用了整数的静态变量，在执行程序的过程中，它只能改变值，而不能改变变量的类型。</p>
<p>与PHP不同的是，你不用在变量的前面加上($)符号。</p>
<p>Zephir的注释和Java, C#, C++等等一些语言的一样。</p>
<p>默认的，变量是不变的，意思是说Zephir期望大部分的变量保持不变。变量保持它们原始的值不变可以优化成静态常量。 如果需要改变变量的值，请使用关键词’let’<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 创建一个数组 */</span></span><br><span class="line">let myArray = [<span class="string">"hello"</span>, <span class="number">0</span>, <span class="number">100.25</span>, <span class="keyword">false</span>, <span class="keyword">null</span>];</span><br></pre></td></tr></table></figure></p>
<p>默认的，数组是一种象PHP一样的动态变量，它包含了许多不同类型的值。令人吃惊的是，PHP内部的函数可以在Zephir中使用，在下面的例子中，’count’ 函数被使用了，编辑器可以以最佳的状态来执行，因为它已经知道了数组的长度了。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*数组有多少个元素 */</span></span><br><span class="line">let length = count(myArray);</span><br></pre></td></tr></table></figure></p>
<p>同样的，我们可以使用花括号来控制程序的流程.<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> i &lt; length &#123;</span><br><span class="line">    <span class="keyword">echo</span> typeof myArray[i], <span class="string">"\n"</span>;</span><br><span class="line">    let i++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>PHP的变量总是动态的，函数总是返回的是可变的动态变量，这就意味着如果一个静态变量在Zphir中被返回了，在PHP的调用中 你得到的却是一个动态变量。</p>
<p><strong>请注意！内存是在编译器中自动管理的，所以你没有必要像C语言一样去分配和释放内存。</strong> 这和PHP是很相似的。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zephir介绍]]></title>
      <url>http://team.jiunile.com/blog/2016/06/zephir-zephir-01.html</url>
      <content type="html"><![CDATA[<h2 id="zephir介绍"><a href="#zephir介绍" class="headerlink" title="zephir介绍"></a>zephir介绍</h2><p>Zephir是一种可以让PHP开发者尝试编写和编译可以被PHP执行代码的一种语言。它是动态/静态类型，它的一些特性对于PHP 开发者来说是非常的相似的。</p>
<p>Zephir的名字是取自Zend Engine/PHP/Intermediate的缩写。建议发音为zephyr相同。事实上Zephir的创造者发音为zaefire_.</p>
<h3 id="简单易于开发"><a href="#简单易于开发" class="headerlink" title="简单易于开发"></a>简单易于开发</h3><p>相信大家和我有一样的经历，看到了yaf和phalcon在想为什么C语言的拓展框架可以这么的快，我自己能不能写一个出来呢？然后屁颠屁颠的跑去找资料找大神了解，大神说你去看一下 “PHP扩展开发及内核应用”，结果大家都知道醉了。</p>
<p>主要原因是需要对C相对的熟悉并且对PHP内核API也要很熟悉，我觉得这已经不是门槛的问题了是太平洋的距离，就草草结束了研究。</p>
<p>当遇到zephir首先了解的就是复杂程度，结果花了10分钟就跟着流程做了一个小DEMO，就这点看来就开发效率这点看来无可厚非的的高效快速，大家感受一下。<br><a id="more"></a><br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Icyboy</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">function</span> <span class="title">hi</span><span class="params">()</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        <span class="keyword">echo</span>  <span class="string">"hello world"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译之后引入到php.ini里面，使用方式如下<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="title">Icyboy</span>\<span class="title">Hello</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">echo</span> Hello::hi() . PHP_EOL;</span><br></pre></td></tr></table></figure></p>
<p>zephir是一个解释器语言和PHP非常近似，通过zephir的机制编译成C语言，然后通过C编译出PHP拓展提供使用，把中间过程高度封装，很大程度让PHP拓展开发简单了很多。</p>
<p><strong>PHP扩展开发及内核应用</strong> <a href="http://www.walu.cc/phpbook" target="_blank" rel="external">http://www.walu.cc/phpbook</a></p>
<h3 id="zephir特性"><a href="#zephir特性" class="headerlink" title="zephir特性"></a>zephir特性</h3><ul>
<li>zephir是静态动态结合语言，在zephir内可以使用传统静态变量，也可以使用动态变量，灵活度高。</li>
<li>内存安全，熟悉C程序的童鞋都知道C可以控制内存指针，其实用的不好是一件很危险的事情，zephir它不允许你使用指针，它提供了一个<strong>task-local垃圾收集器</strong>，以避免内存泄漏。</li>
<li>编译模式，zephir能够编译主流系统Liunx/OSX/Windows能够识别的拓展程序。</li>
<li>开发源代码的高级语言，以面向对象为基础，编写拓展都需要基于面向对象。</li>
</ul>
<h3 id="感受一下"><a href="#感受一下" class="headerlink" title="感受一下"></a>感受一下</h3><p>下面是官方提供的一个让大家感受一下的小例子作用是过滤变量返回字母字符<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">MyLibrary</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">alpha</span><span class="params">(string str)</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        char ch; string filtered = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span> ch in str &#123;</span><br><span class="line">           <span class="keyword">if</span> (ch &gt;= <span class="string">'a'</span> &amp;&amp; ch &lt;= <span class="string">'z'</span>) || (ch &gt;= <span class="string">'A'</span> &amp;&amp; ch &lt;= <span class="string">'Z'</span>) &#123;</span><br><span class="line">              let filtered .= ch;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> filtered;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>从PHP类可以使用如下<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"></span><br><span class="line">$filter = <span class="keyword">new</span> MyLibrary\Filter();</span><br><span class="line"><span class="keyword">echo</span> $filter-&gt;alpha(<span class="string">"01he#l.lo?/1"</span>); <span class="comment">// 结果输出 hello</span></span><br></pre></td></tr></table></figure></p>
<h2 id="为什么是Zephir"><a href="#为什么是Zephir" class="headerlink" title="为什么是Zephir"></a>为什么是Zephir</h2><p>今天的PHP应用程序必须平衡一系列问题包括稳定性、性能和功能。</p>
<p>每一个PHP应用程序是基于一组常见的组件或者说框架，这些公共组件是库/框架或它们的组合。一旦安装后很少改变，作为应用程序的基础，他们必须是有非常快的,</p>
<p>快速和强大的库会很复杂，由于高水平的抽象，一般的做法是约定基础库或框架很少改变，才有机会来改善性能和资源消耗。</p>
<p>Zephir，您可以实现面向对象库/框架/应用程序，使您的应用程序速度提高，改善用户体验。</p>
<h3 id="如果你是一个PHP程序员……"><a href="#如果你是一个PHP程序员……" class="headerlink" title="如果你是一个PHP程序员……"></a>如果你是一个PHP程序员……</h3><p>PHP是在使用的Web应用程序开发中最流行的语言之一。像PHP动态类型和解释语言，由于其灵活性，提供非常高的效率。</p>
<p>PHP是基于Zend引擎的实现。这是执行从字节码表示的PHP代码的虚拟机。Zend引擎是世界上每一个PHP的安装几乎目前，随着Zephir，您可以创建在Zend引擎运行PHP扩展。</p>
<p>PHP托管Zephir，所以他们显然有很多相似的地方，但是，他们有给Zephir自己的个性的重要差异。例如，Zephir更加严格，它可以让你减少编译步骤。</p>
<h3 id="如果你是一个C程序员……"><a href="#如果你是一个C程序员……" class="headerlink" title="如果你是一个C程序员……"></a>如果你是一个C程序员……</h3><p>C是有史以来最强大的和流行的语言之一。 事实上，PHP是用C编写的。</p>
<p>然而，用C开发大型应用程序可以把PHP或Zephir相比比预期的要长很多，一些错误是很难找到。如果你不是一个有经验的开发人员。</p>
<p>Zephir设计是安全的，所以它没有实现指针或手动内存管理，如果你是一个C程序员，你会觉得Zephir强大，比C更加的友好。</p>
<h3 id="编译VS解读"><a href="#编译VS解读" class="headerlink" title="编译VS解读"></a>编译VS解读</h3><p>编译通常会减慢下来的发展；你需要多一点耐心，使你的代码编译运行它之前。此外，该解释趋于降低有利于生产率的性能。</p>
<p>为了更高的效率，Zephir需要编译你的代码，但是他不会影响高生产效率，开发人员可以决定哪些应用程序部分应当在Zephir，哪些不是。</p>
<h3 id="静态类型和动态类型语言"><a href="#静态类型和动态类型语言" class="headerlink" title="静态类型和动态类型语言"></a>静态类型和动态类型语言</h3><p>一般来说，在静态类型语言中，变量是绑定到一个特定类型的一生。 其类型不能改变，只能参考实例和兼容操作。 像C / c++语言实现的方案<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int a = <span class="number">0</span>;</span><br><span class="line">a = <span class="string">"hello"</span>; <span class="comment">// not allowed</span></span><br></pre></td></tr></table></figure></p>
<p>在动态类型，绑定到类型的值，而不是变量。 所以，一个变量可能引用值的类型，然后重新分配后的值类型无关。 Javascript / PHP的例子 动态类型语言<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">0</span>;</span><br><span class="line">a = <span class="string">"hello"</span>; <span class="comment">// allowed</span></span><br></pre></td></tr></table></figure></p>
<p>尽管动态类型有着生产力的优势，但是动态语言并不能成为所有应用的选择，特别是对于非常大型代码库和高性能的应用程序。</p>
<p>优化性能的动态语言像PHP比静态语言(如C)是更具挑战性的。 在静态语言中，优化器可以利用类型信息做出决策。 在动态语言中，只有很有限的信息是可用的，这使得优化器的选择更加困难。</p>
<p>如果你需要非常高的性能,，静态语言可能是一个更安全的选择。</p>
<p>静态语言的另一个好处是编译器执行额外的检查。 编译器无法发现逻辑错误，这更重要但是编译器可以提前发现错误，动态语言只能在运行提示报错信息。</p>
<p>Zephir是静态和动态类型都允许使用的。</p>
<h3 id="代码保护"><a href="#代码保护" class="headerlink" title="代码保护"></a>代码保护</h3><p>在某些情况下，编译不显著提高性能，这可能是因为瓶颈所在。 在应用程序的I / O(很有可能)，而不是计算/内存限制。 然而，编译代码也可能带来某种程度的intelectual保护您的应用程序。 Zephir产生本地二进制文件，你也有能力“隐藏”用户或客户的原始代码。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Zephir不是用来取代PHP或C，相反我们认为这是一个补充，允许开发者进入代码编译和静态类型。Zephir正是试图加入从C和PHP的世界，美好的事物寻找机会使他们的应用程序更快！如果你喜欢PHP，如果你渴望执行效率，那就别犹豫赶快尝试一下Zephir吧！</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Cache 应用中的服务过载案例研究]]></title>
      <url>http://team.jiunile.com/blog/2016/06/cache-server.html</url>
      <content type="html"><![CDATA[<p>简单地说，过载是外部请求对系统的访问量突然激增，造成请求堆积，服务不可用，最终导致系统崩溃。本文主要分析引入Cache可能造成的服务过载，并讨论相关的预防、恢复策略。</p>
<p>Cache在现代系统中使用广泛，由此引入的服务过载隐患无处不在，但却非常隐蔽，容易被忽视。本文希望能为开发者在设计和编写相关类型应用，以及服务过载发生处理时能够有章可循。</p>
<h2 id="一个服务过载案例"><a href="#一个服务过载案例" class="headerlink" title="一个服务过载案例"></a>一个服务过载案例</h2><p>本文讨论的案例是指存在正常调用关系的两个系统（假设调用方为A系统，服务方为B系统），A系统对B系统的访问突然超出B系统的承受能力，造成B系统崩溃。造成服务过载的原因很多，这里分析的是严重依赖Cache的系统服务过载。首先来看一种包含Cache的体系结构（如下图所示）。<br><img src="/images/2_cache.png" alt="Cache应用体系结构"></p>
<p>A系统依赖B系统的读服务，A系统是60台机器组成的集群，B系统是6台机器组成的集群，之所以6台机器能够扛住60台机器的访问，是因为A系统并不是每次都访问B，而是首先请求Cache，只有Cache的相应数据失效时才会请求B。<br><a id="more"></a><br>这正是Cache存在的意义，它让B系统节省了大量机器；如果没有Cache，B系统不得不组成60台机器的集群，如果A也同时依赖除B系统外的另一个系统（假设为C系统）呢？那么C系统也要60台机器，放大的流量将很快耗尽公司的资源。</p>
<p>然而Cache的引入也不是十全十美的，这个结构中如果Cache发生问题，全部的流量将流向依赖方，造成流量激增，从而引发依赖系统的过载。</p>
<p>回到A和B的架构，造成服务过载的原因至少有下面三种：</p>
<ul>
<li>B系统的前置代理发生故障或者其他原因造成B系统暂时不可用，等B系统系统服务恢复时，其流量将远远超过正常值。</li>
<li>Cache系统故障，A系统的流量将全部流到B系统，造成B系统过载。</li>
<li>Cache故障恢复，但这时Cache为空，Cache瞬间命中率为0，相当于Cache被击穿，造成B系统过载。</li>
</ul>
<p>第一个原因不太好理解，为什么B系统恢复后流量会猛增呢？主要原因就是缓存的超时时间。当有数据超时的时候，A系统会访问B系统，但是这时候B系统偏偏故障不可用，那么这个数据只好超时，等发现B系统恢复时，发现缓存里的B系统数据已经都超时了，都成了旧数据，这时当然所有的请求就打到了B。</p>
<p>下文主要介绍服务过载的预防和发生后的一些补救方法，以预防为主，从调用方和服务方的视角阐述一些可行方案。</p>
<h2 id="服务过载的预防"><a href="#服务过载的预防" class="headerlink" title="服务过载的预防"></a>服务过载的预防</h2><p>所谓Client端指的就是上文结构中的A系统，相对于B系统，A系统就是B系统的Client，B系统相当于Server。</p>
<h3 id="Client端的方案"><a href="#Client端的方案" class="headerlink" title="Client端的方案"></a>Client端的方案</h3><p>针对上文阐述的造成服务过载的三个原因：B系统故障恢复、Cache故障、Cache故障恢复，我们看看A系统有哪些方案可以应对。</p>
<blockquote>
<p>合理使用Cache应对B系统宕机</p>
</blockquote>
<p>一般情况下，Cache的每个Key除了对应Value，还对应一个过期时间T，在T内，get操作直接在Cache中拿到Key对应Value并返回。但是在T到达时，get操作主要有五种模式：</p>
<h4 id="基于超时的简单（stupid）模式"><a href="#基于超时的简单（stupid）模式" class="headerlink" title="基于超时的简单（stupid）模式"></a>基于超时的简单（stupid）模式</h4><p>在T到达后，任何线程get操作发现Cache中的Key和对应Value将被清除或标记为不可用，get操作将发起调用远程服务获取Key对应的Value，并更新写回Cache，然后get操作返回新值；如果远程获取Key-Value失败，则get抛出异常。</p>
<p>为了便于理解，举一个码头工人取货的例子：5个工人（线程）去港口取同样Key的货（get），发现货已经过期被扔掉了，这时5个工人各自分别去对岸取新货，然后返回。</p>
<h4 id="基于超时的常规模式"><a href="#基于超时的常规模式" class="headerlink" title="基于超时的常规模式"></a>基于超时的常规模式</h4><p>在T到达后，Cache中的Key和对应Value将被清除或标记为不可用，get操作将调用远程服务获取Key对应的Value，并更新写回Cache；此时，如果另一个线程发现Key和Value已经不可用，get操作还需要判断有没有其他线程发起了远程调用，如果有，那么自己就等待，直到那个线程远程获取操作成功，Cache中得Key变得可用，get操作返回新的Value。如果远程获取操作失败，则get操作抛出异常，不会返回任何Value。</p>
<p>还是码头工人的例子：5个工人（线程）去港口取同样Key的货（get），发现货已经过期被扔掉了，那么只需派出一个人去对岸取货，其他四个人在港口等待即可，而不用5个人全去。</p>
<p>基于超时的简单模式和常规模式区别在于对于同一个超时的Key，前者每个get线程一旦发现Key不存在，则发起远程调用获取值；而后者每个get线程发现Key不存在，则还要判断当前是否有其他线程已经发起了远程调用操作获取新值，如果有，自己就简单的等待即可。</p>
<p>显然基于超时的常规模式比基于超时的简单模式更加优化，减少了超时时并发访问后端的调用量。</p>
<p>实现基于超时的常规模式就需要用到经典的Double-checked locking惯用法了。</p>
<h4 id="基于刷新的简单（stupid）模式"><a href="#基于刷新的简单（stupid）模式" class="headerlink" title="基于刷新的简单（stupid）模式"></a>基于刷新的简单（stupid）模式</h4><p>在T到达后，Cache中的Key和相应Value不动，但是如果有线程调用get操作，将触发refresh操作，根据get和refresh的同步关系，又分为两种模式：</p>
<ul>
<li>同步模式：任何线程发现Key过期，都触发一次refresh操作，get操作等待refresh操作结束，refresh结束后，get操作返回当前Cache中Key对应的Value。注意refresh操作结束并不意味着refresh成功，还可能抛了异常，没有更新Cache，但是get操作不管，get操作返回的值可能是旧值。</li>
<li>异步模式：任何线程发现Key过期，都触发一次refresh操作，get操作触发refresh操作，不等refresh完成，直接返回Cache中的旧值。</li>
</ul>
<p>举上面码头工人的例子说明基于刷新的常规模式：这次还是5工人去港口取货，这时货都在，但是已经旧了，这时5个工人有两种选择：</p>
<ul>
<li>5个人各自去远程取新货，如果取货失败，则拿着旧货返回（同步模式）</li>
<li>5个人各自通知5个雇佣工去取新货，5个工人拿着旧货先回（异步模式）</li>
</ul>
<h4 id="基于刷新的常规模式"><a href="#基于刷新的常规模式" class="headerlink" title="基于刷新的常规模式"></a>基于刷新的常规模式</h4><p>在T到达后，Cache中的Key和相应Value都不会被清除，而是被标记为旧数据，如果有线程调用get操作，将触发refresh更新操作，根据get和refresh的同步关系，又分为两种模式：</p>
<ul>
<li>同步模式：get操作等待refresh操作结束，refresh结束后，get操作返回当前Cache中Key对应的Value，注意：refresh操作结束并不意味着refresh成功，还可能抛了异常，没有更新Cache，但是get操作不管，get操作返回的值可能是旧值。如果其他线程进行get操作，Key已经过期，并且发现有线程触发了refresh操作，则自己不等refresh完成直接返回旧值。</li>
<li>异步模式：get操作触发refresh操作，不等refresh完成，直接返回Cache中的旧值。如果其他线程进行get操作，发现Key已经过期，并且发现有线程触发了refresh操作，则自己不等refresh完成直接返回旧值。</li>
</ul>
<p>再举上面码头工人的例子说明基于刷新的常规模式：这次还是5工人去港口取货，这时货都在，但是已经旧了，这时5个工人有两种选择：</p>
<ul>
<li>派一个人去远方港口取新货，其余4个人拿着旧货先回（同步模式）。</li>
<li>5个人通知一个雇佣工去远方取新货，5个人都拿着旧货先回（异步模式）。</li>
</ul>
<p>基于刷新的简单模式和基于刷新的常规模式区别就在于取数线程之间能否感知当前数据是否正处在刷新状态，因为基于刷新的简单模式中取数线程无法感知当前过期数据是否正处在刷新状态，所以每个取数线程都会触发一个刷新操作，造成一定的线程资源浪费。</p>
<p>而基于超时的常规模式和基于刷新的常规模式区别在于前者过期数据将不能对外访问，所以一旦数据过期，各线程要么拿到数据，要么抛出异常；后者过期数据可以对外访问，所以一旦数据过期，各线程要么拿到新数据，要么拿到旧数据。</p>
<h4 id="基于刷新的续费模式"><a href="#基于刷新的续费模式" class="headerlink" title="基于刷新的续费模式"></a>基于刷新的续费模式</h4><p>该模式和基于刷新的常规模式唯一的区别在于refresh操作超时或失败的处理上。在基于刷新的常规模式中，refresh操作超时或失败时抛出异常，Cache中的相应Key-Value还是旧值，这样下一个get操作到来时又会触发一次refresh操作。</p>
<p>在基于刷新的续费模式中，如果refresh操作失败，那么refresh将把旧值当成新值返回，这样就相当于旧值又被续费了T时间，后续T时间内get操作将取到这个续费的旧值而不会触发refresh操作。</p>
<p>基于刷新的续费模式也像常规模式那样分为同步模式和异步模式，不再赘述。</p>
<p>下面讨论这5种Cache get模式在服务过载发生时的表现，首先假设如下：</p>
<ul>
<li>假设A系统的访问量为每分钟M次。</li>
<li>假设Cache能存Key为C个，并且Key空间有N个。</li>
<li>假设正常状态下，B系统访问量为每分钟W次，显然W&lt;N&lt;M。</li>
</ul>
<p>这时因为某种原因，比如B长时间故障，造成Cache中得Key全部过期，B系统这时从故障中恢复，五种get模式分析表现分析如下：</p>
<ul>
<li>在基于超时和刷新的简单模式中，B系统的瞬间流量将达到和A的瞬时流量M大体等同，相当于Cache被击穿。这就发生了服务过载，这时刚刚恢复的B系统将肯定会被大流量压垮。</li>
<li>在基于超时和刷新的常规模式中，B系统的瞬间流量将和Cache中Key空间N大体等同。这时是否发生服务过载，就要看Key空间N是否超过B系统的流量上限了。</li>
<li>在基于刷新的续费模式中，B系统的瞬间流量为W，和正常情况相同而不会发生服务过载。实际上，在基于刷新的续费模式中，不存在Cache Key全部过期的情况，就算把B系统永久性地干掉，A系统的Cache也会基于旧值长久的平稳运行。</li>
</ul>
<p>第3点，B系统不会发生服务过载的主要原因是基于刷新的续费模式下不会出现chache中的Key全部长时间过期的情况，即使B系统长时间不可用，基于刷新的续费模式也会在一个过期周期内把旧值当成新值继续使用。所以当B系统恢复时，A系统的Cache都处在正常工作状态。</p>
<p>从B系统的角度看，能够抵抗服务过载的基于刷新的续费模式最优。</p>
<p>从A系统的角度看，由于一般情况下A系统是一个高访问量的在线web应用，这种应用最讨厌的一个词就是“线程等待”，因此基于刷新的各种异步模式较优。</p>
<p>综合考虑，基于刷新的异步续费模式是首选。然而凡事有利就有弊，有两点需要注意的地方：</p>
<ul>
<li>基于刷新模式最大的缺点是Key-Value一旦放入Cache就不会被清除，每次更新也是新值覆盖旧值，JVM GC永远无法对其进行垃圾收集，而基于超时的模式中，Key-Value超时后如果新的访问没有到来，内存是可以被GC垃圾回收的。所以如果你使用的是寸土寸金的本地内存做Cache就要小心了。</li>
<li>基于刷新的续费模式需要做好监控，不然有可能Cache中的值已经和真实的值相差很远了，应用还以为是新值而使用。</li>
</ul>
<p>关于具体的Cache，来自Google的Guava本地缓存库支持上文的第二种、第四种和第五种get操作模式。</p>
<p>但是对于Redis等分布式缓存，只提供原始的get、set方法，而提供的get仅仅是获取，与上文提到的五种get操作模式不是一个概念。开发者想用这五种get操作模式的话不得不自己封装和实现。</p>
<p>五种get操作模式中，基于超时和刷新的简单模式是实现起来最简单的模式，但遗憾的是这两种模式对服务过载完全无免疫力，这可能也是服务过载在大量依赖缓存的系统中频繁发生的一个重要原因吧。</p>
<p>本文之所以把第1、3种模式称为stupid模式，是想强调这种模式应该尽量避免，Guava里面根本没有这种模式，而Redis只提供简单的读写操作，很容易就把系统实现成了这种方式。</p>
<blockquote>
<p>应对分布式Cache宕机</p>
</blockquote>
<p>如果是Cache直接挂了，那么就算是基于刷新的异步续费模式也无能为力了。这时A系统铁定无法对Cache进行存取操作，只能将流量完全打到B系统，B系统面对服务过载在劫难逃……</p>
<p>本节讨论的预防Cache宕机仅限于分布式Cache，因为本地Cache一般和A系统应用共享内存和进程，本地Cache挂了A系统也挂了，不会出现本地Cache挂了而A系统应用正常的情况。</p>
<p>首先，A系统请求线程检查分布式Cache状态，如果无应答则说明分布式Cache挂了，则转向请求B系统，这样一来大流量将压垮B系统。这时可选的方案如下：</p>
<ul>
<li>A系统的当前线程不请求B系统，而是打个日志并设置一个默认值。</li>
<li>A系统的当前线程按照一定概率决定是否请求B系统。</li>
<li>A系统的当前线程检查B系统运行情况，如果良好则请求B系统。</li>
</ul>
<p><strong>方案1</strong> 最简单，A系统知道如果没有Cache，B系统可能扛不住自己的全部流量，索性不请求B系统，等待Cache恢复。但这时B系统利用率为0，显然不是最优方案，而且当请求的Value不容易设置默认值时，这个方案就不行了。</p>
<p><strong>方案2</strong> 可以让一部分线程请求B系统，这部分请求肯定能被B系统hold住。可以保守的设置这个概率 u =（B系统的平均流量）/（A系统的峰值流量）。</p>
<p><strong>方案3</strong> 是一种更为智能的方案，如果B系统运行良好，当前线程请求；如果B系统过载，则不请求，这样A系统将让B系统处于一种宕机与不宕机的临界状态，最大限度挖掘B系统性能。这种方案要求B系统提供一个性能评估接口返回Yes和No，Yes表示B系统良好，可以请求；No表示B系统情况不妙，不要请求。这个接口将被频繁调用，必须高效。</p>
<p>方案3的关键在于如何评估一个系统的运行状况。一个系统中当前主机的性能参数有CPU负载、内存使用率、Swap使用率、GC频率和GC时间、各个接口平均响应时间等，性能评估接口需要根据这些参数返回Yes或者No，是不是机器学习里的二分类问题？??关于这个问题已经可以单独写篇文章讨论了，在这里就不展开了，你可以想一个比较简单傻瓜的保守策略，缺点是A系统的请求无法很好的逼近B系统的性能极限。</p>
<p>综合以上分析，方案2比较靠谱。如果选择方案3，建议由专门团队负责研究并提供统一的系统性能实时评估方案和工具。</p>
<blockquote>
<p>应对分布式Cache宕机后的恢复</p>
</blockquote>
<p>不要以为成功hold住分布式Cache宕机就万事大吉了，真正的考验是分布式Cache从宕机过程恢复之后，这时分布式Cache中什么都没有。</p>
<p>即使是上文中提到了基于刷新的异步续费策略这时也没用，因为分布式Cache为空，无论如何都要请求B系统。这时B系统的最大流量是Key的空间取值数量。</p>
<p>如果Key的取值空间数量很少，则相安无事；如果Key的取值空间数量大于B系统的流量上限，服务过载依然在所难免。</p>
<p>这种情况A系统很难处理，关键原因是A系统请求Cache返回Key对应Value为空，A系统无法知道是因为当前Cache是刚刚初始化，所有内容都为空；还是因为仅仅是自己请求的那个Key没在Cache里。</p>
<p>如果是前者，那么当前线程就要像处理Cache宕机那样进行某种策略的回避；如果是后者，直接请求B系统即可，因为这是正常的Cache使用流程。</p>
<p>对于Cache宕机的恢复，A系统真的无能为力，只能寄希望于B系统的方案了。</p>
<h3 id="Server端的方案"><a href="#Server端的方案" class="headerlink" title="Server端的方案"></a>Server端的方案</h3><p>相对于Client端需要应对各种复杂问题，Server端需要应对的问题非常简单，就是如何从容应对过载的问题。无论是缓存击穿也好，还是拒绝服务攻击也罢，对于Server端来说都是过载保护的问题。对于过载保护，主要给出两种可行方案，以及一种比较复杂的方案思路。</p>
<blockquote>
<p>流量控制</p>
</blockquote>
<p>流量控制就是B系统实时监控当前流量，如果超过预设的值或者系统承受能力，则直接拒绝掉一部分请求，以实现对系统的保护。</p>
<p>流量控制根据基于的数据不同，可分为两种：</p>
<ul>
<li>基于流量阈值的流控：流量阈值是每个主机的流量上限，流量超过该阈值主机将进入不稳定状态。阈值提前进行设定，如果主机当前流量超过阈值，则拒绝掉一部分流量，使得实际被处理流量始终低于阈值。</li>
<li>基于主机状态的流控：每个接受每个请求之前先判断当前主机状态，如果主机状况不佳，则拒绝当前请求。</li>
</ul>
<p>基于阈值的流控实现简单，但是最大的问题是需要提前设置阈值，而且随着业务逻辑越来越复杂，接口越来越多，主机的服务能力实际应该是下降的，这样就需要不断下调阈值，增加了维护成本，而且万一忘记调整的话，呵呵……</p>
<p>主机的阈值可以通过压力测试确定，选择的时候可以保守些。</p>
<p>基于主机状态的流控免去了人为控制，但是其最大的确定上文已经提到：如何根据当前主机各个参数判断主机状态呢？想要完美的回答这个问题目测并不容易，因此在没有太好答案之前，我推荐基于阈值的流控。</p>
<p>流量控制基于实现位置的不同，又可以分为两种：</p>
<ul>
<li>反向代理实现流控：在反向代理如Nginx上基于各种策略进行流量控制。这种一般针对HTTP服务。</li>
<li>借助服务治理系统：如果Server端是RMI、RPC等服务，可以构建专门的服务治理系统进行负载均衡、流控等服务。</li>
<li>服务容器实现流控：在应用代码里，业务逻辑之前实现流量控制。</li>
</ul>
<p>第3种在服务器的容器（如Java容器）中实现流控并不推荐，因为流控和业务代码混在一起容易混乱；其次实际上流量已经全量进入到了业务代码里，这时的流控只是阻止其进入真正的业务逻辑，所以流控效果将打折；还有，如果流量策略经常变动，系统将不得不为此经常更改。</p>
<p>因此，推荐前两种方式。</p>
<p>最后提一个注意点：当因为流控而拒绝请求时，务必在返回的数据中带上相关信息（比如“当前请求因为超出流量而被禁止访问”），如果返回值什么都没有将是一个大坑。因为造成调用方请求没有被响应的原因很多，可能是调用方Bug，也可能是服务方Bug，还可能是网络不稳定，这样一来很可能在排查一整天后发现是流控搞的鬼……</p>
<blockquote>
<p>服务降级</p>
</blockquote>
<p>服务降级一般由人为触发，属于服务过载造成崩溃恢复时的策略，但为了和流控对比，将其放到这里。</p>
<p>流量控制本质上是减小访问量，而服务处理能力不变；而服务降级本质上是降低了部分服务的处理能力，增强另一部分服务处理能力，而访问量不变。</p>
<p>服务降级是指在服务过载时关闭不重要的接口（直接拒绝处理请求），而保留重要的接口。比如服务由10个接口，服务降级时关闭了其中五个，保留五个，这时这个主机的服务处理能力将增强到二倍左右。</p>
<p>然而，服务过载发生时动辄就超出系统处理能力10倍，而服务降级能使主机服务处理能力提高10倍么？显然很困难，因此服务过载的应对不能只依靠服务降级策略。</p>
<blockquote>
<p>动态扩展</p>
</blockquote>
<p>动态扩展指的是在流量超过系统服务能力时，自动触发集群扩容，自动部署并上线运行；当流量过去后又自动回收多余机器，完全弹性。</p>
<p>这个方案是不是感觉很不错。但是目前互联网公司的在线应用跑在云上的本身就不多，要完全实现在线应用的自动化弹性运维，要走的路就更多了。</p>
<h2 id="崩溃恢复"><a href="#崩溃恢复" class="headerlink" title="崩溃恢复"></a>崩溃恢复</h2><p>如果服务过载造成系统崩溃还是不幸发生了，这时需要运维控制流量，等后台系统启动完毕后循序渐进的放开流量，主要目的是让Cache慢慢预热。流量控制刚开始可以为10%，然后20%，然后50%，然后80%，最后全量，当然具体的比例，尤其是初始比例，还要看后端承受能力和前端流量的比例，各个系统并不相同。</p>
<p>如果后端系统有专门的工具进行Cache预热，则省去了运维的工作，等Cache热起来再发布后台系统即可。但是如果Cache中的Key空间很大，开发预热工具将比较困难。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>“防患于未然”放在服务过载的应对上也是适合的，预防为主，补救为辅。综合上文分析，具体的预防要点如下：</p>
<ul>
<li>调用方（A系统）采用基于刷新的异步续费模式使用Cache，或者至少不能使用基于超时或刷新的简单（stupid）模式。</li>
<li>调用方（A系统）每次请求Cache时检查Cache是否可用（available），如果不可用则按照一个保守的概率访问后端，而不是无所顾忌的直接访问后端。</li>
<li>服务方（B系统）在反向代理处设置流量控制进行过载保护，阈值需要通过压测获得。</li>
</ul>
<p>崩溃的补救主要还是靠运维和研发在发生时的通力合作：观察流量变化准确定位崩溃原因，运维控流量研发持续关注性能变化。</p>
<p>未来如果有条件的话可以研究下主机应用健康判断问题和动态弹性运维问题，毕竟自动化比人为操作要靠谱。</p>
<hr>
<p>来源：美团点评技术团队-张杨<br>链接：<a href="http://tech.meituan.com/avalanche-study.html" target="_blank" rel="external">http://tech.meituan.com/avalanche-study.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[通过iptables实现端口转发与内网共享上网]]></title>
      <url>http://team.jiunile.com/blog/2016/06/iptables-forward-internet-share.html</url>
      <content type="html"><![CDATA[<p>iptables是一个Linux下优秀的nat+防火墙工具，我使用该工具以较低配置的传统pc配置了一个灵活强劲的防火墙+nat系统,小有心得，看了网上也有很多这方面的文章，但是似乎要么说的比较少，要么就是比较偏，内容不全，容易误导，我研究了一段时间的iptables同时也用了很久，有点滴经验，写来供大家参考，同时也备日后自己翻阅。</p>
<p>首先要说明的是，iptables操作的是2.4以上内核的netfilter。所以需要linux的内核在2.4以上。其功能与安全性远远比其前辈ipfwadm,ipchains强大，iptables大致是工作在OSI七层的二、三、四层，其前辈ipchains不能单独实现对tcp/udp port以及对mac地址的的定义与操作，所以我想ipchains应该是仅仅工作在三层上的。</p>
<h2 id="netfilter工作流程"><a href="#netfilter工作流程" class="headerlink" title="netfilter工作流程"></a>netfilter工作流程</h2><p>我们先简单介绍一下netfilter的大致工作流程，也就是一个数据包（或者叫分组、packet,我个人习惯叫包）在到达linux的网络接口的时候 （网卡）如何处理这个包，然后再介绍一下如何用iptables改变或者说控制对这个数据包进行操作。</p>
<ul>
<li>netfilter内部分为三个表，分别是 filter,nat,mangle，每个表又有不同的操作链（Chains）。</li>
<li>在filter（过滤）表中，也就是他的 防火墙功能 的这个表，定义了三个 Chain。分别是INPUT,FORWARD,OUTPUT。也就是对包的入、转发、出进行定义的三个过滤链。对于这个filter表的操作和控制也是我们实现防火墙功能的一个重要手段</li>
<li>在nat(Network Address Translation、网络地址翻译)表中，也就是我们用以实现地址转换和端口转发功能的这个表，定义了PREROUTING, POSTROUTING,OUTPUT三个链,下面我们会对这三个链作详细的说明</li>
<li>而netfilter的mangle表则是一个自定义表，里面包括上面 的filter以及nat表中的各种chains，它可以让我们进行一些自定义的操作，同时这个mangle表中的chains在netfilter对包 的处理流程中处在一个比较优先的位置。<a id="more"></a>
下面有一张图清晰的描绘了netfilter对包的处理流程（该图摘自网上，不知作者是谁，在此深表敬意！），一般情况下，我们用不到这个mangle表，在这里我们就不做介绍了。<br><img src="/images/iptables_netfilter_chains.png" alt="iptables包处理流程"></li>
<li>ebtables基本使用: <a href="http://www.cnblogs.com/peteryj/archive/2011/07/24/2115602.html" target="_blank" rel="external">http://www.cnblogs.com/peteryj/archive/2011/07/24/2115602.html</a><br><img src="/images/iptables_entables.png" alt="iptables_entables处理流程图"></li>
</ul>
<h3 id="PREROUTING-DNAT"><a href="#PREROUTING-DNAT" class="headerlink" title="PREROUTING(DNAT)"></a>PREROUTING(DNAT)</h3><p>PREROUTING这个chain在最前面，当一个包来到linux的网络接口的时候先过mangle的PREROUTING；然后是nat的PREROUTING,从这个chain的名字我们可以看出，这个chain是在路由之前(pre-routing)要过的。</p>
<p>为什么要在路由之前过呢？大家可以看到这个图上，上面有一个菱形的部分叫ROUTING,这个ROUTING部分就是Linux的route box,也就是路由系统，它同样有很高深的功能，可以实现策略路由等等一些高级特性，此处我们不做详细解释。单说这个PREROUTING链，因为在这个链里面我们对包的操作是DNAT,也就是改变目的地址和（或端口），通常用在端口转发，或者nat到内网的DMZ区，也就是说当一个包过来的时候我们要改变它的目的地址，大家可以想想,如果一个包在改变目的地址之前就被扔进了route box,让系统选好路之后再改变目的地址，那么选路就可能是错的，或者说毫无意义了，所以，PREROUTING这个Chain一定要在进Routing 之前做。</p>
<p>比如说，我们的公网ip是60.1.1.1/24，位于linux中的eth0内网ip是10.1.1.1/24，位于linux中的eth1, 我们的内网有一台web服务器，地址是10.1.1.2/24,我们怎么样能让internet用户通过这个公网ip访问我们内部的这个web服务器呢？ 我们就可以在这个PREROUTING链上面定义一个规则，把访问60.1.1.1:80的用户的目的地址改变一下，改变为10.1.1.2:80,这样 就实现了internet用户对内网服务器的访问了，当然了，这个端口是比较灵活的，我们可以定义任何一个端口的转发，不一定是80–&gt;80，具体的命令我们在下面的例子中介绍，这里我们只谈流程与概念上的实现方法。</p>
<h3 id="FORWARD"><a href="#FORWARD" class="headerlink" title="FORWARD"></a>FORWARD</h3><p>好了，我们接着往下走，这个包已经过了两个PREROUTING链了，这个时候，出现了一个分支转折的地方，也就是图中下方的那个菱形（FORWARD）,转发！这里有一个对目的地址的判断（这里同样说明了PREROUTING一定要在最先，不仅要在route box之前，甚至是这个对目的地址的判断之前，因为我们可能做一个去某某某ip的地方转到自己的ip的规则，所以PREROUTING是最先处理这个包的Chain）！</p>
<p>如果包的目的地是本机ip,那么包向上走，走入INPUT链处理，然后进入LOCAL PROCESS,如果非本地，那么就进入FORWARD链进行过滤，我们在这里就不介绍INPUT,OUTPUT的处理了，因为那主要是对于本机安全的一种处理，我们这里主要说对转发的过滤和nat的实现。</p>
<p>这里的FORWARD我简单说一下，当linux收到了一个 目的ip地址不是本地的包 ，Linux会把这个包丢弃，因为默认情况下，Linux的三层包转发功能是关闭的，如果要让我们的linux实现转发，则需要打开这个转发功能，可以 改变它的一个系统参数，使用sysctl net.ipv4.ip_forward=1或者echo “1” &gt; /proc/sys/net/ipv4/ip_forward命令打开转发功能。</p>
<p>好了，在这里我们让linux允许转发，这个包的目的地址也不是本机，那么它将接着走入FORWARD链，在FORWARD链里面，我们就可以定义详细的规则，也就是是否允许他通过，或者对这个包的方向流程进行一些改变，这也是我们实现访问控制的地方，这里同样也是Mangle_FORWARD然后filter_FORWARD,我们操作任何一个链都会影响到这个包的命运，在下面的介绍中，我们就忽略掉mangle表，我们基本用不到操作它，所以我们假设它是透明的。</p>
<h3 id="POSTROUTING-SNAT"><a href="#POSTROUTING-SNAT" class="headerlink" title="POSTROUTING(SNAT)"></a>POSTROUTING(SNAT)</h3><p>假设这个包被我们的规则放过去了，也就是ACCEPT了，它将进入POSTROUTING部分， 注意！这里我注意到一个细节问题，也就是上面的图中数据包过了FORWARD链之后直接进入了POSTROUITNG链，我觉得这中间缺少一个环节，也就是route box,对于转发的包来说，linux同样需要在选路（路由）之后才能将它送出，这个图却没有标明这一点，我认为它是在过了route box之后才进入的POSTROUITNG，当然了，这对于我们讨论iptables的过滤转发来说不是很重要，只是我觉得流程上有这个问题，还是要说明 一下。</p>
<p>同样的，我们在这里从名字就可以看出，这个POSTROUTING链应该是路由之后的一个链，也就是这个包要送出这台Linux的 最后一个环节了，这也是极其重要的一个环节！！这个时候linux已经完成(has done.._)了对这个包的路由（选路工作），已经找到了合适的接口送出这个包了，在这个链里面我们要进行重要的操作，就是被Linux称为 SNAT 的一个动作，修改源ip地址！为什么修改源ip地址？很多情况需要修改源地址阿，最常见的就是我们内网多台机器需要共享一个或几个公网ip访问internet,因为我们的内网地址是私有的，假如就让linux给路由出去，源地址也不变，这个包应该能访问到目的地，但是却回不来，因为 internet上的N多个路由节点不会转发私有地址的数据包，也就是说，不用合法ip,我们的数据包有去无回。有人会说：“既然是这样，我就不用私有 ip了，我自己分配自己合法的地址不行吗？那样包就会回来了吧？”答案是否定的，ip地址是ICANN来分配的，你的数据包或许能发到目的地，但是回来的 时候人家可不会转到你那里，internet上的路由器中的路由信息会把这个返回包送到那个合法的获得ip的地方去，你同样收不到,而你这种行为有可能被定义为一种ip欺骗，很多设备会把这样的包在接入端就给滤掉了，可能都到不了你要访问的那个服务器，呵呵。</p>
<p>那么Linux如何做SNAT呢？比如一个内网的10.1.1.11的pc访问202.2.2.2的一个web服务器，linux的内网接口10.1.1.1在收到这个包之后把原来的 PC的 ip10.1.1.11改变为60.1.1.1的合法地址然后送出，同时在自己的ip_conntrack表里面做一个记录,记住是内网的哪一个ip的哪 个端口访问的这个web服务器，自己把它的源地址改成多少了，端口改成多少了，以便这个web服务器返回数据包的时候linux将它准确的送回给发送请求 的这个pc.</p>
<p>大体的数据转发流程我们说完了,我们看看iptables使用什么样的参数来完成这些操作。</p>
<h2 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解"></a>概念理解</h2><p>在描述这些具体的操作之前，我还要说几个我对iptables的概念的理解（未必完全正确），这将有助于大家理解这些规则，以实现更精确的控制。</p>
<p>上文中我们提到过，对包的控制是由我们在不同的Chain(链)上面添加不同的规则来实现的，比如我们对过滤表（filter table）添加规则来执行对包的操控。那么既然叫链，一定就是一条或者多条规则组成的了，这时就有一个问题了，如果多个规则对同一种包进行了定义，会发生什么事情呢？ 在Chain中，所有的规则都是从上向下来执行的 ，也就是说，如果匹配了第一行，那么就按照第一行的规则执行，一行一行的往下找，直到找到 符合这个类型的包的规则为止。如果找了一遍没有找到符合这个包的规则怎么办呢？itpables里面有一个概念，就是 Policy ，也就是策略。一说这个东西大家可能就会觉得比较麻烦，什么策略阿，我对于它的理解就是所谓这个策略就是chain中的最后一条规则，也就是说如果找了一遍找不到符合处理这个包的规则，就按照policy来办。这样理解起来就容易多了。iptables 使用-P来设置Chain的策略。</p>
<p>好了，我们言归正传，来说说iptables到底怎样实现对包的控制。</p>
<h4 id="链操作"><a href="#链操作" class="headerlink" title="链操作"></a>链操作</h4><p>先介绍一下iptables如何操作链</p>
<p>对链的操作就那么几种：</p>
<ul>
<li>-I(插入)</li>
<li>-A(追加)</li>
<li>-R(替换)</li>
<li>-D（删除）</li>
<li>-L（列表显示）</li>
</ul>
<p>这里要说明的就是-I将会把规则放在第一行，-A将会放在最后一行。</p>
<p>比如我们要添加一个规则到filter表的FORWARD链：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#意思为：追加一个规则至filter表中的FORWARD链尾，允许（-j ACCEPT）源地址为10.1.1.11目的地址为202.1.1.1的数据包通过。其中-t后面跟的是表名，在-A后面跟Chain名，后面的小写的 -s为源地址，-d为目的地址，-j为处理方向。</span></span><br><span class="line">iptables -t filter -A FORWARD <span class="_">-s</span> 10.1.1.11 <span class="_">-d</span> 202.1.1.1 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#在iptables中，默认的表名就是filter，所以这里可以省略-t filter直接写成: </span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.11 <span class="_">-d</span> 202.1.1.1 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<h4 id="匹配参数"><a href="#匹配参数" class="headerlink" title="匹配参数"></a>匹配参数</h4><p>iptables中的匹配参数： 我们在这里就介绍几种常用的参数，详细地用法可以man iptables看它的联机文档，你会有意外的收获。</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-s</td>
<td style="text-align:left">匹配源地址</td>
</tr>
<tr>
<td style="text-align:left">-d</td>
<td style="text-align:left">匹配目的地址</td>
</tr>
<tr>
<td style="text-align:left">-p</td>
<td style="text-align:left">协议匹配</td>
</tr>
<tr>
<td style="text-align:left">-i</td>
<td style="text-align:left">入接口匹配</td>
</tr>
<tr>
<td style="text-align:left">-o</td>
<td style="text-align:left">出接口匹配</td>
</tr>
<tr>
<td style="text-align:left">–sport，–dport</td>
<td style="text-align:left">源和目的端口匹配</td>
</tr>
<tr>
<td style="text-align:left">-j</td>
<td style="text-align:left">跳转,也就是包的方向</td>
</tr>
<tr>
<td style="text-align:left">!</td>
<td style="text-align:left">取反</td>
</tr>
</tbody>
</table>
<p>其中还有一个!参数，使用!就是取反的意思。下面我们简单举几个例子介绍一下。</p>
<ul>
<li>-s 这个参数呢就是指定源地址的，如果使用这个参数也就是告诉netfilter，对于符合这样一个源地址的包怎么去处理，可以指定某一个单播ip地址，也可以指定一个网络，如果单个的ip地址其实隐含了一个32位的子网掩码，比如-s 10.1.1.11 其实就是-s 10.1.1.11/32，同样我们可以指定不同的掩码用以实现源网络地址的规则，比如一个C类地址我们可以用-s 10.1.1.0/24来指定。</li>
<li>-d参数与-s格式一样。</li>
<li>-i参数是指定入接口的网络接口，比如我仅仅允许从eth3接口过来的包通过FORWARD链，就可以这样指定iptables -A FORWARD -i eth3 -j ACCEPT</li>
<li>-o是出接口,与上同。</li>
</ul>
<p>我们下面用一些简单的实例来step by step看看iptables的具体配置方法。</p>
<h4 id="实例一：简单的nat路由器"><a href="#实例一：简单的nat路由器" class="headerlink" title="实例一：简单的nat路由器"></a>实例一：简单的nat路由器</h4><blockquote>
<p><strong>环境介绍</strong></p>
<ul>
<li>linux 2.4 +</li>
<li>2个网络接口</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>目的：实现内网中的节点（10.1.1.0/24）可控的访问internet。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先将Lan的节点pc的网关指向10.1.1.254。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#确定你的linux的ip配置无误，可以正确的ping通内外的地址。同时用route命令查看linux的本地路由表，确认指定了可用的ISP提供的默认网关。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打开linux的转发功能：</span></span><br><span class="line">sysctl net.ipv4.ip_forward=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#将FORWARD链的策略设置为DROP，这样做的目的是做到对内网ip的控制，你允许哪一个访问internet就可以增加一个规则，不在规则中的ip将无法访问internet.</span></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line"></span><br><span class="line"><span class="comment">#这条规则规定允许任何地址到任何地址的确认包和关联包通过。一定要加这一条，否则你只允许lan IP访问没有用，至于为什么，下面我们再详细说。</span></span><br><span class="line">iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#这条规则做了一个SNAT，也就是源地址转换，将来自10.1.1.0/24的地址转换为60.1.1.1</span></span><br><span class="line"><span class="comment">#(Deven：因为是让内网上网，因此对于代理服务器而言POSTROUTING（经过路由之后的包应该要把源地址改变为60.1.1.1，否则包无法返回）)</span></span><br><span class="line">iptables -t nat -A POSTROUTING <span class="_">-s</span> 10.1.1.0/24 -j SNAT --to 60.1.1.1</span><br><span class="line"><span class="comment">#有这几条规则，一个简单的nat路由器就实现了。这时你可以将允许访问的ip添加至FORWARD链，他们就能访问internet了。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#比如我想让10.1.1.9这个地址访问internet,那么你就加如下的命令就可以了。</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.9 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以精确控制他的访问地址,比如我就允许10.1.1.99访问3.3.3.3这个ip</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.99 <span class="_">-d</span> 3.3.3.3 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者只允许他们访问80端口。</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.0/24 -p tcp --dport http -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#更多的控制可以自己灵活去做,或者查阅iptables的联机文档。</span></span><br></pre></td></tr></table></figure>
<h4 id="实例二：端口转发"><a href="#实例二：端口转发" class="headerlink" title="实例二：端口转发"></a>实例二：端口转发</h4><blockquote>
<p><strong>环境介绍</strong></p>
<ul>
<li>linux 2.4 +</li>
<li>2个网络接口</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>Lan内web server: 10.1.1.1:80</li>
<li>Lan内ftp server: 10.1.1.2:21</li>
<li>Wan口:60.1.1.1/24 eth1</li>
<li>目的：对内部server进行端口转发实现internet用户访问内网服务器。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#同样确认你的linux的各项配置正常，能够访问内外网。</span></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line">iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#也需要加入确认包和关联包的允许通过</span></span><br><span class="line"><span class="comment">#如果你要把访问60.1.1.1:80的数据包转发到Lan内web server,用下面的命令</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 60.1.1.1 -p tcp --dport 80 -j DNAT --to 10.1.1.1:80</span><br><span class="line"></span><br><span class="line"><span class="comment">#ftp服务也同样，命令如下：</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 60.1.1.1 -p tcp --dport 21 -j DNAT --to 10.1.1.2:21</span><br></pre></td></tr></table></figure>
<p>好了，命令完成了，端口转发也做完了，本例能不能转发呢？不能，为什么呢？我下面详细分析一下。</p>
<p>对于iptables好像往外访问的配置比较容易，而对内的转发似乎就有一些问题了，在一开始的时候我就先说了一些关于netfilter的流程问题，那么我就简单说说做了这些配置之后为什么有可能还不行呢？</p>
<p>能引起这个配置失败的原因有很多，我们一个个的来说：</p>
<p><strong>第一</strong> 本例中，我们的FORWARD策略是DROP,那么也就是说，没有符合规则的包将被丢弃，不管内到外还是外到内，我们在这里依然不讨论那个确认包和关联包的问题，我们不用考虑他的问题，下面我会详细说一下这个东西，那么如何让本例可以成功呢？加入下面的规则。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -A FORWARD <span class="_">-d</span> 10.1.1.1 -p tcp --dport 80 -j ACCEPT</span><br><span class="line">iptables -A FORWARD <span class="_">-d</span> 10.1.1.2 -p tcp --dport 21 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<p>有没有觉得有一些晕？为什么目的地址是10.xxx而不是60.xxx人家internet用户不是访问的60.xxx吗？呵呵，回到上面看看那个图吧，FORWARD链在什么位置上，它是在PREROUTING之后，也就是说当这个包到达FORWARD链的时候，目的地址已经变成10.xxx了，假如internet用户的请求是这样202.1.1.1:1333–&gt;60.1.1.1:80，在经过了我们的PREROUTING链之后将变成 202.1.1.1:1333–&gt;10.1.1.1:80,这个时候如果你设置一个目的地址为60.xxx的规则有用吗？呵呵，这是问题一。这个时候应该可以完成端口转发的访问了，但是有一些时候还是不行？为什么？看问题二。</p>
<p><strong>第二</strong> 内网server的ip配置问题，这里我们以web server为例说明一下（ftp情况有一些特殊，下面我们再详细讨论，说确认包和关联包的时候讨论这个问题），上面说到，有的时候可以访问了，有的时候却不行，就是这个web server的ip设置问题了，如果web server没有指定默认的网关，那么在作了上面的配置之后，web server会收到internet的请求，但是，他不知道往哪里回啊，人家的本地路由表不知道你那个internet的ip,202.1.1.1该怎么走。如果你使用截包工具在web server上面察看，你会发现server收到了来自202.1.1.1:1333–&gt;10.1.1.1:80的请求，由于你没有给web server配置默认网关，它不知道怎么回去，所以就出现了不通的情况。怎么办呢？两个解决方法：</p>
<p>一就是给这个server配置一个默认网关，当然要指向这个配置端口转发的linux,本例是10.1.1.254,配置好了，就一定能访问了。有一个疑问？难道不需要在FORWARD链上面设置一个允许web server的ip地址访问外网的规则吗？它的包能出去？答案是肯定的，能出去。因为我们那一条允许确认包与关联包的规则，否则它是出不去的。</p>
<p><strong>第二种方法</strong>，比较麻烦一些，但是对服务器来说这样似乎更安全一些。方法就是对这个包再作一次SNAT，也就是在POSTROUTING链上添加规则。命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING <span class="_">-d</span> 10.1.1.1 -p tcp --dport 80 -j SNAT --to 10.1.1.254</span><br></pre></td></tr></table></figure></p>
<p>ftp 的方法相同。这条命令不太好懂？？其实很简单，如果使用这条命令，那么你的web server不需要再设置默认网关，就能收到这个请求，只要他和linux的lan ip地址是能互访的（也就是说web server和Linux的Lan ip在一个广播域），我们在根据上面的netfilter流程图来分析这个包到底被我们怎么样了：</p>
<ul>
<li>首先一个请求202.1.1.1:1333–&gt; 60.1.1.1:80被linux收到了，进入PREROUTING；</li>
<li>发现一个规则iptables -t nat -A PREROUTING -d 60.1.1.1 -p tcp –dport 80 -j DNAT –to 10.1.1.1:80符合，好了，改你的目的地址，于是这个包变成了202.1.1.1:1333–&gt;10.1.1.1:80，继续往前走；</li>
<li>进入FORWARD链，okay,也有一条规则允许通过iptables -A FORWARD -d 10.1.1.1 -p tcp –dport 80 -j ACCEPT；</li>
<li>进入route box选路，找到合适的路径了，继续进入POSTROUTING链；</li>
<li>耶？又发现一个符合的规则iptables -t nat -A POSTROUTING -d 10.1.1.1 -p tcp –dport 80 -j SNAT –to 10.1.1.254,原来是一个SNAT,改你的源地址，于是这个包变成了10.1.1.254:xxxx–&gt;10.1.1.1:80。为什么用xxxx了，这里的端口是随机的，我也不知道会是什么。</li>
<li>而整个的两次变化的过程都会记录在linux的ip_conntrack中；</li>
<li>当web server收到这个包的时候，发现，原来是一个内网自己兄弟来的请求阿，又在一个广播域，不用找网关，把返回包直接扔给交换机了；</li>
<li>linux在收到返回包之后，会根据他的ip_conntrack中的条目进行两次变换，返回真正的internet用户，于是完成这一次的访问。</li>
</ul>
<p>看了上面的两个例子，不知道大家是否清楚了iptables的转发流程，希望对大家有所帮助。</p>
<h4 id="状态机制"><a href="#状态机制" class="headerlink" title="状态机制"></a>状态机制</h4><p>下面我们就说说我一直在上面提到的关于那个ESTABLISHED,RELATED的规则是怎么回事，到底有什么用处。</p>
<p>说这个东西就要简单说一下网络的数据通讯的方式，我们知道，网络的访问是双向的，也就是说一个Client与Server之间完成数据交换需要双方的发包与收包。在netfilter中，有几种状态，也就是new, established,related,invalid。</p>
<p>当一个客户端，在本文例一中，内网的一台机器访问外网，我们设置了规则允许他出去，但是没有设置允许回来的规则阿，怎么完成访问呢？这就是netfilter的 状态机制 ，当一个lan用户通过这个linux访问外网的时候，它发送了一个请求包，这个包的状态是new,当外网回包的时候他的状态就是established,所以，linux知道，哦，这个包是我的内网的一台机器发出去的应答包，他就放行了。</p>
<p>而外网试图对内发起一个新的连接的时候，他的状态是new,所以linux压根不去理会它。这就是我们为什么要加这一句的原因。</p>
<p>还有那个related,他是一个关联状态，什么会用到呢？tftp,ftp都会用到，因为他们的传输机制决定了，它不像http访问那样，Client_IP: port–&gt;server:80然后server:80–&gt;Client_IP:port，ftp使用tcp21建立连接，使用20端口发送数据，其中又有两种方式，一种主动active mode，一种被动passive mode。主动模式下，client使用port命令告诉server我用哪一个端口接受数据，然后server主动发起对这个端口的请求。被动模式下，server使用port命令告诉客户端，它用那个端口监听，然后客户端发起对他的数据传输，所以这对于一个防火墙来说就是比较麻烦的事情，因为有可能会有new状态的数据包，但是它又是合理的请求，这个时候就用到这个related状态了，他就是一种关联，在linux中，有个叫 ftp_conntrack的模块，它能识别port命令，然后对相应的端口进行放行。</p>
<p>一口气写了这么多东西，不知道质量如何，大家凑和着看吧，希望多多交流共同进步，我还是一个linux的初学者，难免很多谬误，希望高手赐教指正，以期不断进步。</p>
<h4 id="实用命令"><a href="#实用命令" class="headerlink" title="实用命令"></a>实用命令</h4><p>对了，还有几个在实际中比较实用（也比较受用:-)）的命令参数，写出来供大家参考<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">iptables -L -n</span><br><span class="line"><span class="comment">#这样的列表会跳过linux的domain lookup,有的时候使用iptables -L会比较慢，因为linux会尝试解析ip的域名，真是罗嗦，如果你的dns server比较不爽的话，iptables -L就会让你很不爽，加一个-n参数就好了。列表刷的就出来。当然了，如果你的linux就是做防火墙，建议把nameserver去掉，在 /etc/resolve.conf里面，因为有时候使用route命令也会比较慢列出来，很是不爽。</span></span><br><span class="line"></span><br><span class="line">iptables -L -v</span><br><span class="line"><span class="comment">#这个命令会显示链中规则的包和流量计数，嘿嘿，看看哪些小子用的流量那么多，用tc限了他。</span></span><br><span class="line"></span><br><span class="line">iptables -t nat -L -vn</span><br><span class="line"><span class="comment">#查看nat表中的规则。</span></span><br><span class="line"></span><br><span class="line">cat /proc/net/ip_conntrack</span><br><span class="line"><span class="comment">#查看目前的conntrack，可能会比较多哦，最好加一个|grep "关键字"，看看你感兴趣的链接跟踪</span></span><br><span class="line"></span><br><span class="line">wc <span class="_">-l</span> /proc/net/ip_conntrack</span><br><span class="line"><span class="comment">#看看总链接有多少条。</span></span><br><span class="line"></span><br><span class="line">iptables-save &gt;/etc/iptables</span><br><span class="line"><span class="comment">#把当前的所有链备份一下，之所以放到/etc下面叫iptables，因为这样重起机器的时候会自动加载所有的链，经常地备份一下吧，否则如果链多，万一掉电重启，你还是会比较痛苦。</span></span><br></pre></td></tr></table></figure></p>
<p><strong>转发</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#之前因为一个网段被封了，因此通过iptables做转发：</span></span><br><span class="line"><span class="comment">#代理服务器WAN IP：111.**.**.219，LAN IP：192.168.0.219</span></span><br><span class="line"><span class="comment">#内网服务器IP：192.168.0.41</span></span><br><span class="line"><span class="comment">#1.在代理服务器打开转发功能（sysctl.conf）</span></span><br><span class="line"><span class="comment">#2.添加以下规则</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 111.**.**.219 -p tcp --dport 9999 -j DNAT --to-destination 192.168.0.41:9999</span><br><span class="line">iptables -t nat -A POSTROUTING <span class="_">-d</span> 192.168.0.41 -p tcp --dport 9999 -j SNAT --to-source 192.168.0.219</span><br></pre></td></tr></table></figure></p>
<p> 原文：<a href="http://wwdhks.blog.51cto.com/839773/1154032" target="_blank" rel="external">http://wwdhks.blog.51cto.com/839773/1154032</a></p>
]]></content>
    </entry>
    
  
  
</search>
