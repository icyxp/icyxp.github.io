<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[kuberenetes 操作实践]]></title>
      <url>http://icyxp.github.io/blog/2019/03/k8s-best-practices.html</url>
      <content type="html"><![CDATA[<h2 id="常用操作命令"><a href="#常用操作命令" class="headerlink" title="常用操作命令"></a>常用操作命令</h2><h3 id="常规命令"><a href="#常规命令" class="headerlink" title="常规命令"></a>常规命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置主机名</span></span><br><span class="line">hostnamectl --static <span class="built_in">set</span>-hostname  xxxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取集群加入信息</span></span><br><span class="line">kubeadm token create --print-join-command</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker仓库口令</span></span><br><span class="line">kubectl create secret docker-registry registry-secret --docker-server=registry.cn-shanghai.aliyuncs.com --docker-username=xupeng@patsnap --docker-password=patsnap2019! --docker-email=xupeng@patsnap -n ningbo</span><br><span class="line"></span><br><span class="line"><span class="comment"># etcd key 查看</span></span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">--endpoints=https://127.0.0.1:2379 \</span><br><span class="line">--cacert /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">get /registry/minions/ningbo-db --prefix</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="kubernetes-node-常用操作命令"><a href="#kubernetes-node-常用操作命令" class="headerlink" title="kubernetes node 常用操作命令"></a>kubernetes node 常用操作命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置node 不可调度</span></span><br><span class="line">kubectl cordon k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#驱逐node 上的pod </span></span><br><span class="line">kubectl drain k8s-node-1 --delete-local-data --force --ignore-daemonsets</span><br><span class="line"></span><br><span class="line"><span class="comment">#node 重新加入</span></span><br><span class="line">kubectl uncordon k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除node</span></span><br><span class="line">kubectl delete node k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置master不进行调度</span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置label</span></span><br><span class="line">kubectl label node k8s-master project=ipms-app</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置role标签</span></span><br><span class="line">kubectl label node k8s-node-01 node-role.kubernetes.io/node=</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除标签</span></span><br><span class="line">kubectl label nodes k8s-master mtype-</span><br></pre></td></tr></table></figure>
<h2 id="如何调整网络插件"><a href="#如何调整网络插件" class="headerlink" title="如何调整网络插件"></a>如何调整网络插件</h2><blockquote>
<p>calico网络插件调整，原本没使用IPIP模式，现在需要使用CrossSubnet</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先删除原本的calico插件</span></span><br><span class="line">kubectl delete <span class="_">-f</span> calico.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改calico.yaml，将CALICO_IPV4POOL_IPIP调整为CrossSubnet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#启用calico插件</span></span><br><span class="line">kubectl apply <span class="_">-f</span> calico.yaml</span><br></pre></td></tr></table></figure>
<h2 id="如何将kubeProxy的iptables修改为ipvs"><a href="#如何将kubeProxy的iptables修改为ipvs" class="headerlink" title="如何将kubeProxy的iptables修改为ipvs"></a>如何将kubeProxy的iptables修改为ipvs</h2><h3 id="1-加载内核模块"><a href="#1-加载内核模块" class="headerlink" title="1. 加载内核模块"></a>1. 加载内核模块</h3><p>查看内核模块是否加载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod|grep ip_vs</span><br></pre></td></tr></table></figure></p>
<p>如果没有加载，使用如下命令加载ipvs相关模块<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure></p>
<h3 id="2-更改kube-proxy配置"><a href="#2-更改kube-proxy配置" class="headerlink" title="2. 更改kube-proxy配置"></a>2. 更改kube-proxy配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit configmap kube-proxy -n kube-system</span><br></pre></td></tr></table></figure>
<p>找到并修改如下部分的内容<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enableProfiling:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">10256</span></span><br><span class="line"><span class="attr">hostnameOverride:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">iptables:</span></span><br><span class="line"><span class="attr">  masqueradeAll:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  masqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">ipvs:</span></span><br><span class="line"><span class="attr">  excludeCIDRs:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  scheduler:</span> <span class="string">""</span>  <span class="comment">#=========================&gt; 为空表示默认的负载均衡算法为轮询， rr, wrr, lc, wlc, sh, dh, lblc...</span></span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">metricsBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">10249</span></span><br><span class="line"><span class="attr">mode:</span> iptables   <span class="comment">#=========================&gt; 修改此处为ipvs</span></span><br><span class="line"><span class="attr">nodePortAddresses:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">portRange:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">resourceContainer:</span> /kube-proxy</span><br><span class="line"><span class="attr">udpIdleTimeout:</span> <span class="number">250</span>ms</span><br></pre></td></tr></table></figure></p>
<p>编辑完，保存退出</p>
<h3 id="3-删除所有kube-proxy的pod"><a href="#3-删除所有kube-proxy的pod" class="headerlink" title="3. 删除所有kube-proxy的pod"></a>3. 删除所有kube-proxy的pod</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod $(kubectl get pod -n kube-system | grep kube-proxy | awk -F <span class="string">' '</span> <span class="string">'&#123;print $1&#125;'</span>) -n kube-system</span><br></pre></td></tr></table></figure>
<h3 id="4-查看kube-proxy的pod日志"><a href="#4-查看kube-proxy的pod日志" class="headerlink" title="4. 查看kube-proxy的pod日志"></a>4. 查看kube-proxy的pod日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs kube-proxy-xxx -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">#I0308 02:16:02.980965       1 server_others.go:183] Using ipvs Proxier.</span></span><br><span class="line"><span class="comment">#W0308 02:16:02.991188       1 proxier.go:356] IPVS scheduler not specified, use rr by default</span></span><br><span class="line"><span class="comment">#I0308 02:16:02.991338       1 server_others.go:210] Tearing down inactive rules.</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.022123       1 server.go:448] Version: v1.11.6</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.028801       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_max' to 131072</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029030       1 conntrack.go:52] Setting nf_conntrack_max to 131072</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029208       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029296       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029639       1 config.go:102] Starting endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029682       1 controller_utils.go:1025] Waiting for caches to sync for endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029723       1 config.go:202] Starting service config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029777       1 controller_utils.go:1025] Waiting for caches to sync for service config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.129930       1 controller_utils.go:1032] Caches are synced for endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.129931       1 controller_utils.go:1032] Caches are synced for service config controller</span></span><br><span class="line"></span><br><span class="line">看到有 Using ipvs Proxier 即表明切换成功.</span><br></pre></td></tr></table></figure>
<h3 id="5-安装ipvsadm"><a href="#5-安装ipvsadm" class="headerlink" title="5. 安装ipvsadm"></a>5. 安装ipvsadm</h3><p>使用ipvsadm查看ipvs相关规则，如果没有这个命令可以直接yum安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ipvsadm</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 服务质量 Qos 解析]]></title>
      <url>http://icyxp.github.io/blog/2019/03/k8s-pod-qos.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><code>QoS</code>是 Quality of Service 的缩写，即服务质量。为了实现资源被有效调度和分配的同时提高资源利用率，<code>kubernetes</code>针对不同服务质量的预期，通过 QoS（Quality of Service）来对 pod 进行服务质量管理。对于一个 pod 来说，服务质量体现在两个具体的指标：<code>CPU 和内存</code>。当节点上内存资源紧张时，kubernetes 会根据预先设置的不同 QoS 类别进行相应处理。</p>
<p>QoS 主要分为<code>Guaranteed</code>、<code>Burstable</code> 和 <code>Best-Effort</code>三类，优先级从高到低。</p>
<h2 id="Guaranteed-有保证的"><a href="#Guaranteed-有保证的" class="headerlink" title="Guaranteed(有保证的)"></a>Guaranteed(有保证的)</h2><p>对于绑定 CPU 和具有相对可预测性的工作负载（例如，用来处理请求的 Web 服务）来说，这是一个很好的 QoS 等级。属于该级别的pod有以下两种：</p>
<ul>
<li><strong>Pod中的所有容器都且仅设置了 CPU 和内存的 limits</strong></li>
<li><strong>pod中的所有容器都设置了 CPU 和内存的 requests 和 limits ，且单个容器内的requests==limits（requests不等于0）</strong><a id="more"></a>
<strong>示例1：pod中的所有容器都且仅设置了limits</strong><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>示例2： pod 中的所有容器都设置了 requests 和 limits，且单个容器内的<code>requests==limits</code></strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br></pre></td></tr></table></figure></p>
<p>容器foo和bar内resources的requests和limits均相等，该pod的QoS级别属于<code>Guaranteed</code>。</p>
<h2 id="Burstable-不稳定的"><a href="#Burstable-不稳定的" class="headerlink" title="Burstable(不稳定的)"></a>Burstable(不稳定的)</h2><p>这对短时间内需要消耗大量资源或者初始化过程很密集的工作负载非常有用，例如：用来构建 Docker 容器的 Worker 和运行未优化的 JVM 进程的容器都可以使用该 QoS 等级。<strong>pod中只要有一个容器的requests和limits的设置不相同</strong>，该pod的QoS即为<code>Burstable</code>。</p>
<p><strong>示例1：容器foo指定了resource，而容器bar未指定</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br></pre></td></tr></table></figure></p>
<p><strong>示例2：容器foo设置了内存limits，而容器bar设置了CPU limits</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong><code>注意：若容器指定了requests而未指定limits，则limits的值等于节点resource的最大值；若容器指定了limits而未指定requests，则requests的值等于limits。</code></strong></p>
</blockquote>
<h2 id="Best-Effort-尽最大努力"><a href="#Best-Effort-尽最大努力" class="headerlink" title="Best-Effort(尽最大努力)"></a>Best-Effort(尽最大努力)</h2><p>这对于可中断和低优先级的工作负载非常有用，例如：迭代运行的幂等优化过程。<strong>如果Pod中所有容器的resources均未设置requests与limits</strong>，该pod的QoS即为<code>Best-Effort</code>。</p>
<p><strong>示例1：容器foo和容器bar均未设置requests和limits</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br></pre></td></tr></table></figure></p>
<h2 id="根据QoS进行资源回收策略"><a href="#根据QoS进行资源回收策略" class="headerlink" title="根据QoS进行资源回收策略"></a>根据QoS进行资源回收策略</h2><p>Kubernetes 通过<code>cgroup</code>给pod设置QoS级别，当资源不足时先<code>kill</code>优先级低的 pod，在实际使用过程中，通过<code>OOM</code>分数值来实现，<code>OOM</code>分数值范围为0-1000。OOM 分数值根据<code>OOM_ADJ</code>参数计算得出。</p>
<p>对于<code>Guaranteed</code>级别的 Pod，OOM_ADJ参数设置成了-998，对于<code>Best-Effort</code>级别的 Pod，OOM_ADJ参数设置成了1000，对于<code>Burstable</code>级别的 Pod，OOM_ADJ参数取值从2到999。</p>
<p>对于 kuberntes 保留资源，比如kubelet，docker，OOM_ADJ参数设置成了-999，表示不会被OOM kill掉。<strong>OOM_ADJ参数设置的越大，计算出来的OOM分数越高，表明该pod优先级就越低，当出现资源竞争时会越早被kill掉</strong>，对于OOM_ADJ参数是-999的表示kubernetes永远不会因为OOM将其kill掉。</p>
<h2 id="QoS-pods被kill掉场景与顺序"><a href="#QoS-pods被kill掉场景与顺序" class="headerlink" title="QoS pods被kill掉场景与顺序"></a>QoS pods被kill掉场景与顺序</h2><ul>
<li><code>Best-Effort pods</code>：系统用完了全部内存时，该类型 pods 会最先被kill掉。</li>
<li><code>Burstable pods</code>：系统用完了全部内存，且没有 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。</li>
<li><code>Guaranteed pods</code>：系统用完了全部内存，且没有 Burstable 与 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。</li>
</ul>
<h2 id="QoS使用建议"><a href="#QoS使用建议" class="headerlink" title="QoS使用建议"></a>QoS使用建议</h2><p>如果资源充足，可将 QoS pods 类型均设置为<code>Guaranteed</code>。用计算资源换业务性能和稳定性，减少排查问题时间和成本。如果想更好的提高资源利用率，<strong>业务服务</strong>可以设置为<code>Guaranteed</code>，而其他服务根据重要程度可分别设置为<code>Burstable</code>或<code>Best-Effort</code>。</p>
<p>在搞清楚服务什么时候会出现故障以及为什么会出现故障之前，都不应该将其部署到生产环境中。可通过一些技术手段（<strong>负载压测等</strong>）来设置应用的资源 limits 和 requests。这将会为你的系统增加弹性能力和可预测性。</p>
<p>在测试过程中，记录服务失败时做了哪些操作是至关重要的。可以将发现的故障模式添加到相关的书籍和文档中，这对分类生产环境中出现的问题很有用。下面是我们在测试过程中发现的一些故障模式：</p>
<ul>
<li>内存缓慢增加</li>
<li>CPU 使用率达到 100%</li>
<li>响应时间太长</li>
<li>请求被丢弃</li>
<li>不同请求的响应时间差异很大</li>
</ul>
<p>你最好将这些发现都收集起来，以备不时之需，因为有一天它们可能会为你或团队节省一整天的时间。</p>
<h2 id="一些有用的工具"><a href="#一些有用的工具" class="headerlink" title="一些有用的工具"></a>一些有用的工具</h2><h3 id="Loader-io"><a href="#Loader-io" class="headerlink" title="Loader.io"></a>Loader.io</h3><p><a href="http://loader.io/" target="_blank" rel="external">Loader.io</a> 是一个在线负载测试工具，它允许你配置负载增加测试和负载不变测试，在测试过程中可视化应用程序的性能和负载，并能快速启动和停止测试。它也会保存测试结果的历史记录，因此在资源限制发生变化时很容易对结果进行比较。<br><img src="/images/k8s/loader.jpg" alt="Loader"></p>
<h3 id="Kubescope-cli"><a href="#Kubescope-cli" class="headerlink" title="Kubescope cli"></a>Kubescope cli</h3><p><a href="https://github.com/hharnisc/kubescope-cli" target="_blank" rel="external">Kubescope cli</a> 是一个可以运行在本地或 Kubernetes 中的工具，可直接从 Docker Daemon 中收集容器指标并可视化。和 <code>cAdvisor</code> 等其他集群指标收集服务一样， <code>kubescope cli</code> 收集指标的周期是 1 秒（而不是 10-15 秒）。如果周期是 10-15 秒，你可能会在测试期间错过一些引发性能瓶颈的问题。如果你使用 cAdvisor 进行测试，每次都要使用新的 Pod 作为测试对象，因为 Kubernetes 在超过资源限制时就会将 Pod 杀死，然后重新启动一个全新的 Pod。而 <code>kubescope cli</code> 就没有这方面的忧虑，它直接从 Docker Daemon 中收集容器指标（你可以自定义收集指标的时间间隔），并使用正则表达式来选择和过滤你想要显示的容器。<br><img src="/images/k8s/kubescope-cli.gif" alt="kubescope-cli"></p>
<p>来源：</p>
<ul>
<li>阳明的博客</li>
<li>Ryan Yang</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[理解 kubernetes 亲和性调度]]></title>
      <url>http://icyxp.github.io/blog/2019/03/k8s-affinity.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一般情况下我们部署的 Pod 是通过集群的自动调度策略来选择节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 Pod 的调度，比如我们内部的一些服务 gitlab 之类的也是跑在<code>Kubernetes</code>集群上的，我们就不希望对外的一些服务和内部的服务跑在同一个节点上了，害怕内部服务对外部的服务产生影响；但是有的时候我们的服务之间交流比较频繁，又希望能够将这两个服务的 Pod 调度到同一个的节点上。这就需要用到 Kubernetes 里面的一个概念：亲和性和反亲和性。</p>
<p>亲和性有分成节点亲和性(<code>nodeAffinity</code>)和 Pod 亲和性(<code>podAffinity</code>)。</p>
<h2 id="nodeSelector"><a href="#nodeSelector" class="headerlink" title="nodeSelector"></a>nodeSelector</h2><p>在了解亲和性之前，我们先来了解一个非常常用的调度方式：nodeSelector。我们知道label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的一个就是 service 通过匹配 label 去匹配 Pod 资源，而 Pod 的调度也可以根据节点的 label 来进行调度。</p>
<p>我们可以通过下面的命令查看我们的 node 的 label：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    147d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    67d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    127d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>现在我们先给节点node02增加一个com=youdianzhishi的标签，命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label nodes node02 com=youdianzhishi</span><br><span class="line">node <span class="string">"node02"</span> labeled</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>我们可以通过上面的<code>--show-labels</code>参数可以查看上述标签是否生效。当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 Pod 的<code>spec</code>字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label 即可。比如，下面的 Pod 我们要强制调度到 node02 这个节点上去，我们就可以使用 nodeSelector 来表示了：(node-selector-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> busybox-pod</span><br><span class="line"><span class="attr">  name:</span> test-busybox</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> sleep</span><br><span class="line"><span class="bullet">    -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    imagePullPolicy:</span> Always</span><br><span class="line"><span class="attr">    name:</span> test-busybox</span><br><span class="line"><span class="attr">  nodeSelector:</span></span><br><span class="line"><span class="attr">    com:</span> youdianzhishi</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以通过 describe 命令查看调度结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> node-selector-demo.yaml</span><br><span class="line">pod <span class="string">"test-busybox"</span> created</span><br><span class="line">$ kubectl describe pod <span class="built_in">test</span>-busybox</span><br><span class="line">Name:         <span class="built_in">test</span>-busybox</span><br><span class="line">Namespace:    default</span><br><span class="line">Node:         node02/10.151.30.63</span><br><span class="line">......</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  com=youdianzhishi</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason                 Age   From               Message</span><br><span class="line">  ----    ------                 ----  ----               -------</span><br><span class="line">  Normal  SuccessfulMountVolume  55s   kubelet, node02    MountVolume.SetUp succeeded <span class="keyword">for</span> volume <span class="string">"default-token-n9w2d"</span></span><br><span class="line">  Normal  Scheduled              54s   default-scheduler  Successfully assigned <span class="built_in">test</span>-busybox to node02</span><br><span class="line">  Normal  Pulling                54s   kubelet, node02    pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Pulled                 40s   kubelet, node02    Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Created                40s   kubelet, node02    Created container</span><br><span class="line">  Normal  Started                40s   kubelet, node02    Started container</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 Events 下面的信息，我们的 Pod 通过默认的 default-scheduler 调度器被绑定到了node02节点。不过需要注意的是<code>nodeSelector</code>属于强制性的，如果我们的目标节点没有可用的资源，我们的 Pod 就会一直处于 Pending 状态，这就是<code>nodeSelector</code>的用法。</p>
<p>通过上面的例子我们可以感受到<code>nodeSelector</code>的方式比较直观，但是还够灵活，控制粒度偏大，接下来我们再和大家了解下更加灵活的方式：节点亲和性(<code>nodeAffinity</code>)。</p>
<h2 id="亲和性和反亲和性调度"><a href="#亲和性和反亲和性调度" class="headerlink" title="亲和性和反亲和性调度"></a>亲和性和反亲和性调度</h2><p>我们了解了 kubernetes 调度器的一个调度流程，我们知道默认的调度器在使用的时候，经过了 predicates 和 priorities 两个阶段，但是在实际的生产环境中，往往我们需要根据自己的一些实际需求来控制 pod 的调度，这就需要用到 nodeAffinity(节点亲和性)、podAffinity(pod 亲和性) 以及 podAntiAffinity(pod 反亲和性)。</p>
<p>亲和性调度可以分成软策略和硬策略两种方式:</p>
<ul>
<li><code>软策略</code>就是如果你没有满足调度要求的节点的话，pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓了的策略</li>
<li><code>硬策略</code>就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不干的策略。</li>
</ul>
<p>对于亲和性和反亲和性都有这两种规则可以设置： <code>preferredDuringSchedulingIgnoredDuringExecution</code>和<code>requiredDuringSchedulingIgnoredDuringExecution</code>，前面的就是软策略，后面的就是硬策略。</p>
<blockquote>
<p>这命名不觉得有点反人类吗？有点无语……</p>
</blockquote>
<h2 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h2><p>节点亲和性主要是用来控制 pod 要部署在哪些主机上，以及不能部署在哪些主机上的。它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。</p>
<p>比如现在我们用一个 Deployment 来管理3个 pod 副本，现在我们来控制下这些 pod 的调度，如下例子：（node-affinity-demo.yaml）<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">            nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">            - matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> kubernetes.io/hostname</span><br><span class="line"><span class="attr">                operator:</span> NotIn</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> node03</span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 软策略</span></span><br><span class="line"><span class="attr">          - weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            preference:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> com</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> youdianzhishi</span><br></pre></td></tr></table></figure></p>
<p>上面这个 pod 首先是要求不能运行在 node03 这个节点上，如果有个节点满足<code>com=youdianzhishi</code>的话就优先调度到这个节点上。</p>
<p>下面是我们测试的节点列表信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>可以看到 node02 节点有<code>com=youdianzhishi</code>这样的 label，按要求会优先调度到这个节点来的，现在我们来创建这个 pod，然后使用descirbe命令查看具体的调度情况是否满足我们的要求。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> node-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods <span class="_">-l</span> app=affinity -o wide</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7b4c946854-5gfln   1/1       Running   0          47s       10.244.4.214   node02</span><br><span class="line">affinity-7b4c946854<span class="_">-l</span>8b47   1/1       Running   0          47s       10.244.4.215   node02</span><br><span class="line">affinity-7b4c946854-r86p5   1/1       Running   0          47s       10.244.4.213   node02</span><br></pre></td></tr></table></figure></p>
<p>从结果可以看出 pod 都被部署到了 node02，其他节点上没有部署 pod，这里的匹配逻辑是 label 的值在某个列表中，现在<code>Kubernetes</code>提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<blockquote>
<p>如果<code>nodeSelectorTerms</code>下面有多个选项的话，满足任何一个条件就可以了；如果<code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 POD。</p>
</blockquote>
<h2 id="podAffinity"><a href="#podAffinity" class="headerlink" title="podAffinity"></a>podAffinity</h2><p>pod 亲和性主要解决 pod 可以和哪些 pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 pod 反亲和性主要是解决 pod 不能和哪些 pod 部署在同一个拓扑域中的问题，它们都是处理的 pod 与 pod 之间的关系，比如一个 pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 pod 在节点上了，那么我就不想和你待在同一个节点上。</p>
<p>由于我们这里只有一个集群，并没有区域或者机房的概念，所以我们这里直接使用主机名来作为拓扑域，把 pod 创建在同一个主机上面。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>同样，还是针对上面的资源对象，我们来测试下 pod 的亲和性：（pod-affinity-demo.yaml）<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">          - labelSelector:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> app</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> busybox-pod</span><br><span class="line"><span class="attr">            topologyKey:</span> kubernetes.io/hostname</span><br></pre></td></tr></table></figure></p>
<p>上面这个例子中的 pod 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 pod：这个 pod 有一个<code>app=busybox-pod</code>的 label。</p>
<p>我们查看有标签<code>app=busybox-pod</code>的 pod 列表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=busybox-pod</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line"><span class="built_in">test</span>-busybox   1/1       Running   164        7d        10.244.4.205   node02</span><br></pre></td></tr></table></figure></p>
<p>我们看到这个 pod 运行在了 node02 的节点上面，所以按照上面的亲和性来说，上面我们部署的3个 pod 副本也应该运行在 node02 节点上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=affinity</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-564f9d7db9-lzzvq   1/1       Running   0          3m        10.244.4.216   node02</span><br><span class="line">affinity-564f9d7db9-p79cq   1/1       Running   0          3m        10.244.4.217   node02</span><br><span class="line">affinity-564f9d7db9-spfzs   1/1       Running   0          3m        10.244.4.218   node02</span><br></pre></td></tr></table></figure></p>
<p>如果我们把上面的 test-busybox 和 affinity 这个 Deployment 都删除，然后重新创建 affinity 这个资源，看看能不能正常调度呢：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete <span class="_">-f</span> node-selector-demo.yaml</span><br><span class="line">pod <span class="string">"test-busybox"</span> deleted</span><br><span class="line">$ kubectl delete <span class="_">-f</span> pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> deleted</span><br><span class="line">$ kubectl create <span class="_">-f</span> pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=affinity</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP        NODE</span><br><span class="line">affinity-564f9d7db9-fbc8w   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-n8gcf   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-qc7x6   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到处于<code>Pending</code>状态了，这是因为现在没有一个节点上面拥有<code>busybox-pod</code>这个 label 的 pod，而上面我们的调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 test-busybox 这个 pod 调度到 node03 这个节点上，看看上面的 affinity 的3个副本会不会也被调度到 node03 这个节点上去？</p>
<p>我们这个地方使用的是<code>kubernetes.io/hostname</code>这个拓扑域，意思就是我们当前调度的 pod 要和目标的 pod 处于同一个主机上面，因为要处于同一个拓扑域下面，为了说明这个问题，我们把拓扑域改成<code>beta.kubernetes.io/os</code>，同样的我们当前调度的 pod 要和目标的 pod 处于同一个拓扑域中，目标的 pod 是不是拥有<code>beta.kubernetes.io/os=linux</code>的标签，而我们这里3个节点都有这样的标签，这也就意味着我们3个节点都在同一个拓扑域中，所以我们这里的 pod 可能会被调度到任何一个节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7d86749984-glkhz                 1/1       Running     0          3m        10.244.2.16    node03</span><br><span class="line">affinity-7d86749984-h4fb9                 1/1       Running     0          3m        10.244.4.219   node02</span><br><span class="line">affinity-7d86749984-tj7k2                 1/1       Running     0          3m        10.244.2.14    node03</span><br></pre></td></tr></table></figure></p>
<h2 id="podAntiAffinity"><a href="#podAntiAffinity" class="headerlink" title="podAntiAffinity"></a>podAntiAffinity</h2><p>这就是 pod 亲和性的用法，而 pod 反亲和性则是反着来的，比如一个节点上运行了某个 pod，那么我们的 pod 则希望被调度到其他节点上去，同样我们把上面的 podAffinity 直接改成 podAntiAffinity，(pod-antiaffinity-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">          - labelSelector:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> app</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> busybox-pod</span><br><span class="line"><span class="attr">            topologyKey:</span> kubernetes.io/hostname</span><br></pre></td></tr></table></figure></p>
<p>这里的意思就是如果一个节点上面有一个<code>app=busybox-pod</code>这样的 pod 的话，那么我们的 pod 就别调度到这个节点上面来，上面我们把<code>app=busybox-pod</code>这个 pod 固定到了 node03 这个节点上面来，所以正常来说我们这里的 pod 不会出现在 node03 节点上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> pod-antiaffinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-bcbd8854f-br8z8                  1/1       Running     0          5s        10.244.4.222   node02</span><br><span class="line">affinity-bcbd8854f-cdffh                  1/1       Running     0          5s        10.244.4.223   node02</span><br><span class="line">affinity-bcbd8854f-htb52                  1/1       Running     0          5s        10.244.4.224   node02</span><br><span class="line"><span class="built_in">test</span>-busybox                              1/1       Running     0          23m       10.244.2.10    node03</span><br></pre></td></tr></table></figure></p>
<p>这就是 pod 反亲和性的用法。</p>
<h2 id="污点（taints）与容忍（tolerations）"><a href="#污点（taints）与容忍（tolerations）" class="headerlink" title="污点（taints）与容忍（tolerations）"></a>污点（taints）与容忍（tolerations）</h2><p>对于<code>nodeAffinity</code>无论是硬策略还是软策略方式，都是调度 pod 到预期节点上，而<code>Taints</code>恰好与之相反，如果一个节点标记为 Taints ，除非 pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 pod。</p>
<p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 pod，则污点就很有用了，pod 不会再被调度到 taint 标记过的节点。我们使用<code>kubeadm</code>搭建的集群默认就给 master 节点添加了一个污点标记，所以我们看到我们平时的 pod 都没有被调度到 master 上去：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node master</span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/hostname=master</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">......</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      <span class="literal">false</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以使用上面的命令查看 master 节点的信息，其中有一条关于 Taints 的信息：node-role.kubernetes.io/master:NoSchedule，就表示给 master 节点打了一个污点的标记，其中影响的参数是<code>NoSchedule</code>，表示 pod 不会被调度到标记为 taints 的节点，除了 NoSchedule 外，还有另外两个选项：</p>
<ul>
<li><code>PreferNoSchedule</code>：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去</li>
<li><code>NoExecute</code>：该选项意味着一旦 Taint 生效，如该节点内正在运行的 pod 没有对应 Tolerate 设置，会直接被逐出</li>
</ul>
<p>污点 taint 标记节点的命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes node02 <span class="built_in">test</span>=node02:NoSchedule</span><br><span class="line">node <span class="string">"node02"</span> tainted</span><br></pre></td></tr></table></figure></p>
<p>上面的命名将 node02 节点标记为了污点，影响策略是 NoSchedule，只会影响新的 pod 调度，如果仍然希望某个 pod 调度到 taint 节点上，则必须在 Spec 中做出<code>Toleration</code>定义，才能调度到该节点，比如现在我们想要将一个 pod 调度到 master 节点：(taint-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> taint</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> taint</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> taint</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> http</span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node-role.kubernetes.io/master"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>由于 master 节点被标记为了污点节点，所以我们这里要想 pod 能够调度到 master 节点去，就需要增加容忍的声明：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"node-role.kubernetes.io/master"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>然后创建上面的资源，查看结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> taint-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"taint"</span> created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS             RESTARTS   AGE       IP             NODE</span><br><span class="line">......</span><br><span class="line">taint-845d8bb4fb-57mhm                    1/1       Running            0          1m        10.244.4.247   node02</span><br><span class="line">taint-845d8bb4fb-bbvmp                    1/1       Running            0          1m        10.244.0.33    master</span><br><span class="line">taint-845d8bb4fb-zb78x                    1/1       Running            0          1m        10.244.4.246   node02</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到有一个 pod 副本被调度到了 master 节点，这就是容忍的使用方法。</p>
<p>对于 tolerations 属性的写法，其中的 key、value、effect 与 Node 的 Taint 设置需保持一致， 还有以下几点说明：</p>
<ul>
<li>1.如果 operator 的值是 Exists，则 value 属性可省略</li>
<li>2.如果 operator 的值是 Equal，则表示其 key 与 value 之间的关系是 equal(等于)</li>
<li>3.如果不指定 operator 属性，则默认值为 Equal</li>
</ul>
<p>另外，还有两个特殊值：</p>
<ul>
<li>1.空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也是是能容忍所有 node 的所有 Taints</li>
<li>2.空的 effect 匹配所有的 effect</li>
</ul>
<p>最后，如果我们要取消节点的污点标记，可以使用下面的命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes node02 <span class="built_in">test</span>-</span><br><span class="line">node <span class="string">"node02"</span> untainted</span><br></pre></td></tr></table></figure></p>
<p>这就是污点和容忍的使用方法。                                                                                             </p>
<p>来源：www.qikqiak.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在阿里云使用Kubeadm 1.13.x 部署多Master集群]]></title>
      <url>http://icyxp.github.io/blog/2019/02/k8s-kubeadm-ha-1-13-x.html</url>
      <content type="html"><![CDATA[<h2 id="前言（坑）"><a href="#前言（坑）" class="headerlink" title="前言（坑）"></a>前言（坑）</h2><p><strong>负载均衡问题</strong></p>
<ul>
<li>阿里不支持LVS，没有vip可用，必须通过申请SLB来固定VIP</li>
<li>因Kubernetes apiserver为https协议，阿里SLB中能负载均衡HTTPS的只有TCP方式，但TCP协议不能转发到发起主机（<code>apiserver 需要有回环路访问，简单说就是自己给自己发请求</code>）</li>
</ul>
<p>为了解决kubernets apiserver高可用问题，故用以下方式来解决：</p>
<ul>
<li>申请一个内网的SLB（获取VIP），8443为监听端口，6443为apiserver的后端端口</li>
<li>在每台master机器上搭建keepalived+haproxy，VIP 用SLB的VIP</li>
</ul>
<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">申请阿里云的SLB获取到</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.13.3-0 kubelet-1.13.3-0 kubectl-1.13.3-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy"><a href="#搭建keepalived-haproxy" class="headerlink" title="搭建keepalived + haproxy"></a>搭建keepalived + haproxy</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.183']"</span> \ <span class="comment">#master-1地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.184']"</span> \ <span class="comment">#master-2地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.185']"</span> \ <span class="comment">#master-3地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="配置阿里云-SLB"><a href="#配置阿里云-SLB" class="headerlink" title="配置阿里云 SLB"></a>配置阿里云 SLB</h3><blockquote>
<p>在阿里云SLB 管理界面添加8443监听端口，使用<code>TCP</code>协议，后端服务器选择一台你即将初始化的master机器，后端服务器端口为6443，健康检查默认配置即可，保存配置。此时你的SLB是不进行工作的，因为后端服务器的6443端口还未监听。</p>
</blockquote>
<h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.13</span><span class="number">.3</span></span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.100</span>:<span class="number">8443</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line"><span class="attr">  certSANs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.183</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.184</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.185</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.100</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">16</span></span><br><span class="line"><span class="attr">imageRepository:</span> registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">mode:</span> iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:<span class="number">3.1</span>  k8s.gcr.io/pause:<span class="number">3.1</span></span><br><span class="line">kubeadm init --config $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -f /etc/kubernetes/admin.conf $&#123;HOME&#125;/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="初始化其它master节点"><a href="#初始化其它master节点" class="headerlink" title="初始化其它master节点"></a>初始化其它master节点</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化master-2</span></span><br><span class="line">ssh master-2 <span class="string">"kubeadm reset -f </span><br><span class="line">rm -rf /etc/kubernetes/pki/</span><br><span class="line">mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化master-3</span></span><br><span class="line">ssh master-3 <span class="string">"kubeadm reset -f </span><br><span class="line">rm -rf /etc/kubernetes/pki/ </span><br><span class="line">mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件到master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件到master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取加入口令</span></span><br><span class="line">JOIN_CMD=`kubeadm token create --print-join-command`</span><br><span class="line"></span><br><span class="line"><span class="comment">#将master-2 加入</span></span><br><span class="line">ssh master-2 <span class="string">"<span class="variable">$&#123;JOIN_CMD&#125;</span> --experimental-control-plane"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将master-3 加入</span></span><br><span class="line">ssh master-3 <span class="string">"<span class="variable">$&#123;JOIN_CMD&#125;</span> --experimental-control-plane"</span></span><br></pre></td></tr></table></figure>
<h3 id="再次配置阿里云-SLB"><a href="#再次配置阿里云-SLB" class="headerlink" title="再次配置阿里云 SLB"></a>再次配置阿里云 SLB</h3><blockquote>
<p>同样进入阿里云SLB管理界面，将其余的两台master机器加入到后端服务器中。这样你的apiserver 就高可用了</p>
</blockquote>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.3.2</span><br><span class="line">docker pull quay.io/calico/node:v3.3.2</span><br><span class="line">docker pull quay.io/calico/cni:v3.3.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.13.3</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.13.3</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.13.3</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.13.3</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.13.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 调度器介绍]]></title>
      <url>http://icyxp.github.io/blog/2019/02/k8s-scheduler.html</url>
      <content type="html"><![CDATA[<h3 id="kube-scheduler-简介"><a href="#kube-scheduler-简介" class="headerlink" title="kube-scheduler 简介"></a>kube-scheduler 简介</h3><blockquote>
<p><code>kube-scheduler</code>是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。</p>
</blockquote>
<h3 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h3><p>默认情况下，kube-scheduler 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。</p>
<p><code>kube-scheduler</code> 是 kubernetes 的调度器，它的主要作用就是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上去，是一个独立的二进制程序，启动之后会一直监听 API Server，获取到 PodSpec.NodeName 为空的 Pod，对每个 Pod 都会创建一个 binding。<br><img src="/images/k8s/kube-scheduler-structrue.jpg" alt="K8s scheduler structure"><br><a id="more"></a><br>这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：</p>
<ul>
<li>如何保证全部的节点调度的公平性？要知道并不是说有节点资源配置都是一样的</li>
<li>如何保证每个节点都能被分配资源？</li>
<li>集群资源如何能够被高效利用？</li>
<li>集群资源如何才能被最大化使用？</li>
<li>如何保证 Pod 调度的性能和效率？</li>
<li>用户是否可以根据自己的实际需求定制自己的调度策略？</li>
</ul>
<p>考虑到实际环境中的各种复杂情况，kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</p>
<p>kubernetes 调度器的源码位于 kubernetes/pkg/scheduler 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/scheduler</span><br><span class="line">-- scheduler.go         <span class="comment">#调度相关的具体实现</span></span><br><span class="line">|-- algorithm</span><br><span class="line">|   |-- predicates      <span class="comment">#节点筛选策略</span></span><br><span class="line">|   |-- priorities      <span class="comment">#节点打分策略</span></span><br><span class="line">|-- algorithmprovider</span><br><span class="line">|   |-- defaults        <span class="comment">#定义默认的调度器</span></span><br></pre></td></tr></table></figure></p>
<p>其中 Scheduler 创建和运行的核心程序，对应的代码在 pkg/scheduler/scheduler.go，如果要查看<code>kube-scheduler</code>的入口程序，对应的代码在 cmd/kube-scheduler/scheduler.go。</p>
<p>调度主要分为以下几个部分：</p>
<ul>
<li>首先是预选过程，过滤掉不满足条件的节点，这个过程称为<code>Predicates</code></li>
<li>然后是优选过程，对通过的节点按照优先级排序，称之为<code>Priorities</code></li>
<li>最后从中选择优先级最高的节点，如果中间任何一步骤有错误，就直接返回错误</li>
</ul>
<p><code>Predicates</code>阶段首先遍历全部节点，过滤掉不满足条件的节点，属于强制性规则，这一阶段输出的所有满足要求的 Node 将被记录并作为第二阶段的输入，如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试。</p>
<blockquote>
<p>所以我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。</p>
</blockquote>
<p><code>Priorities</code>阶段即再次对节点进行筛选，如果有多个节点都满足条件的话，那么系统会按照节点的优先级(priorites)大小对节点进行排序，最后选择优先级最高的节点来部署 Pod 应用。</p>
<p>下面是调度过程的简单示意图：<br><img src="/images/k8s/kube-scheduler-filter.jpg" alt="K8s scheduler filter"></p>
<p>更详细的流程是这样的：</p>
<ul>
<li>首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源</li>
<li>API Server 收到用户请求后，存储相关数据到 etcd 数据库中</li>
<li>调度器监听 API Server 查看为调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点，这个分配过程就是我们上面提到的两个阶段：<ul>
<li>预选阶段(Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉</li>
<li>优选阶段(Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本分布到不同的主机上，使用最低负载的主机等等策略</li>
</ul>
</li>
<li>经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 binding 操作，然后将结果存储到 etcd 中</li>
<li>最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作</li>
</ul>
<p>其中<code>Predicates</code>过滤有一系列的算法可以使用，我们这里简单列举几个：</p>
<ul>
<li>PodFitsResources：节点上剩余的资源是否大于 Pod 请求的资源</li>
<li>PodFitsHost：如果 Pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配</li>
<li>PodFitsHostPorts：节点上已经使用的 port 是否和 Pod 申请的 port 冲突</li>
<li>PodSelectorMatches：过滤掉和 Pod 指定的 label 不匹配的节点</li>
<li>NoDiskConflict：已经 mount 的 volume 和 Pod 指定的 volume 不冲突，除非它们都是只读的</li>
<li>CheckNodeDiskPressure：检查节点磁盘空间是否符合要求</li>
<li>CheckNodeMemoryPressure：检查节点内存是否够用</li>
</ul>
<p>除了这些过滤算法之外，还有一些其他的算法，更多更详细的我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/predicates.go。</p>
<p>而<code>Priorities</code>优先级是由一系列键值对组成的，键是该优先级的名称，值是它的权重值，同样，我们这里给大家列举几个具有代表性的选项：</p>
<ul>
<li>LeastRequestedPriority：通过计算 CPU 和内存的使用率来决定权重，使用率越低权重越高，当然正常肯定也是资源是使用率越低权重越高，能给别的 Pod 运行的可能性就越大</li>
<li>SelectorSpreadPriority：为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高</li>
<li>ImageLocalityPriority：就是如果在某个节点上已经有要使用的镜像节点了，镜像总大小值越大，权重就越高</li>
<li>NodeAffinityPriority：这个就是根据节点的亲和性来计算一个权重值，后面我们会详细讲解亲和性的使用方法</li>
</ul>
<p>除了这些策略之外，还有很多其他的策略，同样我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/ 了解更多信息。每一个优先级函数会返回一个0-10的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn)</span><br></pre></td></tr></table></figure></p>
<h3 id="自定义调度"><a href="#自定义调度" class="headerlink" title="自定义调度"></a>自定义调度</h3><p>上面就是 kube-scheduler 默认调度的基本流程，除了使用默认的调度器之外，我们也可以自定义调度策略。</p>
<h4 id="调度器扩展"><a href="#调度器扩展" class="headerlink" title="调度器扩展"></a>调度器扩展</h4><p><code>kube-scheduler</code>在启动的时候可以通过 <code>--policy-config-file</code>参数来指定调度策略文件，我们可以根据我们自己的需要来组装<code>Predicates</code>和<code>Priority</code>函数。选择不同的过滤函数和优先级函数、控制优先级函数的权重、调整过滤函数的顺序都会影响调度过程。</p>
<p>下面是官方的 Policy 文件示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"kind"</span> : <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"apiVersion"</span> : <span class="string">"v1"</span>,</span><br><span class="line">    <span class="string">"predicates"</span> : [</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"PodFitsHostPorts"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"PodFitsResources"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"NoDiskConflict"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"NoVolumeZoneConflict"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"MatchNodeSelector"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"HostName"</span>&#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"priorities"</span> : [</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"LeastRequestedPriority"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"BalancedResourceAllocation"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"ServiceSpreadingPriority"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"EqualPriority"</span>, <span class="string">"weight"</span> : 1&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="多调度器"><a href="#多调度器" class="headerlink" title="多调度器"></a>多调度器</h4><p>如果默认的调度器不满足要求，还可以部署自定义的调度器。并且，在整个集群中还可以同时运行多个调度器实例，通过<code>podSpec.schedulerName</code> 来选择使用哪一个调度器（默认使用内置的调度器）。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  schedulerName:</span> my-scheduler  <span class="comment"># 选择使用自定义调度器 my-scheduler</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:<span class="number">1.10</span></span><br></pre></td></tr></table></figure></p>
<p>要开发我们自己的调度器也是比较容易的，比如我们这里的 my-scheduler:</p>
<ul>
<li>首先需要通过指定的 API 获取节点和 Pod</li>
<li>然后选择<code>phase=Pending</code>和<code>schedulerName=my-scheduler</code>的pod</li>
<li>计算每个 Pod 需要放置的位置之后，调度程序将创建一个<code>Binding</code>对象</li>
<li>然后根据我们自定义的调度器的算法计算出最适合的目标节点</li>
</ul>
<h4 id="优先级调度"><a href="#优先级调度" class="headerlink" title="优先级调度"></a>优先级调度</h4><p>与前面所讲的调度优选策略中的优先级（Priorities）不同，前面所讲的优先级指的是节点优先级，而我们这里所说的优先级 pod priority 指的是 Pod 的优先级，高优先级的 Pod 会优先被调度，或者在资源不足低情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。</p>
<p>要定义 Pod 优先级，就需要先定义<code>PriorityClass</code>对象，该对象没有 Namespace 的限制：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> PriorityClass</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> high-priority</span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"This priority class should be used for XYZ service pods only."</span></span><br></pre></td></tr></table></figure></p>
<p>其中：</p>
<ul>
<li><code>value</code>为 32 位整数的优先级，该值越大，优先级越高</li>
<li><code>globalDefault</code>用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个<code>PriorityClass</code>将其设置为 true</li>
</ul>
<p>然后通过在 Pod 的<code>spec.priorityClassName</code>中指定已定义的<code>PriorityClass</code>名称即可：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx</span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">  priorityClassName:</span> high-priority</span><br></pre></td></tr></table></figure></p>
<p>另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 pending 时，抢占（preemption）逻辑就会被触发。<code>Preemption</code>会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。</p>
<p>现在我们通过下面的图再去回顾下 kubernetes 的调度过程是不是就清晰很多了：<br><img src="/images/k8s/kube-scheduler-detail.png" alt="K8s scheduler detail"></p>
<p>来源: k8s技术圈</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.11.x + etcd集群 部署多Master集群]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-kubeadm-etcd-ha-1-11-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.11.5-0 kubelet-1.11.5-0 kubectl-1.11.5-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CADVISOR_ARGS=--cadvisor-port=0"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CADVISOR_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建etcd"><a href="#搭建etcd" class="headerlink" title="搭建etcd"></a>搭建etcd</h2><h3 id="在master-1上执行"><a href="#在master-1上执行" class="headerlink" title="在master-1上执行"></a>在master-1上执行</h3><h4 id="安装cfssl"><a href="#安装cfssl" class="headerlink" title="安装cfssl"></a>安装cfssl</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">wget -O /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">wget -O /bin/cfssl-certinfo  https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line"><span class="keyword">for</span> cfssl <span class="keyword">in</span> `ls /bin/cfssl*`;<span class="keyword">do</span> chmod +x <span class="variable">$cfssl</span>;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<h4 id="配置生成etcd-证书"><a href="#配置生成etcd-证书" class="headerlink" title="配置生成etcd 证书"></a>配置生成etcd 证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置证书</span></span><br><span class="line">mkdir -pv <span class="variable">$HOME</span>/ssl &amp;&amp; <span class="built_in">cd</span> <span class="variable">$HOME</span>/ssl</span><br><span class="line"></span><br><span class="line">cat &gt; ca-config.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"signing"</span>: &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">      <span class="string">"expiry"</span>: <span class="string">"87600h"</span> //10年</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"profiles"</span>: &#123;</span><br><span class="line">      <span class="string">"kubernetes"</span>: &#123;</span><br><span class="line">        <span class="string">"usages"</span>: [</span><br><span class="line">            <span class="string">"signing"</span>,</span><br><span class="line">            <span class="string">"key encipherment"</span>,</span><br><span class="line">            <span class="string">"server auth"</span>,</span><br><span class="line">            <span class="string">"client auth"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; etcd-ca-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"JiangSu"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"SuZhou"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"Etcd Security"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; etcd-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.19.170.183"</span>,</span><br><span class="line">      <span class="string">"172.19.170.184"</span>,</span><br><span class="line">      <span class="string">"172.19.170.185"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"JiangSu"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"SuZhou"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"Etcd Security"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成证书并复制证书至其他etcd节点</span></span><br><span class="line">cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca</span><br><span class="line">cfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动证书</span></span><br><span class="line">mkdir -pv /etc/etcd/ssl</span><br><span class="line">cp etcd*.pem /etc/etcd/ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将证书拷贝到其它节点</span></span><br><span class="line">scp -r /etc/etcd master-2:/etc/</span><br><span class="line">scp -r /etc/etcd master-3:/etc/</span><br></pre></td></tr></table></figure>
<h4 id="安装配置etcd"><a href="#安装配置etcd" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">yum install -y etcd </span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.183:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.183:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd1"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.183:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.183:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span> <span class="comment">#这里要设置为new</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line"><span class="comment">#启动会卡住，等待其它节点的加入</span></span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="在master-2-上执行"><a href="#在master-2-上执行" class="headerlink" title="在master-2 上执行"></a>在master-2 上执行</h3><h4 id="安装配置etcd-1"><a href="#安装配置etcd-1" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">yum install -y etcd </span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.184:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.184:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd2"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.184:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.184:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"existing"</span> <span class="comment">#如果etcd 启动报错，可先把这行注释掉，然后在启动</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="在master-3-上执行"><a href="#在master-3-上执行" class="headerlink" title="在master-3 上执行"></a>在master-3 上执行</h3><h4 id="安装配置etcd-2"><a href="#安装配置etcd-2" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.185:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd3"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.185:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"existing"</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="etcd-集群验证"><a href="#etcd-集群验证" class="headerlink" title="etcd 集群验证"></a>etcd 集群验证</h3><blockquote>
<p>随意在任何一台etcd (master) 节点上进行验证</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">etcdctl --endpoints <span class="string">"https://172.19.170.183:2379,https://172.19.170.184:2379,https://172.19.170.185:2379"</span>   --ca-file=/etc/etcd/ssl/etcd-ca.pem  \</span><br><span class="line">--cert-file=/etc/etcd/ssl/etcd.pem   --key-file=/etc/etcd/ssl/etcd-key.pem   cluster-health</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">token=$(kubeadm token generate)</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$token</span></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.2.4</span><br><span class="line">docker pull quay.io/calico/node:v3.2.4</span><br><span class="line">docker pull quay.io/calico/cni:v3.2.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.11.5</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[chrome 访问k8s dashboard 出现ssl证书错误]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-dashboard-chrome-err.html</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>最近上了k8s后，给开发提供dashboard访问，发现有不少开发无法使用chrome访问，出现问题的大部分都是使用的windows系统，主要有以下两类问题，这里记录下如何解决</p>
</blockquote>
<h3 id="ERR-SSL-SERVER-CERT-BAD-FORMAT"><a href="#ERR-SSL-SERVER-CERT-BAD-FORMAT" class="headerlink" title="ERR_SSL_SERVER_CERT_BAD_FORMAT"></a>ERR_SSL_SERVER_CERT_BAD_FORMAT</h3><blockquote>
<p>解决方法：重新安装chrome最新版本解决</p>
</blockquote>
<h3 id="NET-ERR-CERT-INVALID"><a href="#NET-ERR-CERT-INVALID" class="headerlink" title="NET::ERR_CERT_INVALID"></a>NET::ERR_CERT_INVALID</h3><p><img src="/images/k8s/chrome_err.png" alt="NET::ERR_CERT_INVALID"></p>
<blockquote>
<p>解决方法：创建chrome桌面快捷方式，然后到桌面：右键chrome–&gt;属性–&gt;在目标后面添加如下：<code>--disable-infobars --ignore-certificate-errors</code></p>
<p>示例：<code>&quot;C:\Program Files (x86)\Google\Chrome\Application\chrome.exe&quot; --disable-infobars --ignore-certificate-errors</code></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.12.x 部署多Master集群]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-kubeadm-ha-1-12-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.12.3-0 kubelet-1.12.3-0 kubectl-1.12.3-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.183:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.183:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380</span><br><span class="line">      initial-cluster-state: new</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源 </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.184:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.184:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源 </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-2 https://172.19.170.184:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-2执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.185:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.185:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380,master-3=https://172.19.170.185:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-3 https://172.19.170.185:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-3执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="调整配置，使用vip来进行调度"><a href="#调整配置，使用vip来进行调度" class="headerlink" title="调整配置，使用vip来进行调度"></a>调整配置，使用vip来进行调度</h3><blockquote>
<p>在所有master机器上都执行以下命令</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在master-1 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-2 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-3 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.3.2</span><br><span class="line">docker pull quay.io/calico/node:v3.3.2</span><br><span class="line">docker pull quay.io/calico/cni:v3.3.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.12.3</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.12.3</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.12.3</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.12.3</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.12.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.11.x 部署多Master集群]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-kubeadm-ha-1-11-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.11.5-0 kubelet-1.11.5-0 kubectl-1.11.5-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CADVISOR_ARGS=--cadvisor-port=0"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;300Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CADVISOR_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.183:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.183:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380</span><br><span class="line">      initial-cluster-state: new</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.184:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.184:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-2 https://172.19.170.184:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-2执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.185:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.185:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380,master-3=https://172.19.170.185:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-3 https://172.19.170.185:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-3执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="调整配置，使用vip来进行调度"><a href="#调整配置，使用vip来进行调度" class="headerlink" title="调整配置，使用vip来进行调度"></a>调整配置，使用vip来进行调度</h3><blockquote>
<p>在所有master机器上都执行以下命令</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在master-1 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-2 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-3 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.2.4</span><br><span class="line">docker pull quay.io/calico/node:v3.2.4</span><br><span class="line">docker pull quay.io/calico/cni:v3.2.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.11.5</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[认识Kubernetes Descheduler]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-descheduler.html</url>
      <content type="html"><![CDATA[<p><code>kube-scheduler</code> 是 Kubernetes 中负责调度的组件，它本身的调度功能已经很强大了。但由于 Kubernetes 集群非常活跃，它的状态会随时间而改变，由于各种原因，你可能需要将已经运行的 Pod 移动到其他节点：</p>
<ul>
<li>某些节点负载过高</li>
<li>某些资源对象被添加了 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature" target="_blank" rel="external">node 亲和性</a> 或 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature" target="_blank" rel="external">pod （反）亲和性</a></li>
<li>集群中加入了新节点</li>
</ul>
<p>一旦 Pod 启动之后 <code>kube-scheduler</code> 便不会再尝试重新调度它。根据环境的不同，你可能会有很多需要手动调整 Pod 的分布，例如：如果集群中新加入了一个节点，那么已经运行的 Pod 并不会被分摊到这台节点上，这台节点可能只运行了少量的几个 Pod，这并不理想，对吧？</p>
<h3 id="Descheduler-如何工作？"><a href="#Descheduler-如何工作？" class="headerlink" title="Descheduler 如何工作？"></a>Descheduler 如何工作？</h3><p><a href="https://github.com/kubernetes-incubator/descheduler" target="_blank" rel="external">Descheduler</a> 会检查 Pod 的状态，并根据自定义的策略将不满足要求的 Pod 从该节点上驱逐出去。Descheduler 并不是 <code>kube-scheduler</code> 的替代品，而是要依赖于它。该项目目前放在 Kubernetes 的孵化项目中，还没准备投入生产，但经过我实验发现它的运行效果很好，而且非常稳定。那么该如何安装呢？<br><a id="more"></a></p>
<h3 id="部署方法"><a href="#部署方法" class="headerlink" title="部署方法"></a>部署方法</h3><p>你可以通过 <code>Job</code> 或 <code>CronJob</code> 来运行 descheduler。我已经创建了一个镜像 <code>komljen/descheduler:v0.5.0-4-ga7ceb671</code>（包含在下面的 yaml 文件中），但由于这个项目的更新速度很快，你可以通过以下的命令创建你自己的镜像：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">⚡ git <span class="built_in">clone</span> https://github.com/kubernetes-incubator/descheduler</span><br><span class="line">⚡ <span class="built_in">cd</span> descheduler &amp;&amp; make image</span><br></pre></td></tr></table></figure></p>
<p>然后打好标签 push 到自己的镜像仓库中。</p>
<p>通过我创建的 chart 模板，你可以用 <code>Helm</code> 来部署 descheduler，该模板支持 RBAC 并且已经在 Kubernetes v1.9 上测试通过。</p>
<p>添加我的 helm 私有仓库，然后部署 descheduler：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">⚡ helm repo add akomljen-charts \</span><br><span class="line">    https://raw.githubusercontent.com/komljen/helm-charts/master/charts/</span><br><span class="line"></span><br><span class="line">⚡ helm install --name ds \</span><br><span class="line">    --namespace kube-system \</span><br><span class="line">    akomljen-charts/descheduler</span><br></pre></td></tr></table></figure></p>
<p>你也可以不使用 helm，通过手动部署。首先创建 serviceaccount 和 clusterrolebinding：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a cluster role</span></span><br><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"nodes"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>]</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>, <span class="string">"delete"</span>]</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods/eviction"</span>]</span><br><span class="line">  verbs: [<span class="string">"create"</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a service account</span></span><br><span class="line">⚡ kubectl create sa descheduler -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the cluster role to the service account</span></span><br><span class="line">⚡ kubectl create clusterrolebinding descheduler \</span><br><span class="line">    -n kube-system \</span><br><span class="line">    --clusterrole=descheduler \</span><br><span class="line">    --serviceaccount=kube-system:descheduler</span><br></pre></td></tr></table></figure></p>
<p>然后通过 <code>configmap</code> 创建 descheduler 策略。目前只支持四种策略：</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removeduplicates" target="_blank" rel="external">RemoveDuplicates</a><br>RS、deployment 中的 pod 不能同时出现在一台机器上</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#lownodeutilization" target="_blank" rel="external">LowNodeUtilization</a><br>找到资源使用率比较低的 node，然后驱逐其他资源使用率比较高节点上的 pod，期望调度器能够重新调度让资源更均衡</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removepodsviolatinginterpodantiaffinity" target="_blank" rel="external">RemovePodsViolatingInterPodAntiAffinity</a><br>找到已经违反 Pod Anti Affinity 规则的 pods 进行驱逐，可能是因为反亲和是后面加上去的</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removepodsviolatingnodeaffinity" target="_blank" rel="external">RemovePodsViolatingNodeAffinity</a><br>找到违反 Node Affinity 规则的 pods 进行驱逐，可能是因为 node 后面修改了 label</li>
</ul>
<p>默认这四种策略全部开启，你可以根据需要关闭它们。下面在 <code>kube-system</code> 命名空间中创建一个 configmap：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">data:</span><br><span class="line">  policy.yaml: |-  </span><br><span class="line">    apiVersion: descheduler/v1alpha1</span><br><span class="line">    kind: DeschedulerPolicy</span><br><span class="line">    strategies:</span><br><span class="line">      RemoveDuplicates:</span><br><span class="line">         enabled: <span class="literal">false</span></span><br><span class="line">      LowNodeUtilization:</span><br><span class="line">         enabled: <span class="literal">true</span></span><br><span class="line">         params:</span><br><span class="line">           nodeResourceUtilizationThresholds:</span><br><span class="line">             thresholds:</span><br><span class="line">               cpu: 20</span><br><span class="line">               memory: 20</span><br><span class="line">               pods: 20</span><br><span class="line">             targetThresholds:</span><br><span class="line">               cpu: 50</span><br><span class="line">               memory: 50</span><br><span class="line">               pods: 50</span><br><span class="line">      RemovePodsViolatingInterPodAntiAffinity:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">      RemovePodsViolatingNodeAffinity:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">        params:</span><br><span class="line">          nodeAffinityType:</span><br><span class="line">          - requiredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>在 <code>kube-system</code> 命名空间中创建一个 CronJob，该 CroJob 每 30 分钟运行一次：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">spec:</span><br><span class="line">  schedule: <span class="string">"*/30 * * * *"</span></span><br><span class="line">  jobTemplate:</span><br><span class="line">    metadata:</span><br><span class="line">      name: descheduler</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">"true"</span></span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          serviceAccountName: descheduler</span><br><span class="line">          containers:</span><br><span class="line">          - name: descheduler</span><br><span class="line">            image: komljen/descheduler:v0.6.0</span><br><span class="line">            volumeMounts:</span><br><span class="line">            - mountPath: /policy-dir</span><br><span class="line">              name: policy-volume</span><br><span class="line">            <span class="built_in">command</span>:</span><br><span class="line">            - /bin/descheduler</span><br><span class="line">            - --v=4</span><br><span class="line">            - --max-pods-to-evict-per-node=10</span><br><span class="line">            - --policy-config-file=/policy-dir/policy.yaml</span><br><span class="line">          restartPolicy: <span class="string">"OnFailure"</span></span><br><span class="line">          volumes:</span><br><span class="line">          - name: policy-volume</span><br><span class="line">            configMap:</span><br><span class="line">              name: descheduler</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">⚡ kubectl get cronjobs -n kube-system</span><br><span class="line">NAME             SCHEDULE       SUSPEND   ACTIVE    LAST SCHEDULE   AGE</span><br><span class="line">descheduler      */30 * * * *   False     0         2m              32m</span><br></pre></td></tr></table></figure></p>
<p>当 CronJob 开始工作后，可以通过以下命令查看已经成功结束的 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">⚡ kubectl get pods -n kube-system <span class="_">-a</span> | grep Completed</span><br><span class="line">descheduler-1525520700-297pq          0/1       Completed   0          1h</span><br><span class="line">descheduler-1525521000-tz2ch          0/1       Completed   0          32m</span><br><span class="line">descheduler-1525521300-mrw4t          0/1       Completed   0          2m</span><br></pre></td></tr></table></figure></p>
<p>也可以查看这些 Pod 的日志，然后根据需要调整 descheduler 策略：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">⚡ kubectl logs descheduler-1525521300-mrw4t -n kube-system</span><br><span class="line">I0505 11:55:07.554195       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84</span><br><span class="line">I0505 11:55:07.554255       1 reflector.go:240] Listing and watching *v1.Node from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84</span><br><span class="line">I0505 11:55:07.767903       1 lownodeutilization.go:147] Node <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span> is appropriately utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:41.5, <span class="string">"memory"</span>:1.3635487207675927, <span class="string">"pods"</span>:8.181818181818182&#125;</span><br><span class="line">I0505 11:55:07.767942       1 lownodeutilization.go:149] allPods:9, nonRemovablePods:9, bePods:0, bPods:0, gPods:0</span><br><span class="line">I0505 11:55:07.768141       1 lownodeutilization.go:144] Node <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span> is over utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:48.75, <span class="string">"memory"</span>:61.05259502942694, <span class="string">"pods"</span>:30&#125;</span><br><span class="line">I0505 11:55:07.768156       1 lownodeutilization.go:149] allPods:33, nonRemovablePods:12, bePods:1, bPods:19, gPods:1</span><br><span class="line">I0505 11:55:07.768376       1 lownodeutilization.go:144] Node <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span> is over utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:39.125, <span class="string">"memory"</span>:98.19259268881142, <span class="string">"pods"</span>:33.63636363636363&#125;</span><br><span class="line">I0505 11:55:07.768390       1 lownodeutilization.go:149] allPods:37, nonRemovablePods:8, bePods:0, bPods:29, gPods:0</span><br><span class="line">I0505 11:55:07.768538       1 lownodeutilization.go:147] Node <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span> is appropriately utilized with usage: api.ResourceThresholds&#123;<span class="string">"memory"</span>:43.19826999287199, <span class="string">"pods"</span>:30.90909090909091, <span class="string">"cpu"</span>:35.25&#125;</span><br><span class="line">I0505 11:55:07.768552       1 lownodeutilization.go:149] allPods:34, nonRemovablePods:11, bePods:8, bPods:15, gPods:0</span><br><span class="line">I0505 11:55:07.768556       1 lownodeutilization.go:65] Criteria <span class="keyword">for</span> a node under utilization: CPU: 20, Mem: 20, Pods: 20</span><br><span class="line">I0505 11:55:07.768571       1 lownodeutilization.go:69] No node is underutilized, nothing to <span class="keyword">do</span> here, you might tune your thersholds further</span><br><span class="line">I0505 11:55:07.768576       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.779313       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.796766       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.813303       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.829109       1 node_affinity.go:40] Executing <span class="keyword">for</span> nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">I0505 11:55:07.829133       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.840416       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.856735       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.945566       1 request.go:480] Throttling request took 88.738917ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-41-14.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded</span><br><span class="line">I0505 11:55:07.972702       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:08.145559       1 request.go:480] Throttling request took 172.751657ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-34-29.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded</span><br><span class="line">I0505 11:55:08.160964       1 node_affinity.go:72] Evicted 0 pods</span><br></pre></td></tr></table></figure></p>
<p>哇哦，现在你的集群中已经运行了一个 descheduler！</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Kubernetes 的默认调度器已经做的很好，但由于集群处于不断变化的状态中，某些 Pod 可能运行在错误的节点上，或者你想要均衡集群资源的分配，这时候就需要 descheduler 来帮助你将某些节点上的 Pod 驱逐到正确的节点上去。我很期待正式版的发布！</p>
<p>参考文档:</p>
<ul>
<li>Meet a Kubernetes Descheduler</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubernetes正确的处理用户请求]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-handling-client-requests-properly-with-kubernetes.html</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>毫无疑问，我们希望正确处理客户端请求。当pod正在启动或关闭时，我们显然不希望看到断开的连接。Kubernetes本身并不能确保这种情况永远不会发生。您的应用需要遵循一些规则以防止断开连接。本文讨论这些规则。</p>
</blockquote>
<h3 id="确保正确处理所有客户端请求"><a href="#确保正确处理所有客户端请求" class="headerlink" title="确保正确处理所有客户端请求"></a>确保正确处理所有客户端请求</h3><p>我们先从访问Pod的客户端的视角来看看Pod的生命周期（客户端使用pod提供的服务）。我们希望确保客户的请求得到妥善处理，因为如果当pod启动或关闭时连接开始中断，我们就会遇到麻烦。Kubernetes本身并不保证不会发生这种情况，所以让我们看看我们需要做些什么来防止它发生。</p>
<h3 id="当Pod启动时阻止连接中断"><a href="#当Pod启动时阻止连接中断" class="headerlink" title="当Pod启动时阻止连接中断"></a>当Pod启动时阻止连接中断</h3><p>如果你理解services和service endpoints的工作原理，确保在pod启动时正确处理每个连接都非常简单。当pod启动时，它会作为一个endpoints被添加到所有匹配该Pod标签的Services里。Pod也会发出信号告诉Kubernetes它已就绪。只有它变成一个service endpoints时才可以接收来自客户端的请求。</p>
<p>如果你没有在Pod Spec里指定readiness探针，则会始终认为该pod已准备就绪。这意味着它将立即开始接收请求 - 只要第一个Kube-Proxy在其节点上更新iptables规则并且第一个客户端pod尝试连接到该服务。如果那个时候你的应用并没有做好接收请求的准备，那么客户端将会见到“connection refused”类型的错误。</p>
<p>你所需要做的就是保证你的readiness探针当且仅当你的应用可以正确处理收到的请求时才返回成功结果。所以添加一个HTTP GET readiness探针并让它指向你应用的基础URL会是一个很好的开始。在很多情况下，这可以让你省去实现一个特定的readiness端点的工作量。<br><a id="more"></a></p>
<h3 id="在pod关闭期间防止断开连接"><a href="#在pod关闭期间防止断开连接" class="headerlink" title="在pod关闭期间防止断开连接"></a>在pod关闭期间防止断开连接</h3><p>现在让我们看看当一个Pod生命周期结束时发生了什么——当Pod被删除和它里面的容器被停止时。一旦Pod的容器接收到SIGTERM后它就会开始关闭（或者是在那之前先执行prestop钩子），但这是否能保证所有的客户端请求可以被正确地处理？</p>
<p>我们的应用在收到结束信号时应该如何响应？它是否应该继续接收请求？那么那些已经收到的请求但是还未完成的请求呢？那么那些正在发送请求的间隔中且仍然处理打开状态的持久HTTP连接（连接上没有活跃的请求）呢？在回答这些问题之前，我们需要深入了解一下当Pod结束时集群里发生的一系列事件。</p>
<h3 id="了解Pod删除时发生的一系列事件"><a href="#了解Pod删除时发生的一系列事件" class="headerlink" title="了解Pod删除时发生的一系列事件"></a>了解Pod删除时发生的一系列事件</h3><p>您需要始终牢记Kubernetes的各个组件是独立运行在集群的节点上的。它们之间并不是一个巨大的单体应用。这些组件间同步状态会花费一点时间。让我们一起来看看当Pod删除时发生了什么。</p>
<p>当APIserver收到一个停止Pod的请求时，它首先修改了etcd里Pod的状态，并通知关注这个删除事件所有的watcher。这些watcher里包括Kubelet和Endpoint Controller。这两个序列的事件是并行发生的（标记为A和B），如图1所示。<br><img src="/images/k8s/k8s_pod_03.png" alt="pod停止时的事件"></p>
<p>在A系列事件里，你会看到在Kubelet收到该Pod要停止的通知以后会尽快开始停止Pod的一系列操作（执行prestop钩子，发送SIGTERM信号，等待一段时间然后如果这个容器没有自动退出的话就强行杀掉这个容器）。如果应用响应了SIGTERM并迅速停止接收请求，那么任何尝试连接它的客户端都会收到一个Connection Refusd的错误。因为APIserver是直接向Kubelet发送的请求，那么从Pod被删除开始计算，Pod用于执行这段逻辑的时间相当短。</p>
<p>现在，让我们一起看看另外一系列事件都发生了什么——移除这个Pod相关的iptables规则（图中所示事件系列B）。当Endpoints Controller（运行在在Kubernetes控制平面里的Controller Manager里）收到这个Pod被删除的通知，然后它会把这个Pod从所有关联到这个Pod的Service里剔除。它通过向APIserver发送一个REST请求对Endpoint对象进行修改来实现。APIserver然后通知每个监视Endpoint对象的组件。在这些组件里包含了每个工作节点上运行的Kubeproxy。这些代理负责更新它所在节点上的iptables规则，这些规则可以用来阻止外面的请求被转发到要停止的Pod上。这里有个非常重要的细节，移除这些iptables规则对于已有的连接不会有任何影响——连接到这个Pod的客户端仍然可以通过已有连接向它发送请求。</p>
<p>这些请求都是并行发生的。更确切地，关停应用所需要的时间要比iptables更新花费所需的时间稍短一些。这是因为iptables修改的事件链看起来稍微长一些（见图2），因为这些事件需要先到达Endpoints Controller，然后它向APIServer发送一个新请求，接着在Proxy最终修改iptables规则之前，APIserver必须通知到每个KubeProxy。这意味着SIGTERM可能会在所有节点iptables规则更新前发送。<br><img src="/images/k8s/k8s_pod_04.png" alt="删除pod时的事件时间线"></p>
<p>结果是pod在收到终止信号后仍然可以接收客户端请求。如果应用程序立即停止接受连接，则会导致客户端收到“Connection Refused”类型的错误（就像在没有定义Readiness探针时，Pod启动但无法对外提供服务时一样） 。</p>
<h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>谷歌搜索此问题的解决方案，似乎给Pod添加一个Readiness探针就可以解决这个问题。按照推测，你所需要做的就是让你的Readiness探针在应用收到SIGTERM信号后尽快失败。这可以让Pod从Service上移除。只有在Readiness探针连续失败几次后才会从Service上移除（这可以在Readiness探针里进行配置）。并且显然这个移除事件会在iptables规则更新前先到达Kubeproxy。</p>
<p>实际上，Readiness探针在整个过程中并未起到关键作用。一旦Endpoints Controller收到Pod删除事件后（当Pod配置里的deletionTimestamp域不再为空时），它会尽快从Service上移除这些Pod。从那时起，这已经就与Readiness探测结果不相关了。</p>
<p>那么什么是问题的正确解决方案？我们如何保证所有的请求都可以被正确处理？</p>
<p>好吧，很明显，即使收到终止信号后，pod也需要继续接受连接，直到所有Kube代理完成更新iptables规则。那么，不止是Kubeproxy。可能有一些IngressController或者其他直接向Pod转发请求的负载均衡设备等不需要经过Service的。这还包括使用客户端负载平衡的客户端。<br>为了确保没有任何客户端遇到断开的连接，您必须等到所有客户端以某种方式通知您他们将不再转发到该Pod的连接。</p>
<p>但这是不可能的，因为这些组件都分布在不同的计算机上。即使你知道它们每个所在的位置或者等着它们每个都满足停止Pod的需求，如果它们其中之一没有响应你会该怎么办？你要等待多久？记着，在那段时间里，你需要暂停关闭进程。</p>
<p>你唯一可能的做事情是你可以等待足够长的时间直到所有的代理都完成了它们的工作。但是多长时间才算够？在大多数情况中，短暂的几秒就足够了，但是显然这并不能满足全部的情况。当APIserver或者Endpoints Controller过载时，它们可能需要更长时间来通知到每个代理。重要的是要了解您无法完美地解决问题，但是即使是5秒或者10秒都可以显著提升用户体验。你可以设置更长的延迟，但是别太过分，因为这些延迟推迟了容器的停止时间，并且会导致容器在被关停后仍然被显示在列表里，这会对用户带来一定的困扰。</p>
<p>正确关闭应用程序包括以下步骤：</p>
<ul>
<li>等待几秒，然后停止接收新连接</li>
<li>关闭所有未在请求中的keep-alived连接</li>
<li>等到所有活跃的请求关闭，然后</li>
<li>彻底关闭</li>
</ul>
<p>要了解在此过程中连接和请求发生的情况，请仔细检查图3。<br><img src="/images/k8s/k8s_pod_05.png" alt="在收到终止信号后正确处理现有和新连接"></p>
<p>没有像收到终止信号后立即退出过程一样简单，对吧？这一切是值得的吗？这要靠你自己来决定。但是至少你可以添加一个prestop钩子并等待几秒，就像下面所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span>  </span><br><span class="line"><span class="attr">    preStop:</span>    </span><br><span class="line"><span class="attr">        exec:</span>      </span><br><span class="line"><span class="attr">            command:</span></span><br><span class="line"><span class="bullet">             -</span> sh</span><br><span class="line"><span class="bullet">             -</span> -c</span><br><span class="line"><span class="bullet">             -</span> <span class="string">"sleep 5"</span></span><br></pre></td></tr></table></figure></p>
<p>这样，您根本不需要修改应用程序的代码。如果您的应用要确保完全处理所有正在进行的请求，这个preStop钩子就可以满足所有你的需要。</p>
<p>参考文档:</p>
<ul>
<li>handling-client-requests-properly-with-kubernetes</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm 证书说明]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-kubeadm-ca-desc.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>如果你使用过kubeadm部署过Kubernetes的环境, master主机节点上就一定会在相应的目录创建了一大批证书文件, 本篇文章就来说说kubeadm到底为我们生成了哪些证书</p>
</blockquote>
<p>在Kubernetes的部署中, 创建证书, 配置证书是一道绕不过去坎儿, 好在有kubeadm这样的自动化工具, 帮我们去生成, 配置这些证书. 对于只是想体验Kubernetes或只是想测试的亲来说, 这已经够了, 但是作为Kubernetes的集群维护者来说, kubeadm更像是一个黑盒, 本篇文章就来说说黑盒中关于证书的事儿~</p>
<p>使用kubeadm创建完Kubernetes集群后, 默认会在<code>/etc/kubernetes/pki</code>目录下存放集群中需要用到的证书文件, 整体结构如下图所示:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:/etc/kubernetes/pki<span class="comment"># tree</span></span><br><span class="line">.</span><br><span class="line">|-- apiserver.crt</span><br><span class="line">|-- apiserver-etcd-client.crt</span><br><span class="line">|-- apiserver-etcd-client.key</span><br><span class="line">|-- apiserver.key</span><br><span class="line">|-- apiserver-kubelet-client.crt</span><br><span class="line">|-- apiserver-kubelet-client.key</span><br><span class="line">|-- ca.crt</span><br><span class="line">|-- ca.key</span><br><span class="line">|-- etcd</span><br><span class="line">|   |-- ca.crt</span><br><span class="line">|   |-- ca.key</span><br><span class="line">|   |-- healthcheck-client.crt</span><br><span class="line">|   |-- healthcheck-client.key</span><br><span class="line">|   |-- peer.crt</span><br><span class="line">|   |-- peer.key</span><br><span class="line">|   |-- server.crt</span><br><span class="line">|   `-- server.key</span><br><span class="line">|-- front-proxy-ca.crt</span><br><span class="line">|-- front-proxy-ca.key</span><br><span class="line">|-- front-proxy-client.crt</span><br><span class="line">|-- front-proxy-client.key</span><br><span class="line">|-- sa.key</span><br><span class="line">`-- sa.pub</span><br><span class="line"></span><br><span class="line">1 directory, 22 files</span><br></pre></td></tr></table></figure></p>
<p>以上22个文件就是kubeadm为我们创建的所有证书相关的文件, 下面我们来一一解析</p>
<h3 id="证书分组"><a href="#证书分组" class="headerlink" title="证书分组"></a>证书分组</h3><p>Kubernetes把证书放在了两个文件夹中</p>
<ul>
<li>/etc/kubernetes/pki</li>
<li>/etc/kubernetes/pki/etcd</li>
</ul>
<p>我们再将这22个文件按照更细的粒度去分组<br><a id="more"></a></p>
<h3 id="Kubernetes-集群根证书"><a href="#Kubernetes-集群根证书" class="headerlink" title="Kubernetes 集群根证书"></a>Kubernetes 集群根证书</h3><p>Kubernetes 集群根证书CA(Kubernetes集群组件的证书签发机构)</p>
<ul>
<li>/etc/kubernetes/pki/ca.crt</li>
<li>/etc/kubernetes/pki/ca.key</li>
</ul>
<p>以上这组证书为签发其他Kubernetes组件证书使用的根证书, 可以认为是Kubernetes集群中证书签发机构之一</p>
<p>由此根证书签发的证书有:</p>
<ol>
<li>kube-apiserver 组件持有的服务端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver.crt</li>
<li>/etc/kubernetes/pki/apiserver.key</li>
</ul>
<ol>
<li>kubelet 组件持有的客户端证书, 用作 kube-apiserver 主动向 kubelet 发起请求时的客户端认证</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver-kubelet-client.crt</li>
<li>/etc/kubernetes/pki/apiserver-kubelet-client.key</li>
</ul>
<blockquote>
<p>注意: Kubernetes集群组件之间的交互是双向的, kubelet 既需要主动访问 kube-apiserver, kube-apiserver 也需要主动向 kubelet 发起请求, 所以双方都需要有自己的根证书以及使用该根证书签发的服务端证书和客户端证书. 在 kube-apiserver 中, 一般明确指定用于 https 访问的服务端证书和带有<code>CN 用户名</code>信息的客户端证书. 而在 kubelet 的启动配置中, 一般只指定了 ca 根证书, 而没有明确指定用于 https 访问的服务端证书, 这是因为, 在生成服务端证书时, 一般会指定服务端地址或主机名, kube-apiserver 相对变化不是很频繁, 所以在创建集群之初就可以预先分配好用作 kube-apiserver 的 IP 或主机名/域名, 但是由于部署在 node 节点上的 kubelet 会因为集群规模的变化而频繁变化, 而无法预知 node 的所有 IP 信息, 所以 kubelet 上一般不会明确指定服务端证书, 而是只指定 ca 根证书, 让 kubelet 根据本地主机信息自动生成服务端证书并保存到配置的<code>cert-dir</code>文件夹中.</p>
</blockquote>
<p>好了, 至此, Kubernetes集群根证书所签发的证书都在上面了, 算上根证书一共涉及到6个文件, 22-6=16, 我们还剩下16个文件</p>
<h3 id="汇聚层证书"><a href="#汇聚层证书" class="headerlink" title="汇聚层证书"></a>汇聚层证书</h3><p>kube-apiserver 的另一种访问方式就是使用 <code>kubectl proxy</code> 来代理访问, 而该证书就是用来支持SSL代理访问的. 在该种访问模式下, 我们是以http的方式发起请求到代理服务的, 此时, 代理服务会将该请求发送给 kube-apiserver, 在此之前, 代理会将发送给 kube-apiserver 的请求头里加入证书信息, 以下两个配置</p>
<p>API Aggregation允许在不修改Kubernetes核心代码的同时扩展Kubernetes API. 开启 API Aggregation 需要在 kube-apiserver 中添加如下配置:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-client-ca-file=&lt;path to aggregator CA cert&gt;</span><br><span class="line">--requestheader-allowed-names=front-proxy-client</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class="line">--requestheader-group-headers=X-Remote-Group</span><br><span class="line">--requestheader-username-headers=X-Remote-User</span><br><span class="line">--proxy-client-cert-file=&lt;path to aggregator proxy cert&gt;</span><br><span class="line">--proxy-client-key-file=&lt;path to aggregator proxy key&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>官方警告: 除非你了解保护 CA 使用的风险和机制, 否则不要在不通上下文中重用已经使用过的 CA</strong></p>
<p>如果 kube-proxy 没有和 API server 运行在同一台主机上，那么需要确保启用了如下 apiserver 标记：<code>--enable-aggregator-routing=true</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">客户端 ---发起请求---&gt; 代理 ---Add Header:发起请求---&gt; kube-apiserver</span><br><span class="line">                   (客户端证书)                        (服务端证书)</span><br></pre></td></tr></table></figure>
<p>kube-apiserver 代理根证书(客户端证书)</p>
<p>用在<code>requestheader-client-ca-file</code>配置选项中, kube-apiserver 使用该证书来验证客户端证书是否为自己所签发</p>
<ul>
<li>/etc/kubernetes/pki/front-proxy-ca.crt</li>
<li>/etc/kubernetes/pki/front-proxy-ca.key</li>
</ul>
<p>由此根证书签发的证书只有一组:</p>
<p>代理层(如汇聚层aggregator)使用此套代理证书来向 kube-apiserver 请求认证</p>
<ol>
<li>代理端使用的客户端证书, 用作代用户与 kube-apiserver 认证</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/front-proxy-client.crt</li>
<li>/etc/kubernetes/pki/front-proxy-client.key</li>
</ul>
<p>参考文档:</p>
<ul>
<li>kube-apiserver 配置参数: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver" target="_blank" rel="external">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver</a></li>
<li>使用汇聚层扩展 Kubernetes API: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/" target="_blank" rel="external">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation</a></li>
<li>配置汇聚层: <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer</a></li>
</ul>
<p>至此, 刨除代理专用的证书外, 还剩下 16-4=12 个文件</p>
<h3 id="etcd-集群根证书"><a href="#etcd-集群根证书" class="headerlink" title="etcd 集群根证书"></a>etcd 集群根证书</h3><p>etcd集群所用到的证书都保存在<code>/etc/kubernetes/pki/etcd</code>这路径下, 很明显, 这一套证书是用来专门给etcd集群服务使用的, 设计以下证书文件</p>
<p>etcd 集群根证书CA(etcd 所用到的所有证书的签发机构)</p>
<ul>
<li>/etc/kubernetes/pki/etcd/ca.crt</li>
<li>/etc/kubernetes/pki/etcd/ca.key</li>
</ul>
<p>由此根证书签发机构签发的证书有:</p>
<ol>
<li>etcd server 持有的服务端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/server.crt</li>
<li>/etc/kubernetes/pki/etcd/server.key</li>
</ul>
<ol>
<li>peer 集群中节点互相通信使用的客户端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/peer.crt</li>
<li>/etc/kubernetes/pki/etcd/peer.key</li>
</ul>
<p>注: Peer：对同一个etcd集群中另外一个Member的称呼</p>
<ol>
<li>pod 中定义 Liveness 探针使用的客户端证书<br> kubeadm 部署的 Kubernetes 集群是以 pod 的方式运行 etcd 服务的, 在该 pod 的定义中, 配置了 Liveness 探活探针</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/healthcheck-client.crt</li>
<li>/etc/kubernetes/pki/etcd/healthcheck-client.key<br>当你 describe etcd 的 pod 时, 会看到如下一行配置:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Liveness:       <span class="built_in">exec</span> [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] delay=15s timeout=15s period=10s <span class="comment">#success=1 #failure=8</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li>配置在 kube-apiserver 中用来与 etcd server 做双向认证的客户端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver-etcd-client.crt</li>
<li>/etc/kubernetes/pki/apiserver-etcd-client.key</li>
</ul>
<p>至此, 介绍了涉及到 etcd 服务的10个证书文件, 12-10=2, 仅剩两个没有介绍到的文件啦, 胜利在望, 坚持一下~</p>
<h3 id="Serveice-Account秘钥"><a href="#Serveice-Account秘钥" class="headerlink" title="Serveice Account秘钥"></a>Serveice Account秘钥</h3><p>最后介绍的这组”证书”其实不是证书, 而是一组秘钥. 看着后缀名是不是有点眼熟呢, 没错, 这组秘钥对儿其实跟我们在Linux上创建, 用于免密登录的密钥对儿原理是一样的~</p>
<blockquote>
<p>这组的密钥对儿仅提供给 kube-controller-manager 使用. kube-controller-manager 通过 sa.key 对 token 进行签名, master 节点通过公钥 sa.pub 进行签名的验证</p>
</blockquote>
<ul>
<li>/etc/kubernetes/pki/sa.key</li>
<li>/etc/kubernetes/pki/sa.pub</li>
</ul>
<p>至此, kubeadm 工具帮我们创建的所有证书文件都已经介绍完了, 整个 Kubernetes&amp;etcd 集群中所涉及到的绝大部分证书都差不多在这里了. 有的行家可能会看出来, 至少还少了一组证书呀, 就是 kube-proxy 持有的证书怎么没有自动生成呀. 因为 kubeadm 创建的集群, kube-proxy 是以 pod 形式运行的, 在 pod 中, 直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书了. 如果你的 kube-proxy 是以守护进程的方式直接运行在宿主机的, 那么你就需要为它创建一套证书了. 创建的方式也很简单, 直接使用上面第一条提到的 <code>Kubernetes 集群根证书</code> 进行签发就可以了(注意CN和O的设置)</p>
<p>参考文档:</p>
<ul>
<li><a href="https://kubernetes.io/docs/setup/certificates/" target="_blank" rel="external">https://kubernetes.io/docs/setup/certificates/</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/</a></li>
<li>docs.lvrui.io</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm证书过期时间调整]]></title>
      <url>http://icyxp.github.io/blog/2018/12/k8s-kubeadm-ca-upgdate.html</url>
      <content type="html"><![CDATA[<blockquote>
<p> kubeadm 默认证书为一年，一年过期后，会导致api service不可用，使用过程中会出现：x509: certificate has expired or is not yet valid</p>
</blockquote>
<p><strong><code>如何进行调整，下面给了两个方案，供大家选择</code></strong></p>
<h3 id="方案一-通过修改kubeadm-调整证书过期时间"><a href="#方案一-通过修改kubeadm-调整证书过期时间" class="headerlink" title="方案一 通过修改kubeadm 调整证书过期时间"></a>方案一 通过修改kubeadm 调整证书过期时间</h3><a id="more"></a>
<h4 id="修改代码，调整过期时间"><a href="#修改代码，调整过期时间" class="headerlink" title="修改代码，调整过期时间"></a>修改代码，调整过期时间</h4><p> 克隆代码：<code>git clone https://github.com/kubernetes/kubernetes.git</code>, 切换到指定的tag或者版本修改<code>vendor/k8s.io/client-go/util/cert/cert.go</code>文件，<code>git diff</code> 对比如下：<br><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/staging/src/k8s.io/client-go/util/cert/cert.go b/staging/src/k8s.io/client-go/util/cert/cert.go</span><br><span class="line">index fb7f5fa..e800962 100644</span><br><span class="line"><span class="comment">--- a/staging/src/k8s.io/client-go/util/cert/cert.go</span></span><br><span class="line"><span class="comment">+++ b/staging/src/k8s.io/client-go/util/cert/cert.go</span></span><br><span class="line">@@ -104,7 +104,7 @@ func NewSignedCert(cfg Config, key *rsa.PrivateKey, caCert *x509.Certificate, ca</span><br><span class="line">                IPAddresses:  cfg.AltNames.IPs,</span><br><span class="line">                SerialNumber: serial,</span><br><span class="line">                NotBefore:    caCert.NotBefore,</span><br><span class="line"><span class="deletion">-               NotAfter:     time.Now().Add(duration365d).UTC(),</span></span><br><span class="line"><span class="addition">+               NotAfter:     time.Now().Add(duration365d * 10).UTC(),</span></span><br><span class="line">                KeyUsage:     x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature,</span><br><span class="line">                ExtKeyUsage:  cfg.Usages,</span><br><span class="line">        &#125;</span><br><span class="line">@@ -149,7 +149,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS</span><br><span class="line">                        CommonName: fmt.Sprintf("%s-ca@%d", host, time.Now().Unix()),</span><br><span class="line">                &#125;,</span><br><span class="line">                NotBefore: time.Now(),</span><br><span class="line"><span class="deletion">-               NotAfter:  time.Now().Add(time.Hour * 24 * 365),</span></span><br><span class="line"><span class="addition">+               NotAfter:  time.Now().Add(time.Hour * 24 * 3650),</span></span><br><span class="line"></span><br><span class="line">                KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,</span><br><span class="line">                BasicConstraintsValid: true,</span><br></pre></td></tr></table></figure></p>
<h4 id="编译代码"><a href="#编译代码" class="headerlink" title="编译代码"></a>编译代码</h4><p>编译环境我已经做了对应的1.11.5、1.12.3、1.13.0，已上传到docker hub 上，大家可下载使用，地址如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull icyboy/k8s_build:v1.11.5  <span class="comment"># 基于 golang:1.10.3</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.12.3  <span class="comment"># 基于 golang:1.10.4 </span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.0  <span class="comment"># 基于 golang:1.11.2</span></span><br></pre></td></tr></table></figure></p>
<p>编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -v 你修改后的代码目录:/go/src/k8s.io/kubernetes -it icyboy/k8s_build:v1.11.5 bash</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /go/src/k8s.io/kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubeadm, 这里主要编译kubeadm 即可</span></span><br><span class="line">make all WHAT=cmd/kubeadm GOFLAGS=-v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubelet</span></span><br><span class="line"><span class="comment"># make all WHAT=cmd/kubelet GOFLAGS=-v</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubectl</span></span><br><span class="line"><span class="comment"># make all WHAT=cmd/kubectl GOFLAGS=-v</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译完产物在 _output/bin/kubeadm 目录下</span></span><br><span class="line"><span class="comment">#将kubeadm 文件拷贝出来，替换系统中的kubeadm</span></span><br></pre></td></tr></table></figure></p>
<p>对应的kubeadm 文件我也编译好后放到百度云中，大家可放心下载使用，可通过<code>kubeadm version</code> 查看对应的版本信息和官方的进行比对<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译过后的</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"11+"</span>, GitVersion:<span class="string">"v1.11.5-dirty"</span>, GitCommit:<span class="string">"753b2dbc622f5cc417845f0ff8a77f539a4213ea"</span>, GitTreeState:<span class="string">"dirty"</span>, BuildDate:<span class="string">"2018-12-07T05:58:18Z"</span>, GoVersion:<span class="string">"go1.10.3"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#官方的</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"11"</span>, GitVersion:<span class="string">"v1.11.5"</span>, GitCommit:<span class="string">"753b2dbc622f5cc417845f0ff8a77f539a4213ea"</span>, GitTreeState:<span class="string">"clean"</span>, BuildDate:<span class="string">"2018-11-26T14:38:30Z"</span>, GoVersion:<span class="string">"go1.10.3"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>kubeadm 下载地址：<a href="https://pan.baidu.com/s/1PplHyDkYDTusx46j9uHwDA" target="_blank" rel="external">https://pan.baidu.com/s/1PplHyDkYDTusx46j9uHwDA</a><br>提取码：dy6f</p>
<h4 id="替换证书"><a href="#替换证书" class="headerlink" title="替换证书"></a>替换证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用新的kubeadm 替换官方的kubeadm</span></span><br><span class="line">chmod +x kubeadm &amp;&amp; \cp <span class="_">-f</span> kubeadm /usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#备份原有的证书</span></span><br><span class="line">mv /etc/kubernetes/pki /etc/kubernetes/pki.old</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成新的证书，kubeadm.yaml 指定你自己服务器上的</span></span><br><span class="line">kubeadm alpha phase certs all --config  ~/kubeadm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#备份原有的conf文件</span></span><br><span class="line">mv /etc/kubernetes/*conf /etc/kubernetes/*conf-old</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据新证书重新生成新的配置文件</span></span><br><span class="line">kubeadm alpha phase kubeconfig all --config ~/kubeadm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#替换老的config文件</span></span><br><span class="line">\cp <span class="_">-f</span> /etc/kubernetes/admin.conf ~/.kube/config</span><br></pre></td></tr></table></figure>
<p><strong>验证</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/pki</span><br><span class="line">openssl x509 -in apiserver-etcd-client.crt -text -noout</span><br><span class="line"><span class="comment">#Certificate:</span></span><br><span class="line"><span class="comment">#    Data:</span></span><br><span class="line"><span class="comment">#        Version: 3 (0x2)</span></span><br><span class="line"><span class="comment">#        Serial Number: 2755977466456048186 (0x263f32e76918023a)</span></span><br><span class="line"><span class="comment">#    Signature Algorithm: sha256WithRSAEncryption</span></span><br><span class="line"><span class="comment">#        Issuer: CN=kubernetes</span></span><br><span class="line"><span class="comment">#        Validity</span></span><br><span class="line"><span class="comment">#            Not Before: Dec  7 09:33:32 2018 GMT</span></span><br><span class="line">             Not After : Dec  4 09:33:32 2028 GMT  <span class="comment">#这里变成10年了</span></span><br><span class="line"><span class="comment">#        Subject: O=system:masters, CN=kube-apiserver-etcd-client</span></span><br><span class="line"><span class="comment">#        Subject Public Key Info:</span></span><br><span class="line"><span class="comment">#        ....</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其余证书安装上述方法同样进行验证</span></span><br></pre></td></tr></table></figure></p>
<h3 id="方案二-启用自动轮换kubelet-证书"><a href="#方案二-启用自动轮换kubelet-证书" class="headerlink" title="方案二 启用自动轮换kubelet 证书"></a>方案二 启用自动轮换kubelet 证书</h3><blockquote>
<p>kubelet证书分为server和client两种， <code>k8s 1.9</code>默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启</p>
</blockquote>
<h4 id="增加-kubelet-参数"><a href="#增加-kubelet-参数" class="headerlink" title="增加 kubelet 参数"></a>增加 kubelet 参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true"</span></span><br></pre></td></tr></table></figure>
<h4 id="增加-controller-manager-参数"><a href="#增加-controller-manager-参数" class="headerlink" title="增加 controller-manager 参数"></a>增加 controller-manager 参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数</span></span><br><span class="line">  - <span class="built_in">command</span>:</span><br><span class="line">    - kube-controller-manager</span><br><span class="line">    - --experimental-cluster-signing-duration=87600h0m0s</span><br><span class="line">    - --feature-gates=RotateKubeletServerCertificate=<span class="literal">true</span></span><br><span class="line">    - ....</span><br></pre></td></tr></table></figure>
<h4 id="创建-rbac-对象"><a href="#创建-rbac-对象" class="headerlink" title="创建 rbac 对象"></a>创建 rbac 对象</h4><p>创建rbac对象，允许节点轮换kubelet server证书：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; ca-update.yaml &lt;&lt; EOF</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line"><span class="attr">  name:</span> system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> certificates.k8s.io</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> certificatesigningrequests/selfnodeserver</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> create</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm:node-autoapprove-certificate-server</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> Group</span><br><span class="line"><span class="attr">  name:</span> system:nodes</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create –f ca-update.yaml</span><br></pre></td></tr></table></figure></p>
<h2 id="离线一键安装包"><a href="#离线一键安装包" class="headerlink" title="离线一键安装包"></a>离线一键安装包</h2><blockquote>
<p>k8s 离线一键安装包教程&amp;&amp;地址：<a href="http://team.jiunile.com/pro/k8s/" target="_blank" rel="external">一键安装</a></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes Pod 的生命周期管理]]></title>
      <url>http://icyxp.github.io/blog/2018/11/k8s-k8s-pod-life-cycle.html</url>
      <content type="html"><![CDATA[<h2 id="Pod的生命周期"><a href="#Pod的生命周期" class="headerlink" title="Pod的生命周期"></a>Pod的生命周期</h2><hr>
<h3 id="Pod-phase"><a href="#Pod-phase" class="headerlink" title="Pod phase"></a>Pod phase</h3><p>Pod 的 status 在信息保存在 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471" target="_blank" rel="external">PodStatus</a> 中定义，其中有一个 phase 字段。</p>
<p>Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。</p>
<p>Pod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。</p>
<p>无论你是手动创建 Pod，还是通过 deployment、daemonset 或 statefulset来创建，Pod 的 phase 都有以下几个可能的值：</p>
<ul>
<li><strong>挂起（Pending）</strong>：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。</li>
<li><strong>运行中（Running）</strong>：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。</li>
<li><strong>成功（Successed）</strong>：Pod 中的所有容器都被成功终止，并且不会再重启。</li>
<li><strong>失败（Failed）</strong>：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。</li>
<li><strong>未知（Unkonwn）</strong>：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。</li>
</ul>
<p>下图是 Pod 的生命周期示意图，从图中可以看到 Pod 状态的变化。<br><img src="/images/k8s/k8s_pod_01.jpg" alt="K8s pod 生命周期"></p>
<a id="more"></a>
<h3 id="Pod-状态"><a href="#Pod-状态" class="headerlink" title="Pod 状态"></a>Pod 状态</h3><p>Pod 有一个 PodStatus 对象，其中包含一个 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L1964" target="_blank" rel="external">PodCondition</a> 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 <code>PodScheduled</code>、<code>Ready</code>、<code>Initialized</code> 和 <code>Unschedulable</code>。status 字段是一个字符串，可能的值有 <code>True</code>、<code>False</code> 和 <code>Unknown</code>。</p>
<p>当你通过 <code>kubectl get pod</code> 查看 Pod 时，<code>STATUS</code> 这一列可能会显示与上述5个状态不同的值，例如 <code>Init:0/1</code> 和 <code>CrashLoopBackOff</code>。这是因为 Pod 状态的定义除了包含 phase 之外，还有 <code>InitContainerStatuses</code> 和 <code>containerStatuses</code> 等其他字段，具体代码参考 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471" target="_blank" rel="external">overall status of a pod</a> .</p>
<p>如果想知道究竟发生了什么，可以通过命令 <code>kubectl describe pod/$PODNAME</code> 查看输出信息的 <code>Events</code> 条目。通过 Events 条目可以看到一些具体的信息，比如正在拉取容器镜像，Pod 已经被调度，或者某个 container 处于 unhealthy 状态。</p>
<h2 id="Pod-的启动关闭流程"><a href="#Pod-的启动关闭流程" class="headerlink" title="Pod 的启动关闭流程"></a>Pod 的启动关闭流程</h2><hr>
<p>下面通过一个具体的示例来探究一下 Pod 的整个生命周期流程。为了确定事情发生的顺序，通过下面的 manifest 来部署一个 deployment。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span>                   Deployment</span><br><span class="line"><span class="attr">apiVersion:</span>             apps/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span>                 loap</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span>             <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span>            loap</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span>           init</span><br><span class="line"><span class="attr">        image:</span>          busybox</span><br><span class="line"><span class="attr">        command:</span>       [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): INIT &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span>    /loap</span><br><span class="line"><span class="attr">          name:</span>         timing</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span>           main</span><br><span class="line"><span class="attr">        image:</span>          busybox</span><br><span class="line"><span class="attr">        command:</span>       [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): START &gt;&gt; /loap/timing;</span><br><span class="line">sleep 10; echo $(date +%s): END &gt;&gt; /loap/timing;'</span>]</span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span>    /loap</span><br><span class="line"><span class="attr">          name:</span>         timing</span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): LIVENESS &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): READINESS &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        lifecycle:</span></span><br><span class="line"><span class="attr">          postStart:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): POST-START &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">          preStop:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span>  [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): PRE-HOOK &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span>           timing</span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span>         /tmp/loap</span><br></pre></td></tr></table></figure></p>
<p>等待 Pod 状态变为 <code>Running</code> 之后，通过以下命令来强制停止 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale deployment loap --replicas=0</span><br></pre></td></tr></table></figure></p>
<p>查看 <code>/tmp/loap/timing</code> 文件的内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /tmp/loap/timing</span><br><span class="line"></span><br><span class="line">1525334577: INIT</span><br><span class="line">1525334581: START</span><br><span class="line">1525334581: POST-START</span><br><span class="line">1525334584: READINESS</span><br><span class="line">1525334584: LIVENESS</span><br><span class="line">1525334588: PRE-HOOK</span><br><span class="line">1525334589: END</span><br></pre></td></tr></table></figure></p>
<p><code>/tmp/loap/timing</code> 文件的内容很好地体现了 Pod 的启动和关闭流程，具体过程如下：<br><img src="/images/k8s/k8s_pod_02.jpg" alt="Pod 的启动和关闭流程"></p>
<ol>
<li>首先启动一个 Infra 容器（又叫 Pause 容器），用来和 Pod 中的其他容器共享 linux 命名空间，并开启 init 进程。（上图中忽略了这一步）</li>
<li>然后启动 Init 容器，它是一种专用的容器，在应用程序容器启动之前运行，用来对 Pod 进行一些初始化操作，并包括一些应用镜像中不存在的实用工具和安装脚本。</li>
<li>4 秒之后，应用程序容器和 <code>post-start hook</code> 同时启动。</li>
<li>7 秒之后开始启动 <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank" rel="external">liveness 和 readiness 探针</a>。</li>
<li>11 秒之后，通过手动杀掉 Pod，<code>pre-stop hook</code> 执行，优雅删除期限过期后（默认是 30 秒），应用程序容器停止。实际的 Pod 终止过程要更复杂，具体参考 <a href="https://jimmysong.io/kubernetes-handbook/concepts/pod.html" target="_blank" rel="external">Pod 的终止</a>。</li>
</ol>
<blockquote>
<p>必须主动杀掉 Pod 才会触发 <code>pre-stop hook</code>，如果是 Pod 自己 Down 掉，则不会执行 <code>pre-stop hook</code>。</p>
</blockquote>
<h2 id="如何快速-DEBUG"><a href="#如何快速-DEBUG" class="headerlink" title="如何快速 DEBUG"></a>如何快速 DEBUG</h2><hr>
<p>当 Pod 出现致命的错误时，如果能够快速 DEBUG，将会帮助我们快速定位问题。为了实现这个目的，可以把把致命事件的信息通过 <code>.spec.terminationMessagePath</code> 配置写入指定位置的文件，就像打印错误、异常和堆栈信息一样。该位置的内容可以很方便的通过 dashboards、监控软件等工具检索和展示，默认路径为 <code>/dev/termination-log</code>。</p>
<p>以下是一个小例子：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># termination-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> termination-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> termination-demo-container</span><br><span class="line"><span class="attr">    image:</span> alpine</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>]</span><br><span class="line"><span class="attr">    args:</span> [<span class="string">"-c"</span>, <span class="string">"sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log"</span>]</span><br></pre></td></tr></table></figure></p>
<p>这些消息的最后部分会使用其他的规定来单独存储：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> termination-demo.yaml</span><br><span class="line"></span><br><span class="line">$ sleep 20</span><br><span class="line"></span><br><span class="line">$ kubectl get pod termination-demo -o go-template=<span class="string">'&#123;&#123;range .status.containerStatuses&#125;&#125;&#123;&#123;.lastState.terminated.message&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line">Sleep expired</span><br><span class="line"></span><br><span class="line">$ kubectl get pod termination-demo -o go-template=<span class="string">'&#123;&#123;range .status.containerStatuses&#125;&#125;&#123;&#123;.lastState.terminated.exitCode&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line">0</span><br></pre></td></tr></table></figure></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><hr>
<ul>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/pod-hook.html" target="_blank" rel="external">Pod hook</a></li>
<li><a href="https://blog.openshift.com/kubernetes-pods-life/" target="_blank" rel="external">Kubernetes: A Pod’s Life</a></li>
<li><a href="https://k8smeetup.github.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/" target="_blank" rel="external">确定 Pod 失败的原因</a></li>
<li>Ryan Yang </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java和Docker限制的那些事儿]]></title>
      <url>http://icyxp.github.io/blog/2018/07/docker-java%E4%B8%8Edocker%E7%9A%84%E9%82%A3%E7%82%B9%E4%BA%8B.html</url>
      <content type="html"><![CDATA[<h2 id="揭秘"><a href="#揭秘" class="headerlink" title="揭秘"></a>揭秘</h2><p>Java和Docker不是天然的朋友。 Docker可以设置内存和CPU限制，而Java不能自动检测到。使用Java的Xmx标识（繁琐/重复）或新的实验性JVM标识，我们可以解决这个问题。</p>
<h2 id="虚拟化中的不匹配"><a href="#虚拟化中的不匹配" class="headerlink" title="虚拟化中的不匹配"></a>虚拟化中的不匹配</h2><p>Java和Docker的结合并不是完美匹配的，最初的时候离完美匹配有相当大的距离。对于初学者来说，JVM的全部设想就是，虚拟机可以让程序与底层硬件无关。</p>
<p>那么，把我们的Java应用打包到JVM中，然后整个再塞进Docker容器中，能给我们带来什么好处呢？大多数情况下，你只是在复制JVMs和Linux容器，除了浪费更多的内存，没任何好处。感觉这样子挺傻的。</p>
<p>不过，Docker可以把你的程序，设置，特定的JDK，Linux设置和应用服务器，还有其他工具打包在一起，当做一个东西。站在DevOps/Cloud的角度来看，这样一个完整的容器有着更高层次的封装。</p>
<h3 id="问题一：内存"><a href="#问题一：内存" class="headerlink" title="问题一：内存"></a>问题一：内存</h3><p>时至今日，绝大多数产品级应用仍然在使用Java 8（或者更旧的版本），而这可能会带来问题。Java 8（update 131之前的版本）跟Docker无法很好地一起工作。问题是在你的机器上，JVM的可用内存和CPU数量并不是Docker允许你使用的可用内存和CPU数量。</p>
<p>比如，如果你限制了你的Docker容器只能使用100MB内存，但是呢，旧版本的Java并不能识别这个限制。Java看不到这个限制。JVM会要求更多内存，而且远超这个限制。如果使用太多内存，Docker将采取行动并杀死容器内的进程！JAVA进程被干掉了，很明显，这并不是我们想要的。</p>
<p>为了解决这个问题，你需要给Java指定一个最大内存限制。在旧版本的Java（8u131之前），你需要在容器中通过设置-Xmx来限制堆大小。这感觉不太对，你可不想定义这些限制两次，也不太想在你的容器中来定义。</p>
<p>幸运的是我们现在有了更好的方式来解决这个问题。从Java 9之后（8u131+），JVM增加了如下标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap</span><br></pre></td></tr></table></figure></p>
<p>这些标志强制JVM检查Linux的cgroup配置，Docker是通过cgroup来实现最大内存设置的。现在，如果你的应用到达了Docker设置的限制（比如500MB），JVM是可以看到这个限制的。JVM将会尝试GC操作。如果仍然超过内存限制，JVM就会做它该做的事情，抛出OutOfMemoryException。也就是说，JVM能够看到Docker的这些设置。</p>
<p>从Java 10之后（参考下面的测试），这些体验标志位是默认开启的，也可以使用-XX:+UseContainerSupport来使能（你可以通过设置-XX:-UseContainerSupport来禁止这些行为）。</p>
<h3 id="问题二：CPU"><a href="#问题二：CPU" class="headerlink" title="问题二：CPU"></a>问题二：CPU</h3><p>第二个问题是类似的，但它与CPU有关。简而言之，JVM将查看硬件并检测CPU的数量。它会优化你的runtime以使用这些CPUs。但是同样的情况，这里还有另一个不匹配，Docker可能不允许你使用所有这些CPUs。可惜的是，这在Java 8或Java 9中并没有修复，但是在Java 10中得到了解决。</p>
<p>从Java 10开始，可用的CPUs的计算将采用以不同的方式（默认情况下）解决此问题（同样是通过UseContainerSupport）。<br><a id="more"></a></p>
<h2 id="Java和Docker的内存处理测试"><a href="#Java和Docker的内存处理测试" class="headerlink" title="Java和Docker的内存处理测试"></a>Java和Docker的内存处理测试</h2><p>作为一个有趣的练习，让我们验证并测试Docker如何使用几个不同的JVM版本/标志甚至不同的JVM来处理内存不足。</p>
<p>首先，我们创建一个测试应用程序，它只是简单地“吃”内存并且不释放它。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MemEat</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        List l = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">byte</span> b[] = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1048576</span>];</span><br><span class="line">            l.add(b);</span><br><span class="line">            Runtime rt = Runtime.getRuntime();</span><br><span class="line">            System.out.println( <span class="string">"free memory: "</span> + rt.freeMemory() );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们可以启动Docker容器并运行这个应用程序来查看会发生什么。</p>
<h3 id="测试一：Java-8u111"><a href="#测试一：Java-8u111" class="headerlink" title="测试一：Java 8u111"></a>测试一：Java 8u111</h3><p>首先，我们将从具有旧版本Java 8的容器开始（update 111）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it java:openjdk-8u111 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>我们编译并运行MemEat.java文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 67194416</span><br><span class="line">free memory: 66145824</span><br><span class="line">free memory: 65097232</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>正如所料，Docker已经杀死了我们的Java进程。不是我们想要的（！）。你也可以看到输出，Java认为它仍然有大量的内存需要分配。</p>
<p>我们可以通过使用-Xmx标志为Java提供最大内存来解决此问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java -Xmx100m MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 1155664</span><br><span class="line">free memory: 1679936</span><br><span class="line">free memory: 2204208</span><br><span class="line">free memory: 1315752</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>在提供了我们自己的内存限制之后，进程正常停止，JVM理解它正在运行的限制。然而，问题在于你现在将这些内存限制设置了两次，Docker一次，JVM一次。</p>
<h3 id="测试二：Java-8u144"><a href="#测试二：Java-8u144" class="headerlink" title="测试二：Java 8u144"></a>测试二：Java 8u144</h3><p>如前所述，随着增加新标志来修复问题，JVM现在可以遵循Docker所提供的设置。我们可以使用版本新一点的JVM来测试它。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk8 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>（在撰写本文时，此OpenJDK Java镜像的版本是Java 8u144）</p>
<p>接下来，我们再次编译并运行MemEat.java文件，不带任何标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 67194416</span><br><span class="line">free memory: 66145824</span><br><span class="line">free memory: 65097232</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>依然存在同样的问题。但是我们现在可以提供上面提到的实验性标志来试试看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 1679936</span><br><span class="line">free memory: 2204208</span><br><span class="line">free memory: 1155616</span><br><span class="line">free memory: 1155600</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">   at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>这一次我们没有告诉JVM限制的是什么，我们只是告诉JVM去检查正确的限制设置！现在感觉好多了。</p>
<h3 id="测试三：Java-10u23"><a href="#测试三：Java-10u23" class="headerlink" title="测试三：Java 10u23"></a>测试三：Java 10u23</h3><p>有些人在评论和Reddit上提到Java 10通过使实验标志成为新的默认值来解决所有问题。这种行为可以通过禁用此标志来关闭：-XX：-UseContainerSupport。</p>
<p>当我测试它时，它最初不起作用。在撰写本文时，AdoptAJDK OpenJDK10镜像与jdk-10+23一起打包。这个JVM显然还是不理解UseContainerSupport标志，该进程仍然被Docker杀死。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk10 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>测试了代码（甚至手动提供需要的标志）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 96262112</span><br><span class="line">free memory: 94164960</span><br><span class="line">free memory: 92067808</span><br><span class="line">free memory: 89970656</span><br><span class="line">Killed</span><br><span class="line">java -XX:+UseContainerSupport MemEat</span><br><span class="line">Unrecognized VM option <span class="string">'UseContainerSupport'</span></span><br><span class="line">Error: Could not create the Java Virtual Machine.</span><br><span class="line">Error: A fatal exception has occurred. Program will exit.</span><br></pre></td></tr></table></figure></p>
<h3 id="测试四：Java-10u46（Nightly）"><a href="#测试四：Java-10u46（Nightly）" class="headerlink" title="测试四：Java 10u46（Nightly）"></a>测试四：Java 10u46（Nightly）</h3><p>我决定尝试AdoptAJDK OpenJDK 10的最新nightly构建。它包含的版本是Java 10+46，而不是Java 10+23。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk10:nightly /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>然而，在这个ngithly构建中有一个问题，导出的PATH指向旧的Java 10+23目录，而不是10+46，我们需要修复这个问题。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/opt/java/openjdk/jdk-10+46/bin/</span><br><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 3566824</span><br><span class="line">free memory: 2796008</span><br><span class="line">free memory: 1480320</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">  at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>成功！不提供任何标志，Java 10依然可以正确检测到Dockers内存限制。</p>
<h3 id="测试五：OpenJ9"><a href="#测试五：OpenJ9" class="headerlink" title="测试五：OpenJ9"></a>测试五：OpenJ9</h3><p>我最近也在试用OpenJ9，这个免费的替代JVM已经从IBM J9开源，现在由Eclipse维护。</p>
<p>请在我的下一篇博文<a href="http://royvanrijn.com/blog/2018/05/openj9-jvm-shootout/" target="_blank" rel="external">http://royvanrijn.com/blog/2018/05/openj9-jvm-shootout/</a>阅读关于OpenJ9的更多信息。</p>
<p>它运行速度快，内存管理非常好，性能卓越，经常可以为我们的微服务节省多达30-50％的内存。这几乎可以将Spring Boot应用程序定义为’micro’了，其运行时间只有100-200mb，而不是300mb+。我打算尽快就此写一篇关于这方面的文章。</p>
<p>但令我惊讶的是，OpenJ9还没有类似于Java 8/9/10+中针对cgroup内存限制的标志（backported）的选项。如果我们将以前的测试用例应用到最新的AdoptAJDK OpenJDK 9 + OpenJ9 build：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk9-openj9 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>我们添加OpenJDK标志（OpenJ9会忽略的标志）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 83988984</span><br><span class="line">free memory: 82940400</span><br><span class="line">free memory: 81891816</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>Oops，JVM再次被Docker杀死。</p>
<p>我真的希望类似的选项将很快添加到OpenJ9中，因为我希望在生产环境中运行这个选项，而不必指定最大内存两次。 Eclipse/IBM正在努力修复这个问题，已经提了issues，甚至已经针对issues提交了PR。</p>
<h3 id="更新：（不推荐Hack）"><a href="#更新：（不推荐Hack）" class="headerlink" title="更新：（不推荐Hack）"></a>更新：（不推荐Hack）</h3><p>一个稍微丑陋/hacky的方式来解决这个问题是使用下面的组合标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">java -Xmx`cat /sys/fs/cgroup/memory/memory.limit_<span class="keyword">in</span>_bytes` MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 3171536</span><br><span class="line">free memory: 2127048</span><br><span class="line">free memory: 2397632</span><br><span class="line">free memory: 1344952</span><br><span class="line">JVMDUMP039I Processing dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span> at 2018/05/15 14:04:26 - please wait.</span><br><span class="line">JVMDUMP032I JVM requested System dump using <span class="string">'//core.20180515.140426.125.0001.dmp'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I System dump written to //core.20180515.140426.125.0001.dmp</span><br><span class="line">JVMDUMP032I JVM requested Heap dump using <span class="string">'//heapdump.20180515.140426.125.0002.phd'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Heap dump written to //heapdump.20180515.140426.125.0002.phd</span><br><span class="line">JVMDUMP032I JVM requested Java dump using <span class="string">'//javacore.20180515.140426.125.0003.txt'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Java dump written to //javacore.20180515.140426.125.0003.txt</span><br><span class="line">JVMDUMP032I JVM requested Snap dump using <span class="string">'//Snap.20180515.140426.125.0004.trc'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Snap dump written to //Snap.20180515.140426.125.0004.trc</span><br><span class="line">JVMDUMP013I Processed dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span>.</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">  at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>在这种情况下，堆大小受限于分配给Docker实例的内存，这适用于较旧的JVM和OpenJ9。这当然是错误的，因为容器本身和堆外的JVM的其他部分也使用内存。但它似乎工作，显然Docker在这种情况下是宽松的。也许某些bash大神会做出更好的版本，从其他进程的字节中减去一部分。</p>
<p>无论如何，不要这样做，它可能无法正常工作。</p>
<h3 id="测试六：OpenJ9（Nightly）"><a href="#测试六：OpenJ9（Nightly）" class="headerlink" title="测试六：OpenJ9（Nightly）"></a>测试六：OpenJ9（Nightly）</h3><p>有人建议使用OpenJ9的最新nightly版本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk9-openj9:nightly /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>最新的OpenJ9夜间版本，它有两个东西：</p>
<ol>
<li>另一个有问题的PATH参数，需要先解决这个问题</li>
<li>JVM支持新标志UseContainerSupport（就像Java 10一样）</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/opt/java/openjdk/jdk-9.0.4+12/bin/</span><br><span class="line">javac MemEat.java</span><br><span class="line">java -XX:+UseContainerSupport MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 5864464</span><br><span class="line">free memory: 4815880</span><br><span class="line">free memory: 3443712</span><br><span class="line">free memory: 2391032</span><br><span class="line">JVMDUMP039I Processing dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span> at 2018/05/15 21:32:07 - please wait.</span><br><span class="line">JVMDUMP032I JVM requested System dump using <span class="string">'//core.20180515.213207.62.0001.dmp'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I System dump written to //core.20180515.213207.62.0001.dmp</span><br><span class="line">JVMDUMP032I JVM requested Heap dump using <span class="string">'//heapdump.20180515.213207.62.0002.phd'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Heap dump written to //heapdump.20180515.213207.62.0002.phd</span><br><span class="line">JVMDUMP032I JVM requested Java dump using <span class="string">'//javacore.20180515.213207.62.0003.txt'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Java dump written to //javacore.20180515.213207.62.0003.txt</span><br><span class="line">JVMDUMP032I JVM requested Snap dump using <span class="string">'//Snap.20180515.213207.62.0004.trc'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Snap dump written to //Snap.20180515.213207.62.0004.trc</span><br><span class="line">JVMDUMP013I Processed dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span>.</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br></pre></td></tr></table></figure>
<p>TADAAA，正在修复中！</p>
<p>奇怪的是，这个标志在OpenJ9中默认没有启用，就像它在Java 10中一样。再说一次：确保你测试了这是你想在一个Docker容器中运行Java。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>简言之：注意资源限制的不匹配。测试你的内存设置和JVM标志，不要假设任何东西。</p>
<p>如果您在Docker容器中运行Java，请确保你设置了Docker内存限制和在JVM中也做了限制，或者你的JVM能够理解这些限制。</p>
<p>如果您无法升级您的Java版本，请使用-Xmx设置您自己的限制。</p>
<p>对于Java 8和Java 9，请更新到最新版本并使用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX：+UnlockExperimentalVMOptions -XX：+UseCGroupMemoryLimitForHeap</span><br></pre></td></tr></table></figure></p>
<p>对于Java 10，确保它支持’UseContainerSupport’（更新到最新版本）。</p>
<p>对于OpenJ9（我强烈建议使用，可以在生产环境中有效减少内存占用量），现在使用-Xmx设置限制，但很快会出现一个支持UseContainerSupport标志的版本。</p>
<p>原文链接：<a href="http://royvanrijn.com/blog/2018/05/java-and-docker-memory-limits/" target="_blank" rel="external">http://royvanrijn.com/blog/2018/05/java-and-docker-memory-limits/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Centos 6.x 上升级go 1.10.3]]></title>
      <url>http://icyxp.github.io/blog/2018/07/go-%E5%8D%87%E7%BA%A7go1-10-x%E9%97%AE%E9%A2%98.html</url>
      <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><blockquote>
<p>目前本地Linux (Centos 6.x) 编译环境还滞留在1.8.x上，为了提升go性能，想将go升级到最新版，但在升级过程中遇到如下问题，故此记录下！忘后续的go友能跳过此坑！</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在Centos 6.x 升级go 1.10.x过程中遇到如下问题：</p>
<ul>
<li>step1.  下载go 1.10.x 源码</li>
<li>step2.  解压，进入go/src</li>
<li>step3.  执行./all.bash</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Building Go cmd/dist using /root/go1.4.</span><br><span class="line">Building Go toolchain1 using /root/go1.4.</span><br><span class="line">Building Go bootstrap cmd/go (go_bootstrap) using Go toolchain1.</span><br><span class="line">Building Go toolchain2 using go_bootstrap and Go toolchain1.</span><br><span class="line">Building Go toolchain3 using go_bootstrap and Go toolchain2.</span><br><span class="line">Building packages and commands <span class="keyword">for</span> linux/amd64.</span><br><span class="line"></span><br><span class="line"><span class="comment">##### Testing packages.</span></span><br><span class="line">.... 过程略长，特此省略</span><br><span class="line">ok      cmd/internal/src    0.001s</span><br><span class="line">ok      cmd/internal/<span class="built_in">test</span>2json    0.097s</span><br><span class="line">ok      cmd/link    1.988s</span><br><span class="line">ok      cmd/link/internal/ld    43.529s</span><br><span class="line">ok      cmd/nm    3.417s</span><br><span class="line">ok      cmd/objdump    1.588s</span><br><span class="line">ok      cmd/pack    1.217s</span><br><span class="line">ok      cmd/trace    0.007s</span><br><span class="line">--- FAIL: TestObjFile (0.01s)</span><br><span class="line">    binutils_test.go:231: SourceLine: unexpected error write |1: broken pipe</span><br><span class="line">FAIL</span><br><span class="line">FAIL    cmd/vendor/github.com/google/pprof/internal/binutils    0.018s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/driver    12.194s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/elfexec    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/graph    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/measurement    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/report    0.048s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/symbolizer    0.004s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/symbolz    0.004s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/profile    0.045s</span><br><span class="line">ok      cmd/vendor/github.com/ianlancetaylor/demangle    0.012s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/arm/armasm    0.007s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/arm64/arm64asm    0.043s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/ppc64/ppc64asm    0.003s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/x86/x86asm    0.064s</span><br><span class="line">ok      cmd/vet    1.205s</span><br><span class="line">ok      cmd/vet/internal/cfg    0.002s</span><br><span class="line">2018/07/18 17:59:22 Failed: <span class="built_in">exit</span> status 1</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>为什么在执行binutils_test.go 会Failed，最终查看代码原因是因为如下命令引起：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">120807 3650  execve(<span class="string">"/usr/bin/addr2line"</span>, [<span class="string">"/usr/bin/addr2line"</span>, <span class="string">"-aif"</span>, <span class="string">"-e"</span>, <span class="string">"testdata/exe_linux_64"</span>], [/* 15 vars */] &lt;unfinished ...&gt;</span><br></pre></td></tr></table></figure></p>
<p>有关生成此命令的源代码可查看如下地址：<a href="https://github.com/google/pprof/blob/a74ae6fb3cd7047c79272e3ea0814b08154a2d3c/internal/binutils/addr2liner.go#L92" target="_blank" rel="external">addr2line</a></p>
<p>add2line文件来自于包：binutils</p>
<p>执行命令失败，是因为在CentOS 6.x上，binutils的版本是2.20，<a href="https://sourceware.org/git/gitweb.cgi?p=binutils-gdb.git;a=blob_plain;f=binutils/NEWS;hb=refs/tags/binutils-2_27" target="_blank" rel="external">参考文献</a> ，然后addr2line命令中的-a参数在binutils 2.21版中才添加</p>
<p>因此，为了解决这个问题，我从源码重新进行编译binutils并将其构建的二进制文件添加到PATH中，然后运行测试，并成功通过。</p>
<p>编译安装binutils过程如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://ftp.gnu.org/gnu/binutils/binutils-2.27.tar.gz</span><br><span class="line">tar zxvf binutils-2.27.tar.gz </span><br><span class="line"><span class="built_in">cd</span> binutils-2.27</span><br><span class="line">./configure --prefix=/usr</span><br><span class="line">make </span><br><span class="line">make install</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong><code>特此说明，在Centos 7.x 上能成功避免此坑！！</code></strong></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Logstash filter插件开发]]></title>
      <url>http://icyxp.github.io/blog/2017/08/log-logstash-filter.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Logstash是一个具有实时管线能力的开源数据收集引擎。在ELK Stack中，通常选择更轻量级的Filebeat收集日志，然后将日志输出到Logstash进行加工处理，再将处理后的日志输出到指定的目标（ElasticSearch，Kafka等）当中。</p>
<p>Logstash事件的处理管线是inputs → filters → outputs，三个阶段都可以自定义插件，本文主要介绍如何开发自定义需求最多的filter插件。</p>
<p>Logstash的安装就不详细介绍了，下载传送门：<a href="https://www.elastic.co/downloads/logstash" target="_blank" rel="external">https://www.elastic.co/downloads/logstash</a>。</p>
<h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><p>我们使用docker来讲解如何给logstash定制一个filter插件，首先，我们下载logstash官方最新版本的镜像。下载方式: <code>docker pull logstash</code>，下载好后我们就来Run这个镜像，命令：<code>docker run -it logstash bash</code>，这样我们就进入到logstash容器中了。接下来我们就安装下两个基本的软件包，一个vim，一个rsyslog，命令如下：<code>apt-get update &amp;&amp; apt-get -y install vim rsyslog &amp;&amp; /etc/init.d/rsysylog start</code>。到此我们的准备工作就完毕了。</p>
<a id="more"></a>
<h2 id="生成filter插件"><a href="#生成filter插件" class="headerlink" title="生成filter插件"></a>生成filter插件</h2><p>cd到Logstash的跟目录，使用<code>bin/logstash-plugin</code>生成filter插件模板，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/share/logstash</span><br><span class="line">mkdir -p vendor/localgems</span><br><span class="line">bin/logstash-plugin generate --type filter --name <span class="built_in">test</span> --path vendor/localgems</span><br><span class="line"><span class="comment">#vendor/localgems 可修改为你自己的路径</span></span><br></pre></td></tr></table></figure></p>
<p>查看filter插件的目录结构，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@c11787c4cef3:/usr/share/logstash/vendor/localgems<span class="comment"># tree .</span></span><br><span class="line">.</span><br><span class="line">└── logstash-filter-test</span><br><span class="line">    ├── CHANGELOG.md</span><br><span class="line">    ├── CONTRIBUTORS</span><br><span class="line">    ├── DEVELOPER.md</span><br><span class="line">    ├── Gemfile</span><br><span class="line">    ├── LICENSE</span><br><span class="line">    ├── README.md</span><br><span class="line">    ├── Rakefile</span><br><span class="line">    ├── lib</span><br><span class="line">    │   └── logstash</span><br><span class="line">    │       └── filters</span><br><span class="line">    │           └── test.rb</span><br><span class="line">    ├── logstash-filter-test.gemspec</span><br><span class="line">    ├── spec</span><br><span class="line">    │   ├── filters</span><br><span class="line">    │   │   └── <span class="built_in">test</span>_spec.rb</span><br><span class="line">    │   └── spec_helper.rb</span><br><span class="line">    └── test.conf</span><br><span class="line"></span><br><span class="line">6 directories, 12 files</span><br></pre></td></tr></table></figure></p>
<h2 id="filter插件初探"><a href="#filter插件初探" class="headerlink" title="filter插件初探"></a>filter插件初探</h2><p>Logstash插件是用ruby写的，查看<code>logstash-filter-test/lib/logstash/filters/test.rb</code>文件，如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># logstash依赖于UTF-8编码，需要在插件代码开始处添加</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#引入了插件必备的包</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">"logstash/filters/base"</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">"logstash/namespace"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插件继承自Base基类，并配置插件的使用名称</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogStash::Filters::Test</span> &lt; LogStash::Filters::<span class="title">Base</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插件名称，在Logstash配置的filter块中使用</span></span><br><span class="line">  <span class="comment">#filter&#123;</span></span><br><span class="line">  <span class="comment">#  test&#123;</span></span><br><span class="line">  <span class="comment">#      source =&gt; "message"</span></span><br><span class="line">  <span class="comment">#  &#125;</span></span><br><span class="line">  <span class="comment">#&#125;</span></span><br><span class="line">  config_name <span class="string">"test"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插件参数配置</span></span><br><span class="line">  <span class="comment"># source是插件test的可选参数，默认值是"Hello World!"。</span></span><br><span class="line">  config <span class="symbol">:source</span>, <span class="symbol">:validate</span> =&gt; <span class="symbol">:string</span>, <span class="symbol">:default</span> =&gt; <span class="string">"Hello World!"</span></span><br><span class="line">  <span class="comment"># 下面是参数的通用配置代码</span></span><br><span class="line">  <span class="comment"># config :variable_name, :validate =&gt; :variable_type, :default =&gt; "Default value", :required =&gt; boolean, :deprecated =&gt; boolean, :obsolete =&gt; string</span></span><br><span class="line">  <span class="comment"># :variable_name：参数名称</span></span><br><span class="line">  <span class="comment"># :validate：验证参数类型，如:string, :password, :boolean, :number, :array, :hash, :path等</span></span><br><span class="line">  <span class="comment"># :required：是否必须配置</span></span><br><span class="line">  <span class="comment"># :default：默认值</span></span><br><span class="line">  <span class="comment"># :deprecated：是否废弃</span></span><br><span class="line">  <span class="comment"># :obsolete：声明该配置不再使用，通常提供升级方案</span></span><br><span class="line"></span><br><span class="line">  public</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">register</span></span></span><br><span class="line">    <span class="comment"># 方法相当于初始化方法，不需要手动调用，可以在这个方法里面调用配置变量，如<span class="doctag">@message</span>，也可以初始化自己的实例变量。</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  public</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span><span class="params">(event)</span></span></span><br><span class="line">    <span class="comment"># 方法是插件的数据处理逻辑，其中event变量封装了数据流，可以通过接口访问event中的内容，具体参见 https://www.elastic.co/guide/en/logstash/5.1/event-api.html。</span></span><br><span class="line">    <span class="keyword">if</span> (source = event.get(@source))      </span><br><span class="line">      datas = source.split(<span class="string">"|"</span>)</span><br><span class="line">      event.set(<span class="string">"data"</span>, datas[<span class="number">0</span>])</span><br><span class="line">      event.set(<span class="string">"data2"</span>, datas[<span class="number">1</span>])</span><br><span class="line">      event.set(<span class="string">"user"</span>, <span class="string">"xp"</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#这个方法用于保证Logstash的配置`add_field`, `remove_field`, `add_tag`和`remove_tag`会被正确执行。</span></span><br><span class="line">    filter_matched(event)</span><br><span class="line">  <span class="keyword">end</span> <span class="comment"># def filter</span></span><br><span class="line"><span class="keyword">end</span> <span class="comment"># class LogStash::Filters::Test</span></span><br></pre></td></tr></table></figure></p>
<p>查看<code>logstash-filter-test/logstash-filter-test.gemspec</code>文件，如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Gem::Specification.new <span class="keyword">do</span> <span class="params">|s|</span></span><br><span class="line">  s.name          = <span class="string">'logstash-filter-test'</span>  </span><br><span class="line">  s.version       = <span class="string">'0.1.0'</span></span><br><span class="line">  s.licenses      = [<span class="string">'Apache License (2.0)'</span>]</span><br><span class="line">  s.summary       = <span class="string">'描述这个插件的概要'</span></span><br><span class="line">  s.description   = <span class="string">'这个插件的详细说明'</span></span><br><span class="line">  s.homepage      = <span class="string">'http://icyxp.github.io'</span></span><br><span class="line">  s.authors       = [<span class="string">'icyboy'</span>]</span><br><span class="line">  s.email         = <span class="string">'icyboy@me.com'</span></span><br><span class="line">  s.require_paths = [<span class="string">'lib'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Files</span></span><br><span class="line">  s.files = Dir[<span class="string">'lib/**/*'</span>,<span class="string">'spec/**/*'</span>,<span class="string">'vendor/**/*'</span>,<span class="string">'*.gemspec'</span>,<span class="string">'*.md'</span>,<span class="string">'CONTRIBUTORS'</span>,<span class="string">'Gemfile'</span>,<span class="string">'LICENSE'</span>,<span class="string">'NOTICE.TXT'</span>]</span><br><span class="line">   <span class="comment"># Tests</span></span><br><span class="line">  s.test_files = s.files.grep(<span class="regexp">%r&#123;^(test|spec|features)/&#125;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Special flag to let us know this is actually a logstash plugin</span></span><br><span class="line">  s.metadata = &#123; <span class="string">"logstash_plugin"</span> =&gt; <span class="string">"true"</span>, <span class="string">"logstash_group"</span> =&gt; <span class="string">"filter"</span> &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Gem dependencies</span></span><br><span class="line">  s.add_runtime_dependency <span class="string">"logstash-core-plugin-api"</span>, <span class="string">"~&gt; 2.0"</span></span><br><span class="line">  s.add_development_dependency <span class="string">'logstash-devutils'</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<h2 id="在Logstash中配置定制的插件"><a href="#在Logstash中配置定制的插件" class="headerlink" title="在Logstash中配置定制的插件"></a>在Logstash中配置定制的插件</h2><p>cd到Logstash根目录下<code>cd /usr/shar/logstash</code>，在<code>Gemfile</code>末尾添加以下配置：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="string">"logstash-filter-test"</span>, <span class="symbol">:path</span> =&gt; <span class="string">"vendor/localgems/logstash-filter-test"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="启动Logstash"><a href="#启动Logstash" class="headerlink" title="启动Logstash"></a>启动Logstash</h2><p>先编写一个配置文件<code>test.conf</code>，这里我使用了rsyslog<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#来源rsyslog</span></span><br><span class="line">input&#123;</span><br><span class="line">    file&#123;</span><br><span class="line">	    path =&gt; <span class="string">"/var/log/messages"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#或者可以设置为终端输入</span></span><br><span class="line"><span class="comment">#input&#123;</span></span><br><span class="line"><span class="comment">#    stdin&#123;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    &#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line">filter&#123;</span><br><span class="line">    <span class="comment"># 定制的filter插件</span></span><br><span class="line">    <span class="built_in">test</span>&#123;</span><br><span class="line">	    <span class="built_in">source</span> =&gt; <span class="string">"message"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#输出到终端</span></span><br><span class="line">output&#123;</span><br><span class="line">    stdout&#123;</span><br><span class="line">        codec =&gt; rubydebug</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行起来<code>logstash -f test.conf</code>，然后往rsyslog里写日志你就可以看下如下情况就说明ok了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#logger "测试一下"</span></span><br><span class="line">&#123;</span><br><span class="line">          <span class="string">"path"</span> =&gt; <span class="string">"/var/log/messages"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2017-08-16T15:27:46.606Z,</span><br><span class="line">          <span class="string">"data"</span> =&gt; <span class="string">"Aug 16 15:27:45 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">         <span class="string">"data2"</span> =&gt; nil,</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"c11787c4cef3"</span>,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"Aug 16 15:27:45 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">          <span class="string">"user"</span> =&gt; <span class="string">"xp"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#logger "测试一下|from by icyboy"</span></span><br><span class="line">&#123;</span><br><span class="line">          <span class="string">"path"</span> =&gt; <span class="string">"/var/log/messages"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2017-08-16T15:28:17.661Z,</span><br><span class="line">          <span class="string">"data"</span> =&gt; <span class="string">"Aug 16 15:28:16 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">         <span class="string">"data2"</span> =&gt; <span class="string">"from by icyboy"</span>,</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"c11787c4cef3"</span>,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"Aug 16 15:28:16 c11787c4cef3 root: 测试一下|from by icyboy"</span>,</span><br><span class="line">          <span class="string">"user"</span> =&gt; <span class="string">"xp"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一语点醒技术人：你不是Google]]></title>
      <url>http://icyxp.github.io/blog/2017/07/gossip-%E4%BD%A0%E4%B8%8D%E6%98%AFGoogle.html</url>
      <content type="html"><![CDATA[<p><img src="/images/gossip/01/cover.png" alt="01"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在为问题寻找解决方案时要先充分了解问题本身，而不是一味地盲目崇拜那些巨头公司。Ozan Onay以Amazon、LinkedIn和Google为例，为执迷不悟的人敲响警钟。以下内容已获得作者翻译授权，查看原文：<a href="https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb" target="_blank" rel="external">You Are Not Google</a>。</p>
<p>软件工程师总是着迷于荒唐古怪的事。我们看起来似乎很理性，但在面对技术选型时，总是陷入抓狂——从Hacker News到各种博客，像一只飞蛾一样，来回折腾，最后精疲力尽，无助地飞向一团亮光，跪倒在它的前面——那就是我们一直在寻找的东西。</p>
<p>真正理性的人不是这样做决定的。不过工程师一贯如此，比如决定是否使用MapReduce。</p>
<p>Joe Hellerstein在他的大学数据库教程视频中说道：</p>
<blockquote>
<p>世界上只有差不多5个公司需要运行这么大规模的作业。至于其他公司……他们使用了所有的IO来实现不必要的容错。在2000年代，人们狂热地追随着Google：“我们要做Google做过的每一件事，因为我们也运行着世界上最大的互联网数据服务。”</p>
</blockquote>
<p>超出实际需求的容错没有什么问题，但我们却为此付出了的惨重的代价：不仅增加了IO，还有可能让原先成熟的系统——包含了事务、索引和查询优化器——变得破碎不堪。这是一个多么严重的历史倒退！有多少个Hadoop用户是有意识地做出这种决定的？有多少人知道他们的决定到底是不是一个明智之举？</p>
<p>MapReduce已经成为一个众矢之的，那些盲目崇拜者也意识到事情不对劲。但这种情况却普遍存在：虽然你使用了大公司的技术，但你的情况却与他们大不一样，而且你的决定并没有经过深思熟虑，你只是习以为常地认为，模仿巨头公司就一定也能给你带来同样的财富。</p>
<p>是的，这又是一篇劝大家“不要盲目崇拜”的文章。不过这次我列出了一长串有用的清单，或许能够帮助你们做出更好的决定。</p>
<a id="more"></a>
<h2 id="很酷的技术？UNPHAT"><a href="#很酷的技术？UNPHAT" class="headerlink" title="很酷的技术？UNPHAT"></a>很酷的技术？UNPHAT</h2><p>如果你还在使用Google搜索新技术来重建你的软件架构，那么我建议你不要再这么做了。相反，你可以考虑应用UNPHAT原则。</p>
<ol>
<li>在彻底了解（Understand）你的问题之前，不要急着去寻找解决方案。你的目标应该是在问题领域内“解决”问题，而不是在方案领域内解决问题。</li>
<li>列出（eNumerate）多种方案，不要只把眼睛盯在你最喜欢的方案上。</li>
<li>选择一个候选方案，并阅读相关论文（Paper）。</li>
<li>了解候选方案的产生背景（Historical context）。</li>
<li>比较优点（Advantages）和缺点，扬长避短。</li>
<li>思考（Think）！冷静地思考候选方案是否适合用于解决你的问题。要出现怎样异常的情况才会让你改变注意？例如，数据要少到什么程度才会让你打消使用Hadoop的念头？</li>
</ol>
<h2 id="你不是Amazon"><a href="#你不是Amazon" class="headerlink" title="你不是Amazon"></a>你不是Amazon</h2><p>UNPHAT原则十分直截了当。最近我与一个公司有过一次对话，这个公司打算在一个读密集的系统里使用Cassandra，他们的数据是在夜间加载到系统里的。</p>
<p>他们阅读了Dynamo的相关论文，并且知道Cassandra是最接近Dynamo的一个产品。我们知道，这些分布式数据库优先保证写可用性（Amazon是不会让“添加到购物车”这种操作出现失败的）。为了达到这个目的，他们在一致性以及几乎所有在传统RDBMS中出现过的特性上做出了妥协。但这家公司其实没有必要优先考虑写可用性，因为他们每天只有一次写入操作，只是数据量比较大。</p>
<p>他们之所以考虑使用Cassandra，是因为PostgreSQL查询需要耗费几分钟的时间。他们认为是硬件的问题，经过排查，我们发现数据表里有5000万条数据，每条数据最多80个字节。如果从SSD上整块地读取所有数据大概需要5秒钟，这个不算快，但比起实际的查询，它要快上两个数量级。</p>
<p>我真的很想多问他们几个问题（了解问题！），在问题变得愈加严重时，我为他们准备了5个方案（列出多个候选方案！），不过很显然，Cassandra对于他们来说完全是一个错误的方案。他们只需要耐心地做一些调优，比如对部分数据重新建模，或许可以考虑使用（当然也有可能没有）其他技术……但一定不是这种写高可用的键值存储系统，Amazon当初创建Cassandra是用来解决他们的购物车问题的！</p>
<h2 id="你不是LinkedIn"><a href="#你不是LinkedIn" class="headerlink" title="你不是LinkedIn"></a>你不是LinkedIn</h2><p>我发现一个学生创办的小公司居然在他们的系统里使用Kafka，这让我感到很惊讶。因为据我所知，他们每天只有很少的事务需要处理——最好的情况下，一天最多只有几百个。这样的吞吐量几乎可以直接记在记事本上。</p>
<p>Kafka被设计用于处理LinkedIn内部的吞吐量，那可是一个天文数字。即使是在几年前，这个数字已经达到了每天数万亿，在高峰时段每秒钟需要处理1000万个消息。不过Kafka也可以用于处理低吞吐量的负载，或许再低10个数量级？</p>
<p>或许工程师们在做决定时确实是基于他们的预期需求，并且也很了解Kafka的适用场景。但我猜测他们是抵挡不住社区对Kafka的追捧，并没有仔细想过Kafka是否适合他们。要知道，那可是10个数量级的差距！</p>
<h2 id="再一次，你不是Amazon"><a href="#再一次，你不是Amazon" class="headerlink" title="再一次，你不是Amazon"></a>再一次，你不是Amazon</h2><p>比Amazon的分布式数据库更为著名的是它的可伸缩架构模式，也就是面向服务架构。Werner Vogels在2006年的一次访谈中指出，Amazon在2001年时就意识到他们的前端需要横向伸缩，而面向服务架构有助于他们实现前端伸缩。工程师们面面相觑，最后只有少数几个工程师着手去做这件事情，而几乎没有人愿意将他们的静态网页拆分成小型的服务。</p>
<p>不过Amazon还是决定向SOA转型，他们当时有7800个员工和30亿美元的销售规模。</p>
<p>当然，并不是说你也要等到有7800个员工的时候才能转向SOA……只是你要多想想，它真的能解决你的问题吗？你的问题的根源是什么？可以通过其他的方式解决它们吗？</p>
<p>如果你告诉我说，你那50个人的公司打算转向SOA，那么我不禁感到疑惑：为什么很多大型的公司仍然在乐此不彼地使用具有模块化的大型单体应用？</p>
<h2 id="甚至Google也不是Google"><a href="#甚至Google也不是Google" class="headerlink" title="甚至Google也不是Google"></a>甚至Google也不是Google</h2><p>使用Hadoop和Spark这样的大规模数据流引擎会非常有趣，但在很多情况下，传统的DBMS更适合当前的负载，有时候数据量小到可以直接放进内存。你是否愿意花10,000美金去购买1TB的内存？如果你有十亿个用户，每个用户仅能使用1KB的内存，所以你的投入远远不够。</p>
<p>或许你的负载大到需要把数据写回磁盘。那么你需要多少磁盘？你到底有多少数据量？Google之所以要创建GFS和MapReduce，是要解决整个Web的计算问题，比如重建整个Web的搜索索引。</p>
<p>或许你已经阅读过GFS和MapReduce的论文，Google的部分问题在于吞吐量，而不是容量，他们之所以需要分布式的存储，是因为从磁盘读取字节流要花费太多的时间。那么你在2017年需要使用多少设备吞吐量？你一定不需要像Google那么大的吞吐量，所以你可能会考虑使用更好的设备。如果都用上SSD会给你增加多少成本？</p>
<p>或许你还想要伸缩性。但你有仔细算过吗，你的数据增长速度会快过SSD降价的速度吗？在你的数据撑爆所有的机器之前，你的业务会有多少增长？截止2016年，Stack Exchange每天要处理2亿个请求，但是他们只用了4个SQL Server，一个用于Stack Overflow，一个用于其他用途，另外两个作为备份复本。</p>
<p>或许你在应用UNPHAT原则之后，仍然决定要使用Hadoop或Spark。或许你的决定是对的，但关键的是你要用对工具。Google非常明白这个道理，当他们意识到MapReduce不再适合用于构建索引之后，他们就不再使用它。</p>
<h2 id="先了解你的问题"><a href="#先了解你的问题" class="headerlink" title="先了解你的问题"></a>先了解你的问题</h2><p>我所说的也不是什么新观点，不过或许UNPHAT对于你们来说已经足够了。如果你觉得还不够，可以听听Rich Hickey的演讲“<a href="https://www.youtube.com/watch?v=f84n5oFoZBc" target="_blank" rel="external">吊床驱动开发</a>”，或者看看Polya的书《<a href="https://www.amazon.com/How-Solve-Mathematical-Princeton-Science/dp/069111966X?ie=UTF8&amp;%2aVersion%2a=1&amp;%2aentries%2a=0" target="_blank" rel="external">How to Solve It</a>》， 或者学习一下Hamming的课程“<a href="https://www.youtube.com/playlist?list=PL2FF649D0C4407B30" target="_blank" rel="external">The Art of Doing Science and Engineering</a>”。我恳请你们一定要多思考！在尝试解决问题之前先对它们有充分的了解。最后送上Polya的一个金句名言：</p>
<blockquote>
<p>回答一个你不了解的问题是愚蠢的，到达一个你不期望的终点是悲哀的。</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[高负载微服务系统的诞生过程]]></title>
      <url>http://icyxp.github.io/blog/2017/07/microservice-%E9%AB%98%E8%B4%9F%E8%BD%BD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%9E%E7%94%9F%E8%BF%87%E7%A8%8B.html</url>
      <content type="html"><![CDATA[<p><img src="/images/ms/02/cover.png" alt="封面"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在2016 <a href="http://highload.co/" target="_blank" rel="external">LighLoad++</a>大会上，“M-Tex”的开发经理Vadim Madison讲述了从一个由数百个微服务组成的系统到包含数千个微服务的高负载项目的发展历程。本文已获得翻译授权，查看英文原文：<a href="https://kukuruku.co/post/microservices-in-a-high-load-project/" target="_blank" rel="external">Microservices in a High-Load Project</a> 。</p>
<p>我将告诉大家我们是如何开始一个高负载微服务项目的。在讲述我们的经历之前，先让我们简单地自我介绍一下。</p>
<p>简单地说，我们从事视频输出方面的工作——我们提供实时的视频。我们负责“NTV-Plus”和“Match TV”频道的视频平台。该平台有30万的并发用户，每小时输出300TB的内容。这是一个很有意思的任务。那么我们是如何做到的呢？</p>
<p>这背后都有哪些故事？这些故事都是关于项目的开发和成长，关于我们对项目的思考。总而言之，是关于如何提升项目的伸缩能力，承受更大的负载，在不宕机和不丢失关键特性的情况下为客户提供更多的功能。我们总是希望能够满足客户的需求。当然，这也涉及到我们是如何实现这一切，以及这一切是如何开始的。</p>
<a id="more"></a>
<p><strong>在最开始，我们有两台运行在Docker集群里的服务器，数据库运行在相同机器的容器里。没有专用的存储，基础设施非常简单。</strong></p>
<p>我们就是这样开始的，只有两台运行在Docker集群里的服务器。那个时候，数据库也运行在同一个集群里。我们的基础设施里没有什么专用的组件，十分简单。<br><img src="/images/ms/02/00.jpg" alt="00"></p>
<p>我们的基础设施最主要的组件就是Docker和TeamCity，我们用它们来交付和构建代码。</p>
<p>在接下来的时期——我称其为我们的发展中期——是我们项目发展的关键时期。我们拥有了80台服务器，并在一组特殊的机器上为数据库搭建了一个单独的专用集群。我们开始使用基于CEPH的分布式存储，并开始思考服务之间的交互问题，同时要更新我们的监控系统。</p>
<p>现在，让我们来看看我们在这一时期都做了哪些事情。Docker集群里已经有数百台服务器，微服务就运行在它们上面。这个时候，我们开始根据数据总线和逻辑分离原则将我们的系统拆分成服务子系统。当微服务越来越多时，我们决定拆分我们的系统，这样维护起来就容易得多（也更容易理解）。<br><img src="/images/ms/02/01.jpg" alt="01"></p>
<p>这张图展示的是我们系统其中的一小部分。这部分系统负责视频剪切。半年前，我在“RIT++”也展示过类似的图片。那个时候只有17个绿色的微服务，而现在有28个绿色的微服务。这些服务只占我们整个系统的二十分之一，所以可以想象我们系统大致的规模有多大。</p>
<h2 id="深入细节"><a href="#深入细节" class="headerlink" title="深入细节"></a>深入细节</h2><p>服务间的通信是一件很有趣的事情。一般来说，我们应该尽可能提升服务间通信效率。我们使用了<a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="external">protobuf</a>，我们认为它就是我们需要的东西。</p>
<p>它看起来是这样的：<br><img src="/images/ms/02/02.jpg" alt="02"></p>
<p>微服务的前面有一个负载均衡器。请求到达前端，或者直接发送给提供了JSON API的服务。protobuf被用于内部服务之间的交互。</p>
<p>protobuf真是一个好东西。它为消息提供了很好的压缩率。现如今有很多框架，只要使用很小的开销就能实现序列化和反序列化。我们可以将其视为有条件的请求类型。</p>
<p>但如果从微服务角度来看，我们会发现，微服务之间也存在某种私有的协议。如果只有一两个或者五个微服务，我们可以为每个微服务打开一个控制台，通过它们来访问微服务，并获得响应结果。如果出现了问题，我们可以对其进行诊断。不过这在一定程度上让微服务的支持工作变得复杂。</p>
<p>在一定时期内，这倒不是什么问题，因为并没有太多的微服务。另外，Google发布了<a href="https://grpc.io/" target="_blank" rel="external">gRPC</a>。在那个时候，gRPC满足了所有我们想做的事情。于是我们逐渐迁移到gRPC。于是我们的技术栈里出现了另一个组件。<br><img src="/images/ms/02/03.jpg" alt="03"></p>
<p>实现的细节也是很有趣的。gRPC默认是基于HTTP/2的。如果你的环境相对稳定，应用程序不怎么发生变更，也不需要在机器间迁移，那么gRCP对于你来说就是个不错的东西。另外，gRPC支持很多客户端和服务器端的编程语言。</p>
<p>现在，我们从微服务角度来看待这个问题。从一方面来看，gRPC是一个好东西，但从另一方面来看，它也有不足之处。当我们开始对日志进行标准化（这样就可以将它们聚合到一个独立的系统里）时，我们发现，从gRPC中抽取日志非常麻烦。</p>
<p>于是，我们决定开发自己的日志系统。它解析消息，并将它们转成我们需要的格式。这样我们才可以获得我们想要的日志。还有一个问题，添加新的微服务会让服务间的依赖变得更加复杂。这是微服务一直存在的问题，这也是除版本问题之外的另一个具有一定复杂性的问题。</p>
<p>于是，我们开始考虑使用JSON。在很长的一段时间里，我们无法相信，在使用了紧凑的二进制协议之后会转回使用JSON。有一天，我们看到一篇文章，来自<a href="http://engineering.dailymotion.com/" target="_blank" rel="external">DailyMotion</a>的一个家伙在文章里提到了同样的事情：“我们知道该如何使用JSON，每个人都可以使用JSON。既然如此，为什么还要自寻烦恼呢？”<br><img src="/images/ms/02/04.jpg" alt="04"></p>
<p>于是，我们逐渐从gRPC转向我们自己实现的JSON。我们保留了HTTP/2，它与JSON组合起来可以带来更快的速度。</p>
<p>现在，我们具备了所有必要的特性。我们可以通过cURL访问我们的服务。我们的QA团队使用<a href="https://www.getpostman.com/" target="_blank" rel="external">Postman</a>，所以他们也感觉很满意。一切都变得简单起来。这是一个有争议性的决定，但却为我们带来了很多好处。</p>
<p>JSON唯一的缺点就是它的紧凑性不足。根据我们的测试结果，它与MessagePack之间有30%的差距。不过对于一个支持系统来说，这不算是个大问题。</p>
<p>况且，我们在转到JSON之后还获得了更多的特性，比如协议版本。有时候，当我们在新版本的协议上使用protobuf时，客户端也必须改用protobuf。如果你有数百个服务，就算只有10%的服务进行了迁移，这也会引起很大的连锁反应。你在一个服务上做了一些变更，就会有十多个服务也需要跟着改动。</p>
<p>因此，我们就会面临这样的一种情况，一个服务的开发人员已经发布了第五个、第六个，甚至第七个版本，但生产环境里仍然在运行第四个版本，就因为其他相关服务的开发人员有他们自己的优先级和截止日期。他们无法持续地更新他们的服务，并使用新版本的协议。所以，新版本的服务虽然发布了，但还派不上用场。然后，我们却要以一种很奇怪的方式来修复旧版本的bug，这让支持工作变得更加复杂。</p>
<p>最后，我们决定停止发布新版本的协议。我们提供协议的基础版本，可以往里面添加少量的属性。服务的消费者开始使用JSON schema。</p>
<p>标准看起来是这样的：<br><img src="/images/ms/02/05.jpg" alt="05"></p>
<p>我们没有使用版本1、2和3，而是只使用版本1和指向它的schema。<br><img src="/images/ms/02/06.jpg" alt="06"></p>
<p>这是从我们服务返回的一个典型的响应结果。它是一个内容管理器，返回有关广播的信息。这里有一个消费者schema的例子。<br><img src="/images/ms/02/07.jpg" alt="07"></p>
<p>最底下的字符串最有意思，也就是”required”那块。我们可以看到，这个服务只需要4个字段——id、content、date和status。如果我们使用了这个schema，那么消费者就只会得到这样的数据。<br><img src="/images/ms/02/08.jpg" alt="08"></p>
<p>它们可以被用在每一个协议版本里，从第一个版本到后来的每一个变更版本。这样，在版本之间迁移就容易很多。在我们发布新版本之后，客户端的迁移就会简单很多。</p>
<p>下一个重要的议题是系统的稳定性问题。这是微服务和其他任何一个系统都需要面临的问题（在微服务架构里，我们可以更强烈地感觉到它的重要性）。系统总会在某个时候变得不稳定。</p>
<p>如果服务间的调用链只包含了一两个服务，那么就没有什么问题。在这种情况下，你看不出单体和分布式系统之间有多大区别。但当调用链里包含了5到7个调用，那么问题就会接踵而至。你根本不知道为什么会这样，也不知道能做些什么。在这种情况下，调试会变得很困难。在单体系统里，你可以通过逐步调试来找出错误。但对于微服务来说，网络不稳定性或高负载下的性能不稳定性也会对微服务造成影响。特别是对于拥有大量节点的分布式系统来说，这些情况就更加显而易见了。<br><img src="/images/ms/02/09.jpg" alt="09"></p>
<p>在一开始，我们采用了传统的办法。我们监控所有的东西，查看问题和问题的发生点，然后尝试尽快修复它们。我们将微服务的度量指标收集到一个独立的数据库里。我们使用<a href="https://github.com/python-diamond/Diamond" target="_blank" rel="external">Diamond</a>来收集系统度量指标。我们使用<a href="https://github.com/google/cadvisor" target="_blank" rel="external">cAdvisor</a>来分析容器的资源使用情况和性能特征。所有的结果都被保存到<a href="https://github.com/influxdata/influxdb" target="_blank" rel="external">InfluxDB</a>，然后我们在Grafana里创建仪表盘。<br><img src="/images/ms/02/10.jpg" alt="10"></p>
<p>于是，我们现在的基础设施里又多了三个组件。</p>
<p>我们比以往更加关注所发生的一切。我们对问题的反应速度更快了。不过，这并没有阻止问题的出现。</p>
<p>奇怪的是，微服务架构的主要问题出在那些不稳定的服务上。它们有的今天运行正常，明天就不行，而且有各种各样的原因。如果服务出现超载，而你继续向它发送负载，它就会宕机一段时间。如果它在一段时间不提供服务，负载就会下降，然后它就又活过来了。这类系统很难维护，也很难知道到底出了什么问题。</p>
<p>最后，我们决定把这些服务停掉，而不是让它们来回折腾。我们因此需要改变服务的实现方式。</p>
<p>我们做了一件很重要的事情。我们对每个服务接收的请求数量设定了一个上限。每个服务知道自己可以处理多少个来自客户端的请求（我们稍后会详细说明）。如果请求数量达到上限，服务将抛出503 Service Unavailable异常。客户端知道这个节点无法提供服务，就会选择另一个节点。</p>
<p>当系统出现问题时，我们就可以通过这种方式来减少请求时间。另外，我们也提升了服务的稳定性。</p>
<p>我们引入了第二种模式——<strong>回路断路器</strong>（Circuit Breaker）。我们在客户端实现了这种模式。</p>
<p>假设有一个服务A，它有4个可以访问的服务B的实例。它向注册中心索要服务B的地址：“给我这些服务的地址”。它得到了服务B的4个地址。服务A向第一个服务B的实例发起了请求。第一个服务B实例正常返回响应。服务A将其标记为可访问：“是的，我可以访问它”。然后，服务A向第二个服务B实例发起请求，不过它没有在期望的时间内得到响应。我们禁用了这个实例，然后向下一个实例发起请求。下一个实例因为某些原因返回了不正确的协议版本。于是我们也将其禁用，然后转向第四个实例。</p>
<p>总得来说，只有一半的服务能够为客户端提供服务。于是服务A将会向能够正常返回响应的两个服务发起请求。而另外两个无法满足要求的实例被禁用了一段时间。</p>
<p>我们通过这种方式来提升性能的稳定性。如果服务出现了问题，我们就将其关闭，并发出告警，然后尝试找出问题所在。</p>
<p>因为引入了回路断路器模式，我们的基础设施里又多了一个组件——<a href="https://github.com/Netflix/Hystrix" target="_blank" rel="external">Hystrix</a>。<br><img src="/images/ms/02/11.jpg" alt="11"></p>
<p>Hystrix不仅实现了回路断路器模式，它也有助于我们了解系统里出现了哪些问题：<br><img src="/images/ms/02/12.jpg" alt="12"></p>
<p>圆环的大小表示服务与其他组件之间的流量大小。颜色表示系统的健康状况。如果圆环是绿色的，那么说明一切正常。如果圆环是红色的，那么就有问题了。</p>
<p>如果一个服务应该被停掉，那么它看起来是这个样子的。圆环是打开的。<br><img src="/images/ms/02/13.jpg" alt="13"></p>
<p>我们的系统变得相对稳定。每个服务至少都有两个可用的实例，这样我们就可以选择停掉其中的一个。不过，尽管是这样，我们仍然不知道我们的系统究竟发生了什么问题。在处理请求期间如果出现了问题，我们应该怎样才能知道问题的根源是什么呢？</p>
<p>这是一个标准的请求：<br><img src="/images/ms/02/14.jpg" alt="14"></p>
<p>这是一个处理链条。用户发送请求到第一个服务，然后是第二个。从第二个服务开始，链条将请求发送到第三个和第四个服务。</p>
<p>然后一个分支不明原因地消失了。在经历了这类场景之后，我们尝试着提升这种场景的可见性，于是我们找到了Appdash。Appdash是一个跟踪服务。<br><img src="/images/ms/02/16.jpg" alt="16"></p>
<p>它看起来是这个样子的：<br><img src="/images/ms/02/17.jpg" alt="17"></p>
<p>可以这么说，我们只是想尝试一下，看看它是否适合我们。将它用在我们的系统里是一件很容易的事情，因为我们那个时候使用的是Go语言。Appdash提供了一个开箱即用的包。我们认为Appdash是一个好东西，只是它的实现并不是很适合我们。<br><img src="/images/ms/02/18.jpg" alt="18"></p>
<p>于是，我们决定使用<a href="http://zipkin.io/" target="_blank" rel="external">Zipkin</a>来代替Appdash。Zipkin是由Twitter开源的。它看起来是这个样子的：<br><img src="/images/ms/02/19.jpg" alt="19"></p>
<p>我认为这样会更清楚一些。我们可以从中看到一些服务，也可以看到我们的请求是如何通过请求链的，还可以看到请求在每个服务里都做了哪些事情。一方面，我们可以看到服务的总时长和每个分段的时长，另一方面，我们完全可以添加描述服务内容的信息。</p>
<p>我们可以在这里添加一些与数据库的调用、文件系统的读取、缓存的访问有关的信息，这样就可以知道请求里哪一部分使用了最多的时间。TraceID可以帮助我们做到这一点。稍后我会介绍更多细节。<br><img src="/images/ms/02/20.jpg" alt="20"></p>
<p>我们就是通过这种方式知道请求在处理过程中发生了什么问题，以及为什么有时候无法被正常处理。刚开始一切都正常，然后突然间，其中的一个出现了问题。我们稍作排查，就知道出问题的服务发生了什么。<br><img src="/images/ms/02/21.jpg" alt="21"></p>
<p>不久前，一些厂商推出了一个跟踪系统的标准。为了简化系统的实现，主要的几个跟踪系统厂商在如何设计客户端API和客户端类库上达成了一致。现在已经有了<a href="http://opentracing.io/" target="_blank" rel="external">OpenTracing</a>的实现，支持几乎所有的主流开发语言。现在就可以使用它了。</p>
<p>我们已经有办法知道那些突然间崩溃的服务。我们可以看到其中的某部分在垂死挣扎，但是不知道为什么。光有环境信息是不够的，</p>
<p>我们还需要日志。是的，这应该成为标准的一部分，它就是Elasticsearch、Logstash和Kibana（ELK）。不过我们对它们做了一些改动。<br><img src="/images/ms/02/22.jpg" alt="22"></p>
<p>我们并没有将大量的日志直接通过forward传给Logstash，而是先传给syslog，让它把日志聚合到构建机器上，然后再通过forward导入到<strong>Elasticsearch</strong>和<strong>Kibana</strong>。这是一个很标准的流程，那么巧妙的地方在哪里呢？<br><img src="/images/ms/02/23.jpg" alt="23"></p>
<p>巧妙的是，我们可以在任何可能的地方往日志里加入Zipkin的TraceID。</p>
<p>这样一来，我们就可以在Kibana仪表盘上看到完整的用户请求执行情况。也就是说，一旦服务进入生产环境，就为运营做好了准备。它已经通过了自动化测试，如果有必要，QA可以再进行手动检查。它应该没有什么问题。如果它出现了问题，那说明有一些先决条件没有得到满足。日志里详细地记录了这些先决条件，通过过滤，我们可以看到某个请求的跟踪信息。我们因此可以快速地查出问题的根源，为我们节省了很多时间。</p>
<p>我们后来引入了动态调试模式。现在的日志数量还不是很大，大概只有100 GB到150 GB，我记不太清楚具体数字了。不过，这些日志是在正常的日志模式下生成的。如果我们添加更多的细节，那么日志就可能变成TB级别的，处理起来就很耗费资源。</p>
<p>当我们发现某些服务出现问题，就打开调试模式（通过一个API），看看发生了什么事情。有时候，我们找到出现问题的服务，在不将它关闭的情况下打开调试模式，尝试找出问题所在。</p>
<p>最后，我们在ELK端查找问题。我们还对关键服务的错误进行聚合。服务知道哪些错误是关键性的，哪些不是关键性的，然后将它们传给<a href="https://sentry.io/" target="_blank" rel="external">Sentry</a>。<br><img src="/images/ms/02/24.jpg" alt="24"></p>
<p>Sentry能够智能地收集错误日志，并形成度量指标，还会进行一些基本的过滤。我们在很多服务上使用了Sentry。我们从单体应用时期就开始使用它了。</p>
<p>那么最有趣的问题是，我们是如何进行伸缩的？这里需要先介绍一些概念。我们把每个机器看成一个黑盒。<br><img src="/images/ms/02/25.jpg" alt="25"></p>
<p>我们有一个编排系统，最开始使用<a href="https://www.nomadproject.io/" target="_blank" rel="external">Nomad</a>。确切地说，应该是<a href="https://www.ansible.com/" target="_blank" rel="external">Ansible</a>。我们自己编写脚本，但光是这些还不能满足要求。那个时候，Nomad的某些版本可以简化我们的工作，于是我们决定迁移到Nomad。<br><img src="/images/ms/02/26.jpg" alt="26"></p>
<p>同时还使用了<a href="https://www.consul.io/" target="_blank" rel="external">Consul</a>，将它作为服务发现的注册中心。还有Vault，用于存储敏感数据，比如密码、秘钥和其他所有不能保存在Git上的东西。</p>
<p>这样，所有的机器几乎都变得一模一样。每个机器上都安装了<strong>Docker</strong>，还有<strong>Consul</strong>和<strong>Nomad</strong>代理。总的来说，每一个机器都处于备用状态，可以在任何时候投入使用。如果不用了，我们就让它们下线。如果你构建了云平台，你就可以先准备好机器，在高峰期时将它们打开，在负载下降时将它们关闭。这会节省大量的成本。<br><img src="/images/ms/02/27.jpg" alt="27"></p>
<p>后来，我们决定从<strong>Nomad</strong>迁移到<a href="https://github.com/kubernetes/kubernetes" target="_blank" rel="external">Kubernetes</a>，<strong>Consul</strong>也因此成为了集中式的配置系统。</p>
<p>这样一来，部分栈可以进行自动伸缩。那么我们是怎么做的呢？</p>
<p>第一步，我们对内存、CPU和网络进行限制。<br><img src="/images/ms/02/28.jpg" alt="28"></p>
<p>我们分别将这三个元素分成三个等级，砍掉其中的一部分。例如，<br><img src="/images/ms/02/29.jpg" alt="29"></p>
<p>R3-C2-N1，我们已经限定只给某个服务一小部分网络流量、多一点点的CPU和更多的内存。这个服务真的很耗费资源。</p>
<p>我们在这里使用了助记符，我们的决策服务可以设置很多的组合值，这些值看起来是这样的：<br><img src="/images/ms/02/30.jpg" alt="30"></p>
<p>事实上，我们还有C4和R4，不过它们已经超出了这些标准的限制。标准看起来是这样的：<br><img src="/images/ms/02/31.jpg" alt="31"></p>
<p>下一步开始做一些预备工作。我们先确定服务的伸缩类型。</p>
<p>独立的服务最容易伸缩，它可以进行线性地伸缩。如果用户增长了两倍，我们就运行两倍的服务实例。这就万事大吉了。</p>
<p>第二种伸缩类型：服务依赖了外部的资源，比如那些使用了数据库的服务。数据库有它自己的容量上限，这个一定要注意。你还要知道，如果系统性能出现衰退，就不应该再增加更多的实例，而且你要知道这种情况会在什么时候发生。</p>
<p>第三种情况是，服务受到外部系统的牵制。例如，外部的账单系统。就算运行了100个服务实例，它也没办法处理超过500个请求。我们要考虑到这些限制。在确定了服务类型并设置了相应的标记之后，是时候看看它们是如何通过我们的构建管道的。<br><img src="/images/ms/02/32.jpg" alt="32"></p>
<p>我们在CI服务器上运行了一些单元测试，然后在测试环境运行集成测试，我们的QA团队会对它们做一些检查。在这之后，我们就进入了预生产环境的负载测试。<br><img src="/images/ms/02/33.jpg" alt="33"></p>
<p>如果是第一种类型的服务，我们使用一个实例，并在这个环境里运行它，给它最大的负载。在运行了几轮之后，我们取其中的最小值，将它存入<strong>InfluxDB</strong>，将它作为该服务的负载上限。</p>
<p>如果是第二种类型的服务，我们逐渐加大负载，直到出现了性能衰退。我们对这个过程进行评估，如果我们知道该系统的负载，那么就比较当前负载是否已经足够，否则，我们就会设置告警，不会把这个服务发布到生产环境。我们会告诉开发人员：“你们需要分离出一些东西，或者加进去另一个工具，让这个服务可以更好地伸缩。”<br><img src="/images/ms/02/34.jpg" alt="34"></p>
<p>因为我们知道第三种类型服务的上限，所以我们只运行一个实例。我们也会给它一些负载，看看它可以服务多少个用户。如果我们知道账单系统的上限是1000个请求，并且每个服务实例可以处理200个请求，那么就需要5个实例。</p>
<p>我们把这些信息都保存到了InfluxDB。我们的决策服务开始派上用场了。它会检查两个边界：上限和下限。如果超出了上限，那么就应该增加服务实例。如果超出下限，那么就减少实例。如果负载下降（比如晚上的时候），我们就不需要这么多机器，可以减少它们的数量，并关掉一部分机器，省下一些费用。</p>
<p>整体看起来是这样的：<br><img src="/images/ms/02/35.jpg" alt="35"></p>
<p>每个服务的度量指标表明了它们当前的负载。负载信息被保存到InfluxDB，如果决策服务发现服务实例达到了上限，它会向Nomad和Kubernetes发送命令，要求增加服务实例。有可能在云端已经有可用的实例，或者开始做一些准备工作。不管怎样，发出要求增加新服务实例的告警才是关键所在。</p>
<p>一些受限的服务如果达到上限，也会发出相关的告警。对于这类情况，我们除了加大等待队列，也做不了其他什么事情。不过最起码我们知道我们很快就会面临这样的问题，并开始做好应对措施。</p>
<p>这就是我想告诉大家有关伸缩性方面的事情。除了这些，还有另外一个东西——<a href="https://about.gitlab.com/gitlab-ci/" target="_blank" rel="external">Gitlab CI</a>。<br><img src="/images/ms/02/36.jpg" alt="36"></p>
<p>我们一般是通过TeamCity来开发服务的。后来，我们意识到，所有的服务都有一个共性，这些服务都是不一样的，并且知道自己该如何部署到容器里。要生成这么的项目真的很困难，不过如果使用yml文件来描述它们，并把这个文件与服务放在一起，就会方便很多。虽然我们只做了一些小的改变，不过却为我们带来了非常多的可能性。</p>
<p>现在，我想说一些一直想对自己说的话。</p>
<p>关于微服务开发，我建议在一开始就使用<strong>编排系统</strong>。可以使用最简单的编排系统，比如Nomad，通过nomad agent -dev命令启动一个编排系统，包括Consul和其他东西。</p>
<p>我们仿佛是在一个黑盒子工作。你试图避免被绑定到某台特定的机器上，或者被附加到某台特定机器的文件系统上。这些事情会让你开始重新思考。</p>
<p>在开发阶段，<strong>每个服务至少需要两个实例</strong>，如果其中一个出现问题，就可以关掉它，由另一个接管继续服务。</p>
<p>还有一些有关架构的问题。在微服务架构里，<strong>消息总线</strong>是一个非常重要的组件。</p>
<p>假设你有一个用户注册系统，那么如何以最简单的方式实现它呢？对于注册系统来说，需要创建账户，然后在账单系统里创建一个用户，并为他创建头像和其他东西。你有一组服务，其中的超级服务收到了一个请求，它将请求分发给其他服务。经过几次之后，它就知道该触发哪些服务来完成注册。</p>
<p>不过，我们可以使用一种更简单、更可靠、更高效的方式来实现。我们使用一个服务来处理注册，它注册了一个用户，然后发送一个事件到消息总线，比如“我已经注册了一个yoghurt，ID是……”。相关的服务会收到这个事件，其中的一个服务会在账单系统里创建一个账户，另一个服务会发送一封欢迎邮件。</p>
<p>不过，系统会因此失去强一致性。这个时候你没有超级服务，也不知道每个服务的状态。不过，这样的系统很容易维护。</p>
<p>现在，我再说一些之前提到过的问题。<strong>不要试图修复</strong>出问题的服务。如果某些服务实例出现了问题，将它找出来，然后把流量定向到其他服务实例（可能是新增的实例）上，然后再诊断问题。这样可以显著提升系统的可用性。</p>
<p>通过<strong>收集度量指标</strong>来了解系统的状态自然不在话下。</p>
<p>不过要注意，如果你对某个度量指标不了解，不知道怎么使用它，或者它对你来说没有什么意义，就不要收集它。因为有时候，这样的度量指标会有数百万个。你在这些无用的度量指标上面浪费了很多资源和时间。这些是无效的负载。</p>
<p>如果你认为你需要某些度量指标，那么就收集它们。如果不需要，就不要收集。</p>
<p>如果你发现了一个问题，不要急着去修复。在很多情况下，<strong>系统会对此作出反应</strong>。当系统需要你采取行动的时候，它会给你发出告警。如果它不要求你在半夜跑去修复问题，那么它就不算是一个告警。它只不过是一种警告，你可以在把它当成一般的问题来处理。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[微服务，够了]]></title>
      <url>http://icyxp.github.io/blog/2017/07/microservice-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%A4%9F%E4%BA%86.html</url>
      <content type="html"><![CDATA[<p><img src="/images/ms/01/cover.png" alt="01"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  资深架构师<a href="https://twitter.com/aadrake" target="_blank" rel="external">Adam Drake</a>在他的博客上分享了他对微服务的看法,他 从自己的经验出发,结合Martin Fowler对微服务的见解,帮助想要采用 微服务的公司重新审视微服务。以下内容已获得作者翻译授权,查看英文 原文 <a href="https://aadrake.com/posts/2017-05-20-enough-with-the-microservices.html" target="_blank" rel="external">Enough with the microservices</a>。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  关于微服务的优势和劣势已经有过太多的讨论,不过我仍然看到很多 成长型初创公司对它进行着盲目崇拜。冒着“重复发明轮子”的风险(Martin Fowler已经写过“Microservice Premium”的文章),我想把我的一些 想法写下来,在必要的时候可以发给客户,也希望能够帮助人们避免犯下我之前见过的那些错误。在进行架构或技术选型时,将网络上找到的一些所谓的最佳实践文章作为指南,一旦做出了错误的决定,就要付出惨重的代价。如果能够帮助哪怕一个公司避免犯下这种错误,那么写这篇文章都是值得的。</p>
<p>  如今微服务是个热门技术,微服务架构一直以来都存在(面向服务架构也算是吧?),但对于我所见过的大部分公司来说,微服务不仅浪费了他们的时间,分散了他们的注意力,而且让事情变得更糟糕。</p>
<p>  这听起来似乎很奇怪,因为大部分关于微服务的文章都会肯定微服务 的各种好处,比如解耦系统、更好的伸缩性、消除开发团队之间的依赖, 等等。如果你的公司有 Uber、Airbnb、Facebook 或 Twitter 那样的规模, 那么就没有什么问题。我曾经帮助一些大型组织转型到微服务架构,包括 搭建消息系统和采用一些能够提升伸缩性的技术。不过,对于成长型初创 公司来说,很少需要这些技术和服务。</p>
<p>  Russ Miles在他的《<a href="http://www.russmiles.com/essais/8-ways-to-lose-at-microservices-adoption" target="_blank" rel="external">让微服务失效的八种方式</a>》这篇文章中表达了 他的首要观点,而在我看来,这些场景却到处可见。成长型初创公司总是 想模仿那些大公司的最佳实践,用它们来弥补自身的不足。但是,最佳实 践是要视情况而定的。有些东西对于 Facebook 来说是最佳实践,但对于 只有不到百人的初创公司来说,它们就不一定也是最佳实践。</p>
<p>  如果你的公司比那些大公司小一些,在一定程度上你仍然能够从微服务架构中获益。但是,对于成长型初创公司来说,大规模地迁移到微服务是一种过错,而且对技术人来说是不公平的。</p>
<a id="more"></a>
<h2 id="为什么选择微服务"><a href="#为什么选择微服务" class="headerlink" title="为什么选择微服务?"></a>为什么选择微服务?</h2><p>  一般来说,成长型初创公司采用微服务架构最主要的目的为了减少或消除开发团队之间的依赖,或者提升系统处理大流量负载的能力(比如伸缩性)。开发人员经常抱怨的问题和常见的症状包括合并冲突、由未完整实现的功能引起的部署错误以及伸缩性问题。接下来让我们逐个说明这些问题。</p>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>  在初创公司的早期阶段,开发团队规模不大,使用的技术也很简单。人们在一起工作,不会出现混乱,要实现一些功能也比较快。一切看起来都很美好。</p>
<p>  随着公司的不断发展,开发团队也在壮大,代码库也在增长,然后就出现了多个团队在同一个代码库上工作的情况。这些团队的大部分成员都是公司早期的员工。因为初创公司的早期员工一般都是初级开发人员,他们并没有意识到一个问题,那就是在团队规模增长和代码库增长的同时,沟通效率也需要随之提升。对于缺乏经验的技术人员来说,他们倾向于通过技术问题来解决人的问题,并希望通过微服务来减少开发团队之间的依赖和耦合。</p>
<p>  实际上,他们真正需要做的是通过有效的沟通来解决人的问题。当一个初创公司有多个开发团队时,团队之间需要协调,团队成员需要知道每个人都在做什么,他们需要协作。在这样规模的企业里,软件开发其实具备了社交的性质。如果团队之间缺乏沟通或者缺乏信息分享,不管用不用微服务,一样存在依赖问题,而就算使用了微服务,也仍然存在负面的技术问题。</p>
<p>  将代码模块化作为解决这个问题的技术方案,确实能够缓解软件开发固有的团队依赖问题,但团队间的沟通仍然要随着团队规模的增长而不断改进。</p>
<p>  不要混淆了解耦和分布式二者的含义。由模块和接口组成的单体可以帮助你达到解耦的目的,而且你也应该这么做。你没有必要把应用程序拆分成分布式的多个独立服务,在模块间定义清晰的接口同样能达到解耦的目的。</p>
<h2 id="部分功能实现"><a href="#部分功能实现" class="headerlink" title="部分功能实现"></a>部分功能实现</h2><p>  微服务里需要用到<a href="https://en.wikipedia.org/wiki/Feature_toggle" target="_blank" rel="external">功能标志</a> (feature flag),微服务开发人员需要熟悉这种技术。特别是在进行快速开发(下面会深入讨论)的时候,你可能需要部署一些功能,这些功能在某些平台上还没有实现,或者前端已经完全实现,但后端还没有。随着公司的发展,部署和运维系统变得越来越自动化和复杂,功能标志也变得越来越重要。</p>
<h2 id="水平伸缩"><a href="#水平伸缩" class="headerlink" title="水平伸缩"></a>水平伸缩</h2><p>  通过部署同一个微服务的多个实例来获得伸缩性,这是微服务的优点 之一。不过,大多数过早采用微服务的公司在这些微服务背后使用了同一 个存储系统。也就是说,这些服务具备了伸缩性,但整个应用并不具备伸 缩性。如果你正打算使用这样的伸缩方式,那为什么不直接在负载均衡器 后面部署多个单体实例呢?你可以用更简单的方式达到相同的目的。再者, 水平伸缩应该被作为杀手锏来使用。你首先要关注的应该是如何提升应用 程序的性能。一些简单的优化常常能带来数百倍的性能提升,这里也包括 如何正确地使用其他服务。例如,我在一篇博文里提到的<a href="https://aadrake.com/posts/2017-05-15-redis-performance-triage-handbook.html" target="_blank" rel="external">Redis性能诊断</a>。</p>
<h2 id="我们为微服务做好准备了吗"><a href="#我们为微服务做好准备了吗" class="headerlink" title="我们为微服务做好准备了吗?"></a>我们为微服务做好准备了吗?</h2><p>  在讨论架构选型时,人们经常会忽略这个问题,但其实却是最重要的。 高级技术人员在了解了开发人员或业务人员的抱怨或痛点之后,在网上找 寻找解决方案,他们总是宣称能解决这些问题。但在这些信誓旦旦的观点 背后,有很多需要注意的地方。微服务有利也有弊。如果你的企业足够成 熟,并且具有一定的技术积累,那么采用微服务所面临的挑战会小很多, 并且能够带来更多正面好处。那么怎样才算已经为微服务做好准备了呢? Martin Fowler在多年前表达了他对<a href="https://martinfowler.com/bliki/MicroservicePrerequisites.html" target="_blank" rel="external">微服务先决条件</a>的看法,但是从我的 经验来看,大多数成长型初创公司完全忽略了他的观点。Martin 的观点 是一个很好的切入点,让我们来逐个说明。</p>
<p>  我敢说,大部分成长型初创公司几乎连一个先决条件都无法满足,更不用说满足所有的条件了。如果你的技术团队不具备快速配置、部署和监控能力,那么在迁移到微服务前必须先获得这些能力。接下来让我们更详细地讨论这些先决条件。</p>
<h2 id="快速配置"><a href="#快速配置" class="headerlink" title="快速配置"></a>快速配置</h2><p>  如果你的开发团队里只有少数几个人可以配置新服务、虚拟环境或其 他配套设施,那说明你们还没有为微服务做好准备。你的每个团队里都应 该要有几个这样的人,他们具备了配置基础设施和部署服务的能力,而且 不需要求助于外部。要注意,光是有一个 DevOps 团队并不意味着你在实 施 DevOps。开发人员应该参与管理与应用程序相关的组件,包括基础设施。</p>
<p>  类似的,如果你没有灵活的基础设施(易于伸缩并且可以由团队里的不同人员来管理)来支撑当前的架构,那么在迁移到微服务前必须先解决这个问题。你当然可以在裸机上运行微服务,以更低的成本获得出众的性能,但在服务的运维和部署方面也必须具备灵活性。</p>
<h2 id="基本的监控"><a href="#基本的监控" class="headerlink" title="基本的监控"></a>基本的监控</h2><p>  如果你不曾对你的单体应用进行过性能监控,那么在迁移到微服务时, 你的日子会很难过。你需要熟悉系统级别的度量指标(比如 CPU 和内存)、 应用级别的度量指标(比如端点的请求延迟或端点的错误)和业务级别的 度量指标(比如每秒事务数或每秒收益),这样才可以更好地理解系统的 性能。在性能方面,微服务生态系统比单体系统要复杂得多,就更不用提 诊断问题的复杂性了。你可以搭建一个监控系统(如 Prometheus),在 将单体应用拆分成微服务之前对应用做一些增强,以便进行监控。</p>
<h2 id="快速部署"><a href="#快速部署" class="headerlink" title="快速部署"></a>快速部署</h2><p>  如果你的单体系统没有一个很好的持续集成流程和部署系统,那么要 集成和部署好你的微服务几乎是件不可能的事。想象一下这样的场景:10 个团队和 100 个服务,它们都需要进行手动测试和部署,然后再将这些工 作与测试和部署一个单体所需要的工作进行对比。100 个服务会出现多少 种问题?而单体系统呢?这些先决条件很好地说明了微服务的复杂性。</p>
<p>  Phil Calcado在Fowler的先决条件清单里添加了一些东西,不过我 认为它们更像是重要的扩展,而不是真正的先决条件。</p>
<h2 id="如果我们具备了这些先决条件呢"><a href="#如果我们具备了这些先决条件呢" class="headerlink" title="如果我们具备了这些先决条件呢?"></a>如果我们具备了这些先决条件呢?</h2><p>  就算具备了这些条件,仍然需要注意微服务的负面因素,确保微服务能够为你的业务带来真正的价值。事实上,很多技术人员对微服务中存在的<a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="external">分布式计算谬论</a>视而不见,但为了确保能够成功,这些问题是必须要考虑到的。对于大部分成长型初创公司来说,基于各种原因,他们应该避免使用微服务。</p>
<h2 id="运营成本的增加"><a href="#运营成本的增加" class="headerlink" title="运营成本的增加"></a>运营成本的增加</h2><p>  快速部署这一先决条件已经涵盖了一部分成本,除此之外,对 微服务进行容器化(可能使用 Docker)和使用容器编排系统(比如 Kubernetes)也需要耗费很多成本。Docker 和 Kubernetes 都是很优秀的 技术,但是对于大部分成长型初创公司来说,它们都是一种负担。我见过 初创公司使用 rsync 作为部署和编排工具,我也见过很多的初创公司陷入 运维工具的复杂性泥潭里,他们因此浪费了很多时间,而这些时间本来可 以用于为用户开发更多的功能。</p>
<h2 id="你的应用会被拖慢"><a href="#你的应用会被拖慢" class="headerlink" title="你的应用会被拖慢"></a>你的应用会被拖慢</h2><p>  如果你的单体系统里包含了多个模块,并且在模块间定义了良好的 API,那么 API 之间的交互就几乎没有什么额外开销。但对于微服务来说 就不是这么一回事了,因为它们一般运行在不同的机器上,它们之间需要 通过网络进行交互。这样会在一定程度上拖慢整个系统。如果一个请求需 要多个服务进行同步交互,那么情况会变得更加糟糕。我曾经工作过的一 个公司,他们需要调用将近 10 个服务才能处理完某些请求。处理请求的 每一个步骤都需要额外的网络开销和延迟,但实际上,他们可以把这些服 务放在单个软件包里,按照不同的模块来区分,或者把它们设计成异步的。这样可以为他们节省大量的基础设施成本。</p>
<h2 id="本地开发变得更加困难"><a href="#本地开发变得更加困难" class="headerlink" title="本地开发变得更加困难"></a>本地开发变得更加困难</h2><p>  如果你有一个单体应用,后端只有一个数据库,那么在开发过程中, 在本地运行这个应用是很容易的。如果你有 100 个服务,并使用了多个数 据存储系统,而且它们之间互相依赖,那么本地开发就会变成一个噩梦。即使是 Docker 也无法把你从这种复杂性泥潭中拯救出来。虽然事情原本 可以简单一些,不过仍然需要处理依赖问题。理论上说,微服务不存在这 些问题,因为微服务被认为是相互独立的。不过,对于成长型初创公司来说,就不是这么一回事了。技术人员一般需要在本地运行所有(或者几乎 所有)的服务才能进行新功能的开发和测试。这种复杂性是对资源的巨大浪费。</p>
<h2 id="难以伸缩"><a href="#难以伸缩" class="headerlink" title="难以伸缩"></a>难以伸缩</h2><p>  对单体系统进行伸缩的最简单方式是在负载均衡器后面部署单体系 统的多个实例。在流量增长的情况下,这是一种非常简单的伸缩方式, 而且从运维角度来讲,它的复杂性是最低的。你的系统在编排平台(如 <a href="https://aws.amazon.com/cn/elasticbeanstalk/" target="_blank" rel="external">Elastic Beanstalk</a>)上运行的时间越长越好,你和你的团队就可以集中 精力开发客户需要的东西,而不是忙于解决部署管道问题。使用合适的 CI/CD 系统可以缓解这个问题,但在微服务生态系统里,事情要复杂得多, 而且这些复杂性所造成的麻烦已经超过了它们所能带来的好处。</p>
<h2 id="然后呢"><a href="#然后呢" class="headerlink" title="然后呢?"></a>然后呢?</h2><p>  如果你刚好身处一个成长型初创公司里,需要对架构做一些调整,而微服务似乎不能解决你的问题,这个时候应该怎么办?</p>
<p>  Fowler 提出的先决条件可以说是技术领域的<a href="https://en.wikipedia.org/wiki/Capability_Maturity_Model" target="_blank" rel="external">能力成熟度模型</a>, Fowler 在他的文章里对成熟度模型进行过介绍。如果这种成熟度模型对于公司来说是说得通的,那么我们可以按照 Fowler 提出的先决条件,并使用其他的一些中间步骤为向微服务迁移做好准备。下面的内容引用自 Fowler 的文章。</p>
<p>  关键是你要认识到,成熟度模型的评估结果并不代表你的当前水平,它们只是在告诉你需要做哪些工作才能朝着改进的目标前进。你当前的水平只是一种中间工作,用于确定下一步该获得什么样的技能。</p>
<p>  那么,我们该做出怎样的改进,以及如何达成这些目标?我们需要经过一些简单的步骤,其中前面两步就可以解决很多在向微服务迁移过程中会出现的问题,而且不会带来相关的复杂性。</p>
<ul>
<li>清理应用程序。确保应用程序具有良好的自动化测试套件,并使 用了最新版本的软件包、框架和编程语言。</li>
<li>重构应用程序,把它拆分成多个模块,为模块定义清晰的API。不 要让外部代码直接触及模块内部,所有的交互都应该通过模块提 供的API来进行。</li>
<li>从应用程序中选择一个模块,并把它拆分成独立的应用程序,部 署在相同的主机上。你可以从中获得一些好处,而不会带来太多 的运维麻烦。不过,你仍然需要解决这两个应用之间的交互问 题,虽然它们都部署在同一个主机上。不过你可以无视微服务架 构里固有的网络分区问题和分布式系统的可用性问题。</li>
<li>把独立出来的模块移动到不同的主机上。现在,你需要处理跨网 络交互问题,不过这样可以让这两个系统之间的耦合降得更低。</li>
<li>如果有可能,可以重构数据存储系统,让另一个主机上的模块负 责自己的数据存储。</li>
</ul>
<p>在我所见过的公司里,如果他们能够完成前面两个步骤就算万事大吉了。如果他们能够完成前面两个步骤,那么剩下的步骤一般不会像他们最初想象的那么重要了。如果你决定在这个过程的某个点上停下来,而系统仍然具有可维护性和比刚开始时更好的状态,那么就再好不过了。</p>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>  我不能说这些想法都是独一无二的,也不能说是我所独有的。我只是 从其他遭遇了相同问题的人那里收集想法,并连同观察到的现象在这里作 了一次总结。还有其他很多比我更有经验的人也写过这方面的文章,他们 剖析地更加深入,比如Sander Mak写的有关模块和<a href="https://www.oreilly.com/ideas/modules-vs-microservices" target="_blank" rel="external">微服务的文章</a>。不管 怎样,对于正在考虑对他们的未来架构做出调整的公司来说,这些经验都 是非常重要的。认真地思考每一个问题,确保微服务对你们的组织来说是 一个正确的选择。</p>
<p>  最起码在完成了上述的前面两个步骤之后,再慎重考虑一下微服务对于你的组织来说是否是正确的方向。你之前的很多问题可能会迎刃而解。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL性能监控慢日志分析利器]]></title>
      <url>http://icyxp.github.io/blog/2017/07/mysql-mysql-performance-monitoring.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>入题之前先讲讲为什么写这篇文章，这就不得不提起MySQL与percona，阿里基于mysql开发了AliSQL，写这篇文章的时候阿里已经将其开源，percona是一家领先的MySQL咨询公司，该公司基于mysql开发了Percona Server，Percona Server是一款独立的数据库产品，为用户提供了换出其MySQL安装并换入Percona Server产品的能力。percona除了开发了多款数据库产品，还开发了数据库监控程序：pmm（Percona Monitoring and Management）服务器，我们都知道mysql自身缺乏实时的监控功能，而此时pmm-server就恰好解决了我们这一难题，好了废话不多说，先看一张pmm server的监控图。<br><img src="/images/mysql_pmm_1.png" alt="PMM"></p>
<p>常规的监测项目都有了，最吸引我的一点在于它的慢日志分析功能，如下图所示：<br><img src="/images/mysql_pmm_2.png" alt="PMM"></p>
<a id="more"></a>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="1-在vmware或者virtualbox上安装ubuntu14-04-Server镜像，可以选择清华大学的镜像，下载速度快"><a href="#1-在vmware或者virtualbox上安装ubuntu14-04-Server镜像，可以选择清华大学的镜像，下载速度快" class="headerlink" title="1. 在vmware或者virtualbox上安装ubuntu14.04 Server镜像，可以选择清华大学的镜像，下载速度快"></a>1. 在vmware或者virtualbox上安装ubuntu14.04 Server镜像，可以选择清华大学的镜像，下载速度快</h3><h3 id="2-系统装完后接下来就要在ubuntu上安装docker了，执行命令："><a href="#2-系统装完后接下来就要在ubuntu上安装docker了，执行命令：" class="headerlink" title="2. 系统装完后接下来就要在ubuntu上安装docker了，执行命令："></a>2. 系统装完后接下来就要在ubuntu上安装docker了，执行命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="_">-s</span>SL https://get.daocloud.io/docker | sh</span><br></pre></td></tr></table></figure>
<p>等待完成即可，这是一种安装docker比较快的方式，而且安装的docker版本也比较高，安装完成后输入docker -v看到下面信息说明安装完成：<br><code>Docker version 17.04.0-ce, build 4845c56</code></p>
<h3 id="3-安装完docker，接下来就需要下载pmm-server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像："><a href="#3-安装完docker，接下来就需要下载pmm-server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像：" class="headerlink" title="3. 安装完docker，接下来就需要下载pmm server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像："></a>3. 安装完docker，接下来就需要下载pmm server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像：</h3><p>在 Docker 的启动参数中加入:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--registry-mirror=https://docker.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure></p>
<p>Ubuntu 用户（包括使用 systemd 的 Ubuntu 15.04）可以修改 /etc/default/docker 文件，加入如下参数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_OPTS=<span class="string">"--registry-mirror=https://docker.mirrors.ustc.edu.cn"</span></span><br></pre></td></tr></table></figure></p>
<p>其他 systemd 用户可以通过执行 sudo systemctl edit docker.service 来修改设置, 覆盖默认的启动参数:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/docker <span class="_">-d</span> -H fd:// --registry-mirror=https://docker.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure></p>
<h3 id="4-接下来下载pmm镜像的速度就会大大提升，执行下面命令："><a href="#4-接下来下载pmm镜像的速度就会大大提升，执行下面命令：" class="headerlink" title="4. 接下来下载pmm镜像的速度就会大大提升，执行下面命令："></a>4. 接下来下载pmm镜像的速度就会大大提升，执行下面命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull percona/pmm-server:1.2.0</span><br></pre></td></tr></table></figure>
<p>然后等待完成即可。</p>
<h3 id="5-创建PMM-数据容器："><a href="#5-创建PMM-数据容器：" class="headerlink" title="5. 创建PMM 数据容器："></a>5. 创建PMM 数据容器：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker create \</span><br><span class="line">   -v /opt/prometheus/data \</span><br><span class="line">   -v /opt/consul-data \</span><br><span class="line">   -v /var/lib/mysql \</span><br><span class="line">   -v /var/lib/grafana \</span><br><span class="line">   --name pmm-data \</span><br><span class="line">   percona/pmm-server:1.2.0 /bin/<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="6-运行PMM-server容器"><a href="#6-运行PMM-server容器" class="headerlink" title="6. 运行PMM server容器:"></a>6. 运行PMM server容器:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="_">-d</span> \</span><br><span class="line">   -p 80:80 \</span><br><span class="line">   --volumes-from pmm-data \</span><br><span class="line">   --name pmm-server \</span><br><span class="line">   --restart always \</span><br><span class="line">   percona/pmm-server:1.2.0</span><br></pre></td></tr></table></figure>
<h3 id="7-安装PMM客户端："><a href="#7-安装PMM客户端：" class="headerlink" title="7. 安装PMM客户端："></a>7. 安装PMM客户端：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh https://www.percona.com/downloads/pmm-client/pmm-client-1.2.0/binary/redhat/7/x86_64/pmm-client-1.2.0-1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<h3 id="8-连接PMM服务器："><a href="#8-连接PMM服务器：" class="headerlink" title="8. 连接PMM服务器："></a>8. 连接PMM服务器：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmm-admin config --server pmm服务器主机地址</span><br></pre></td></tr></table></figure>
<h3 id="9-配置mysql监控："><a href="#9-配置mysql监控：" class="headerlink" title="9. 配置mysql监控："></a>9. 配置mysql监控：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmm-admin add mysql --user root --password 123456 --host mysql ip地址 --port 3306 instace3306</span><br></pre></td></tr></table></figure>
<p><code>注：pmm-client收的监控数据来源有这么几方面</code></p>
<ul>
<li>MySQL所在机器的系统指标</li>
<li>MySQL的performance_schema库</li>
<li>slow-log(慢查询日志–mysql要开启慢日志功能)</li>
</ul>
<p><code>如果我们想收集a和c中的指标的话，最好还是将pmm-client部署在MySQL所在机器</code></p>
<h3 id="10-至此访问pmm服务器ip地址即可查看接口"><a href="#10-至此访问pmm服务器ip地址即可查看接口" class="headerlink" title="10. 至此访问pmm服务器ip地址即可查看接口"></a>10. 至此访问pmm服务器ip地址即可查看接口</h3>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL too many connection 问题分析]]></title>
      <url>http://icyxp.github.io/blog/2016/09/mysql-mysql-sleep.html</url>
      <content type="html"><![CDATA[<h2 id="问题缘由"><a href="#问题缘由" class="headerlink" title="问题缘由"></a>问题缘由</h2><p>线上一个网站在运行一段时间后，页面打开速度变慢随之出现<code>502 bad gateway</code>的错误。</p>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>由<code>502 bad gateway</code>的错误，我们知道就是nginx链接后端的php，而php程序没有及时返回，造成了超时。<br>那么造成php超时的原因也有多种，比如：</p>
<ol>
<li>php-fpm资源消耗光</li>
<li>调用外部资源超时，比如外部的web service、数据库等等</li>
<li>….</li>
</ol>
<p>出现问题，我们先登录服务器查看相关日志。</p>
<p>结合我们的业务，首先想到了mysql数据库，先查看了一下mysql数据库的状态，通过show full processlist命令发现有大量的链接处于sleep状态。</p>
<p><code>sleep</code>状态的意思就是说，某个客户端一直占着这个链接，但是什么事也不干，或者是客户端压根儿就已经断开了，而服务端却不知道。</p>
<p>我们知道，mysql的连接数是有限制的，比如默认是151个，那么当大量的链接处于sleep状态时，php程序就无法同mysql建立链接，就会发生超时现象。</p>
<a id="more"></a>
<p>那么造成sleep的原因，有三个，下面是mysql手册给出的解释：</p>
<ol>
<li>客户端程序在退出之前没有调用<code>mysql_close()</code>。[写程序的疏忽，或者数据库的db类库没有自动关闭每次的连接。。。]</li>
<li>客户端sleep的时间在<code>wait_timeout</code>或<code>interactive_timeout</code>规定的秒内没有发出任何请求到服务器. [类似常连，类似于不完整的tcp ip协议构造，服务端一直认为客户端仍然存在（有可能客户端已经断掉了）]</li>
<li>客户端程序在结束之前向服务器发送了请求还没得到返回结果就结束掉了。 [参看：tcp ip协议的三次握手]</li>
</ol>
<p>那么知道了问题所在，就要找到是什么原因导致的sleep线程的存在，</p>
<p>通过上面的信息，我们知道是 192.168.1.2这个IP的20318端口和mysql建立的链接，而192.168.1.2正是我们的web服务器，</p>
<p>于是ssh登录服务器，通过<code>netstat -tunp</code>找到端口20318所对应的进程和pid，一看就是php-fpm引起的。</p>
<p>下面就是要看一下这个php-fpm是调用的哪一个php文件，找到了具体的php文件就好办了。</p>
<p>具体可以通过<code>lsof</code>列出这个<code>pid</code>打开的文件，也可以通过<code>strace</code>跟踪进程的系统调用。</p>
<p>下面是lsof的部分输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lsof -p 23018</span></span><br><span class="line">COMMAND   PID   USER   FD   TYPE     DEVICE     SIZE       NODE NAME</span><br><span class="line">php-fpm 15687 daemon  cwd    DIR      104,2     4096   69437193 /xxx/daemon.php</span><br><span class="line">php-fpm 15687 daemon  rtd    DIR      104,6     4096          2 /</span><br><span class="line">php-fpm 15687 daemon  txt    REG      104,5 27714205    3466635 /app/php/sbin/php-fpm</span><br></pre></td></tr></table></figure></p>
<p>从中可以看到，是我们的<code>daemon.php</code>引起的，这个程序是我们向ios设备推送通知的程序，其中要跟苹果（Apple）的服务器建立链接，可能是苹果服务器不稳定,超时引起的。</p>
<p>程序大致流程：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql_connect();  <span class="comment">//建立链接</span></span><br><span class="line"></span><br><span class="line">$queryBid = mysql_query(<span class="string">"select bundleId from apps"</span>);</span><br><span class="line"><span class="keyword">while</span> ($row = mysql_fetch_assoc($queryBid)) &#123;</span><br><span class="line">	<span class="comment">//取出通知内容，连接苹果服务器，进行推送</span></span><br><span class="line">	<span class="comment">//这里有时候会花一二十分钟，此时mysql服务器那里的连接一直是sleep状态</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mysql_close();</span><br></pre></td></tr></table></figure></p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>方案一：修改的思路也很简单，我们先通过mysql把数据取出来，之后马上关掉mysql连接，释放mysql资源，剩下的就慢慢干好了。 修改后的程序是这样的：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql_connect();  <span class="comment">//建立链接</span></span><br><span class="line"></span><br><span class="line">$arr_bid=<span class="keyword">array</span>();</span><br><span class="line">$queryBid = mysql_query(<span class="string">"select bundleId from apps"</span>);</span><br><span class="line"><span class="keyword">while</span>($row = mysql_fetch_assoc($queryBid)) &#123;</span><br><span class="line">	$arr_bid[] = $row;</span><br><span class="line">&#125;</span><br><span class="line">mysql_close(); <span class="comment">//从mysql中取完数据就马上关闭连接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">foreach</span>($arr_bid <span class="keyword">as</span> $row)&#123;</span><br><span class="line">	<span class="comment">//取出通知内容，连接苹果服务器，进行推送</span></span><br><span class="line">	<span class="comment">//这里有时候会花一二十分钟</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>方案二：[不治本，有弊端]<br>写一个定时脚本，每分钟检查下mysql连接数，超过sleep时间的自动kill掉<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span>  </span><br><span class="line">define(<span class="string">'MAX_SLEEP_TIME'</span>， <span class="number">120</span>);  </span><br><span class="line">  </span><br><span class="line">$hostname = <span class="string">"localhost"</span>;  </span><br><span class="line">$username = <span class="string">"root"</span>;  </span><br><span class="line">$password = <span class="string">"password"</span>;  </span><br><span class="line">  </span><br><span class="line">$connect = mysql_connect($hostname， $username， $password);  </span><br><span class="line">$result = mysql_query(<span class="string">"SHOW PROCESSLIST"</span>， $connect);  </span><br><span class="line"><span class="keyword">while</span> ($proc = mysql_fetch_assoc($result)) &#123;  </span><br><span class="line">	<span class="keyword">if</span> ($proc[<span class="string">"Command"</span>] == <span class="string">"Sleep"</span> &amp;&amp; $proc[<span class="string">"Time"</span>] &gt; MAX_SLEEP_TIME) &#123;  </span><br><span class="line">	@mysql_query(<span class="string">"KILL "</span> . $proc[<span class="string">"Id"</span>]， $connect);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br><span class="line">mysql_close($connect);</span><br></pre></td></tr></table></figure></p>
<p>加入到crontab定时计划里<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * php /usr/<span class="built_in">local</span>/sbin/<span class="built_in">kill</span>-mysql-sleep-proc.php</span><br></pre></td></tr></table></figure></p>
<p>方案三：[不推荐]<br>修改mysql配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="built_in">wait</span>_timeout=10</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">mysql&gt; <span class="built_in">set</span> global <span class="built_in">wait</span>_timeout=10;</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MaxScale 在 slave 有故障后如何处理？]]></title>
      <url>http://icyxp.github.io/blog/2016/08/mysql-maxscale-02.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前介绍了 <a href="/blog/2016/08/maxscale-01.html">MaxScale</a> 可以实现 Mysql 的读写分离和读负载均衡，那么当 slave 出现故障后，MaxScale 会如何处理呢？</p>
<p>例如有 3 台数据库服务器，一主二从的结构，数据库名称分别为 master, slave1, slave2</p>
<p>现在我们实验以下两种情况：</p>
<ol>
<li>当一台从服务器（ slave1 或者 slave2 ）出现故障后，查看 MaxScale 如何应对，及故障服务器重新上线后的情况</li>
<li>当两台从服务器（ slave1 和 slave2 ）都出现故障后，查看 MaxScale 如何应对，及故障服务器重新上线后的情况</li>
</ol>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>为了更深入的查看 MaxScale 的状态，需要把 MaxScale 的日志打开</p>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [maxscale] 部分，这里用来进行全局设置，在其中添加日志配置<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log_info</span>=<span class="number">1</span></span><br><span class="line"><span class="attr">logdir</span>=/tmp/</span><br></pre></td></tr></table></figure></p>
<p>通过开启 log_info 级别，可以看到 MaxScale 的路由日志</p>
<p>修改配置后，重启 MaxScale </p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><a id="more"></a>
<h3 id="单个-slave-故障的情况"><a href="#单个-slave-故障的情况" class="headerlink" title="单个 slave 故障的情况"></a>单个 slave 故障的情况</h3><p>初始状态是一切正常<br><img src="/images/maxscale_08.png" alt="Maxscale"></p>
<p>停掉 slave2 的复制，登录 slave2 的 mysql 执行<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; stop slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_09.png" alt="Maxscale"></p>
<p>slave2 已经失效了</p>
<p>查看日志信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/maxscale1.log</span><br><span class="line"><span class="comment">#尾部显示：</span></span><br><span class="line">2016-08-15 12:26:02   notice : Server changed state: slave2[172.17.0.4:3306]: lost_slave</span><br></pre></td></tr></table></figure></p>
<p>提示 slave2 已经丢失</p>
<p>查看客户端查询结果<br><img src="/images/maxscale_10.png" alt="Maxscale"></p>
<p>查询操作全都转到了 <code>slave1</code></p>
<p>可以看到， 在有 slave 故障后，MaxScale 会自动进行排除，不再向其转发请求</p>
<p>下面看下 slave2 再次<strong>上线后的情况</strong></p>
<p>登录 slave2 的 mysql 执行<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_11.png" alt="Maxscale"></p>
<p>恢复了正常状态，重新识别到了 slave2</p>
<p>查看日志信息，显示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-08-15 12:32:36   notice : Server changed state: slave2[172.17.0.4:3306]: new_slave</span><br></pre></td></tr></table></figure></p>
<p>查看客户端查询结果<br><img src="/images/maxscale_12.png" alt="Maxscale"></p>
<p>slave2 又可以正常接受查询请求</p>
<p>通过实验可以看到，在部分 slave 发生故障时，MaxScale 可以自动识别出来，并移除路由列表，当故障恢复重新上线后，MaxScale 也能自动将其加入路由，过程透明</p>
<h3 id="全部-slave-故障的情况"><a href="#全部-slave-故障的情况" class="headerlink" title="全部 slave 故障的情况"></a>全部 slave 故障的情况</h3><p>分别登陆 slave1 和 slave2 的 mysql，执行停止复制的命令<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; stop slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_13.png" alt="Maxscale"></p>
<p>发现各个服务器的角色都识别不出来了</p>
<p>查看日志<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2016-08-15 12:44:11   notice : Server changed state: master[172.17.0.2:3306]: lost_master</span><br><span class="line">2016-08-15 12:44:11   notice : Server changed state: slave1[172.17.0.3:3306]: lost_slave</span><br><span class="line">2016-08-15 12:44:11   notice : Server changed state: slave2[172.17.0.4:3306]: lost_slave</span><br><span class="line">2016-08-15 12:44:11   error  : No Master can be determined. Last known was 172.17.0.2:3306</span><br></pre></td></tr></table></figure></p>
<p>从日志中看到，MaxScale 发现2个slave 和 master 都丢了，然后报错：没有 master 了</p>
<p>客户端连接 MaxScale 时也失败了<br><img src="/images/maxscale_14.png" alt="Maxscale"></p>
<p>说明从服务器全部失效后，会导致 master 也无法识别，使整个数据库服务都失效了</p>
<p>对于 slave 全部失效的情况，能否让 master 还可用？这样至少可以正常提供数据库服务</p>
<p>这需要修改 MaxScale 的配置，告诉 MaxScale 我们需要一个稳定的 master</p>
<h4 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h4><p>先恢复两个 slave，让集群回到正常状态，登陆两个 slave 的mysql<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure></p>
<p>修改 MaxScale 配置文件，添加新的配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [MySQL Monitor] 部分，添加：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">detect_stale_master</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>保存退出，然后重启 MaxScale</p>
<h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>停掉两台 slave ，查看 MaxScale 服务器状态<br><img src="/images/maxscale_15.png" alt="Maxscale"></p>
<p>可以看到，虽然 slave 都无法识别了，但 master 还在，并提示处于稳定状态</p>
<p>客户端执行请求<br><img src="/images/maxscale_16.png" alt="Maxscale"></p>
<p>客户端可以连接 MaxScale，而且请求都转到了 master 上，说明 slave 全部失效时，由 master 支撑了全部请求</p>
<p>当恢复两个 slave 后，整体状态自动恢复正常，从客户端执行请求时，又可以转到 slave 上<br><img src="/images/maxscale_17.png" alt="Maxscale"></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>通过测试发现，在部分 slave 故障情况下，对于客户端是完全透明的，当全部 slave 故障时，经过简单的配置，MaxScale 也可以很好的处理</p>
<hr>
<p>来源：杜亦舒</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Mysql 读写分离中间件 MaxScale]]></title>
      <url>http://icyxp.github.io/blog/2016/08/mysql-maxscale-01.html</url>
      <content type="html"><![CDATA[<h2 id="MaxScale-是干什么的？"><a href="#MaxScale-是干什么的？" class="headerlink" title="MaxScale 是干什么的？"></a>MaxScale 是干什么的？</h2><p>配置好了 Mysql 的主从复制结构后，我们希望实现读写分离，把读操作分散到从服务器中，并且对多个从服务器能实现负载均衡</p>
<p><strong>读写分离</strong>和<strong>负载均衡</strong>是 Mysql 集群的基础需求，<strong>MaxScale</strong> 就可以帮着我们方便的实现这些功能<br><img src="/images/maxscale_01.png" alt="Maxscale"></p>
<h2 id="MaxScale-的基础构成"><a href="#MaxScale-的基础构成" class="headerlink" title="MaxScale 的基础构成"></a>MaxScale 的基础构成</h2><ul>
<li>MaxScale 是 Mysql 的兄弟公司 MariaDB 开发的，现在已经发展得非常成熟</li>
<li>MaxScale 是插件式结构，允许用户开发适合自己的插件</li>
<li>MaxScale 目前提供的插件功能分为<strong>5类</strong></li>
</ul>
<h3 id="认证插件"><a href="#认证插件" class="headerlink" title="认证插件"></a>认证插件</h3><p>提供了登录认证功能，MaxScale 会读取并缓存数据库中 user 表中的信息，当有连接进来时，先从缓存信息中进行验证，如果没有此用户，会从后端数据库中更新信息，再次进行验证</p>
<h3 id="协议插件"><a href="#协议插件" class="headerlink" title="协议插件"></a>协议插件</h3><p>包括客户端连接协议，和连接数据库的协议</p>
<h3 id="路由插件"><a href="#路由插件" class="headerlink" title="路由插件"></a>路由插件</h3><p>决定如何把客户端的请求转发给后端数据库服务器，读写分离和负载均衡的功能就是由这个模块实现的</p>
<h3 id="监控插件"><a href="#监控插件" class="headerlink" title="监控插件"></a>监控插件</h3><p>对各个数据库服务器进行监控，例如发现某个数据库服务器响应很慢，那么就不向其转发请求了</p>
<h3 id="日志和过滤插件"><a href="#日志和过滤插件" class="headerlink" title="日志和过滤插件"></a>日志和过滤插件</h3><p>提供简单的数据库防火墙功能，可以对SQL进行过滤和容错</p>
<a id="more"></a>
<h2 id="MaxScale-的安装使用"><a href="#MaxScale-的安装使用" class="headerlink" title="MaxScale 的安装使用"></a>MaxScale 的安装使用</h2><p>例如有 3 台数据库服务器，是一主二从的结构</p>
<h3 id="过程概述"><a href="#过程概述" class="headerlink" title="过程概述"></a>过程概述</h3><ol>
<li>配置好集群环境</li>
<li>下载安装 MaxScale</li>
<li>配置 MaxScale，添加各数据库信息</li>
<li>启动 MaxScale，查看是否正确连接数据库</li>
<li>客户端连接 MaxScale，进行测试</li>
</ol>
<h3 id="详细过程"><a href="#详细过程" class="headerlink" title="详细过程"></a>详细过程</h3><h4 id="配置一主二从的集群环境"><a href="#配置一主二从的集群环境" class="headerlink" title="配置一主二从的集群环境"></a>配置一主二从的集群环境</h4><p>准备3台服务器，安装 Mysql，配置一主二从的复制结构<br>主从复制的配置过程略过</p>
<h4 id="安装-MaxScale"><a href="#安装-MaxScale" class="headerlink" title="安装 MaxScale"></a>安装 MaxScale</h4><p>最好在另一台服务器上安装，如果资源不足，可以和某个 Mysql 放在一起</p>
<p>MaxScale 的下载地址<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://downloads.mariadb.com/files/MaxScale</span><br></pre></td></tr></table></figure></p>
<p>根据自己的服务器选择合适的安装包</p>
<p>以 centos 7 为例 安装步骤如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install libaio.x86_64 libaio-devel.x86_64 novacom-server.x86_64 libedit -y</span><br><span class="line">rpm -ivh maxscale-1.4.3-1.centos.7.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<h4 id="配置-MaxScale"><a href="#配置-MaxScale" class="headerlink" title="配置 MaxScale"></a>配置 MaxScale</h4><p>在开始配置之前，需要在 master 中为 MaxScale 创建两个用户，用于监控模块和路由模块</p>
<p>创建监控用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user scalemon@'%' identified by "111111";</span><br><span class="line">mysql&gt; grant replication slave, replication client on *.* to scalemon@'%';</span><br></pre></td></tr></table></figure></p>
<p>创建路由用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user maxscale@'%' identified by "111111";</span><br><span class="line">mysql&gt; grant select on mysql.* to maxscale@'%';</span><br></pre></td></tr></table></figure></p>
<p>用户创建完成后，开始配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [server1] 部分，修改其中的 address 和 port，指向 master 的 IP 和端口</p>
<p>复制2次 [server1] 的整块儿内容，改为 [server2] 与 [server3]，同样修改其中的 address 和 port，分别指向 slave1 和 slave2<br><img src="/images/maxscale_02.png" alt="Maxscale"></p>
<p>找到 <code>[MySQL Monitor]</code> 部分，修改 servers 为 server1,server2,server3，修改 user 和 passwd 为之前创建的监控用户的信息（scalemon,111111）<br><img src="/images/maxscale_03.png" alt="Maxscale"></p>
<p>找到 <code>[Read-Write Service]</code> 部分，修改 servers 为 server1,server2,server3，修改 user 和 passwd 为之前创建的路由用户的信息（maxscale,111111）<br><img src="/images/maxscale_04.png" alt="Maxscale"></p>
<p>由于我们使用了 <code>[Read-Write Service]</code>，需要删除另一个服务 <code>[Read-Only Service]</code>，删除其整块儿内容即可</p>
<p>配置完成，保存并退出编辑器</p>
<h4 id="启动-MaxScale"><a href="#启动-MaxScale" class="headerlink" title="启动 MaxScale"></a>启动 MaxScale</h4><p>执行启动命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maxscale --config=/etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 的响应端口是否已经就绪<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ntelp</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/maxscale_05.png" alt="Maxscale"></p>
<ul>
<li>4006 是连接 MaxScale 时使用的端口</li>
<li>6603 是 MaxScale 管理器的端口</li>
</ul>
<p>登录 MaxScale 管理器，查看一下数据库连接状态，默认的用户名和密码是 admin/mariadb<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maxadmin --user=admin --password=mariadb</span><br><span class="line">MaxScale&gt; list servers</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/maxscale_06.png" alt="Maxscale"></p>
<p>可以看到，MaxScale 已经连接到了 master 和 slave</p>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>先在 master 上创建一个测试用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant ALL PRIVILEGES on *.* to rtest@"%" Identified by "111111";</span><br></pre></td></tr></table></figure></p>
<p>使用 Mysql 客户端到连接 MaxScale<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h MaxScale所在的IP -P 4006 -u rtest -p111111</span><br></pre></td></tr></table></figure></p>
<p>执行查看数据库服务器名的操作来知道当前实际所在的数据库<br><img src="/images/maxscale_07.png" alt="Maxscale"></p>
<p>开启事务后，就自动路由到了 master，普通的查询操作，是在 slave上</p>
<p>MaxScale 的配置完成了</p>
<hr>
<p>来源：杜亦舒</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[双机高可用-负载均衡-MySQL(读写分离、主从自动切换)架构设计]]></title>
      <url>http://icyxp.github.io/blog/2016/08/ha-load-balance-mysql-master-slave-architecture.html</url>
      <content type="html"><![CDATA[<h2 id="架构简介"><a href="#架构简介" class="headerlink" title="架构简介"></a>架构简介</h2><p>只有两台机器，需要实现其中一台死机之后另一台能接管这台机器的服务，并且在两台机器正常服务时，两台机器都能用上。如何设计这样的架构场景。<br><img src="/images/high_availability.png" alt="High Availability"></p>
<p>此架构主要是由keepalived实现双机高可用，维护了一个外网VIP，一个内网VIP。正常情况时，外网VIP和内网VIP都绑定在server1服务器，web请求发送到server1的nginx，nginx对于静态资源请求就直接在本机检索并返回，对于php的动态请求，则负载均衡到server1和server2。对于SQL请求，会将此类请求发送到Atlas MySQL中间件，Atlas接收到请求之后，把涉及写操作的请求发送到内网VIP，读请求操作发送到mysql从，这样就实现了读写分离。</p>
<p>当主服务器server1宕机时，keepalived检测到后，立即把外网VIP和内网VIP绑定到server2，并把server2的mysql切换成主库。此时由于外网VIP已经转移到了server2，web请求将发送给server2的nginx。nginx检测到server1宕机，不再把请求转发到server1的php-fpm。之后的sql请求照常发送给本地的atlas，atlas把写操作发送给内网VIP，读操作发送给mysql从，由于内网VIP已经绑定到server2了，server2的mysql同时接受写操作和读操作。</p>
<p>当主服务器server1恢复后，server1的mysql自动设置为从，与server2的mysql主同步。keepalived不抢占server2的VIP，继续正常服务。</p>
<h2 id="架构要求"><a href="#架构要求" class="headerlink" title="架构要求"></a>架构要求</h2><p>要实现此架构，需要三个条件：</p>
<ol>
<li>服务器可以设置内网IP，并且设置的内网IP互通。</li>
<li>服务器可以随意绑定IDC分配给我们使用的外网IP，即外网IP没有绑定MAC地址。</li>
<li>MySQL服务器支持GTID，即MySQL-5.6.5以上版本。</li>
</ol>
<h2 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h2><blockquote>
<p><strong>对外VIP</strong>：10.96.153.239<br><strong>对内VIP</strong>：192.168.1.150</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">描述</th>
<th style="text-align:left">网卡1：eth0(公网IP)</th>
<th style="text-align:left">网卡2：eth1(内网IP)</th>
<th style="text-align:left">操作系统</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">server1</td>
<td style="text-align:left">10.96.153.110</td>
<td style="text-align:left">192.168.1.100</td>
<td style="text-align:left">Centos 6.X</td>
</tr>
<tr>
<td style="text-align:left">server2</td>
<td style="text-align:left">10.96.153.114</td>
<td style="text-align:left">192.168.1.101</td>
<td style="text-align:left">Centos 6.X</td>
</tr>
</tbody>
</table>
<h2 id="架构配置"><a href="#架构配置" class="headerlink" title="架构配置"></a>架构配置</h2><a id="more"></a>
<h3 id="hosts设置"><a href="#hosts设置" class="headerlink" title="hosts设置"></a>hosts设置</h3><p>Server1与Server2 hosts设置如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/hosts</span></span><br><span class="line">192.168.1.100 server1</span><br><span class="line">192.168.1.101 server2</span><br></pre></td></tr></table></figure></p>
<h3 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h3><p>Nginx、PHP、MySQL、Memcached安装略</p>
<h3 id="解决session共享问题"><a href="#解决session共享问题" class="headerlink" title="解决session共享问题"></a>解决session共享问题</h3><p>php默认的session存储是在/tmp目录下，现在我们是用两台服务器作php请求的负载，这样会造成session分布在两台服务器的/tmp目录下，导致依赖于session的功能不正常。我们可以使用memcached来解决此问题。</p>
<p>上一步我们已经安装好了memcached，现在只需要配置php.ini来使用memcached，配置如下，打开php.ini配置文件，修改为如下两行的值：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session.save_handler = memcache</span><br><span class="line">session.save_path = "tcp://192.168.1.100:11211,tcp://192.168.1.101:11211"</span><br></pre></td></tr></table></figure></p>
<p>之后重启php-fpm生效。</p>
<h3 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h3><h4 id="Server1配置"><a href="#Server1配置" class="headerlink" title="Server1配置"></a>Server1配置</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    [...]</span><br><span class="line">    upstream php-server &#123;</span><br><span class="line">        server 192.168.1.101:9000;</span><br><span class="line">        server 127.0.0.1:9000;</span><br><span class="line">        keepalive 100;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">    server &#123;</span><br><span class="line">        [...]</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            fastcgi_pass   php-server;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line">        [...]</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Server2配置"><a href="#Server2配置" class="headerlink" title="Server2配置"></a>Server2配置</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    [...]</span><br><span class="line">    upstream php-server &#123;</span><br><span class="line">        server 192.168.1.100:9000;</span><br><span class="line">        server 127.0.0.1:9000;</span><br><span class="line">        keepalive 100;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">    server &#123;</span><br><span class="line">        [...]</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            fastcgi_pass   php-server;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line">        [...]</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个配置主要的作用是设置php请求的负载均衡。</p>
<h3 id="MySQL配置"><a href="#MySQL配置" class="headerlink" title="MySQL配置"></a>MySQL配置</h3><h4 id="mysql-util安装"><a href="#mysql-util安装" class="headerlink" title="mysql util安装"></a>mysql util安装</h4><p>我们需要安装mysql util里的主从配置工具来实现主从切换。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-utilities-1.5.3.tar.gz</span><br><span class="line">tar xzf mysql-utilities-1.5.3.tar.gz</span><br><span class="line"><span class="built_in">cd</span> mysql-utilities-1.5.3</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p>
<h4 id="mysql-my-cnf配置"><a href="#mysql-my-cnf配置" class="headerlink" title="mysql my.cnf配置"></a>mysql my.cnf配置</h4><p><strong>server1：</strong><br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mysql]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="attr">protocol</span> = tcp</span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="comment"># BINARY LOGGING</span></span><br><span class="line"><span class="attr">log-bin</span> = /usr/local/mysql/data/mysql-bin</span><br><span class="line"><span class="attr">expire-logs-days</span> = <span class="number">14</span></span><br><span class="line"><span class="attr">binlog-format</span> = row</span><br><span class="line"><span class="attr">log-slave-updates</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">gtid-mode</span> = <span class="literal">on</span></span><br><span class="line"><span class="attr">enforce-gtid-consistency</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">master-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">relay-log-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">server-id</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">report-host</span> = server1</span><br><span class="line"><span class="attr">report-port</span> = <span class="number">3306</span></span><br><span class="line"><span class="section">[...]</span></span><br></pre></td></tr></table></figure></p>
<p><strong>server2：</strong><br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mysql]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="attr">protocol</span> = tcp</span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="comment"># BINARY LOGGING</span></span><br><span class="line"><span class="attr">log-bin</span> = /usr/local/mysql/data/mysql-bin</span><br><span class="line"><span class="attr">expire-logs-days</span> = <span class="number">14</span></span><br><span class="line"><span class="attr">binlog-format</span> = row</span><br><span class="line"><span class="attr">log-slave-updates</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">gtid-mode</span> = <span class="literal">on</span></span><br><span class="line"><span class="attr">enforce-gtid-consistency</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">master-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">relay-log-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">server-id</span> = <span class="number">2</span></span><br><span class="line"><span class="attr">report-host</span> = server2</span><br><span class="line"><span class="attr">report-port</span> = <span class="number">3306</span></span><br><span class="line"><span class="section">[...]</span></span><br></pre></td></tr></table></figure></p>
<p>这两个配置主要是设置了binlog和启用gtid-mode，并且需要设置不同的server-id和report-host。</p>
<h4 id="开放root帐号远程权限"><a href="#开放root帐号远程权限" class="headerlink" title="开放root帐号远程权限"></a>开放root帐号远程权限</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant all on *.* to 'root'@'192.168.1.%' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; grant all on *.* to 'root'@'server1' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; grant all on *.* to 'root'@'server2' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<h4 id="设置mysql主从"><a href="#设置mysql主从" class="headerlink" title="设置mysql主从"></a>设置mysql主从</h4><p>在任意一台执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysqlreplicate --master=root:Xp29at5F37@server1:3306 --slave=root:Xp29at5F37@server2:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># slave on server2: … connected.</span></span><br><span class="line"><span class="comment"># Checking for binary logging on master…</span></span><br><span class="line"><span class="comment"># Setting up replication…</span></span><br><span class="line"><span class="comment"># …done.</span></span><br></pre></td></tr></table></figure></p>
<h4 id="显示主从关系"><a href="#显示主从关系" class="headerlink" title="显示主从关系"></a>显示主从关系</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysqlrplshow --master=root:Xp29at5F37@server1 --discover-slaves-login=root:Xp29at5F37</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># Finding slaves for master: server1:3306</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Replication Topology Graph</span></span><br><span class="line">server1:3306 (MASTER)</span><br><span class="line">|</span><br><span class="line">+— server2:3306 – (SLAVE)</span><br></pre></td></tr></table></figure>
<h4 id="检查主从状态"><a href="#检查主从状态" class="headerlink" title="检查主从状态"></a>检查主从状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mysqlrplcheck --master=root:Xp29at5F37@server1 --slave=root:Xp29at5F37@server2</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># slave on server2: … connected.</span></span><br><span class="line">Test Description Status</span><br><span class="line">—————————————————————————</span><br><span class="line">Checking <span class="keyword">for</span> binary logging on master [pass]</span><br><span class="line">Are there binlog exceptions? [pass]</span><br><span class="line">Replication user exists? [pass]</span><br><span class="line">Checking server_id values [pass]</span><br><span class="line">Checking server_uuid values [pass]</span><br><span class="line">Is slave connected to master? [pass]</span><br><span class="line">Check master information file [pass]</span><br><span class="line">Checking InnoDB compatibility [pass]</span><br><span class="line">Checking storage engines compatibility [pass]</span><br><span class="line">Checking lower_<span class="keyword">case</span>_table_names settings [pass]</span><br><span class="line">Checking slave delay (seconds behind master) [pass]</span><br><span class="line"><span class="comment"># …done.</span></span><br></pre></td></tr></table></figure>
<h3 id="Keepalived配置"><a href="#Keepalived配置" class="headerlink" title="Keepalived配置"></a>Keepalived配置</h3><h4 id="安装-两台都装"><a href="#安装-两台都装" class="headerlink" title="安装(两台都装)"></a>安装(两台都装)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install keepalived</span><br><span class="line">chkconfig keepalived on</span><br></pre></td></tr></table></figure>
<h4 id="keepalived配置-server1"><a href="#keepalived配置-server1" class="headerlink" title="keepalived配置(server1)"></a>keepalived配置(server1)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">vrrp_sync_group VG_1 &#123;</span><br><span class="line">	group &#123;</span><br><span class="line">		inside_network</span><br><span class="line">		outside_network</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance inside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth1</span><br><span class="line">	virtual_router_id 51</span><br><span class="line">	priority 101</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		192.168.1.150/24</span><br><span class="line">	&#125;</span><br><span class="line">	nopreempt</span><br><span class="line">	notify /data/sh/mysqlfailover-server1.sh</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance outside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth0</span><br><span class="line">	virtual_router_id 50</span><br><span class="line">	priority 101</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">	auth_<span class="built_in">type</span> PASS</span><br><span class="line">	auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		10.96.153.239/24</span><br><span class="line">	&#125;</span><br><span class="line">	nopreempt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="keepalived配置-server2"><a href="#keepalived配置-server2" class="headerlink" title="keepalived配置(server2)"></a>keepalived配置(server2)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">vrrp_sync_group VG_1 &#123;</span><br><span class="line">	group &#123;</span><br><span class="line">		inside_network</span><br><span class="line">		outside_network</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance inside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth1</span><br><span class="line">	virtual_router_id 51</span><br><span class="line">	priority 100</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		192.168.1.150</span><br><span class="line">	&#125;</span><br><span class="line">	notify /data/sh/mysqlfailover-server2.sh</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance outside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth0</span><br><span class="line">	virtual_router_id 50</span><br><span class="line">	priority 100</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		10.96.153.239/24</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此keepalived配置需要注意的是：</p>
<ol>
<li>两台server的state都设置为backup，server1增加nopreempt配置，并且server1 priority比server2高，这样用来实现当server1从宕机恢复时，不抢占VIP;</li>
<li>server1设置notify /data/sh/mysqlfailover-server1.sh,server2设置notify /data/sh/mysqlfailover-server2.sh,作用是自动切换主从</li>
</ol>
<p>/data/sh/mysqlfailover-server1.sh脚本内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"> </span></span><br><span class="line">sleep 10</span><br><span class="line">state=<span class="variable">$3</span></span><br><span class="line">result=`mysql -h127.0.0.1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status;'</span>`</span><br><span class="line">[[ <span class="string">"<span class="variable">$result</span>"</span> == <span class="string">""</span> ]] &amp;&amp; mysqlState=<span class="string">"master"</span> || mysqlState=<span class="string">"slave"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"MASTER"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"slave"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlrpladmin --slave=root:Xp29at5F37@server1:3306 failover</span><br><span class="line">	<span class="keyword">fi</span> </span><br><span class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"BACKUP"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"master"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlreplicate --master=root:Xp29at5F37@server2:3306 --slave=root:Xp29at5F37@server1:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/proxy-read-only-backend-addresses.*/proxy-read-only-backend-addresses = 192.168.1.150:3306/'</span> /usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br><span class="line">mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"REMOVE BACKEND 2;"</span></span><br></pre></td></tr></table></figure></p>
<p>/data/sh/mysqlfailover-server2.sh脚本内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line">sleep 10</span><br><span class="line">state=<span class="variable">$3</span></span><br><span class="line">result=`mysql -h127.0.0.1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status;'</span>`</span><br><span class="line">[[ <span class="string">"<span class="variable">$result</span>"</span> == <span class="string">""</span> ]] &amp;&amp; mysqlState=<span class="string">"master"</span> || mysqlState=<span class="string">"slave"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"MASTER"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"slave"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlrpladmin --slave=root:Xp29at5F37@server2:3306 failover</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"BACKUP"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"master"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlreplicate --master=root:Xp29at5F37@server1:3306 --slave=root:Xp29at5F37@server2:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/proxy-read-only-backend-addresses.*/proxy-read-only-backend-addresses = 192.168.1.150:3306/'</span> /usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br><span class="line">mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"REMOVE BACKEND 2;"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Atlas设置"><a href="#Atlas设置" class="headerlink" title="Atlas设置"></a>Atlas设置</h3><h4 id="atlas安装（两台服务器）"><a href="#atlas安装（两台服务器）" class="headerlink" title="atlas安装（两台服务器）"></a>atlas安装（两台服务器）</h4><p>到这里下载最新版本，<a href="https://github.com/Qihoo360/Atlas/releases" target="_blank" rel="external">https://github.com/Qihoo360/Atlas/releases</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/Qihoo360/Atlas/releases/download/2.2.1/Atlas-2.2.1.el6.x86_64.rpm</span><br><span class="line">rpm -i Atlas-2.2.1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<h4 id="atlas配置（两台服务器）"><a href="#atlas配置（两台服务器）" class="headerlink" title="atlas配置（两台服务器）"></a>atlas配置（两台服务器）</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/mysql-proxy/conf</span><br><span class="line">cp test.cnf my.cnf</span><br><span class="line">vi my.cnf</span><br><span class="line">#调整如下参数</span><br><span class="line">proxy-backend-addresses = 192.168.1.150:3306</span><br><span class="line">proxy-read-only-backend-addresses = 192.168.1.101:3306</span><br><span class="line">pwds = root:qtyU1btXOo074Itvx0UR9Q==</span><br><span class="line">event-threads = 8</span><br></pre></td></tr></table></figure>
<p><strong><code>注意</code></strong>：</p>
<ul>
<li>proxy-backend-addresse设置为内网VIP</li>
<li>proxy-read-only-backend-addresses设置为server2的IP</li>
<li>root:qtyU1btXOo074Itvx0UR9Q==设置数据库的用户和密码，密码是通过/usr/local/mysql-proxy/bin/encrypt Xp29at5F37生成。</li>
<li>更详细参数解释请查看，<a href="https://github.com/Qihoo360/Atlas/wiki/Atlas%E9%83%A8%E5%88%86%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E5%8F%8A%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3" target="_blank" rel="external">Atlas配置详解</a>。</li>
</ul>
<h4 id="启动atlas（两台服务器）"><a href="#启动atlas（两台服务器）" class="headerlink" title="启动atlas（两台服务器）"></a>启动atlas（两台服务器）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/mysql-proxy/bin/mysql-proxy --defaults-file=/usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br></pre></td></tr></table></figure>
<p>之后程序里配置mysql就配置127.0.0.1:1234就好。</p>
<h4 id="部署atlas自动维护脚本（两台服务器）"><a href="#部署atlas自动维护脚本（两台服务器）" class="headerlink" title="部署atlas自动维护脚本（两台服务器）"></a>部署atlas自动维护脚本（两台服务器）</h4><p>添加定时任务（如每2分钟运行一次）我们把脚本放在/data/sh/auto_maintain_atlas.sh,脚本内容为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"> </span></span><br><span class="line">count=`mysql -N -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"select * from backends;"</span> | wc <span class="_">-l</span>`</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$count</span>"</span> == <span class="string">"1"</span> ]];<span class="keyword">then</span></span><br><span class="line">    result=`mysql -hserver1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status\G'</span>`</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | grep Slave_IO_State;<span class="keyword">then</span></span><br><span class="line">        slaveIP=192.168.1.100</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        result=`mysql -hserver2 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status\G'</span>`</span><br><span class="line">        slaveIP=192.168.1.101</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    slaveIORunning=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_IO_Running:/&#123;print $2&#125;'</span>`</span><br><span class="line">    slaveSQLRunning=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_SQL_Running:/&#123;print $2&#125;'</span>`</span><br><span class="line">    SlaveSQLRunning_State=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_SQL_Running_State:/&#123;print $2&#125;'</span>`</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$slaveIORunning</span>"</span> =~ <span class="string">"Yes"</span> &amp;&amp; <span class="string">"<span class="variable">$slaveSQLRunning</span>"</span> =~ <span class="string">"Yes"</span> &amp;&amp; <span class="string">"<span class="variable">$SlaveSQLRunning_State</span>"</span> =~ <span class="string">"Slave has read all relay log"</span> ]];<span class="keyword">then</span></span><br><span class="line">        mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"add slave <span class="variable">$&#123;slaveIP&#125;</span>:3306;"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></p>
<p>为什么需要这个脚本呢？假设目前mysql主服务器在s1，s1宕机后，s2接管VIP，接着删除atlas中设置的slave backend，将mysql提升为主。过一段时间后，s1从宕机中恢复，这时候s1的mysql自动切换为从，接着删除atlas中设置的slave backend，开始连接s2的mysql主同步数据。到这个时候我们发现，已经不存在读写分离了，所有的sql都发送给了s2的mysql。auto_maintain_atlas.sh脚本就派上用场了，此脚本会定时的检查主从是否已经同步完成，如果完成就自动增加slave backend，这样读写分离又恢复了，完全不需要人工干预。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="测试keepalived是否工作正常"><a href="#测试keepalived是否工作正常" class="headerlink" title="测试keepalived是否工作正常"></a>测试keepalived是否工作正常</h4><p>我们来模拟server1宕机。<br>在server1上执行shutdown关机命令。<br>此时我们登录server2，执行ip addr命令，输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1: lo: mtu 16436 qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet6 ::1/128 scope host</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:81:9d:42 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 10.96.153.114/24 brd 10.96.153.255 scope global eth0</span><br><span class="line">inet 10.96.153.239/24 scope global secondary eth0</span><br><span class="line">inet6 fe80::20c:29ff:fe81:9d42/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">3: eth1: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:81:9d:4c brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 192.168.1.101/24 brd 192.168.1.255 scope global eth1</span><br><span class="line">inet 192.168.1.150/32 scope global eth1</span><br><span class="line">inet6 fe80::20c:29ff:fe81:9d4c/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>我们看到对外VIP 10.96.153.239和对内IP 192.168.1.150已经转移到server2了，证明keepalived运行正常。</p>
<h4 id="测试是否自动切换了主从"><a href="#测试是否自动切换了主从" class="headerlink" title="测试是否自动切换了主从"></a>测试是否自动切换了主从</h4><p>登录server2的mysql服务器，执行show slave status;命令，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show slave status\G</span><br><span class="line">Empty <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>我们发现从状态已经为空，证明已经切换为主了。</p>
<h4 id="测试server1是否抢占VIP"><a href="#测试server1是否抢占VIP" class="headerlink" title="测试server1是否抢占VIP"></a>测试server1是否抢占VIP</h4><p>为什么要测试这个呢？如果server1恢复之后抢占了VIP，而我们的Atlas里后端设置的是VIP，这样server1启动之后，sql的写操作就会向server1的mysql发送，而server1的mysql数据是旧于server2的，所以这样会造成数据不一致，这个是非常重要的测试。<br>我们先来启动server1，之后执行ip addr，输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1: lo: mtu 16436 qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet6 ::1/128 scope host</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:f1:4f:4e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 10.96.153.110/24 brd 10.96.153.255 scope global eth0</span><br><span class="line">inet6 fe80::20c:29ff:fef1:4f4e/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">3: eth1: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:f1:4f:58 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 192.168.1.100/24 brd 192.168.1.255 scope global eth1</span><br><span class="line">inet6 fe80::20c:29ff:fef1:4f58/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>我们看到，server1并没有抢占VIP，测试正常。不过另人郁闷的是，在虚拟机的环境并没有测试成功，不知道为什么。</p>
<h4 id="测试server2的atlas是否已经删除slave-backend"><a href="#测试server2的atlas是否已经删除slave-backend" class="headerlink" title="测试server2的atlas是否已经删除slave backend"></a>测试server2的atlas是否已经删除slave backend</h4><p>我们测试这个是为了保证atlas已经没有slave backend，也就是没有从库的设置了，否则当server1恢复时，有可能会把读请求发送给server1的mysql，造成读取了旧数据的问题。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]<span class="comment"># mysql -h127.0.0.1 -P2345 -uuser -ppwd</span></span><br><span class="line">mysql&gt; select * from backends;</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| backend_ndx | address | state | <span class="built_in">type</span> |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| 1 | 192.168.1.150:3306 | up | rw |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">1 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>如果看到只有一个后端，证明运作正常。</p>
<h4 id="测试server1-mysql是否设置为从"><a href="#测试server1-mysql是否设置为从" class="headerlink" title="测试server1 mysql是否设置为从"></a>测试server1 mysql是否设置为从</h4><p>serve1恢复后，登录server1的mysql服务器，执行show slave status;命令，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">Slave_IO_State: Opening tables</span><br><span class="line">Master_Host: server1</span><br><span class="line">Master_User: rpl</span><br><span class="line">Master_Port: 3306</span><br><span class="line">Connect_Retry: 60</span><br><span class="line">Master_Log_File: mysql-bin.000015</span><br><span class="line">Read_Master_Log_Pos: 48405991</span><br><span class="line">Relay_Log_File: mysql-relay-bin.000002</span><br><span class="line">Relay_Log_Pos: 361</span><br><span class="line">Relay_Master_Log_File: mysql-bin.000015</span><br><span class="line">Slave_IO_Running: Yes</span><br><span class="line">Slave_SQL_Running: yes</span><br></pre></td></tr></table></figure></p>
<h4 id="测试是否自动恢复读写分离"><a href="#测试是否自动恢复读写分离" class="headerlink" title="测试是否自动恢复读写分离"></a>测试是否自动恢复读写分离</h4><p>server1恢复后一段时间，我们可以看是读写分离是否已经恢复。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]<span class="comment"># mysql -h127.0.0.1 -P2345 -uuser -ppwd</span></span><br><span class="line">Warning: Using a password on the <span class="built_in">command</span> line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor. Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1</span><br><span class="line">Server version: 5.0.99-agent-admin</span><br><span class="line">Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line">Type ‘<span class="built_in">help</span>;’ or ‘\h’ <span class="keyword">for</span> help. Type ‘\c’ to clear the current input statement.</span><br><span class="line">mysql&gt; select * from backends;</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| backend_ndx | address | state | <span class="built_in">type</span> |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| 1 | 192.168.1.150:3306 | up | rw |</span><br><span class="line">| 2 | 192.168.1.100:3306 | up | ro |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">2 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>我们看到server1已经被添加为slave backend了。这表示已经成功恢复读写分离。</p>
<hr>
<p>来自：<a href="https://www.centos.bz" target="_blank" rel="external">https://www.centos.bz</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用lsyncd实时同步文件]]></title>
      <url>http://icyxp.github.io/blog/2016/07/lsyncd.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Lysncd 实际上是lua语言封装了 inotify 和 rsync 工具，采用了 Linux 内核（2.6.13 及以后）里的 inotify 触发机制，然后通过rsync去差异同步，达到实时的效果。我认为它最令人称道的特性是，完美解决了 inotify + rsync海量文件同步带来的文件频繁发送文件列表的问题 —— 通过时间延迟或累计触发事件次数实现。另外，它的配置方式很简单，lua本身就是一种配置语言，可读性非常强。lsyncd也有多种工作模式可以选择，本地目录cp，本地目录rsync，远程目录rsyncssh。</p>
<p>实现简单高效的本地目录同步备份（网络存储挂载也当作本地目录），一个命令搞定。</p>
<p>github地址：<a href="https://github.com/axkibe/lsyncd" target="_blank" rel="external">https://github.com/axkibe/lsyncd</a> </p>
<h2 id="使用-lsyncd-本地目录实时备份"><a href="#使用-lsyncd-本地目录实时备份" class="headerlink" title="使用 lsyncd 本地目录实时备份"></a>使用 lsyncd 本地目录实时备份</h2><p>这一节实现的功能是，本地目录source实时同步到另一个目录target，而在source下有大量的文件，并且有部分目录和临时文件不需要同步。</p>
<h3 id="安装lsyncd"><a href="#安装lsyncd" class="headerlink" title="安装lsyncd"></a>安装lsyncd</h3><p>安装<code>lsyncd</code>极为简单，已经收录在ubuntu的官方镜像源里，直接通过<code>apt-get install lsyncd</code>就可以。<br>在Redhat系（我的环境是CentOS 6.2 x86_64 ），可以手动去下载 <a href="ftp://195.220.108.108/linux/fedora/linux/updates/21/x86_64/l/lsyncd-2.1.5-6.fc21.x86_64.rpm" target="_blank" rel="external">lsyncd-2.1.5-6.fc21.x86_64.rpm</a>，但首先你得安装两个依赖<code>yum install lua lua-devel</code>。也可以通过在线安装，需要<code>epel-release</code>扩展包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">yum install lsyncd</span><br></pre></td></tr></table></figure></p>
<p><strong>源码编译安装</strong><br>从源码编译安装可以使用最新版的lsyncd程序，但必须要相应的依赖库文件和编译工具：<code>yum install lua lua-devel asciidoc cmake</code>。</p>
<p>从 <a href="http://code.google.com/p/lsyncd/downloads/list" target="_blank" rel="external">googlecode lsyncd</a> 上下载的<code>lsyncd-2.1.5.tar.gz</code>，直接<code>./configure、make &amp;&amp; make install</code>就可以了。</p>
<p>从github上下载<a href="https://github.com/axkibe/lsyncd/archive/master.zip" target="_blank" rel="external">lsyncd-master.zip</a> 的2.1.5版本使用的是 cmake 编译工具，无法<code>./configure</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uzip lsyncd-master.zip</span><br><span class="line"><span class="built_in">cd</span> lsyncd-master</span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span>/lsyncd-2.1.5</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></p>
<p>我这个版本编译时有个小bug，如果按照<code>INSTALL</code>在    <code>build</code>目录中make，会提示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[100%] Generating doc/lsyncd.1</span><br><span class="line">Updating the manpage</span><br><span class="line">a2x: failed: <span class="built_in">source</span> file not found: doc/lsyncd.1.txt</span><br><span class="line">make[2]: *** [doc/lsyncd.1] Error 1</span><br><span class="line">make[1]: *** [CMakeFiles/manpage.dir/all] Error 2</span><br><span class="line">make: *** [all] Error 2</span><br></pre></td></tr></table></figure></p>
<p>解决办法是要么直接在解压目录下cmake，不要<code>mkdir build</code>，要么在<code>CMakeList.txt</code>中搜索doc字符串，在前面加上<code>${PROJECT_SOURCE_DIR}</code>。</p>
<a id="more"></a>
<h3 id="lsyncd-conf"><a href="#lsyncd-conf" class="headerlink" title="lsyncd.conf"></a>lsyncd.conf</h3><p>下面都是在编译安装的情况下操作。</p>
<h4 id="lsyncd同步配置"><a href="#lsyncd同步配置" class="headerlink" title="lsyncd同步配置"></a>lsyncd同步配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /usr/local/lsyncd-2.1.5</span></span><br><span class="line"><span class="comment"># mkdir etc var</span></span><br><span class="line"><span class="comment"># vi etc/lsyncd.conf</span></span><br><span class="line">settings &#123;</span><br><span class="line">    logfile      =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.log"</span>,</span><br><span class="line">    statusFile   =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.status"</span>,</span><br><span class="line">    inotifyMode  = <span class="string">"CloseWrite"</span>,</span><br><span class="line">    maxProcesses = 7,</span><br><span class="line">    -- nodaemon =<span class="literal">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest"</span>,</span><br><span class="line">    -- excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary    = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive   = <span class="literal">true</span>,</span><br><span class="line">        compress  = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>到这启动 lsycnd 就可以完成实时同步了，默认的许多参数可以满足绝大部分需求，非常简单。</p>
<h4 id="lsyncd-conf配置选项说明"><a href="#lsyncd-conf配置选项说明" class="headerlink" title="lsyncd.conf配置选项说明"></a>lsyncd.conf配置选项说明</h4><p><strong>settings</strong><br>里面是全局设置，<code>--</code>开头表示注释，下面是几个常用选项说明：</p>
<ul>
<li><code>logfile</code> 定义日志文件</li>
<li><code>stausFile</code> 定义状态文件</li>
<li><code>nodaemon=true</code> 表示不启用守护模式，默认</li>
<li><code>statusInterval</code> 将lsyncd的状态写入上面的statusFile的间隔，默认10秒</li>
<li><code>inotifyMode</code> 指定inotify监控的事件，默认是<code>CloseWrite</code>，还可以是<code>Modify</code>或<code>CloseWrite or Modify</code></li>
<li><code>maxProcesses</code> 同步进程的最大个数。假如同时有20个文件需要同步，而<code>maxProcesses = 8</code>，则最大能看到有8个rysnc进程</li>
<li><code>maxDelays</code> 累计到多少所监控的事件激活一次同步，即使后面的<code>delay</code>延迟时间还未到</li>
</ul>
<p><strong>sync</strong><br>里面是定义同步参数，可以继续使用<code>maxDelays</code>来重写settings的全局变量。一般第一个参数指定lsyncd以什么模式运行：<code>rsync</code>、<code>rsyncssh</code>、<code>direct</code>三种模式：</p>
<ul>
<li><p><code>default.rsync</code>：本地目录间同步，使用rsync，也可以达到使用ssh形式的远程rsync效果，或daemon方式连接远程rsyncd进程；<br><code>default.direct</code> ：本地目录间同步，使用<code>cp</code>、<code>rm</code>等命令完成差异文件备份；<br><code>default.rsyncssh</code> ：同步到远程主机目录，rsync的ssh模式，需要使用key来认证</p>
</li>
<li><p><code>source</code> 同步的源目录，使用绝对路径。</p>
</li>
<li><p><code>target</code> 定义目的地址.对应不同的模式有几种写法：<br><code>/tmp/dest</code> ：本地目录同步，可用于<code>direct</code>和<code>rsync</code>模式<br><code>172.29.88.223:/tmp/dest</code> ：同步到远程服务器目录，可用于<code>rsync</code>和<code>rsyncssh</code>模式，拼接的命令类似于<code>/usr/bin/rsync -ltsd --delete --include-from=- --exclude=* SOURCE TARGET</code>，剩下的就是rsync的内容了，比如指定username，免密码同步<br><code>172.29.88.223::module</code> ：同步到远程服务器目录，用于<code>rsync</code>模式<br>三种模式的示例会在后面给出。</p>
</li>
<li><p><code>init</code> 这是一个优化选项，当<code>init = false</code>，只同步进程启动以后发生改动事件的文件，原有的目录即使有差异也不会同步。默认是<code>true</code></p>
</li>
<li><p><code>delay</code> 累计事件，等待rsync同步延时时间，默认15秒（最大累计到1000个不可合并的事件）。也就是15s内监控目录下发生的改动，会累积到一次rsync同步，避免过于频繁的同步。（可合并的意思是，15s内两次修改了同一文件，最后只同步最新的文件）</p>
</li>
<li><p><code>excludeFrom</code> 排除选项，后面指定排除的列表文件，如<code>excludeFrom = &quot;/etc/lsyncd.exclude&quot;</code>，如果是简单的排除，可以使用<code>exclude = LIST</code>。<br>这里的排除规则写法与原生rsync有点不同，更为简单：</p>
<ul>
<li>监控路径里的任何部分匹配到一个文本，都会被排除，例如<code>/bin/foo/bar</code>可以匹配规则<code>foo</code></li>
<li>如果规则以斜线<code>/</code>开头，则从头开始要匹配全部</li>
<li>如果规则以<code>/</code>结尾，则要匹配监控路径的末尾</li>
<li><code>?</code>匹配任何字符，但不包括<code>/</code></li>
<li><code>*</code>匹配0或多个字符，但不包括<code>/</code></li>
<li><code>**</code>匹配0或多个字符，可以是<code>/</code></li>
</ul>
</li>
<li><p><code>delete</code> 为了保持target与souce完全同步，Lsyncd默认会<code>delete = true</code>来允许同步删除。它除了<code>false</code>，还有<code>startup</code>、<code>running</code>值，请参考 <a href="https://github.com/axkibe/lsyncd/wiki/Lsyncd%202.1.x%20%E2%80%96%20Layer%204%20Config%20%E2%80%96%20Default%20Behavior" target="_blank" rel="external">Lsyncd 2.1.x ‖ Layer 4 Config ‖ Default Behavior</a>。</p>
</li>
</ul>
<p><strong>rsync</strong><br>（提示一下，<code>delete</code>和<code>exclude</code>本来都是<strong>rsync</strong>的选项，上面是配置在sync中的，我想这样做的原因是为了减少rsync的开销）</p>
<ul>
<li><code>bwlimit</code> 限速，单位kb/s，与rsync相同（这么重要的选项在文档里竟然没有标出）</li>
<li><code>compress</code> 压缩传输默认为true。在带宽与cpu负载之间权衡，本地目录同步可以考虑把它设为<code>false</code></li>
<li><code>perms</code> 默认保留文件权限。</li>
<li>其它rsync的选项</li>
</ul>
<p>其它还有rsyncssh模式独有的配置项，如<code>host</code>、<code>targetdir</code>、<code>rsync_path</code>、<code>password_file</code>，见后文示例。<code>rsyncOps={&quot;-avz&quot;,&quot;--delete&quot;}</code>这样的写法在2.1.*版本已经不支持。</p>
<p><code>lsyncd.conf</code>可以有多个<code>sync</code>，各自的source，各自的target，各自的模式，互不影响。</p>
<h3 id="启动lsyncd"><a href="#启动lsyncd" class="headerlink" title="启动lsyncd"></a>启动lsyncd</h3><p>使用命令加载配置文件，启动守护进程，自动同步目录操作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsyncd -log Exec /usr/<span class="built_in">local</span>/lsyncd-2.1.5/etc/lsyncd.conf</span><br></pre></td></tr></table></figure></p>
<h3 id="lsyncd-conf其它模式示例"><a href="#lsyncd-conf其它模式示例" class="headerlink" title="lsyncd.conf其它模式示例"></a>lsyncd.conf其它模式示例</h3><p>以下配置本人都已经过验证可行，必须根据实际需要裁剪配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">settings &#123;</span><br><span class="line">    logfile =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.log"</span>,</span><br><span class="line">    statusFile =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.status"</span>,</span><br><span class="line">    inotifyMode = <span class="string">"CloseWrite"</span>,</span><br><span class="line">    maxProcesses = 8,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- I. 本地目录同步，direct：cp/rm/mv。 适用：500+万文件，变动不大</span><br><span class="line">sync &#123;</span><br><span class="line">    default.direct,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest"</span>,</span><br><span class="line">    delay = 1</span><br><span class="line">    maxProcesses = 1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- II. 本地目录同步，rsync模式：rsync</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest1"</span>,</span><br><span class="line">    excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        bwlimit   = 2000</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- III. 远程目录同步，rsync模式 + rsyncd daemon</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"syncuser@172.29.88.223::module1"</span>,</span><br><span class="line">    delete=<span class="string">"running"</span>,</span><br><span class="line">    exclude = &#123; <span class="string">".*"</span>, <span class="string">".tmp"</span> &#125;,</span><br><span class="line">    delay = 30,</span><br><span class="line">    init = <span class="literal">false</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span>,</span><br><span class="line">        password_file = <span class="string">"/etc/rsyncd.d/rsync.pwd"</span>,</span><br><span class="line">        _extra    = &#123;<span class="string">"--bwlimit=200"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- IV. 远程目录同步，rsync模式 + ssh shell</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"172.29.88.223:/tmp/dest"</span>,</span><br><span class="line">    -- target    = <span class="string">"root@172.29.88.223:/remote/dest"</span>,</span><br><span class="line">    -- 上面target，注意如果是普通用户，必须拥有写权限</span><br><span class="line">    maxDelays = 5,</span><br><span class="line">    delay = 30,</span><br><span class="line">    -- init = <span class="literal">true</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        bwlimit   = 2000</span><br><span class="line">        -- rsh = <span class="string">"/usr/bin/ssh -p 22 -o StrictHostKeyChecking=no"</span></span><br><span class="line">        -- 如果要指定其它端口，请用上面的rsh</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- V. 远程目录同步，rsync模式 + rsyncssh，效果与上面相同</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsyncssh,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src2"</span>,</span><br><span class="line">    host      = <span class="string">"172.29.88.223"</span>,</span><br><span class="line">    targetdir = <span class="string">"/remote/dir"</span>,</span><br><span class="line">    excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    -- maxDelays = 5,</span><br><span class="line">    delay = 0,</span><br><span class="line">    -- init = <span class="literal">false</span>,</span><br><span class="line">    rsync    = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span>,</span><br><span class="line">        _extra = &#123;<span class="string">"--bwlimit=2000"</span>&#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">    ssh      = &#123;</span><br><span class="line">        port  =  1234</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的内容几乎涵盖了所有同步的模式，其中第III个要求像rsync一样配置rsyncd服务端，见本文开头。第IV、V配置ssh方式同步，达到的效果相同，但实际同步时你会发现每次同步都会提示输入ssh的密码，可以通过以下方法解决：</p>
<p>在远端被同步的服务器上开启ssh无密码登录，请注意用户身份：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user$ ssh-keygen -t rsa</span><br><span class="line">一路回车...</span><br><span class="line">user$ <span class="built_in">cd</span> ~/.ssh</span><br><span class="line">user$ cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>把<code>id_rsa</code>私钥拷贝到执行lsyncd的机器上<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user$ chmod 600 ~/.ssh/id_rsa</span><br><span class="line">测试能否无密码登录</span><br><span class="line">user$ ssh user@172.29.88.223</span><br></pre></td></tr></table></figure></p>
<h2 id="lsyncd的其它功能"><a href="#lsyncd的其它功能" class="headerlink" title="lsyncd的其它功能"></a>lsyncd的其它功能</h2><p><code>lsyncd</code>的功能不仅仅是同步，官方手册 <a href="https://github.com/axkibe/lsyncd/wiki/Lsyncd%202.1.x%20%E2%80%96%20Layer%202%20Config%20%E2%80%96%20Advanced%20onAction" target="_blank" rel="external">Lsyncd 2.1.x ‖ Layer 2 Config ‖ Advanced onAction</a> 高级功能提到，还可以监控某个目录下的文件，根据触发的事件自己定义要执行的命令，example是监控某个某个目录，只要是有jpg、gif、png格式的文件参数，就把它们转成pdf，然后同步到另一个目录。正好在我运维的一个项目中有这个需求，现在都是在java代码里转换，还容易出现异常，通过lsyncd可以代替这样的功能。但，门槛在于要会一点点lua语言（根据官方example还是可以写出来）。</p>
<p>另外偶然想到个问题，同时设置了<code>maxDelays</code>和<code>delay</code>，当监控目录一直没有文件变化了，也会发生同步操作，虽然没有可rsync的文件。</p>
<hr>
<p>来源：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL数据库开发规范]]></title>
      <url>http://icyxp.github.io/blog/2016/07/mysql-mysql-develop-standard.html</url>
      <content type="html"><![CDATA[<h2 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h2><h3 id="库名、表名、字段名必须使用小写字母，并采用下划线分割"><a href="#库名、表名、字段名必须使用小写字母，并采用下划线分割" class="headerlink" title="库名、表名、字段名必须使用小写字母，并采用下划线分割"></a>库名、表名、字段名必须使用小写字母，并采用下划线分割</h3><ul>
<li>MySQL有配置参数lower_case_table_names=1，即库表名以小写存储，大小写不敏感。如果是0，则库表名以实际情况存储，大小写敏感；如果是2，以实际情况存储，但以小写比较。</li>
<li>如果大小写混合使用，可能存在abc，Abc，ABC等多个表共存，容易导致混乱。</li>
<li>字段名显示区分大小写，但实际使⽤时不区分，即不可以建立两个名字一样但大小写不一样的字段。</li>
<li>为了统一规范， 库名、表名、字段名使用小写字母。</li>
</ul>
<h3 id="库名以-d-开头，表名以-t-开头，字段名以-f-开头"><a href="#库名以-d-开头，表名以-t-开头，字段名以-f-开头" class="headerlink" title="库名以 d 开头，表名以 t 开头，字段名以 f_ 开头"></a>库名以 d 开头，表名以 t 开头，字段名以 f_ 开头</h3><ul>
<li>比如表 <code>t_crm_relation</code>，中间的 crm 代表业务模块名</li>
<li>视图以<code>view_</code>开头，事件以<code>event_</code>开头，触发器以<code>trig_</code>开头，存储过程以<code>proc_</code>开头，函数以<code>func_</code>开头</li>
<li>普通索引以<code>idx_col1_col2</code>命名，唯一索引以<code>uk_col1_col2</code>命名（可去掉f_公共部分）。如 <code>idx_companyid_corpid_contacttime</code>(f_company_id, f_corp_id, f_contact_time)</li>
</ul>
<h3 id="库名、表名、字段名禁止超过32个字符，需见名知意"><a href="#库名、表名、字段名禁止超过32个字符，需见名知意" class="headerlink" title="库名、表名、字段名禁止超过32个字符，需见名知意"></a>库名、表名、字段名禁止超过32个字符，需见名知意</h3><p>库名、表名、字段名支持最多64个字符，但为了统一规范、易于辨识以及减少传输量，禁止超过32个字符</p>
<h3 id="临时库、表名须以tmp加日期为后缀"><a href="#临时库、表名须以tmp加日期为后缀" class="headerlink" title="临时库、表名须以tmp加日期为后缀"></a>临时库、表名须以tmp加日期为后缀</h3><p>如 t_crm_relation_tmp0425。备份表也类似，形如 <code>_bak20160425</code> 。</p>
<h3 id="按日期时间分表须符合-YYYY-MM-DD-格式"><a href="#按日期时间分表须符合-YYYY-MM-DD-格式" class="headerlink" title="按日期时间分表须符合_YYYY[MM][DD]格式"></a>按日期时间分表须符合_YYYY[MM][DD]格式</h3><p>这也是为将来有可能分表做准备的，比如<code>t_crm_ec_record_201403</code>，但像 t_crm_contact_at201506就打破了这种规范。<br>不具有时间特性的，直接以 <code>t_tbname_001</code> 这样的方式命名。</p>
<h2 id="库表基础规范"><a href="#库表基础规范" class="headerlink" title="库表基础规范"></a>库表基础规范</h2><h3 id="使用Innodb存储引擎"><a href="#使用Innodb存储引擎" class="headerlink" title="使用Innodb存储引擎"></a>使用Innodb存储引擎</h3><p>5.5版本开始mysql默认存储引擎就是InnoDB，5.7版本开始，系统表都放弃MyISAM了。</p>
<a id="more"></a>
<h3 id="表字符集统一使用UTF8"><a href="#表字符集统一使用UTF8" class="headerlink" title="表字符集统一使用UTF8"></a>表字符集统一使用UTF8</h3><ul>
<li>UTF8字符集存储汉字占用3个字节，存储英文字符占用一个字节</li>
<li>校对字符集使用默认的 utf8_general_ci</li>
<li>连接的客户端也使用utf8，建立连接时指定charset或SET NAMES UTF8;。（对于已经在项目中长期使用latin1的，救不了了）</li>
<li>如果遇到EMOJ等表情符号的存储需求，可申请使用UTF8MB4字符集</li>
</ul>
<h3 id="所有表都要添加注释"><a href="#所有表都要添加注释" class="headerlink" title="所有表都要添加注释"></a>所有表都要添加注释</h3><ul>
<li>尽量给字段也添加注释</li>
<li>类status型需指明主要值的含义，如”0-离线，1-在线”</li>
</ul>
<h3 id="控制单表字段数量"><a href="#控制单表字段数量" class="headerlink" title="控制单表字段数量"></a>控制单表字段数量</h3><ul>
<li>单表字段数上限30左右，再多的话考虑垂直分表，一是冷热数据分离，二是大字段分离，三是常在一起做条件和返回列的不分离。</li>
<li>表字段控制少而精，可以提高IO效率，内存缓存更多有效数据，从而提高响应速度和并发能力，后续 alter table 也更快。</li>
</ul>
<h3 id="所有表都必须要显式指定主键"><a href="#所有表都必须要显式指定主键" class="headerlink" title="所有表都必须要显式指定主键"></a>所有表都必须要显式指定主键</h3><ul>
<li>主键尽量采用自增方式，InnoDB表实际是一棵索引组织表，顺序存储可以提高存取效率，充分利用磁盘空间。还有对一些复杂查询可能需要自连接来优化时需要用到。</li>
<li>需要全局唯一主键时，使用外部发号器ticket server（建设中）</li>
<li>如果没有主键或唯一索引，update/delete是通过所有字段来定位操作的行，相当于每行就是一次全表扫描</li>
<li>少数情况可以使用联合唯一主键，需与DBA协商</li>
</ul>
<h3 id="不强制使用外键参考"><a href="#不强制使用外键参考" class="headerlink" title="不强制使用外键参考"></a>不强制使用外键参考</h3><p>即使2个表的字段有明确的外键参考关系，也不使用 FOREIGN KEY ，因为新纪录会去主键表做校验，影响性能。</p>
<h3 id="适度使用存储过程、视图，禁止使用触发器、事件"><a href="#适度使用存储过程、视图，禁止使用触发器、事件" class="headerlink" title="适度使用存储过程、视图，禁止使用触发器、事件"></a>适度使用存储过程、视图，禁止使用触发器、事件</h3><ul>
<li>存储过程（procedure）虽然可以简化业务端代码，在传统企业写复杂逻辑时可能会用到，而在互联网企业变更是很频繁的，在分库分表的情况下要升级一个存储过程相当麻烦。又因为它是不记录log的，所以也不方便debug性能问题。如果使用过程，一定考虑如果执行失败的情况。</li>
<li>使用视图一定程度上也是为了降低代码里SQL的复杂度，但有时候为了视图的通用性会损失性能（比如返回不必要的字段）。</li>
<li>触发器（trigger）也是同样，但也不应该通过它去约束数据的强一致性，mysql只支持“基于行的触发”，也就是说，触发器始终是针对一条记录的，而不是针对整个sql语句的，如果变更的数据集非常大的话，效率会很低。掩盖一条sql背后的工作，一旦出现问题将是灾难性的，但又很难快速分析和定位。再者需要ddl时无法使用pt-osc工具。放在transaction执行。</li>
<li>事件（event）也是一种偷懒的表现，目前已经遇到数次由于定时任务执行失败影响业务的情况，而且mysql无法对它做失败预警。建立专门的 job scheduler 平台。</li>
</ul>
<h3 id="单表数据量控制在5000w以内"><a href="#单表数据量控制在5000w以内" class="headerlink" title="单表数据量控制在5000w以内"></a>单表数据量控制在5000w以内</h3><h3 id="数据库中不允许存储明文密码"><a href="#数据库中不允许存储明文密码" class="headerlink" title="数据库中不允许存储明文密码"></a>数据库中不允许存储明文密码</h3><h2 id="字段规范"><a href="#字段规范" class="headerlink" title="字段规范"></a>字段规范</h2><h3 id="char、varchar、text等字符串类型定义"><a href="#char、varchar、text等字符串类型定义" class="headerlink" title="char、varchar、text等字符串类型定义"></a>char、varchar、text等字符串类型定义</h3><ul>
<li>对于长度基本固定的列，如果该列恰好更新又特别频繁，适合char</li>
<li>varchar虽然存储变长字符串，但不可太小也不可太大。UTF8最多能存21844个汉字，或65532个英文</li>
<li>varbinary(M)保存的是二进制字符串，它保存的是字节而不是字符，所以没有字符集的概念，M长度0-255（字节）。只用于排序或比较时大小写敏感的类型，不包括密码存储</li>
<li>TEXT类型与VARCHAR都类似，存储可变长度，最大限制也是2^16，但是它20bytes以后的内容是在数据页以外的空间存储（row_format=dynamic），对它的使用需要多一次寻址，没有默认值。<br>一般用于存放容量平均都很大、操作没有其它字段那样频繁的值。<br>网上部分文章说要避免使用text和blob，要知道如果纯用varchar可能会导致行溢出，效果差不多，但因为每行占用字节数过多，会导致buffer_pool能缓存的数据行、页下降。另外text和blob上面一般不会去建索引，而是利用sphinx之类的第三方全文搜索引擎，如果确实要创建（前缀）索引，那就会影响性能。凡事看具体场景。<br>另外尽可能把text/blob拆到另一个表中</li>
<li>BLOB可以看出varbinary的扩展版本，内容以二进制字符串存储，无字符集，区分大小写，有一种经常提但不用的场景：不要在数据库里存储图片。</li>
</ul>
<h3 id="int、tinyint、decimal等数字类型定义"><a href="#int、tinyint、decimal等数字类型定义" class="headerlink" title="int、tinyint、decimal等数字类型定义"></a>int、tinyint、decimal等数字类型定义</h3><ul>
<li>使用tinyint来代替 enum和boolean<br>ENUM类型在需要修改或增加枚举值时，需要在线DDL，成本较高；ENUM列值如果含有数字类型，可能会引起默认值混淆<br>tinyint使用1个字节，一般用于status,type,flag的列</li>
<li>建议使用 UNSIGNED 存储非负数值<br>相比不使用 unsigned，可以扩大一倍使用数值范围</li>
<li>int使用固定4个字节存储，int(11)与int(4)只是显示宽度的区别</li>
<li>使用Decimal 代替float/double存储精确浮点数<br>对于货币、金额这样的类型，使用decimal，如 decimal(9,2)。float默认只能能精确到6位有效数字</li>
</ul>
<h3 id="timestamp与datetime选择"><a href="#timestamp与datetime选择" class="headerlink" title="timestamp与datetime选择"></a>timestamp与datetime选择</h3><ul>
<li>datetime 和 timestamp类型所占的存储空间不同，前者8个字节，后者4个字节，这样造成的后果是两者能表示的时间范围不同。前者范围为1000-01-01 00:00:00 ~ 9999-12-31 23:59:59，后者范围为 1970-01-01 08:00:01 到 2038-01-19 11:14:07 。所以 TIMESTAMP 支持的范围比 DATATIME 要小。</li>
<li>timestamp可以在insert/update行时，自动更新时间字段（如 f_set_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP），但一个表只能有一个这样的定义。</li>
<li>timestamp显示与时区有关，内部总是以 UTC 毫秒 来存的。还受到严格模式的限制</li>
<li>优先使用timestamp，datetime也没问题</li>
<li>where条件里不要对时间列上使用时间函数</li>
</ul>
<h3 id="建议字段都定义为NOT-NULL"><a href="#建议字段都定义为NOT-NULL" class="headerlink" title="建议字段都定义为NOT NULL"></a>建议字段都定义为NOT NULL</h3><ul>
<li>如果是索引字段，一定要定义为not null 。因为null值会影响cordinate统计，影响优化器对索引的选择</li>
<li>如果不能保证insert时一定有值过来，定义时使用default ‘’ ，或 0</li>
</ul>
<h3 id="同一意义的字段定义必须相同"><a href="#同一意义的字段定义必须相同" class="headerlink" title="同一意义的字段定义必须相同"></a>同一意义的字段定义必须相同</h3><p>比如不同表中都有 f_user_id 字段，那么它的类型、字段长度要设计成一样</p>
<h2 id="索引规范"><a href="#索引规范" class="headerlink" title="索引规范"></a>索引规范</h2><h3 id="任何新的select-update-delete上线，都要先explain，看索引使用情况"><a href="#任何新的select-update-delete上线，都要先explain，看索引使用情况" class="headerlink" title="任何新的select,update,delete上线，都要先explain，看索引使用情况"></a>任何新的select,update,delete上线，都要先explain，看索引使用情况</h3><p>尽量避免extra列出现：Using File Sort，Using Temporary，rows超过1000的要谨慎上线。<br><strong><code>explain解读</code></strong></p>
<ul>
<li><code>type</code>：ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）</li>
<li><code>possible_keys</code>：指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用</li>
<li><code>key</code>：表示MySQL实际决定使用的键（索引）<br>如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX</li>
<li><code>ref</code>：表示选择 key 列上的索引，哪些列或常量被用于查找索引列上的值</li>
<li><code>rows</code>：根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数</li>
<li><code>Extra</code><ul>
<li><code>Using temporary</code>：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询</li>
<li><code>Using filesort</code>：MySQL中无法利用索引完成的排序操作称为“文件排序”</li>
</ul>
</li>
</ul>
<h3 id="索引个数限制"><a href="#索引个数限制" class="headerlink" title="索引个数限制"></a>索引个数限制</h3><ul>
<li>索引是双刃剑，会增加维护负担，增大IO压力，索引占用空间是成倍增加的</li>
<li>单张表的索引数量控制在5个以内，或不超过表字段个数的20%。若单张表多个字段在查询需求上都要单独用到索引，需要经过DBA评估。</li>
</ul>
<h3 id="避免冗余索引"><a href="#避免冗余索引" class="headerlink" title="避免冗余索引"></a>避免冗余索引</h3><ul>
<li>InnoDB表是一棵索引组织表，主键是和数据放在一起的聚集索引，普通索引最终指向的是主键地址，所以把主键做最后一列是多余的。如f_crm_id作为主键，联合索引(f_user_id,f_crm_id)上的f_crm_id就完全多余</li>
<li>(a,b,c)、(a,b)，后者为冗余索引。可以利用前缀索引来达到加速目的，减轻维护负担</li>
</ul>
<h3 id="没有特殊要求，使用自增id作为主键"><a href="#没有特殊要求，使用自增id作为主键" class="headerlink" title="没有特殊要求，使用自增id作为主键"></a>没有特殊要求，使用自增id作为主键</h3><ul>
<li>主键是一种聚集索引，顺序写入。组合唯一索引作为主键的话，是随机写入，适合写少读多的表</li>
<li>主键不允许更新</li>
</ul>
<h3 id="索引尽量建在选择性高的列上"><a href="#索引尽量建在选择性高的列上" class="headerlink" title="索引尽量建在选择性高的列上"></a>索引尽量建在选择性高的列上</h3><ul>
<li>不在低基数列上建立索引，例如性别、类型。但有一种情况，idx_feedbackid_type (f_feedback_id,f_type)，如果经常用 f_type=1 比较，而且能过滤掉90%行，那这个组合索引就值得创建。有时候同样的查询语句，由于条件取值不同导致使用不同的索引，也是这个道理。</li>
<li>索引选择性计算方法（基数 ÷ 数据行数）<br>Selectivity = Cardinality / Total Rows = select count(distinct col1)/count(*) from tbname，越接近1说明col1上使用索引的过滤效果越好</li>
<li>走索引扫描行数超过30%时，改全表扫描</li>
</ul>
<h3 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h3><ul>
<li>mysql使用联合索引时，从左向右匹配，遇到断开或者范围查询时，无法用到后续的索引列<br>比如索引idx_c1_c2_c3 (c1,c2,c3)，相当于创建了(c1)、(c1,c2)、(c1,c2,c3)三个索引，where条件包含上面三种情况的字段比较则可以用到索引，但像 where c1=a and c3=c 只能用到c1列的索引，像 c2=b and c3=c等情况就完全用不到这个索引</li>
<li>遇到范围查询(&gt;、&lt;、between、like)也会停止索引匹配，比如 c1=a and c2 &gt; 2 and c3=c，只有c1,c2列上的比较能用到索引，(c1,c2,c3)排列的索引才可能会都用上</li>
<li>where条件里面字段的顺序与索引顺序无关，mysql优化器会自动调整顺序</li>
</ul>
<h3 id="前缀索引"><a href="#前缀索引" class="headerlink" title="前缀索引"></a>前缀索引</h3><ul>
<li>对超过30个字符长度的列创建索引时，考虑使用前缀索引，如 idx_cs_guid2 (f_cs_guid(26))表示截取前26个字符做索引，既可以提高查找效率，也可以节省空间</li>
<li>前缀索引也有它的缺点是，如果在该列上 ORDER BY 或 GROUP BY 时无法使用索引，也不能把它们用作覆盖索引(Covering Index)</li>
<li>如果在varbinary或blob这种以二进制存储的列上建立前缀索引，要考虑字符集，括号里表示的是字节数</li>
</ul>
<h3 id="合理使用覆盖索引减少IO"><a href="#合理使用覆盖索引减少IO" class="headerlink" title="合理使用覆盖索引减少IO"></a>合理使用覆盖索引减少IO</h3><p>INNODB存储引擎中，secondary index(非主键索引，又称为辅助索引、二级索引)没有直接存储行地址，而是存储主键值。<br>如果用户需要查询secondary index中所不包含的数据列，则需要先通过secondary index查找到主键值，然后再通过主键查询到其他数据列，因此需要查询两次。覆盖索引则可以在一个索引中获取所有需要的数据列，从而避免回表进行二次查找，节省IO因此效率较高。<br>例如SELECT email，uid FROM user_email WHERE uid=xx，如果uid不是主键，适当时候可以将索引添加为index(uid，email)，以获得性能提升。</p>
<h3 id="尽量不要在频繁更新的列上创建索引"><a href="#尽量不要在频繁更新的列上创建索引" class="headerlink" title="尽量不要在频繁更新的列上创建索引"></a>尽量不要在频繁更新的列上创建索引</h3><p>如不在定义了 ON UPDATE CURRENT_STAMP 的列上创建索引，维护成本太高（好在mysql有insert buffer，会合并索引的插入）</p>
<h2 id="SQL设计"><a href="#SQL设计" class="headerlink" title="SQL设计"></a>SQL设计</h2><h3 id="杜绝直接-SELECT-读取全部字段"><a href="#杜绝直接-SELECT-读取全部字段" class="headerlink" title="杜绝直接 SELECT * 读取全部字段"></a>杜绝直接 SELECT * 读取全部字段</h3><p>即使需要所有字段，减少网络带宽消耗，能有效利用覆盖索引，表结构变更对程序基本无影响</p>
<h3 id="能确定返回结果只有一条时，使用-limit-1"><a href="#能确定返回结果只有一条时，使用-limit-1" class="headerlink" title="能确定返回结果只有一条时，使用 limit 1"></a>能确定返回结果只有一条时，使用 limit 1</h3><p><strong>在保证数据不会有误的前提下</strong>，能确定结果集数量时，多使用limit，尽快的返回结果。</p>
<h3 id="小心隐式类型转换"><a href="#小心隐式类型转换" class="headerlink" title="小心隐式类型转换"></a>小心隐式类型转换</h3><ul>
<li><p>转换规则</p>
<blockquote>
<p>a. 两个参数至少有一个是 NULL 时，比较的结果也是 NULL，例外是使用 &lt;=&gt; 对两个 NULL 做比较时会返回 1，这两种情况都不需要做类型转换<br>b. 两个参数都是字符串，会按照字符串来比较，不做类型转换<br>c. 两个参数都是整数，按照整数来比较，不做类型转换<br>d. 十六进制的值和非数字做比较时，会被当做二进制串<br>e. 有一个参数是 TIMESTAMP 或 DATETIME，并且另外一个参数是常量，常量会被转换为 timestamp<br>f. 有一个参数是 decimal 类型，如果另外一个参数是 decimal 或者整数，会将整数转换为 decimal 后进行比较，如果另外一个参数是浮点数，则会把 decimal 转换为浮点数进行比较<br>g. 所有其他情况下，两个参数都会被转换为浮点数再进行比较。</p>
</blockquote>
</li>
<li><p>如果一个索引建立在string类型上，如果这个字段和一个int类型的值比较，符合第 g 条。如f_phone定义的类型是varchar，但where使用f_phone in (098890)，两个参数都会被当成成浮点型。发生这个隐式转换并不是最糟的，最糟的是string转换后的float，mysql无法使用索引，这才导致了性能问题。如果是 f_user_id = ‘1234567’ 的情况，符合第 b 条,直接把数字当字符串比较。</p>
</li>
</ul>
<h3 id="禁止在where条件列上使用函数"><a href="#禁止在where条件列上使用函数" class="headerlink" title="禁止在where条件列上使用函数"></a>禁止在where条件列上使用函数</h3><ul>
<li>会导致索引失效，如lower(email)，f_qq % 4。可放到右边的常量上计算</li>
<li>返回小结果集不是很大的情况下，可以对返回列使用函数，简化程序开发</li>
</ul>
<h3 id="使用like模糊匹配，-不要放首位"><a href="#使用like模糊匹配，-不要放首位" class="headerlink" title="使用like模糊匹配，%不要放首位"></a>使用like模糊匹配，%不要放首位</h3><p>会导致索引失效，有这种搜索需求是，考虑其它方案，如sphinx全文搜索</p>
<h3 id="涉及到复杂sql时，务必先参考已有索引设计，先explain"><a href="#涉及到复杂sql时，务必先参考已有索引设计，先explain" class="headerlink" title="涉及到复杂sql时，务必先参考已有索引设计，先explain"></a>涉及到复杂sql时，务必先参考已有索引设计，先explain</h3><ul>
<li>简单SQL拆分，不以代码处理复杂为由。</li>
<li>比如 OR 条件： f_phone=’10000’ or f_mobile=’10000’，两个字段各自有索引，但只能用到其中一个。可以拆分成2个sql，或者union all。</li>
<li>先explain的好处是可以为了利用索引，增加更多查询限制条件</li>
</ul>
<h3 id="使用join时，where条件尽量使用充分利用同一表上的索引"><a href="#使用join时，where条件尽量使用充分利用同一表上的索引" class="headerlink" title="使用join时，where条件尽量使用充分利用同一表上的索引"></a>使用join时，where条件尽量使用充分利用同一表上的索引</h3><ul>
<li>如 select t1.a,t2.b * from t1,t2 and t1.a=t2.a and t1.b=123 and t2.c= 4 ，如果t1.c与t2.c字段相同，那么t1上的索引(b,c)就只用到b了。此时如果把where条件中的t2.c=4改成t1.c=4，那么可以用到完整的索引</li>
<li>这种情况可能会在字段冗余设计（反范式）时出现</li>
<li>正确选取inner join和left join</li>
</ul>
<h3 id="少用子查询，改用join"><a href="#少用子查询，改用join" class="headerlink" title="少用子查询，改用join"></a>少用子查询，改用join</h3><p>小于5.6版本时，子查询效率很低，不像Oracle那样先计算子查询后外层查询。5.6版本开始得到优化</p>
<h3 id="考虑使用union-all，少使用union，注意考虑去重"><a href="#考虑使用union-all，少使用union，注意考虑去重" class="headerlink" title="考虑使用union all，少使用union，注意考虑去重"></a>考虑使用union all，少使用union，注意考虑去重</h3><ul>
<li>union all不去重，而少了排序操作，速度相对比union要快，如果没有去重的需求，优先使用union all</li>
<li>如果UNION结果中有使用limit，在2个子SQL可能有许多返回值的情况下，各自加上limit。如果还有order by，请找DBA。</li>
</ul>
<h3 id="IN的内容尽量不超过200个"><a href="#IN的内容尽量不超过200个" class="headerlink" title="IN的内容尽量不超过200个"></a>IN的内容尽量不超过200个</h3><p>超过500个值使用批量的方式，否则一次执行会影响数据库的并发能力，因为单SQL只能且一直占用单CPU，而且可能导致主从复制延迟</p>
<h3 id="拒绝大事务"><a href="#拒绝大事务" class="headerlink" title="拒绝大事务"></a>拒绝大事务</h3><p>比如在一个事务里进行多个select，多个update，如果是高频事务，会严重影响MySQL并发能力，因为事务持有的锁等资源只在事务rollback/commit时才能释放。但同时也要权衡数据写入的一致性。</p>
<h3 id="避免使用is-null-is-not-null这样的比较"><a href="#避免使用is-null-is-not-null这样的比较" class="headerlink" title="避免使用is null, is not null这样的比较"></a>避免使用is null, is not null这样的比较</h3><h3 id="order-by-limit"><a href="#order-by-limit" class="headerlink" title="order by .. limit"></a>order by .. limit</h3><p>这种查询更多的是通过索引去优化，但order by的字段有讲究，比如主键id与f_time都是顺序递增，那就可以考虑order by id而非 f_time 。</p>
<h3 id="c1-lt-a-order-by-c2"><a href="#c1-lt-a-order-by-c2" class="headerlink" title="c1 &lt; a order by c2"></a>c1 &lt; a order by c2</h3><p>与上面不同的是，order by之前有个范围查询，由前面的内容可知，用不到类似(c1,c2)的索引，但是可以利用(c2,c1)索引。另外还可以改写成join的方式实现。</p>
<h3 id="分页优化"><a href="#分页优化" class="headerlink" title="分页优化"></a>分页优化</h3><p>建议使用合理的分页方式以提高分页效率，大页情况下不使用跳跃式分页<br>假如有类似下面分页语句:<br>SELECT FROM table1 ORDER BY ftime DESC LIMIT 10000,10;<br>这种分页方式会导致大量的io，因为MySQL使用的是提前读取策略。<br>推荐分页方式：<br><code>SELECT FROM table1 WHERE ftime &lt; last_time ORDER BY ftime DESC LIMIT 10</code><br>即传入上一次分页的界值</p>
<p>SELECT * FROM table as t1 inner JOIN (SELECT id FROM table ORDER BY time LIMIT 10000，10) as t2 ON t1.id=t2.id</p>
<h3 id="count计数"><a href="#count计数" class="headerlink" title="count计数"></a>count计数</h3><ul>
<li>首先count()、count(1)、count(col1)是有区别的，count()表示整个结果集有多少条记录，count(1)表示结果集里以primary key统计数量，绝大多数情况下count()与count(1)效果一样的，但count(col1)表示的是结果集里 col1 列 NOT null 的记录数。优先采用count()</li>
<li>大数据量count是消耗资源的操作，甚至会拖慢整个库，查询性能问题无法解决的，应从产品设计上进行重构。例如当频繁需要count的查询，考虑使用汇总表</li>
<li>遇到distinct的情况，group by方式可能效率更高。</li>
</ul>
<h3 id="delete-update语句改成select再explain"><a href="#delete-update语句改成select再explain" class="headerlink" title="delete,update语句改成select再explain"></a>delete,update语句改成select再explain</h3><p>select最多导致数据库慢，写操作才是锁表的罪魁祸首</p>
<h3 id="减少与数据库交互的次数，尽量采用批量SQL语句"><a href="#减少与数据库交互的次数，尽量采用批量SQL语句" class="headerlink" title="减少与数据库交互的次数，尽量采用批量SQL语句"></a>减少与数据库交互的次数，尽量采用批量SQL语句</h3><ul>
<li><code>INSERT ... ON DUPLICATE KEY UPDATE ...</code>，插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行旧行UPDATE，如果不重复则直接插入，影响1行。</li>
<li><code>REPLACE INTO</code>类似，但它是冲突时删除旧行。<code>INSERT IGNORE</code>相反，保留旧行，丢弃要插入的新行。</li>
<li>INSERT INTO VALUES(),(),()，合并插入。</li>
</ul>
<h3 id="杜绝危险SQL"><a href="#杜绝危险SQL" class="headerlink" title="杜绝危险SQL"></a>杜绝危险SQL</h3><ul>
<li>去掉where 1=1 这样无意义或恒真的条件，如果遇到update/delete或遭到sql注入就恐怖了</li>
<li>SQL中不允许出现DDL语句。一般也不给予create/alter这类权限，但阿里云RDS只区分读写用户</li>
</ul>
<h2 id="行为规范"><a href="#行为规范" class="headerlink" title="行为规范"></a>行为规范</h2><ul>
<li>不允许在DBA不知情的情况下导现网数据</li>
<li>大批量更新，如修复数据，避开高峰期，并通知DBA。直接执行sql的由运维或DBA同事操作</li>
<li>及时处理已下线业务的SQL</li>
<li>复杂sql上线审核</li>
<li>重要项目的数据库方案选型和设计必须提前通知DBA参与</li>
</ul>
<hr>
<p>原文地址：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Advanced MySQL Query Tuning]]></title>
      <url>http://icyxp.github.io/blog/2016/07/mysql-mysql-query.html</url>
      <content type="html"><![CDATA[<iframe src="//www.slideshare.net/slideshow/embed_code/key/3HLJJcJmM9KLGT" width="100%" height="550" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" allowfullscreen> </iframe>

<p>Youtube: <a href="https://www.youtube.com/watch?v=TPFibi2G_oo" target="_blank" rel="external">https://www.youtube.com/watch?v=TPFibi2G_oo</a></p>
<p>Percona webinars上有许多类似的分享，传送门： <a href="https://www.percona.com/resources/webinars" target="_blank" rel="external">https://www.percona.com/resources/webinars</a> 。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[nginx配置location与rewrite规则教程]]></title>
      <url>http://icyxp.github.io/blog/2016/07/nginx-localtion-rewrite.html</url>
      <content type="html"><![CDATA[<h2 id="location教程"><a href="#location教程" class="headerlink" title="location教程"></a>location教程</h2><p><strong>示例：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">location  = / &#123;</span><br><span class="line">    # 精确匹配 / ，主机名后面不能带任何字符串</span><br><span class="line">    [ configuration A ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location  / &#123;</span><br><span class="line">    # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求</span><br><span class="line">    # 但是正则和最长字符串会优先匹配</span><br><span class="line">    [ configuration B ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /documents/ &#123;</span><br><span class="line">    # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">    # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">    [ configuration C ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ /documents/Abc &#123;</span><br><span class="line">    # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">    # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">    [ configuration CC ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ^~ /images/ &#123;</span><br><span class="line">    # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。</span><br><span class="line">    [ configuration D ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~* \.(gif|jpg|jpeg)$ &#123;</span><br><span class="line">    # 匹配所有以 gif,jpg或jpeg 结尾的请求</span><br><span class="line">    # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则</span><br><span class="line">    [ configuration E ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /images/ &#123;</span><br><span class="line">    # 字符匹配到 /images/，继续往下，会发现 ^~ 存在</span><br><span class="line">    [ configuration F ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /images/abc &#123;</span><br><span class="line">    # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在</span><br><span class="line">    # F与G的放置顺序是没有关系的</span><br><span class="line">    [ configuration G ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ /images/abc/ &#123;</span><br><span class="line">    # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用</span><br><span class="line">    [ configuration H ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~* /js/.*/\.js</span><br></pre></td></tr></table></figure></p>
<ul>
<li>已=开头表示精确匹配<br>如 A 中只匹配根目录结尾的请求，后面不能带任何字符串。</li>
<li>^~ 开头表示uri以某个常规字符串开头，不是正则匹配</li>
<li>~ 开头表示区分大小写的正则匹配</li>
<li>~* 开头表示不区分大小写的正则匹配</li>
<li>/ 通用匹配, 如果没有其它匹配,任何请求都会匹配到</li>
</ul>
<a id="more"></a>
<p><strong><code>顺序&amp;&amp;优先级</code></strong></p>
<blockquote>
<p>(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/)</p>
</blockquote>
<p>按照上面的location写法，以下的匹配示例成立：</p>
<ul>
<li><p>/ —&gt; config A</p>
<blockquote>
<p>精确完全匹配，即使/index.html也匹配不了</p>
</blockquote>
</li>
<li><p>/downloads/download.html —&gt; config B</p>
<blockquote>
<p>匹配B以后，往下没有任何匹配，采用B</p>
</blockquote>
</li>
<li><p>/images/1.gif —&gt; configuration D</p>
<blockquote>
<p>匹配到F，往下匹配到D，停止往下</p>
</blockquote>
</li>
<li><p>/images/abc/def —&gt; config D</p>
<blockquote>
<p>最长匹配到G，往下匹配D，停止往下<br>你可以看到 任何以/images/开头的都会匹配到D并停止，FG写在这里是没有任何意义的，H是永远轮不到的，这里只是为了说明匹配顺序</p>
</blockquote>
</li>
<li><p>/documents/document.html —&gt; config C</p>
<blockquote>
<p>匹配到C，往下没有任何匹配，采用C</p>
</blockquote>
</li>
<li><p>/documents/1.jpg —&gt; configuration E</p>
<blockquote>
<p>匹配到C，往下正则匹配到E</p>
</blockquote>
</li>
<li><p>/documents/Abc.jpg —&gt; config CC</p>
<blockquote>
<p>最长匹配到C，往下正则顺序匹配到CC，不会往下到E</p>
</blockquote>
</li>
</ul>
<h3 id="实际使用建议"><a href="#实际使用建议" class="headerlink" title="实际使用建议"></a>实际使用建议</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">所以实际使用中，个人觉得至少有三个匹配规则定义，如下：</span><br><span class="line">#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。</span><br><span class="line">#这里是直接转发给后端应用服务器了，也可以是一个静态首页</span><br><span class="line"># 第一个必选规则</span><br><span class="line">location = / &#123;</span><br><span class="line">    proxy_pass http://tomcat:8080/index</span><br><span class="line">&#125;</span><br><span class="line"># 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项</span><br><span class="line"># 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用</span><br><span class="line">location ^~ /static/ &#123;</span><br><span class="line">    root /webroot/static/;</span><br><span class="line">&#125;</span><br><span class="line">location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ &#123;</span><br><span class="line">    root /webroot/res/;</span><br><span class="line">&#125;</span><br><span class="line">#第三个规则就是通用规则，用来转发动态请求到后端应用服务器</span><br><span class="line">#非静态文件请求就默认是动态请求，自己根据实际把握</span><br><span class="line">#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了</span><br><span class="line">location / &#123;</span><br><span class="line">    proxy_pass http://tomcat:8080/</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Rewrite教程"><a href="#Rewrite教程" class="headerlink" title="Rewrite教程"></a>Rewrite教程</h2><p>rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在<code>server{},location{},if{}</code>中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 <code>http://seanlook.com/a/we/index.php?id=1&amp;u=str</code> 只对<code>/a/we/index.php</code>重写。语法<code>rewrite regex replacement [flag];</code></p>
<p>如果相对域名或参数字符串起作用，可以使用全局变量匹配，也可以使用proxy_pass反向代理。</p>
<p>表明看rewrite和location功能有点像，都能实现跳转，主要区别在于rewrite是在同一域名内更改获取资源的路径，而location是对一类路径做控制访问或反向代理，可以proxy_pass到其他机器。很多情况下rewrite也会写在location里，它们的执行顺序是：</p>
<ol>
<li>执行server块的rewrite指令</li>
<li>执行location匹配</li>
<li>执行选定的location中的rewrite指令</li>
</ol>
<p>如果其中某步URI被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 Internal Server Error错误。</p>
<h3 id="flag标志位"><a href="#flag标志位" class="headerlink" title="flag标志位"></a>flag标志位</h3><ul>
<li><code>last</code> : 相当于Apache的[L]标记，表示完成rewrite</li>
<li><code>break</code>: 停止执行当前虚拟主机的后续rewrite指令集</li>
<li><code>redirect</code> : 返回302临时重定向，地址栏会显示跳转后的地址</li>
<li><code>permanent</code> : 返回301永久重定向，地址栏会显示跳转后的地址</li>
</ul>
<p>因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解：</p>
<ol>
<li>last一般写在server和if中，而break一般使用在location中</li>
<li>last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配</li>
<li>break和last都能组织继续执行后面的rewrite指令</li>
</ol>
<h3 id="if指令与全局变量"><a href="#if指令与全局变量" class="headerlink" title="if指令与全局变量"></a>if指令与全局变量</h3><p><strong>if判断指令</strong><br>语法为<code>if(condition){...}</code>，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：</p>
<ul>
<li>当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false</li>
<li>直接比较变量和内容时，使用<code>=</code>或<code>!=</code></li>
<li><code>~</code>正则表达式匹配，<code>~*</code>不区分大小写的匹配，<code>!~</code>区分大小写的不匹配</li>
<li><code>-f</code>和<code>!-f</code>用来判断是否存在文件</li>
<li><code>-d</code>和<code>!-d</code>用来判断是否存在目录</li>
<li><code>-e</code>和<code>!-e</code>用来判断是否存在文件或目录</li>
<li><code>-x</code>和<code>!-x</code>用来判断文件是否可执行</li>
</ul>
<p><strong>例如：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">if ($http_user_agent ~ MSIE) &#123;</span><br><span class="line">    rewrite ^(.*)$ /msie/$1 break;</span><br><span class="line">&#125; #如果UA包含"MSIE"，rewrite请求到/msid/目录下</span><br><span class="line"></span><br><span class="line">if ($http_cookie ~* "id=([^;]+)(?:;|$)") &#123;</span><br><span class="line">    set $id $1;</span><br><span class="line"> &#125; #如果cookie匹配正则，设置变量$id等于正则引用部分</span><br><span class="line"></span><br><span class="line">if ($request_method = POST) &#123;</span><br><span class="line">    return 405;</span><br><span class="line">&#125; #如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302</span><br><span class="line"></span><br><span class="line">if ($slow) &#123;</span><br><span class="line">    limit_rate 10k;</span><br><span class="line">&#125; #限速，$slow可以通过 set 指令设置</span><br><span class="line"></span><br><span class="line">if (!-f $request_filename)&#123;</span><br><span class="line">    break;</span><br><span class="line">    proxy_pass  http://127.0.0.1; </span><br><span class="line">&#125; #如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查</span><br><span class="line"></span><br><span class="line">if ($args ~ post=140)&#123;</span><br><span class="line">    rewrite ^ http://example.com/ permanent;</span><br><span class="line">&#125; #如果query string中包含"post=140"，永久重定向到example.com</span><br><span class="line"></span><br><span class="line">location ~* \.(gif|jpg|png|swf|flv)$ &#123;</span><br><span class="line">    valid_referers none blocked www.jefflei.com www.leizhenfang.com;</span><br><span class="line">    if ($invalid_referer) &#123;</span><br><span class="line">        return 404;</span><br><span class="line">    &#125; #防盗链</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>全局变量</strong><br>下面是可以用作if判断的全局变量</p>
<ul>
<li><code>$args</code>： #这个变量等于请求行中的参数，同<code>$query_string</code></li>
<li><code>$content_length</code> ： 请求头中的Content-length字段。</li>
<li><code>$content_type</code> ： 请求头中的Content-Type字段。</li>
<li><code>$document_root</code> ： 当前请求在root指令中指定的值。</li>
<li><code>$host</code> ： 请求主机头字段，否则为服务器名称。</li>
<li><code>$http_user_agent</code> ： 客户端agent信息</li>
<li><code>$http_cookie</code> ： 客户端cookie信息</li>
<li><code>$limit_rate</code> ： 这个变量可以限制连接速率。</li>
<li><code>$request_method</code> ： 客户端请求的动作，通常为GET或POST。</li>
<li><code>$remote_addr</code> ： 客户端的IP地址。</li>
<li><code>$remote_port</code> ： 客户端的端口。</li>
<li><code>$remote_user</code> ： 已经经过Auth Basic Module验证的用户名。</li>
<li><code>$request_filename</code> ： 当前请求的文件路径，由root或alias指令与URI请求生成。</li>
<li><code>$scheme</code> ： HTTP方法（如http，https）。</li>
<li><code>$server_protocol</code> ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。</li>
<li><code>$server_addr</code> ： 服务器地址，在完成一次系统调用后可以确定这个值。</li>
<li><code>$server_name</code> ： 服务器名称。</li>
<li><code>$server_port</code> ： 请求到达服务器的端口号。</li>
<li><code>$request_uri</code> ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。</li>
<li><code>$uri</code> ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。</li>
<li><code>$document_uri</code> ： 与$uri相同。</li>
</ul>
<p>示例：<code>http://localhost:88/test1/test2/test.php</code></p>
<ul>
<li><code>$host</code>：localhost</li>
<li><code>$server_port</code>：88</li>
<li><code>$request_uri</code>：<a href="http://localhost:88/test1/test2/test.php" target="_blank" rel="external">http://localhost:88/test1/test2/test.php</a></li>
<li><code>$document_uri</code>：/test1/test2/test.php</li>
<li><code>$document_root</code>：/var/www/html</li>
<li><code>$request_filename</code>：/var/www/html/test1/test2/test.php</li>
</ul>
<h3 id="常用正则"><a href="#常用正则" class="headerlink" title="常用正则"></a>常用正则</h3><ul>
<li><code>.</code> ： 匹配除换行符以外的任意字符</li>
<li><code>?</code> ： 重复0次或1次</li>
<li><code>+</code> ： 重复1次或更多次</li>
<li><code>*</code> ： 重复0次或更多次</li>
<li><code>\d</code> ：匹配数字</li>
<li><code>^</code> ： 匹配字符串的开始</li>
<li><code>$</code> ： 匹配字符串的介绍</li>
<li><code>{n}</code> ： 重复n次</li>
<li><code>{n,}</code> ： 重复n次或更多次</li>
<li><code>[c]</code> ： 匹配单个字符c</li>
<li><code>[a-z]</code> ： 匹配a-z小写字母的任意一个</li>
</ul>
<p>小括号<code>()</code>之间匹配的内容，可以在后面通过<code>$1</code>来引用，<code>$2</code>表示的是前面第二个<code>()</code>里的内容。正则里面容易让人困惑的是<code>\</code>转义特殊字符。</p>
<h3 id="rewrite实例"><a href="#rewrite实例" class="headerlink" title="rewrite实例"></a>rewrite实例</h3><p><strong>例1：</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="comment"># 定义image日志格式</span></span><br><span class="line">    <span class="attribute">log_format</span> imagelog <span class="string">'[<span class="variable">$time_local</span>] '</span> <span class="variable">$image_file</span> <span class="string">' '</span> <span class="variable">$image_type</span> <span class="string">' '</span> <span class="variable">$body_bytes_sent</span> <span class="string">' '</span> <span class="variable">$status</span>;</span><br><span class="line">    <span class="comment"># 开启重写日志</span></span><br><span class="line">    <span class="attribute">rewrite_log</span> <span class="literal">on</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">root</span> /home/www;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">location</span> / &#123;</span><br><span class="line">                <span class="comment"># 重写规则信息</span></span><br><span class="line">                <span class="attribute">error_log</span> logs/rewrite.log <span class="literal">notice</span>; </span><br><span class="line">                <span class="comment"># 注意这里要用‘’单引号引起来，避免&#123;&#125;</span></span><br><span class="line">                <span class="attribute">rewrite</span> <span class="string">'^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$'</span> /data?file=<span class="variable">$3</span>.<span class="variable">$4</span>;</span><br><span class="line">                <span class="comment"># 注意不能在上面这条规则后面加上“last”参数，否则下面的set指令不会执行</span></span><br><span class="line">                <span class="attribute">set</span> <span class="variable">$image_file</span> <span class="variable">$3</span>;</span><br><span class="line">                <span class="attribute">set</span> <span class="variable">$image_type</span> <span class="variable">$4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">location</span> /data &#123;</span><br><span class="line">                <span class="comment"># 指定针对图片的日志格式，来分析图片类型和大小</span></span><br><span class="line">                <span class="attribute">access_log</span> logs/images.log mian;</span><br><span class="line">                <span class="attribute">root</span> /data/images;</span><br><span class="line">                <span class="comment"># 应用前面定义的变量。判断首先文件在不在，不在再判断目录在不在，如果还不在就跳转到最后一个url里</span></span><br><span class="line">                <span class="attribute">try_files</span> /<span class="variable">$arg_file</span> /image404.html;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="attribute">location</span> = /image404.html &#123;</span><br><span class="line">                <span class="comment"># 图片不存在返回特定的信息</span></span><br><span class="line">                <span class="attribute">return</span> <span class="number">404</span> <span class="string">"image not found\n"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对形如<code>/images/ef/uh7b3/test.png</code>的请求，重写到<code>/data?file=test.png</code>，于是匹配到<code>location /data</code>，先看<code>/data/images/test.png</code>文件存不存在，如果存在则正常响应，如果不存在则重写<code>tryfiles</code>到新的<code>image404 location</code>，直接返回404状态码。</p>
<p><strong>例2：</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rewrite</span><span class="regexp"> ^/images/(.*)_(\d+)x(\d+)\.(png|jpg|gif)$</span> /resizer/<span class="variable">$1</span>.<span class="variable">$4</span>?width=<span class="variable">$2</span>&amp;height=<span class="variable">$3</span>? <span class="literal">last</span>;</span><br></pre></td></tr></table></figure></p>
<p>对形如<code>/images/bla_500x400.jpg</code>的文件请求，重写到<code>/resizer/bla.jpg?width=500&amp;height=400</code>地址，并会继续尝试匹配location。</p>
<hr>
<p>来源：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL事务隔离级别]]></title>
      <url>http://icyxp.github.io/blog/2016/07/mysql-mysql-transaction-level.html</url>
      <content type="html"><![CDATA[<h2 id="四类隔离级别"><a href="#四类隔离级别" class="headerlink" title="四类隔离级别"></a>四类隔离级别</h2><p>SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。</p>
<ul>
<li>Read Uncommitted（读取未提交内容）</li>
</ul>
<p>在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 </p>
<ul>
<li>Read Committed（读取提交内容）</li>
</ul>
<p>这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。</p>
<ul>
<li>Repeatable Read（可重读）</li>
</ul>
<p>这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。</p>
<ul>
<li>Serializable（可串行化）</li>
</ul>
<p>这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。<br><a id="more"></a></p>
<h2 id="隔离级别与一致性"><a href="#隔离级别与一致性" class="headerlink" title="隔离级别与一致性"></a>隔离级别与一致性</h2><p>这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：</p>
<ul>
<li>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</li>
<li>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</li>
<li>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</li>
</ul>
<p>在MySQL中，实现了这四种隔离级别，分别有可能产生问题如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:left">隔离级别</th>
<th style="text-align:left">脏读</th>
<th style="text-align:left">不可重复读</th>
<th style="text-align:left">幻读</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读未提交(Read Uncommitted)</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">读已提交(Read Committed)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">可重复读(Repeatable Read)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">可串行化(Serializable)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
</tr>
</tbody>
</table>
<h2 id="设置当前隔离级别"><a href="#设置当前隔离级别" class="headerlink" title="设置当前隔离级别"></a>设置当前隔离级别</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 取消autocommit</span></span><br><span class="line"><span class="keyword">set</span> autocommit=<span class="number">0</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">"%autocommit%"</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 查看隔离级别</span></span><br><span class="line"><span class="keyword">SELECT</span> @@global.tx_isolation;</span><br><span class="line"><span class="keyword">SELECT</span> @@session.tx_isolation;</span><br><span class="line"><span class="keyword">SELECT</span> @@tx_isolation;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%iso%'</span>;</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| Variable_name | Value           |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| tx_isolation  | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> <span class="keyword">global</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%iso%'</span>;</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| Variable_name | Value           |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| tx_isolation  | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 设置隔离级别</span></span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">read</span> uncommitted;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">read</span> committed;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> repeatable <span class="keyword">read</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">serializable</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 事务操作</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> text.tx;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> text.tx;</span><br><span class="line"><span class="keyword">update</span> text.tx <span class="keyword">set</span> <span class="keyword">num</span> =<span class="number">10</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> text.tx(<span class="keyword">id</span>,<span class="keyword">num</span>) <span class="keyword">values</span>(<span class="number">9</span>,<span class="number">9</span>);</span><br><span class="line"><span class="keyword">rollback</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<h2 id="my-cnf设置"><a href="#my-cnf设置" class="headerlink" title="my.cnf设置"></a>my.cnf设置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MySQL支持4种事务隔离级别，他们分别是：</span></span><br><span class="line"><span class="comment"># READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE.</span></span><br><span class="line"><span class="comment"># 如没有指定，MySQL默认采用的是REPEATABLE-READ，ORACLE默认的是READ-COMMITTED</span></span><br><span class="line">transaction_isolation = REPEATABLE-READ</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL为什么需要一个自增主键]]></title>
      <url>http://icyxp.github.io/blog/2016/07/mysql-mysql-auto-increment-primary-key.html</url>
      <content type="html"><![CDATA[<h2 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h2><p>表中每一行都应该有可以唯一标识自己的一列（或一组列）。</p>
<p>一个顾客可以使用顾客编号列，而订单可以使用订单ID，雇员可以使用雇员ID 或 雇员社会保险号。</p>
<p>主键（primary key） 一列（或一组列），其值能够唯一区分表中的每个行。<br>唯一标识表中每行的这个列（或这组列）称为主键。<strong><code>没有主键，更新或删除表中特定行很困难，因为没有安全的方法保证只涉及相关的行。</code></strong></p>
<p>虽然并不总是都需要主键，但大多数数据库设计人员都应保证他们创建的每个表有一个主键，以便于以后数据操纵和管理</p>
<p>表中的任何列都可以作为主键，只要它满足一下条件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">任何两行都不具有相同的主键值</span><br><span class="line">每个行都必须具有一个主键值（主键列不允许NULL值）</span><br></pre></td></tr></table></figure></p>
<p>主键值规范：这里列出的规则是MySQL本身强制实施的。</p>
<p>主键的最好习惯：<br>除MySQL强制实施的规则外，应该坚持的几个普遍认为的最好习惯为:</p>
<pre><code class="plain">1、不更新主键列的值
2、不重用主键列的值
3、不在主键列中使用可能会更改的值（例如，如果使用一个名字作为主键以标识某个供应商，应该供应商合并和更改其名字时，必须更改这个主键）
</code></pre>
<p>总之：不应该使用一个具有意义的column（id 本身并不保存表 有意义信息） 作为主键，并且一个表必须要有一个主键，为方便扩展、松耦合，高可用的系统做铺垫。<br><a id="more"></a></p>
<h3 id="无特殊需求下Innodb建议使用与业务无关的自增ID作为主键"><a href="#无特殊需求下Innodb建议使用与业务无关的自增ID作为主键" class="headerlink" title="无特殊需求下Innodb建议使用与业务无关的自增ID作为主键"></a>无特殊需求下Innodb建议使用与业务无关的自增ID作为主键</h3><p>InnoDB引擎使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）</p>
<p>1、如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示：<br><img src="/images/mysql_aipk_1.jpg" alt="mysql_primary_key"><br>这样就会形成一个紧凑的索引结构，近似顺序填满。<strong><code>由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。</code></strong></p>
<p>2、 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置：<br><img src="/images/mysql_aipk_2.jpg" alt="mysql_primary_key"><br><strong><code>此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片。</code></strong>得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p>
<p>在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。</p>
<p><strong><code>mysql 在频繁的更新、删除操作，会产生碎片。而含碎片比较大的表，查询效率会降低。此时需对表进行优化，这样才会使查询变得更有效率。</code></strong></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zephir安装和初体验]]></title>
      <url>http://icyxp.github.io/blog/2016/06/zephir-zephir-02.html</url>
      <content type="html"><![CDATA[<h2 id="Zephir安装"><a href="#Zephir安装" class="headerlink" title="Zephir安装"></a>Zephir安装</h2><h3 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h3><p>Zephir主要依赖于下面环境</p>
<ul>
<li>gcc &gt;= 4.x/clang &gt;= 3.x</li>
<li>re2c 0.13或更高版本</li>
<li>gnu 3.81或更高版本</li>
<li>autoconf 2.31或更高版本</li>
<li>automake 1.14或更高版本</li>
<li>libpcre3</li>
<li>php开发工具-phpize</li>
</ul>
<p>如果你使用Ubuntu，你可以安装所需要的包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install git gcc make re2c php5 php5-json php5-dev libpcre3-dev</span><br></pre></td></tr></table></figure></p>
<p>由于Zephir是用PHP编写的，所以你需要安装php<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ php -v</span><br><span class="line">PHP 5.6.5 (cli) (built: Jan 24 2015 20:04:31)</span><br><span class="line">Copyright (c) 1997-2014 The PHP Group</span><br><span class="line">Zend Engine v2.6.0, Copyright (c) 1998-2014 Zend Technologies</span><br><span class="line">with Zend OPcache v7.0.4-dev, Copyright (c) 1999-2014, by Zend Technologies</span><br></pre></td></tr></table></figure></p>
<p>同时也必须确保安装了PHP开发库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ phpize -v</span><br><span class="line">Configuring <span class="keyword">for</span>:</span><br><span class="line">PHP Api Version:         20131106</span><br><span class="line">Zend Module Api No:      20131226</span><br><span class="line">Zend Extension Api No:   220131226</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="安装Zephir"><a href="#安装Zephir" class="headerlink" title="安装Zephir"></a>安装Zephir</h3><ol>
<li><p>下载最新稳定版</p>
</li>
<li><p>运行Zephir安装程序(编译/创建解析器)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> zephir</span><br><span class="line">$ ./install-json</span><br><span class="line">$ ./install -c</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zephir <span class="built_in">help</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>会得到如下返回</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> _____              __    _</span><br><span class="line">/__  /  ___  ____  / /_  (_)____</span><br><span class="line">  / /  / _ \/ __ \/ __ \/ / ___/</span><br><span class="line"> / /__/  __/ /_/ / / / / / /</span><br><span class="line">/____/\___/ .___/_/ /_/_/_/</span><br><span class="line">         /_/</span><br><span class="line"></span><br><span class="line">Zephir version 0.9.2a-dev</span><br><span class="line"></span><br><span class="line">Usage: </span><br><span class="line">    <span class="built_in">command</span> [options]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">    install             Installs the extension (requires root password)</span><br><span class="line">    builddev            Generate/Compile/Install a Zephir extension <span class="keyword">in</span> development mode</span><br><span class="line">    <span class="built_in">help</span>                Displays this <span class="built_in">help</span></span><br><span class="line">    build               Generate/Compile/Install a Zephir extension</span><br><span class="line">    compile             Compile a Zephir extension</span><br><span class="line">    stubs               Generates extension PHP stubs</span><br><span class="line">    version             Shows the Zephir version</span><br><span class="line">    init [namespace]    Initializes a Zephir extension</span><br><span class="line">    fullclean           Cleans the generated object files <span class="keyword">in</span> compilation</span><br><span class="line">    api [--theme-path=/path][--output-directory=/path][--theme-options=&#123;json&#125;|/path]Generates a HTML API</span><br><span class="line">    generate            Generates C code from the Zephir code</span><br><span class="line">    clean               Cleans the generated object files <span class="keyword">in</span> compilation</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">    <span class="_">-f</span>([a-z0-9\-]+)     Enables compiler optimizations</span><br><span class="line">    -fno-([a-z0-9\-]+)  Disables compiler optimizations</span><br><span class="line">    -w([a-z0-9\-]+)     Turns a warning on</span><br><span class="line">    -W([a-z0-9\-]+)     Turns a warning off</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Zephir初体验"><a href="#Zephir初体验" class="headerlink" title="Zephir初体验"></a>Zephir初体验</h2><p>还记得在开篇那个Helloword例子吗？我们先来简单介绍一下Zephir编译机制，在用例子介绍一下Zephir的语法。</p>
<h3 id="编译-解释"><a href="#编译-解释" class="headerlink" title="编译/解释"></a>编译/解释</h3><p>每一种语言都会有它们的”Hello World!”例子，对于Zehpir来说也不例外，下面的这个引导例子列举了许多它重要的特性。</p>
<p>Zephir的代码必须放置在类中。Zephir是基于面向对象类/框架打造的。所以代码放置在类的外面是不允许的。另外，一个命名空间也是必须的。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Test</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">say</span><span class="params">()</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"Hello World!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>一但这个类被编译完成，它会产生下面的一段C代码（gcc/clang/vc++编译）<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">EPHIR_INIT_CLASS(Test_Hello) &#123;</span><br><span class="line">    ZEPHIR_REGISTER_CLASS(Test, Hello, hello, test_hello_method_entry, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> SUCCESS;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PHP_METHOD(Test_Hello, say) &#123;</span><br><span class="line">    php_printf(<span class="string">"%s"</span>, <span class="string">"Hello World!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>事实上，使用Zephir的开发者无需懂得C语言，如果你有使用编译器，或者php内部的构造，或者C语言本身的经验， 在使用Zephir的时候你将会感到更加的清晰。</p>
<h3 id="Zephir初试"><a href="#Zephir初试" class="headerlink" title="Zephir初试"></a>Zephir初试</h3><p>在接下来的例子中，我们将会尽详细的描述，以便你知道是怎么回事。 我们的目标是让你感觉一下到底Zephir是怎么样的一个东西。 随便我们将会详细的探索Zephir的新特性。    </p>
<p>下面的例子很简单，它提供一个类和一个函数，检测一个数组的类型</p>
<p>让我们认真的检查下面的代码，开始认真的的学习Zephir. 这几行代码包括了很多详细的东西，我们将会慢慢的解释。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * MyTest (test/mytest.zep)</span><br><span class="line"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTest</span> </span>&#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">someMethod</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">/* 变量必须声明 */</span></span><br><span class="line">		 <span class="keyword">var</span> myArray;</span><br><span class="line">		 int i = <span class="number">0</span>, length;</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/*创建一个数组 */</span></span><br><span class="line">		 let myArray = [<span class="string">"hello"</span>, <span class="number">0</span>, <span class="number">100.25</span>, <span class="keyword">false</span>, <span class="keyword">null</span>];</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/* 数组有多少个元素*/</span></span><br><span class="line">		 let length = count(myArray);</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/* 打印值类型 */</span></span><br><span class="line">		 <span class="keyword">while</span> i &lt; length &#123;</span><br><span class="line">		     <span class="keyword">echo</span> typeof myArray[i], <span class="string">"\n"</span>;</span><br><span class="line">		     let i++;</span><br><span class="line">		 &#125;</span><br><span class="line">		 </span><br><span class="line">		 <span class="keyword">return</span> myArray;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在函数中，第一行使用了’var’ 和 ‘int’ 关键词来声明一个函数内的私有变量。 在函数中的每一个变量必须事先声明它们自己的类型。这些声明并不是随意的，它帮助编译器来报告给你关于 错误的变量，或者变量的使用是否超出的它的范围，通常它会在最后抛出错误。</p>
<p>动态的变量必须以关键词’var’来声明。这些变量可以被指定或再指定成不同的变量类型。另一方面，’i’ and ‘length’使用了整数的静态变量，在执行程序的过程中，它只能改变值，而不能改变变量的类型。</p>
<p>与PHP不同的是，你不用在变量的前面加上($)符号。</p>
<p>Zephir的注释和Java, C#, C++等等一些语言的一样。</p>
<p>默认的，变量是不变的，意思是说Zephir期望大部分的变量保持不变。变量保持它们原始的值不变可以优化成静态常量。 如果需要改变变量的值，请使用关键词’let’<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 创建一个数组 */</span></span><br><span class="line">let myArray = [<span class="string">"hello"</span>, <span class="number">0</span>, <span class="number">100.25</span>, <span class="keyword">false</span>, <span class="keyword">null</span>];</span><br></pre></td></tr></table></figure></p>
<p>默认的，数组是一种象PHP一样的动态变量，它包含了许多不同类型的值。令人吃惊的是，PHP内部的函数可以在Zephir中使用，在下面的例子中，’count’ 函数被使用了，编辑器可以以最佳的状态来执行，因为它已经知道了数组的长度了。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*数组有多少个元素 */</span></span><br><span class="line">let length = count(myArray);</span><br></pre></td></tr></table></figure></p>
<p>同样的，我们可以使用花括号来控制程序的流程.<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> i &lt; length &#123;</span><br><span class="line">    <span class="keyword">echo</span> typeof myArray[i], <span class="string">"\n"</span>;</span><br><span class="line">    let i++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>PHP的变量总是动态的，函数总是返回的是可变的动态变量，这就意味着如果一个静态变量在Zphir中被返回了，在PHP的调用中 你得到的却是一个动态变量。</p>
<p><strong>请注意！内存是在编译器中自动管理的，所以你没有必要像C语言一样去分配和释放内存。</strong> 这和PHP是很相似的。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zephir介绍]]></title>
      <url>http://icyxp.github.io/blog/2016/06/zephir-zephir-01.html</url>
      <content type="html"><![CDATA[<h2 id="zephir介绍"><a href="#zephir介绍" class="headerlink" title="zephir介绍"></a>zephir介绍</h2><p>Zephir是一种可以让PHP开发者尝试编写和编译可以被PHP执行代码的一种语言。它是动态/静态类型，它的一些特性对于PHP 开发者来说是非常的相似的。</p>
<p>Zephir的名字是取自Zend Engine/PHP/Intermediate的缩写。建议发音为zephyr相同。事实上Zephir的创造者发音为zaefire_.</p>
<h3 id="简单易于开发"><a href="#简单易于开发" class="headerlink" title="简单易于开发"></a>简单易于开发</h3><p>相信大家和我有一样的经历，看到了yaf和phalcon在想为什么C语言的拓展框架可以这么的快，我自己能不能写一个出来呢？然后屁颠屁颠的跑去找资料找大神了解，大神说你去看一下 “PHP扩展开发及内核应用”，结果大家都知道醉了。</p>
<p>主要原因是需要对C相对的熟悉并且对PHP内核API也要很熟悉，我觉得这已经不是门槛的问题了是太平洋的距离，就草草结束了研究。</p>
<p>当遇到zephir首先了解的就是复杂程度，结果花了10分钟就跟着流程做了一个小DEMO，就这点看来就开发效率这点看来无可厚非的的高效快速，大家感受一下。<br><a id="more"></a><br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Icyboy</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">function</span> <span class="title">hi</span><span class="params">()</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        <span class="keyword">echo</span>  <span class="string">"hello world"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译之后引入到php.ini里面，使用方式如下<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="title">Icyboy</span>\<span class="title">Hello</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">echo</span> Hello::hi() . PHP_EOL;</span><br></pre></td></tr></table></figure></p>
<p>zephir是一个解释器语言和PHP非常近似，通过zephir的机制编译成C语言，然后通过C编译出PHP拓展提供使用，把中间过程高度封装，很大程度让PHP拓展开发简单了很多。</p>
<p><strong>PHP扩展开发及内核应用</strong> <a href="http://www.walu.cc/phpbook" target="_blank" rel="external">http://www.walu.cc/phpbook</a></p>
<h3 id="zephir特性"><a href="#zephir特性" class="headerlink" title="zephir特性"></a>zephir特性</h3><ul>
<li>zephir是静态动态结合语言，在zephir内可以使用传统静态变量，也可以使用动态变量，灵活度高。</li>
<li>内存安全，熟悉C程序的童鞋都知道C可以控制内存指针，其实用的不好是一件很危险的事情，zephir它不允许你使用指针，它提供了一个<strong>task-local垃圾收集器</strong>，以避免内存泄漏。</li>
<li>编译模式，zephir能够编译主流系统Liunx/OSX/Windows能够识别的拓展程序。</li>
<li>开发源代码的高级语言，以面向对象为基础，编写拓展都需要基于面向对象。</li>
</ul>
<h3 id="感受一下"><a href="#感受一下" class="headerlink" title="感受一下"></a>感受一下</h3><p>下面是官方提供的一个让大家感受一下的小例子作用是过滤变量返回字母字符<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">MyLibrary</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">alpha</span><span class="params">(string str)</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        char ch; string filtered = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span> ch in str &#123;</span><br><span class="line">           <span class="keyword">if</span> (ch &gt;= <span class="string">'a'</span> &amp;&amp; ch &lt;= <span class="string">'z'</span>) || (ch &gt;= <span class="string">'A'</span> &amp;&amp; ch &lt;= <span class="string">'Z'</span>) &#123;</span><br><span class="line">              let filtered .= ch;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> filtered;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>从PHP类可以使用如下<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"></span><br><span class="line">$filter = <span class="keyword">new</span> MyLibrary\Filter();</span><br><span class="line"><span class="keyword">echo</span> $filter-&gt;alpha(<span class="string">"01he#l.lo?/1"</span>); <span class="comment">// 结果输出 hello</span></span><br></pre></td></tr></table></figure></p>
<h2 id="为什么是Zephir"><a href="#为什么是Zephir" class="headerlink" title="为什么是Zephir"></a>为什么是Zephir</h2><p>今天的PHP应用程序必须平衡一系列问题包括稳定性、性能和功能。</p>
<p>每一个PHP应用程序是基于一组常见的组件或者说框架，这些公共组件是库/框架或它们的组合。一旦安装后很少改变，作为应用程序的基础，他们必须是有非常快的,</p>
<p>快速和强大的库会很复杂，由于高水平的抽象，一般的做法是约定基础库或框架很少改变，才有机会来改善性能和资源消耗。</p>
<p>Zephir，您可以实现面向对象库/框架/应用程序，使您的应用程序速度提高，改善用户体验。</p>
<h3 id="如果你是一个PHP程序员……"><a href="#如果你是一个PHP程序员……" class="headerlink" title="如果你是一个PHP程序员……"></a>如果你是一个PHP程序员……</h3><p>PHP是在使用的Web应用程序开发中最流行的语言之一。像PHP动态类型和解释语言，由于其灵活性，提供非常高的效率。</p>
<p>PHP是基于Zend引擎的实现。这是执行从字节码表示的PHP代码的虚拟机。Zend引擎是世界上每一个PHP的安装几乎目前，随着Zephir，您可以创建在Zend引擎运行PHP扩展。</p>
<p>PHP托管Zephir，所以他们显然有很多相似的地方，但是，他们有给Zephir自己的个性的重要差异。例如，Zephir更加严格，它可以让你减少编译步骤。</p>
<h3 id="如果你是一个C程序员……"><a href="#如果你是一个C程序员……" class="headerlink" title="如果你是一个C程序员……"></a>如果你是一个C程序员……</h3><p>C是有史以来最强大的和流行的语言之一。 事实上，PHP是用C编写的。</p>
<p>然而，用C开发大型应用程序可以把PHP或Zephir相比比预期的要长很多，一些错误是很难找到。如果你不是一个有经验的开发人员。</p>
<p>Zephir设计是安全的，所以它没有实现指针或手动内存管理，如果你是一个C程序员，你会觉得Zephir强大，比C更加的友好。</p>
<h3 id="编译VS解读"><a href="#编译VS解读" class="headerlink" title="编译VS解读"></a>编译VS解读</h3><p>编译通常会减慢下来的发展；你需要多一点耐心，使你的代码编译运行它之前。此外，该解释趋于降低有利于生产率的性能。</p>
<p>为了更高的效率，Zephir需要编译你的代码，但是他不会影响高生产效率，开发人员可以决定哪些应用程序部分应当在Zephir，哪些不是。</p>
<h3 id="静态类型和动态类型语言"><a href="#静态类型和动态类型语言" class="headerlink" title="静态类型和动态类型语言"></a>静态类型和动态类型语言</h3><p>一般来说，在静态类型语言中，变量是绑定到一个特定类型的一生。 其类型不能改变，只能参考实例和兼容操作。 像C / c++语言实现的方案<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int a = <span class="number">0</span>;</span><br><span class="line">a = <span class="string">"hello"</span>; <span class="comment">// not allowed</span></span><br></pre></td></tr></table></figure></p>
<p>在动态类型，绑定到类型的值，而不是变量。 所以，一个变量可能引用值的类型，然后重新分配后的值类型无关。 Javascript / PHP的例子 动态类型语言<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">0</span>;</span><br><span class="line">a = <span class="string">"hello"</span>; <span class="comment">// allowed</span></span><br></pre></td></tr></table></figure></p>
<p>尽管动态类型有着生产力的优势，但是动态语言并不能成为所有应用的选择，特别是对于非常大型代码库和高性能的应用程序。</p>
<p>优化性能的动态语言像PHP比静态语言(如C)是更具挑战性的。 在静态语言中，优化器可以利用类型信息做出决策。 在动态语言中，只有很有限的信息是可用的，这使得优化器的选择更加困难。</p>
<p>如果你需要非常高的性能,，静态语言可能是一个更安全的选择。</p>
<p>静态语言的另一个好处是编译器执行额外的检查。 编译器无法发现逻辑错误，这更重要但是编译器可以提前发现错误，动态语言只能在运行提示报错信息。</p>
<p>Zephir是静态和动态类型都允许使用的。</p>
<h3 id="代码保护"><a href="#代码保护" class="headerlink" title="代码保护"></a>代码保护</h3><p>在某些情况下，编译不显著提高性能，这可能是因为瓶颈所在。 在应用程序的I / O(很有可能)，而不是计算/内存限制。 然而，编译代码也可能带来某种程度的intelectual保护您的应用程序。 Zephir产生本地二进制文件，你也有能力“隐藏”用户或客户的原始代码。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Zephir不是用来取代PHP或C，相反我们认为这是一个补充，允许开发者进入代码编译和静态类型。Zephir正是试图加入从C和PHP的世界，美好的事物寻找机会使他们的应用程序更快！如果你喜欢PHP，如果你渴望执行效率，那就别犹豫赶快尝试一下Zephir吧！</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Cache 应用中的服务过载案例研究]]></title>
      <url>http://icyxp.github.io/blog/2016/06/cache-server.html</url>
      <content type="html"><![CDATA[<p>简单地说，过载是外部请求对系统的访问量突然激增，造成请求堆积，服务不可用，最终导致系统崩溃。本文主要分析引入Cache可能造成的服务过载，并讨论相关的预防、恢复策略。</p>
<p>Cache在现代系统中使用广泛，由此引入的服务过载隐患无处不在，但却非常隐蔽，容易被忽视。本文希望能为开发者在设计和编写相关类型应用，以及服务过载发生处理时能够有章可循。</p>
<h2 id="一个服务过载案例"><a href="#一个服务过载案例" class="headerlink" title="一个服务过载案例"></a>一个服务过载案例</h2><p>本文讨论的案例是指存在正常调用关系的两个系统（假设调用方为A系统，服务方为B系统），A系统对B系统的访问突然超出B系统的承受能力，造成B系统崩溃。造成服务过载的原因很多，这里分析的是严重依赖Cache的系统服务过载。首先来看一种包含Cache的体系结构（如下图所示）。<br><img src="/images/2_cache.png" alt="Cache应用体系结构"></p>
<p>A系统依赖B系统的读服务，A系统是60台机器组成的集群，B系统是6台机器组成的集群，之所以6台机器能够扛住60台机器的访问，是因为A系统并不是每次都访问B，而是首先请求Cache，只有Cache的相应数据失效时才会请求B。<br><a id="more"></a><br>这正是Cache存在的意义，它让B系统节省了大量机器；如果没有Cache，B系统不得不组成60台机器的集群，如果A也同时依赖除B系统外的另一个系统（假设为C系统）呢？那么C系统也要60台机器，放大的流量将很快耗尽公司的资源。</p>
<p>然而Cache的引入也不是十全十美的，这个结构中如果Cache发生问题，全部的流量将流向依赖方，造成流量激增，从而引发依赖系统的过载。</p>
<p>回到A和B的架构，造成服务过载的原因至少有下面三种：</p>
<ul>
<li>B系统的前置代理发生故障或者其他原因造成B系统暂时不可用，等B系统系统服务恢复时，其流量将远远超过正常值。</li>
<li>Cache系统故障，A系统的流量将全部流到B系统，造成B系统过载。</li>
<li>Cache故障恢复，但这时Cache为空，Cache瞬间命中率为0，相当于Cache被击穿，造成B系统过载。</li>
</ul>
<p>第一个原因不太好理解，为什么B系统恢复后流量会猛增呢？主要原因就是缓存的超时时间。当有数据超时的时候，A系统会访问B系统，但是这时候B系统偏偏故障不可用，那么这个数据只好超时，等发现B系统恢复时，发现缓存里的B系统数据已经都超时了，都成了旧数据，这时当然所有的请求就打到了B。</p>
<p>下文主要介绍服务过载的预防和发生后的一些补救方法，以预防为主，从调用方和服务方的视角阐述一些可行方案。</p>
<h2 id="服务过载的预防"><a href="#服务过载的预防" class="headerlink" title="服务过载的预防"></a>服务过载的预防</h2><p>所谓Client端指的就是上文结构中的A系统，相对于B系统，A系统就是B系统的Client，B系统相当于Server。</p>
<h3 id="Client端的方案"><a href="#Client端的方案" class="headerlink" title="Client端的方案"></a>Client端的方案</h3><p>针对上文阐述的造成服务过载的三个原因：B系统故障恢复、Cache故障、Cache故障恢复，我们看看A系统有哪些方案可以应对。</p>
<blockquote>
<p>合理使用Cache应对B系统宕机</p>
</blockquote>
<p>一般情况下，Cache的每个Key除了对应Value，还对应一个过期时间T，在T内，get操作直接在Cache中拿到Key对应Value并返回。但是在T到达时，get操作主要有五种模式：</p>
<h4 id="基于超时的简单（stupid）模式"><a href="#基于超时的简单（stupid）模式" class="headerlink" title="基于超时的简单（stupid）模式"></a>基于超时的简单（stupid）模式</h4><p>在T到达后，任何线程get操作发现Cache中的Key和对应Value将被清除或标记为不可用，get操作将发起调用远程服务获取Key对应的Value，并更新写回Cache，然后get操作返回新值；如果远程获取Key-Value失败，则get抛出异常。</p>
<p>为了便于理解，举一个码头工人取货的例子：5个工人（线程）去港口取同样Key的货（get），发现货已经过期被扔掉了，这时5个工人各自分别去对岸取新货，然后返回。</p>
<h4 id="基于超时的常规模式"><a href="#基于超时的常规模式" class="headerlink" title="基于超时的常规模式"></a>基于超时的常规模式</h4><p>在T到达后，Cache中的Key和对应Value将被清除或标记为不可用，get操作将调用远程服务获取Key对应的Value，并更新写回Cache；此时，如果另一个线程发现Key和Value已经不可用，get操作还需要判断有没有其他线程发起了远程调用，如果有，那么自己就等待，直到那个线程远程获取操作成功，Cache中得Key变得可用，get操作返回新的Value。如果远程获取操作失败，则get操作抛出异常，不会返回任何Value。</p>
<p>还是码头工人的例子：5个工人（线程）去港口取同样Key的货（get），发现货已经过期被扔掉了，那么只需派出一个人去对岸取货，其他四个人在港口等待即可，而不用5个人全去。</p>
<p>基于超时的简单模式和常规模式区别在于对于同一个超时的Key，前者每个get线程一旦发现Key不存在，则发起远程调用获取值；而后者每个get线程发现Key不存在，则还要判断当前是否有其他线程已经发起了远程调用操作获取新值，如果有，自己就简单的等待即可。</p>
<p>显然基于超时的常规模式比基于超时的简单模式更加优化，减少了超时时并发访问后端的调用量。</p>
<p>实现基于超时的常规模式就需要用到经典的Double-checked locking惯用法了。</p>
<h4 id="基于刷新的简单（stupid）模式"><a href="#基于刷新的简单（stupid）模式" class="headerlink" title="基于刷新的简单（stupid）模式"></a>基于刷新的简单（stupid）模式</h4><p>在T到达后，Cache中的Key和相应Value不动，但是如果有线程调用get操作，将触发refresh操作，根据get和refresh的同步关系，又分为两种模式：</p>
<ul>
<li>同步模式：任何线程发现Key过期，都触发一次refresh操作，get操作等待refresh操作结束，refresh结束后，get操作返回当前Cache中Key对应的Value。注意refresh操作结束并不意味着refresh成功，还可能抛了异常，没有更新Cache，但是get操作不管，get操作返回的值可能是旧值。</li>
<li>异步模式：任何线程发现Key过期，都触发一次refresh操作，get操作触发refresh操作，不等refresh完成，直接返回Cache中的旧值。</li>
</ul>
<p>举上面码头工人的例子说明基于刷新的常规模式：这次还是5工人去港口取货，这时货都在，但是已经旧了，这时5个工人有两种选择：</p>
<ul>
<li>5个人各自去远程取新货，如果取货失败，则拿着旧货返回（同步模式）</li>
<li>5个人各自通知5个雇佣工去取新货，5个工人拿着旧货先回（异步模式）</li>
</ul>
<h4 id="基于刷新的常规模式"><a href="#基于刷新的常规模式" class="headerlink" title="基于刷新的常规模式"></a>基于刷新的常规模式</h4><p>在T到达后，Cache中的Key和相应Value都不会被清除，而是被标记为旧数据，如果有线程调用get操作，将触发refresh更新操作，根据get和refresh的同步关系，又分为两种模式：</p>
<ul>
<li>同步模式：get操作等待refresh操作结束，refresh结束后，get操作返回当前Cache中Key对应的Value，注意：refresh操作结束并不意味着refresh成功，还可能抛了异常，没有更新Cache，但是get操作不管，get操作返回的值可能是旧值。如果其他线程进行get操作，Key已经过期，并且发现有线程触发了refresh操作，则自己不等refresh完成直接返回旧值。</li>
<li>异步模式：get操作触发refresh操作，不等refresh完成，直接返回Cache中的旧值。如果其他线程进行get操作，发现Key已经过期，并且发现有线程触发了refresh操作，则自己不等refresh完成直接返回旧值。</li>
</ul>
<p>再举上面码头工人的例子说明基于刷新的常规模式：这次还是5工人去港口取货，这时货都在，但是已经旧了，这时5个工人有两种选择：</p>
<ul>
<li>派一个人去远方港口取新货，其余4个人拿着旧货先回（同步模式）。</li>
<li>5个人通知一个雇佣工去远方取新货，5个人都拿着旧货先回（异步模式）。</li>
</ul>
<p>基于刷新的简单模式和基于刷新的常规模式区别就在于取数线程之间能否感知当前数据是否正处在刷新状态，因为基于刷新的简单模式中取数线程无法感知当前过期数据是否正处在刷新状态，所以每个取数线程都会触发一个刷新操作，造成一定的线程资源浪费。</p>
<p>而基于超时的常规模式和基于刷新的常规模式区别在于前者过期数据将不能对外访问，所以一旦数据过期，各线程要么拿到数据，要么抛出异常；后者过期数据可以对外访问，所以一旦数据过期，各线程要么拿到新数据，要么拿到旧数据。</p>
<h4 id="基于刷新的续费模式"><a href="#基于刷新的续费模式" class="headerlink" title="基于刷新的续费模式"></a>基于刷新的续费模式</h4><p>该模式和基于刷新的常规模式唯一的区别在于refresh操作超时或失败的处理上。在基于刷新的常规模式中，refresh操作超时或失败时抛出异常，Cache中的相应Key-Value还是旧值，这样下一个get操作到来时又会触发一次refresh操作。</p>
<p>在基于刷新的续费模式中，如果refresh操作失败，那么refresh将把旧值当成新值返回，这样就相当于旧值又被续费了T时间，后续T时间内get操作将取到这个续费的旧值而不会触发refresh操作。</p>
<p>基于刷新的续费模式也像常规模式那样分为同步模式和异步模式，不再赘述。</p>
<p>下面讨论这5种Cache get模式在服务过载发生时的表现，首先假设如下：</p>
<ul>
<li>假设A系统的访问量为每分钟M次。</li>
<li>假设Cache能存Key为C个，并且Key空间有N个。</li>
<li>假设正常状态下，B系统访问量为每分钟W次，显然W&lt;N&lt;M。</li>
</ul>
<p>这时因为某种原因，比如B长时间故障，造成Cache中得Key全部过期，B系统这时从故障中恢复，五种get模式分析表现分析如下：</p>
<ul>
<li>在基于超时和刷新的简单模式中，B系统的瞬间流量将达到和A的瞬时流量M大体等同，相当于Cache被击穿。这就发生了服务过载，这时刚刚恢复的B系统将肯定会被大流量压垮。</li>
<li>在基于超时和刷新的常规模式中，B系统的瞬间流量将和Cache中Key空间N大体等同。这时是否发生服务过载，就要看Key空间N是否超过B系统的流量上限了。</li>
<li>在基于刷新的续费模式中，B系统的瞬间流量为W，和正常情况相同而不会发生服务过载。实际上，在基于刷新的续费模式中，不存在Cache Key全部过期的情况，就算把B系统永久性地干掉，A系统的Cache也会基于旧值长久的平稳运行。</li>
</ul>
<p>第3点，B系统不会发生服务过载的主要原因是基于刷新的续费模式下不会出现chache中的Key全部长时间过期的情况，即使B系统长时间不可用，基于刷新的续费模式也会在一个过期周期内把旧值当成新值继续使用。所以当B系统恢复时，A系统的Cache都处在正常工作状态。</p>
<p>从B系统的角度看，能够抵抗服务过载的基于刷新的续费模式最优。</p>
<p>从A系统的角度看，由于一般情况下A系统是一个高访问量的在线web应用，这种应用最讨厌的一个词就是“线程等待”，因此基于刷新的各种异步模式较优。</p>
<p>综合考虑，基于刷新的异步续费模式是首选。然而凡事有利就有弊，有两点需要注意的地方：</p>
<ul>
<li>基于刷新模式最大的缺点是Key-Value一旦放入Cache就不会被清除，每次更新也是新值覆盖旧值，JVM GC永远无法对其进行垃圾收集，而基于超时的模式中，Key-Value超时后如果新的访问没有到来，内存是可以被GC垃圾回收的。所以如果你使用的是寸土寸金的本地内存做Cache就要小心了。</li>
<li>基于刷新的续费模式需要做好监控，不然有可能Cache中的值已经和真实的值相差很远了，应用还以为是新值而使用。</li>
</ul>
<p>关于具体的Cache，来自Google的Guava本地缓存库支持上文的第二种、第四种和第五种get操作模式。</p>
<p>但是对于Redis等分布式缓存，只提供原始的get、set方法，而提供的get仅仅是获取，与上文提到的五种get操作模式不是一个概念。开发者想用这五种get操作模式的话不得不自己封装和实现。</p>
<p>五种get操作模式中，基于超时和刷新的简单模式是实现起来最简单的模式，但遗憾的是这两种模式对服务过载完全无免疫力，这可能也是服务过载在大量依赖缓存的系统中频繁发生的一个重要原因吧。</p>
<p>本文之所以把第1、3种模式称为stupid模式，是想强调这种模式应该尽量避免，Guava里面根本没有这种模式，而Redis只提供简单的读写操作，很容易就把系统实现成了这种方式。</p>
<blockquote>
<p>应对分布式Cache宕机</p>
</blockquote>
<p>如果是Cache直接挂了，那么就算是基于刷新的异步续费模式也无能为力了。这时A系统铁定无法对Cache进行存取操作，只能将流量完全打到B系统，B系统面对服务过载在劫难逃……</p>
<p>本节讨论的预防Cache宕机仅限于分布式Cache，因为本地Cache一般和A系统应用共享内存和进程，本地Cache挂了A系统也挂了，不会出现本地Cache挂了而A系统应用正常的情况。</p>
<p>首先，A系统请求线程检查分布式Cache状态，如果无应答则说明分布式Cache挂了，则转向请求B系统，这样一来大流量将压垮B系统。这时可选的方案如下：</p>
<ul>
<li>A系统的当前线程不请求B系统，而是打个日志并设置一个默认值。</li>
<li>A系统的当前线程按照一定概率决定是否请求B系统。</li>
<li>A系统的当前线程检查B系统运行情况，如果良好则请求B系统。</li>
</ul>
<p><strong>方案1</strong> 最简单，A系统知道如果没有Cache，B系统可能扛不住自己的全部流量，索性不请求B系统，等待Cache恢复。但这时B系统利用率为0，显然不是最优方案，而且当请求的Value不容易设置默认值时，这个方案就不行了。</p>
<p><strong>方案2</strong> 可以让一部分线程请求B系统，这部分请求肯定能被B系统hold住。可以保守的设置这个概率 u =（B系统的平均流量）/（A系统的峰值流量）。</p>
<p><strong>方案3</strong> 是一种更为智能的方案，如果B系统运行良好，当前线程请求；如果B系统过载，则不请求，这样A系统将让B系统处于一种宕机与不宕机的临界状态，最大限度挖掘B系统性能。这种方案要求B系统提供一个性能评估接口返回Yes和No，Yes表示B系统良好，可以请求；No表示B系统情况不妙，不要请求。这个接口将被频繁调用，必须高效。</p>
<p>方案3的关键在于如何评估一个系统的运行状况。一个系统中当前主机的性能参数有CPU负载、内存使用率、Swap使用率、GC频率和GC时间、各个接口平均响应时间等，性能评估接口需要根据这些参数返回Yes或者No，是不是机器学习里的二分类问题？??关于这个问题已经可以单独写篇文章讨论了，在这里就不展开了，你可以想一个比较简单傻瓜的保守策略，缺点是A系统的请求无法很好的逼近B系统的性能极限。</p>
<p>综合以上分析，方案2比较靠谱。如果选择方案3，建议由专门团队负责研究并提供统一的系统性能实时评估方案和工具。</p>
<blockquote>
<p>应对分布式Cache宕机后的恢复</p>
</blockquote>
<p>不要以为成功hold住分布式Cache宕机就万事大吉了，真正的考验是分布式Cache从宕机过程恢复之后，这时分布式Cache中什么都没有。</p>
<p>即使是上文中提到了基于刷新的异步续费策略这时也没用，因为分布式Cache为空，无论如何都要请求B系统。这时B系统的最大流量是Key的空间取值数量。</p>
<p>如果Key的取值空间数量很少，则相安无事；如果Key的取值空间数量大于B系统的流量上限，服务过载依然在所难免。</p>
<p>这种情况A系统很难处理，关键原因是A系统请求Cache返回Key对应Value为空，A系统无法知道是因为当前Cache是刚刚初始化，所有内容都为空；还是因为仅仅是自己请求的那个Key没在Cache里。</p>
<p>如果是前者，那么当前线程就要像处理Cache宕机那样进行某种策略的回避；如果是后者，直接请求B系统即可，因为这是正常的Cache使用流程。</p>
<p>对于Cache宕机的恢复，A系统真的无能为力，只能寄希望于B系统的方案了。</p>
<h3 id="Server端的方案"><a href="#Server端的方案" class="headerlink" title="Server端的方案"></a>Server端的方案</h3><p>相对于Client端需要应对各种复杂问题，Server端需要应对的问题非常简单，就是如何从容应对过载的问题。无论是缓存击穿也好，还是拒绝服务攻击也罢，对于Server端来说都是过载保护的问题。对于过载保护，主要给出两种可行方案，以及一种比较复杂的方案思路。</p>
<blockquote>
<p>流量控制</p>
</blockquote>
<p>流量控制就是B系统实时监控当前流量，如果超过预设的值或者系统承受能力，则直接拒绝掉一部分请求，以实现对系统的保护。</p>
<p>流量控制根据基于的数据不同，可分为两种：</p>
<ul>
<li>基于流量阈值的流控：流量阈值是每个主机的流量上限，流量超过该阈值主机将进入不稳定状态。阈值提前进行设定，如果主机当前流量超过阈值，则拒绝掉一部分流量，使得实际被处理流量始终低于阈值。</li>
<li>基于主机状态的流控：每个接受每个请求之前先判断当前主机状态，如果主机状况不佳，则拒绝当前请求。</li>
</ul>
<p>基于阈值的流控实现简单，但是最大的问题是需要提前设置阈值，而且随着业务逻辑越来越复杂，接口越来越多，主机的服务能力实际应该是下降的，这样就需要不断下调阈值，增加了维护成本，而且万一忘记调整的话，呵呵……</p>
<p>主机的阈值可以通过压力测试确定，选择的时候可以保守些。</p>
<p>基于主机状态的流控免去了人为控制，但是其最大的确定上文已经提到：如何根据当前主机各个参数判断主机状态呢？想要完美的回答这个问题目测并不容易，因此在没有太好答案之前，我推荐基于阈值的流控。</p>
<p>流量控制基于实现位置的不同，又可以分为两种：</p>
<ul>
<li>反向代理实现流控：在反向代理如Nginx上基于各种策略进行流量控制。这种一般针对HTTP服务。</li>
<li>借助服务治理系统：如果Server端是RMI、RPC等服务，可以构建专门的服务治理系统进行负载均衡、流控等服务。</li>
<li>服务容器实现流控：在应用代码里，业务逻辑之前实现流量控制。</li>
</ul>
<p>第3种在服务器的容器（如Java容器）中实现流控并不推荐，因为流控和业务代码混在一起容易混乱；其次实际上流量已经全量进入到了业务代码里，这时的流控只是阻止其进入真正的业务逻辑，所以流控效果将打折；还有，如果流量策略经常变动，系统将不得不为此经常更改。</p>
<p>因此，推荐前两种方式。</p>
<p>最后提一个注意点：当因为流控而拒绝请求时，务必在返回的数据中带上相关信息（比如“当前请求因为超出流量而被禁止访问”），如果返回值什么都没有将是一个大坑。因为造成调用方请求没有被响应的原因很多，可能是调用方Bug，也可能是服务方Bug，还可能是网络不稳定，这样一来很可能在排查一整天后发现是流控搞的鬼……</p>
<blockquote>
<p>服务降级</p>
</blockquote>
<p>服务降级一般由人为触发，属于服务过载造成崩溃恢复时的策略，但为了和流控对比，将其放到这里。</p>
<p>流量控制本质上是减小访问量，而服务处理能力不变；而服务降级本质上是降低了部分服务的处理能力，增强另一部分服务处理能力，而访问量不变。</p>
<p>服务降级是指在服务过载时关闭不重要的接口（直接拒绝处理请求），而保留重要的接口。比如服务由10个接口，服务降级时关闭了其中五个，保留五个，这时这个主机的服务处理能力将增强到二倍左右。</p>
<p>然而，服务过载发生时动辄就超出系统处理能力10倍，而服务降级能使主机服务处理能力提高10倍么？显然很困难，因此服务过载的应对不能只依靠服务降级策略。</p>
<blockquote>
<p>动态扩展</p>
</blockquote>
<p>动态扩展指的是在流量超过系统服务能力时，自动触发集群扩容，自动部署并上线运行；当流量过去后又自动回收多余机器，完全弹性。</p>
<p>这个方案是不是感觉很不错。但是目前互联网公司的在线应用跑在云上的本身就不多，要完全实现在线应用的自动化弹性运维，要走的路就更多了。</p>
<h2 id="崩溃恢复"><a href="#崩溃恢复" class="headerlink" title="崩溃恢复"></a>崩溃恢复</h2><p>如果服务过载造成系统崩溃还是不幸发生了，这时需要运维控制流量，等后台系统启动完毕后循序渐进的放开流量，主要目的是让Cache慢慢预热。流量控制刚开始可以为10%，然后20%，然后50%，然后80%，最后全量，当然具体的比例，尤其是初始比例，还要看后端承受能力和前端流量的比例，各个系统并不相同。</p>
<p>如果后端系统有专门的工具进行Cache预热，则省去了运维的工作，等Cache热起来再发布后台系统即可。但是如果Cache中的Key空间很大，开发预热工具将比较困难。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>“防患于未然”放在服务过载的应对上也是适合的，预防为主，补救为辅。综合上文分析，具体的预防要点如下：</p>
<ul>
<li>调用方（A系统）采用基于刷新的异步续费模式使用Cache，或者至少不能使用基于超时或刷新的简单（stupid）模式。</li>
<li>调用方（A系统）每次请求Cache时检查Cache是否可用（available），如果不可用则按照一个保守的概率访问后端，而不是无所顾忌的直接访问后端。</li>
<li>服务方（B系统）在反向代理处设置流量控制进行过载保护，阈值需要通过压测获得。</li>
</ul>
<p>崩溃的补救主要还是靠运维和研发在发生时的通力合作：观察流量变化准确定位崩溃原因，运维控流量研发持续关注性能变化。</p>
<p>未来如果有条件的话可以研究下主机应用健康判断问题和动态弹性运维问题，毕竟自动化比人为操作要靠谱。</p>
<hr>
<p>来源：美团点评技术团队-张杨<br>链接：<a href="http://tech.meituan.com/avalanche-study.html" target="_blank" rel="external">http://tech.meituan.com/avalanche-study.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[通过iptables实现端口转发与内网共享上网]]></title>
      <url>http://icyxp.github.io/blog/2016/06/iptables-forward-internet-share.html</url>
      <content type="html"><![CDATA[<p>iptables是一个Linux下优秀的nat+防火墙工具，我使用该工具以较低配置的传统pc配置了一个灵活强劲的防火墙+nat系统,小有心得，看了网上也有很多这方面的文章，但是似乎要么说的比较少，要么就是比较偏，内容不全，容易误导，我研究了一段时间的iptables同时也用了很久，有点滴经验，写来供大家参考，同时也备日后自己翻阅。</p>
<p>首先要说明的是，iptables操作的是2.4以上内核的netfilter。所以需要linux的内核在2.4以上。其功能与安全性远远比其前辈ipfwadm,ipchains强大，iptables大致是工作在OSI七层的二、三、四层，其前辈ipchains不能单独实现对tcp/udp port以及对mac地址的的定义与操作，所以我想ipchains应该是仅仅工作在三层上的。</p>
<h2 id="netfilter工作流程"><a href="#netfilter工作流程" class="headerlink" title="netfilter工作流程"></a>netfilter工作流程</h2><p>我们先简单介绍一下netfilter的大致工作流程，也就是一个数据包（或者叫分组、packet,我个人习惯叫包）在到达linux的网络接口的时候 （网卡）如何处理这个包，然后再介绍一下如何用iptables改变或者说控制对这个数据包进行操作。</p>
<ul>
<li>netfilter内部分为三个表，分别是 filter,nat,mangle，每个表又有不同的操作链（Chains）。</li>
<li>在filter（过滤）表中，也就是他的 防火墙功能 的这个表，定义了三个 Chain。分别是INPUT,FORWARD,OUTPUT。也就是对包的入、转发、出进行定义的三个过滤链。对于这个filter表的操作和控制也是我们实现防火墙功能的一个重要手段</li>
<li>在nat(Network Address Translation、网络地址翻译)表中，也就是我们用以实现地址转换和端口转发功能的这个表，定义了PREROUTING, POSTROUTING,OUTPUT三个链,下面我们会对这三个链作详细的说明</li>
<li>而netfilter的mangle表则是一个自定义表，里面包括上面 的filter以及nat表中的各种chains，它可以让我们进行一些自定义的操作，同时这个mangle表中的chains在netfilter对包 的处理流程中处在一个比较优先的位置。<a id="more"></a>
下面有一张图清晰的描绘了netfilter对包的处理流程（该图摘自网上，不知作者是谁，在此深表敬意！），一般情况下，我们用不到这个mangle表，在这里我们就不做介绍了。<br><img src="/images/iptables_netfilter_chains.png" alt="iptables包处理流程"></li>
<li>ebtables基本使用: <a href="http://www.cnblogs.com/peteryj/archive/2011/07/24/2115602.html" target="_blank" rel="external">http://www.cnblogs.com/peteryj/archive/2011/07/24/2115602.html</a><br><img src="/images/iptables_entables.png" alt="iptables_entables处理流程图"></li>
</ul>
<h3 id="PREROUTING-DNAT"><a href="#PREROUTING-DNAT" class="headerlink" title="PREROUTING(DNAT)"></a>PREROUTING(DNAT)</h3><p>PREROUTING这个chain在最前面，当一个包来到linux的网络接口的时候先过mangle的PREROUTING；然后是nat的PREROUTING,从这个chain的名字我们可以看出，这个chain是在路由之前(pre-routing)要过的。</p>
<p>为什么要在路由之前过呢？大家可以看到这个图上，上面有一个菱形的部分叫ROUTING,这个ROUTING部分就是Linux的route box,也就是路由系统，它同样有很高深的功能，可以实现策略路由等等一些高级特性，此处我们不做详细解释。单说这个PREROUTING链，因为在这个链里面我们对包的操作是DNAT,也就是改变目的地址和（或端口），通常用在端口转发，或者nat到内网的DMZ区，也就是说当一个包过来的时候我们要改变它的目的地址，大家可以想想,如果一个包在改变目的地址之前就被扔进了route box,让系统选好路之后再改变目的地址，那么选路就可能是错的，或者说毫无意义了，所以，PREROUTING这个Chain一定要在进Routing 之前做。</p>
<p>比如说，我们的公网ip是60.1.1.1/24，位于linux中的eth0内网ip是10.1.1.1/24，位于linux中的eth1, 我们的内网有一台web服务器，地址是10.1.1.2/24,我们怎么样能让internet用户通过这个公网ip访问我们内部的这个web服务器呢？ 我们就可以在这个PREROUTING链上面定义一个规则，把访问60.1.1.1:80的用户的目的地址改变一下，改变为10.1.1.2:80,这样 就实现了internet用户对内网服务器的访问了，当然了，这个端口是比较灵活的，我们可以定义任何一个端口的转发，不一定是80–&gt;80，具体的命令我们在下面的例子中介绍，这里我们只谈流程与概念上的实现方法。</p>
<h3 id="FORWARD"><a href="#FORWARD" class="headerlink" title="FORWARD"></a>FORWARD</h3><p>好了，我们接着往下走，这个包已经过了两个PREROUTING链了，这个时候，出现了一个分支转折的地方，也就是图中下方的那个菱形（FORWARD）,转发！这里有一个对目的地址的判断（这里同样说明了PREROUTING一定要在最先，不仅要在route box之前，甚至是这个对目的地址的判断之前，因为我们可能做一个去某某某ip的地方转到自己的ip的规则，所以PREROUTING是最先处理这个包的Chain）！</p>
<p>如果包的目的地是本机ip,那么包向上走，走入INPUT链处理，然后进入LOCAL PROCESS,如果非本地，那么就进入FORWARD链进行过滤，我们在这里就不介绍INPUT,OUTPUT的处理了，因为那主要是对于本机安全的一种处理，我们这里主要说对转发的过滤和nat的实现。</p>
<p>这里的FORWARD我简单说一下，当linux收到了一个 目的ip地址不是本地的包 ，Linux会把这个包丢弃，因为默认情况下，Linux的三层包转发功能是关闭的，如果要让我们的linux实现转发，则需要打开这个转发功能，可以 改变它的一个系统参数，使用sysctl net.ipv4.ip_forward=1或者echo “1” &gt; /proc/sys/net/ipv4/ip_forward命令打开转发功能。</p>
<p>好了，在这里我们让linux允许转发，这个包的目的地址也不是本机，那么它将接着走入FORWARD链，在FORWARD链里面，我们就可以定义详细的规则，也就是是否允许他通过，或者对这个包的方向流程进行一些改变，这也是我们实现访问控制的地方，这里同样也是Mangle_FORWARD然后filter_FORWARD,我们操作任何一个链都会影响到这个包的命运，在下面的介绍中，我们就忽略掉mangle表，我们基本用不到操作它，所以我们假设它是透明的。</p>
<h3 id="POSTROUTING-SNAT"><a href="#POSTROUTING-SNAT" class="headerlink" title="POSTROUTING(SNAT)"></a>POSTROUTING(SNAT)</h3><p>假设这个包被我们的规则放过去了，也就是ACCEPT了，它将进入POSTROUTING部分， 注意！这里我注意到一个细节问题，也就是上面的图中数据包过了FORWARD链之后直接进入了POSTROUITNG链，我觉得这中间缺少一个环节，也就是route box,对于转发的包来说，linux同样需要在选路（路由）之后才能将它送出，这个图却没有标明这一点，我认为它是在过了route box之后才进入的POSTROUITNG，当然了，这对于我们讨论iptables的过滤转发来说不是很重要，只是我觉得流程上有这个问题，还是要说明 一下。</p>
<p>同样的，我们在这里从名字就可以看出，这个POSTROUTING链应该是路由之后的一个链，也就是这个包要送出这台Linux的 最后一个环节了，这也是极其重要的一个环节！！这个时候linux已经完成(has done.._)了对这个包的路由（选路工作），已经找到了合适的接口送出这个包了，在这个链里面我们要进行重要的操作，就是被Linux称为 SNAT 的一个动作，修改源ip地址！为什么修改源ip地址？很多情况需要修改源地址阿，最常见的就是我们内网多台机器需要共享一个或几个公网ip访问internet,因为我们的内网地址是私有的，假如就让linux给路由出去，源地址也不变，这个包应该能访问到目的地，但是却回不来，因为 internet上的N多个路由节点不会转发私有地址的数据包，也就是说，不用合法ip,我们的数据包有去无回。有人会说：“既然是这样，我就不用私有 ip了，我自己分配自己合法的地址不行吗？那样包就会回来了吧？”答案是否定的，ip地址是ICANN来分配的，你的数据包或许能发到目的地，但是回来的 时候人家可不会转到你那里，internet上的路由器中的路由信息会把这个返回包送到那个合法的获得ip的地方去，你同样收不到,而你这种行为有可能被定义为一种ip欺骗，很多设备会把这样的包在接入端就给滤掉了，可能都到不了你要访问的那个服务器，呵呵。</p>
<p>那么Linux如何做SNAT呢？比如一个内网的10.1.1.11的pc访问202.2.2.2的一个web服务器，linux的内网接口10.1.1.1在收到这个包之后把原来的 PC的 ip10.1.1.11改变为60.1.1.1的合法地址然后送出，同时在自己的ip_conntrack表里面做一个记录,记住是内网的哪一个ip的哪 个端口访问的这个web服务器，自己把它的源地址改成多少了，端口改成多少了，以便这个web服务器返回数据包的时候linux将它准确的送回给发送请求 的这个pc.</p>
<p>大体的数据转发流程我们说完了,我们看看iptables使用什么样的参数来完成这些操作。</p>
<h2 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解"></a>概念理解</h2><p>在描述这些具体的操作之前，我还要说几个我对iptables的概念的理解（未必完全正确），这将有助于大家理解这些规则，以实现更精确的控制。</p>
<p>上文中我们提到过，对包的控制是由我们在不同的Chain(链)上面添加不同的规则来实现的，比如我们对过滤表（filter table）添加规则来执行对包的操控。那么既然叫链，一定就是一条或者多条规则组成的了，这时就有一个问题了，如果多个规则对同一种包进行了定义，会发生什么事情呢？ 在Chain中，所有的规则都是从上向下来执行的 ，也就是说，如果匹配了第一行，那么就按照第一行的规则执行，一行一行的往下找，直到找到 符合这个类型的包的规则为止。如果找了一遍没有找到符合这个包的规则怎么办呢？itpables里面有一个概念，就是 Policy ，也就是策略。一说这个东西大家可能就会觉得比较麻烦，什么策略阿，我对于它的理解就是所谓这个策略就是chain中的最后一条规则，也就是说如果找了一遍找不到符合处理这个包的规则，就按照policy来办。这样理解起来就容易多了。iptables 使用-P来设置Chain的策略。</p>
<p>好了，我们言归正传，来说说iptables到底怎样实现对包的控制。</p>
<h4 id="链操作"><a href="#链操作" class="headerlink" title="链操作"></a>链操作</h4><p>先介绍一下iptables如何操作链</p>
<p>对链的操作就那么几种：</p>
<ul>
<li>-I(插入)</li>
<li>-A(追加)</li>
<li>-R(替换)</li>
<li>-D（删除）</li>
<li>-L（列表显示）</li>
</ul>
<p>这里要说明的就是-I将会把规则放在第一行，-A将会放在最后一行。</p>
<p>比如我们要添加一个规则到filter表的FORWARD链：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#意思为：追加一个规则至filter表中的FORWARD链尾，允许（-j ACCEPT）源地址为10.1.1.11目的地址为202.1.1.1的数据包通过。其中-t后面跟的是表名，在-A后面跟Chain名，后面的小写的 -s为源地址，-d为目的地址，-j为处理方向。</span></span><br><span class="line">iptables -t filter -A FORWARD <span class="_">-s</span> 10.1.1.11 <span class="_">-d</span> 202.1.1.1 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#在iptables中，默认的表名就是filter，所以这里可以省略-t filter直接写成: </span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.11 <span class="_">-d</span> 202.1.1.1 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<h4 id="匹配参数"><a href="#匹配参数" class="headerlink" title="匹配参数"></a>匹配参数</h4><p>iptables中的匹配参数： 我们在这里就介绍几种常用的参数，详细地用法可以man iptables看它的联机文档，你会有意外的收获。</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-s</td>
<td style="text-align:left">匹配源地址</td>
</tr>
<tr>
<td style="text-align:left">-d</td>
<td style="text-align:left">匹配目的地址</td>
</tr>
<tr>
<td style="text-align:left">-p</td>
<td style="text-align:left">协议匹配</td>
</tr>
<tr>
<td style="text-align:left">-i</td>
<td style="text-align:left">入接口匹配</td>
</tr>
<tr>
<td style="text-align:left">-o</td>
<td style="text-align:left">出接口匹配</td>
</tr>
<tr>
<td style="text-align:left">–sport，–dport</td>
<td style="text-align:left">源和目的端口匹配</td>
</tr>
<tr>
<td style="text-align:left">-j</td>
<td style="text-align:left">跳转,也就是包的方向</td>
</tr>
<tr>
<td style="text-align:left">!</td>
<td style="text-align:left">取反</td>
</tr>
</tbody>
</table>
<p>其中还有一个!参数，使用!就是取反的意思。下面我们简单举几个例子介绍一下。</p>
<ul>
<li>-s 这个参数呢就是指定源地址的，如果使用这个参数也就是告诉netfilter，对于符合这样一个源地址的包怎么去处理，可以指定某一个单播ip地址，也可以指定一个网络，如果单个的ip地址其实隐含了一个32位的子网掩码，比如-s 10.1.1.11 其实就是-s 10.1.1.11/32，同样我们可以指定不同的掩码用以实现源网络地址的规则，比如一个C类地址我们可以用-s 10.1.1.0/24来指定。</li>
<li>-d参数与-s格式一样。</li>
<li>-i参数是指定入接口的网络接口，比如我仅仅允许从eth3接口过来的包通过FORWARD链，就可以这样指定iptables -A FORWARD -i eth3 -j ACCEPT</li>
<li>-o是出接口,与上同。</li>
</ul>
<p>我们下面用一些简单的实例来step by step看看iptables的具体配置方法。</p>
<h4 id="实例一：简单的nat路由器"><a href="#实例一：简单的nat路由器" class="headerlink" title="实例一：简单的nat路由器"></a>实例一：简单的nat路由器</h4><blockquote>
<p><strong>环境介绍</strong></p>
<ul>
<li>linux 2.4 +</li>
<li>2个网络接口</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>目的：实现内网中的节点（10.1.1.0/24）可控的访问internet。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先将Lan的节点pc的网关指向10.1.1.254。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#确定你的linux的ip配置无误，可以正确的ping通内外的地址。同时用route命令查看linux的本地路由表，确认指定了可用的ISP提供的默认网关。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打开linux的转发功能：</span></span><br><span class="line">sysctl net.ipv4.ip_forward=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#将FORWARD链的策略设置为DROP，这样做的目的是做到对内网ip的控制，你允许哪一个访问internet就可以增加一个规则，不在规则中的ip将无法访问internet.</span></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line"></span><br><span class="line"><span class="comment">#这条规则规定允许任何地址到任何地址的确认包和关联包通过。一定要加这一条，否则你只允许lan IP访问没有用，至于为什么，下面我们再详细说。</span></span><br><span class="line">iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#这条规则做了一个SNAT，也就是源地址转换，将来自10.1.1.0/24的地址转换为60.1.1.1</span></span><br><span class="line"><span class="comment">#(Deven：因为是让内网上网，因此对于代理服务器而言POSTROUTING（经过路由之后的包应该要把源地址改变为60.1.1.1，否则包无法返回）)</span></span><br><span class="line">iptables -t nat -A POSTROUTING <span class="_">-s</span> 10.1.1.0/24 -j SNAT --to 60.1.1.1</span><br><span class="line"><span class="comment">#有这几条规则，一个简单的nat路由器就实现了。这时你可以将允许访问的ip添加至FORWARD链，他们就能访问internet了。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#比如我想让10.1.1.9这个地址访问internet,那么你就加如下的命令就可以了。</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.9 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以精确控制他的访问地址,比如我就允许10.1.1.99访问3.3.3.3这个ip</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.99 <span class="_">-d</span> 3.3.3.3 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者只允许他们访问80端口。</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.0/24 -p tcp --dport http -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#更多的控制可以自己灵活去做,或者查阅iptables的联机文档。</span></span><br></pre></td></tr></table></figure>
<h4 id="实例二：端口转发"><a href="#实例二：端口转发" class="headerlink" title="实例二：端口转发"></a>实例二：端口转发</h4><blockquote>
<p><strong>环境介绍</strong></p>
<ul>
<li>linux 2.4 +</li>
<li>2个网络接口</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>Lan内web server: 10.1.1.1:80</li>
<li>Lan内ftp server: 10.1.1.2:21</li>
<li>Wan口:60.1.1.1/24 eth1</li>
<li>目的：对内部server进行端口转发实现internet用户访问内网服务器。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#同样确认你的linux的各项配置正常，能够访问内外网。</span></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line">iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#也需要加入确认包和关联包的允许通过</span></span><br><span class="line"><span class="comment">#如果你要把访问60.1.1.1:80的数据包转发到Lan内web server,用下面的命令</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 60.1.1.1 -p tcp --dport 80 -j DNAT --to 10.1.1.1:80</span><br><span class="line"></span><br><span class="line"><span class="comment">#ftp服务也同样，命令如下：</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 60.1.1.1 -p tcp --dport 21 -j DNAT --to 10.1.1.2:21</span><br></pre></td></tr></table></figure>
<p>好了，命令完成了，端口转发也做完了，本例能不能转发呢？不能，为什么呢？我下面详细分析一下。</p>
<p>对于iptables好像往外访问的配置比较容易，而对内的转发似乎就有一些问题了，在一开始的时候我就先说了一些关于netfilter的流程问题，那么我就简单说说做了这些配置之后为什么有可能还不行呢？</p>
<p>能引起这个配置失败的原因有很多，我们一个个的来说：</p>
<p><strong>第一</strong> 本例中，我们的FORWARD策略是DROP,那么也就是说，没有符合规则的包将被丢弃，不管内到外还是外到内，我们在这里依然不讨论那个确认包和关联包的问题，我们不用考虑他的问题，下面我会详细说一下这个东西，那么如何让本例可以成功呢？加入下面的规则。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -A FORWARD <span class="_">-d</span> 10.1.1.1 -p tcp --dport 80 -j ACCEPT</span><br><span class="line">iptables -A FORWARD <span class="_">-d</span> 10.1.1.2 -p tcp --dport 21 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<p>有没有觉得有一些晕？为什么目的地址是10.xxx而不是60.xxx人家internet用户不是访问的60.xxx吗？呵呵，回到上面看看那个图吧，FORWARD链在什么位置上，它是在PREROUTING之后，也就是说当这个包到达FORWARD链的时候，目的地址已经变成10.xxx了，假如internet用户的请求是这样202.1.1.1:1333–&gt;60.1.1.1:80，在经过了我们的PREROUTING链之后将变成 202.1.1.1:1333–&gt;10.1.1.1:80,这个时候如果你设置一个目的地址为60.xxx的规则有用吗？呵呵，这是问题一。这个时候应该可以完成端口转发的访问了，但是有一些时候还是不行？为什么？看问题二。</p>
<p><strong>第二</strong> 内网server的ip配置问题，这里我们以web server为例说明一下（ftp情况有一些特殊，下面我们再详细讨论，说确认包和关联包的时候讨论这个问题），上面说到，有的时候可以访问了，有的时候却不行，就是这个web server的ip设置问题了，如果web server没有指定默认的网关，那么在作了上面的配置之后，web server会收到internet的请求，但是，他不知道往哪里回啊，人家的本地路由表不知道你那个internet的ip,202.1.1.1该怎么走。如果你使用截包工具在web server上面察看，你会发现server收到了来自202.1.1.1:1333–&gt;10.1.1.1:80的请求，由于你没有给web server配置默认网关，它不知道怎么回去，所以就出现了不通的情况。怎么办呢？两个解决方法：</p>
<p>一就是给这个server配置一个默认网关，当然要指向这个配置端口转发的linux,本例是10.1.1.254,配置好了，就一定能访问了。有一个疑问？难道不需要在FORWARD链上面设置一个允许web server的ip地址访问外网的规则吗？它的包能出去？答案是肯定的，能出去。因为我们那一条允许确认包与关联包的规则，否则它是出不去的。</p>
<p><strong>第二种方法</strong>，比较麻烦一些，但是对服务器来说这样似乎更安全一些。方法就是对这个包再作一次SNAT，也就是在POSTROUTING链上添加规则。命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING <span class="_">-d</span> 10.1.1.1 -p tcp --dport 80 -j SNAT --to 10.1.1.254</span><br></pre></td></tr></table></figure></p>
<p>ftp 的方法相同。这条命令不太好懂？？其实很简单，如果使用这条命令，那么你的web server不需要再设置默认网关，就能收到这个请求，只要他和linux的lan ip地址是能互访的（也就是说web server和Linux的Lan ip在一个广播域），我们在根据上面的netfilter流程图来分析这个包到底被我们怎么样了：</p>
<ul>
<li>首先一个请求202.1.1.1:1333–&gt; 60.1.1.1:80被linux收到了，进入PREROUTING；</li>
<li>发现一个规则iptables -t nat -A PREROUTING -d 60.1.1.1 -p tcp –dport 80 -j DNAT –to 10.1.1.1:80符合，好了，改你的目的地址，于是这个包变成了202.1.1.1:1333–&gt;10.1.1.1:80，继续往前走；</li>
<li>进入FORWARD链，okay,也有一条规则允许通过iptables -A FORWARD -d 10.1.1.1 -p tcp –dport 80 -j ACCEPT；</li>
<li>进入route box选路，找到合适的路径了，继续进入POSTROUTING链；</li>
<li>耶？又发现一个符合的规则iptables -t nat -A POSTROUTING -d 10.1.1.1 -p tcp –dport 80 -j SNAT –to 10.1.1.254,原来是一个SNAT,改你的源地址，于是这个包变成了10.1.1.254:xxxx–&gt;10.1.1.1:80。为什么用xxxx了，这里的端口是随机的，我也不知道会是什么。</li>
<li>而整个的两次变化的过程都会记录在linux的ip_conntrack中；</li>
<li>当web server收到这个包的时候，发现，原来是一个内网自己兄弟来的请求阿，又在一个广播域，不用找网关，把返回包直接扔给交换机了；</li>
<li>linux在收到返回包之后，会根据他的ip_conntrack中的条目进行两次变换，返回真正的internet用户，于是完成这一次的访问。</li>
</ul>
<p>看了上面的两个例子，不知道大家是否清楚了iptables的转发流程，希望对大家有所帮助。</p>
<h4 id="状态机制"><a href="#状态机制" class="headerlink" title="状态机制"></a>状态机制</h4><p>下面我们就说说我一直在上面提到的关于那个ESTABLISHED,RELATED的规则是怎么回事，到底有什么用处。</p>
<p>说这个东西就要简单说一下网络的数据通讯的方式，我们知道，网络的访问是双向的，也就是说一个Client与Server之间完成数据交换需要双方的发包与收包。在netfilter中，有几种状态，也就是new, established,related,invalid。</p>
<p>当一个客户端，在本文例一中，内网的一台机器访问外网，我们设置了规则允许他出去，但是没有设置允许回来的规则阿，怎么完成访问呢？这就是netfilter的 状态机制 ，当一个lan用户通过这个linux访问外网的时候，它发送了一个请求包，这个包的状态是new,当外网回包的时候他的状态就是established,所以，linux知道，哦，这个包是我的内网的一台机器发出去的应答包，他就放行了。</p>
<p>而外网试图对内发起一个新的连接的时候，他的状态是new,所以linux压根不去理会它。这就是我们为什么要加这一句的原因。</p>
<p>还有那个related,他是一个关联状态，什么会用到呢？tftp,ftp都会用到，因为他们的传输机制决定了，它不像http访问那样，Client_IP: port–&gt;server:80然后server:80–&gt;Client_IP:port，ftp使用tcp21建立连接，使用20端口发送数据，其中又有两种方式，一种主动active mode，一种被动passive mode。主动模式下，client使用port命令告诉server我用哪一个端口接受数据，然后server主动发起对这个端口的请求。被动模式下，server使用port命令告诉客户端，它用那个端口监听，然后客户端发起对他的数据传输，所以这对于一个防火墙来说就是比较麻烦的事情，因为有可能会有new状态的数据包，但是它又是合理的请求，这个时候就用到这个related状态了，他就是一种关联，在linux中，有个叫 ftp_conntrack的模块，它能识别port命令，然后对相应的端口进行放行。</p>
<p>一口气写了这么多东西，不知道质量如何，大家凑和着看吧，希望多多交流共同进步，我还是一个linux的初学者，难免很多谬误，希望高手赐教指正，以期不断进步。</p>
<h4 id="实用命令"><a href="#实用命令" class="headerlink" title="实用命令"></a>实用命令</h4><p>对了，还有几个在实际中比较实用（也比较受用:-)）的命令参数，写出来供大家参考<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">iptables -L -n</span><br><span class="line"><span class="comment">#这样的列表会跳过linux的domain lookup,有的时候使用iptables -L会比较慢，因为linux会尝试解析ip的域名，真是罗嗦，如果你的dns server比较不爽的话，iptables -L就会让你很不爽，加一个-n参数就好了。列表刷的就出来。当然了，如果你的linux就是做防火墙，建议把nameserver去掉，在 /etc/resolve.conf里面，因为有时候使用route命令也会比较慢列出来，很是不爽。</span></span><br><span class="line"></span><br><span class="line">iptables -L -v</span><br><span class="line"><span class="comment">#这个命令会显示链中规则的包和流量计数，嘿嘿，看看哪些小子用的流量那么多，用tc限了他。</span></span><br><span class="line"></span><br><span class="line">iptables -t nat -L -vn</span><br><span class="line"><span class="comment">#查看nat表中的规则。</span></span><br><span class="line"></span><br><span class="line">cat /proc/net/ip_conntrack</span><br><span class="line"><span class="comment">#查看目前的conntrack，可能会比较多哦，最好加一个|grep "关键字"，看看你感兴趣的链接跟踪</span></span><br><span class="line"></span><br><span class="line">wc <span class="_">-l</span> /proc/net/ip_conntrack</span><br><span class="line"><span class="comment">#看看总链接有多少条。</span></span><br><span class="line"></span><br><span class="line">iptables-save &gt;/etc/iptables</span><br><span class="line"><span class="comment">#把当前的所有链备份一下，之所以放到/etc下面叫iptables，因为这样重起机器的时候会自动加载所有的链，经常地备份一下吧，否则如果链多，万一掉电重启，你还是会比较痛苦。</span></span><br></pre></td></tr></table></figure></p>
<p><strong>转发</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#之前因为一个网段被封了，因此通过iptables做转发：</span></span><br><span class="line"><span class="comment">#代理服务器WAN IP：111.**.**.219，LAN IP：192.168.0.219</span></span><br><span class="line"><span class="comment">#内网服务器IP：192.168.0.41</span></span><br><span class="line"><span class="comment">#1.在代理服务器打开转发功能（sysctl.conf）</span></span><br><span class="line"><span class="comment">#2.添加以下规则</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 111.**.**.219 -p tcp --dport 9999 -j DNAT --to-destination 192.168.0.41:9999</span><br><span class="line">iptables -t nat -A POSTROUTING <span class="_">-d</span> 192.168.0.41 -p tcp --dport 9999 -j SNAT --to-source 192.168.0.219</span><br></pre></td></tr></table></figure></p>
<p> 原文：<a href="http://wwdhks.blog.51cto.com/839773/1154032" target="_blank" rel="external">http://wwdhks.blog.51cto.com/839773/1154032</a></p>
]]></content>
    </entry>
    
  
  
</search>
