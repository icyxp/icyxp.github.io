<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[使用 Go 实现 Async/Await 模式]]></title>
      <url>http://team.jiunile.com/blog/2020/12/go-async-await.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Golang 是一种并发编程语言。它具有强大的特性，如 <code>Goroutines</code> 和 <code>Channels</code>，可以很好地处理异步任务。另外，<code>goroutines</code> 不是 OS 线程，这就是为什么您可以在不增加开销的情况下根据需要启动任意数量的 <code>goroutine</code> 的原因，它的堆栈大小初始化时仅 <strong>2KB</strong>。那么为什么要 <code>async/await</code> 呢？ <code>Async/Await</code> 是一种很好的语言特点，它为异步编程提供了更简单的接口。</p>
<p>项目链接：<a href="https://github.com/icyxp/AsyncGoDemo" target="_blank" rel="external">https://github.com/icyxp/AsyncGoDemo</a></p>
<a id="more"></a>
<h2 id="它是如何工作的？"><a href="#它是如何工作的？" class="headerlink" title="它是如何工作的？"></a>它是如何工作的？</h2><p>从 F# 开始，然后是 C#，到现在 Python 和 Javascript 中，<code>async/await</code> 是一种非常流行的语言特点。它简化了异步方法的执行结构并且读起来像同步代码。对于开发人员来说更容易理解。让我们看看 c# 中的一个简单示例 <code>async/await</code> 是如何工作的。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> async Task <span class="title">Main</span><span class="params">(<span class="built_in">string</span>[] args)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    Console.WriteLine(<span class="string">"Let's start ..."</span>);</span><br><span class="line">    var done = DoneAsync();</span><br><span class="line">    Console.WriteLine(<span class="string">"Done is running ..."</span>);</span><br><span class="line">    Console.WriteLine(await done);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> async Task&lt;<span class="keyword">int</span>&gt; DoneAsync()</span><br><span class="line">&#123;</span><br><span class="line">    Console.WriteLine(<span class="string">"Warming up ..."</span>);</span><br><span class="line">    await Task.Delay(<span class="number">3000</span>);</span><br><span class="line">    Console.WriteLine(<span class="string">"Done ..."</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当程序运行时，我们的 <code>Main</code> 函数将被执行。我们有异步函数 <code>DoneAsync</code>。我们使用 <code>Delay</code> 方法停止执行代码 3 秒钟。Delay 本身是一个异步函数，所以我们用 <code>await</code> 来调用它。</p>
<blockquote>
<p><code>await</code> 只阻塞异步函数内的代码执行</p>
</blockquote>
<p>在 <code>Main</code> 函数中，我们不使用 <code>await</code> 来调用 <code>DoneAsync</code>。但 <code>DoneAsync</code> 开始执行后，只有当我们 <code>await</code> 它的时候，我们才会得到结果。执行流程如下所示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Let<span class="string">'s start ...</span><br><span class="line">Warming up ...</span><br><span class="line">Done is running ...</span><br><span class="line">Done ...</span><br><span class="line">1</span></span><br></pre></td></tr></table></figure></p>
<p>对于异步执行，这看起来非常简单。让我们看看如何使用 Golang 的 <code>Goroutines</code> 和 <code>Channels</code> 来做到这一点。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DoneAsync</span><span class="params">()</span> <span class="title">chan</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	r := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">	fmt.Println(<span class="string">"Warming up ..."</span>)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		time.Sleep(<span class="number">3</span> * time.Second)</span><br><span class="line">		r &lt;- <span class="number">1</span></span><br><span class="line">		fmt.Println(<span class="string">"Done ..."</span>)</span><br><span class="line">	&#125;()</span><br><span class="line">	<span class="keyword">return</span> r</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span> <span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">"Let's start ..."</span>)</span><br><span class="line">	val := DoneAsync()</span><br><span class="line">	fmt.Println(<span class="string">"Done is running ..."</span>)</span><br><span class="line">	fmt.Println(&lt;- val)</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">``</span><span class="string">` </span><br><span class="line">在这里，`</span>DoneAsync<span class="string">` 异步运行并返回一个 `</span>channel<span class="string">`。执行完异步任务后，它会将值写入 `</span>channel<span class="string">`。在 `</span>main<span class="string">` 函数中，我们调用 `</span>DoneAsync<span class="string">` 并继续执行后续操作，然后从返回的 `</span>channel<span class="string">` 读取值。它是一个阻塞调用，等待直到将值写入 `</span>channel<span class="string">`，并在获得值后将其输出到终端。</span><br><span class="line">`</span><span class="string">``</span><span class="keyword">go</span></span><br><span class="line">Let<span class="string">'s start ...</span><br><span class="line">Warming up ...</span><br><span class="line">Done is running ...</span><br><span class="line">Done ...</span><br><span class="line">1</span></span><br></pre></td></tr></table></figure></p>
<p>我们看到，我们实现了与 C# 程序相同的结果，但它看起来不像 <code>async/await</code> 那样优雅。尽管这确实不错，但是我们可以使用这种方法轻松地完成很多细粒度的事情，我们还可以用一个简单的结构和接口在 Golang 中实现 <code>async/await</code> 关键字。让我们试试。</p>
<h2 id="实现-Async-Await"><a href="#实现-Async-Await" class="headerlink" title="实现 Async/Await"></a>实现 Async/Await</h2><p>完整代码可在项目链接中找到（在文章开始的地方）。要在 Golang 中实现 <code>async/await</code>，我们将从一个名为 <code>async</code> 的包目录开始。项目结构看起来是这样的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── async</span><br><span class="line">│   └── async.go</span><br><span class="line">├── main.go</span><br><span class="line">└── README.md</span><br></pre></td></tr></table></figure>
<p>在 <code>async</code> 文件中，我们编写了可以处理异步任务最简单的 <code>Future</code> 接口。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> async</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"context"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Future interface has the method signature for await</span></span><br><span class="line"><span class="keyword">type</span> Future <span class="keyword">interface</span> &#123;</span><br><span class="line">	Await() <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> future <span class="keyword">struct</span> &#123;</span><br><span class="line">	await <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context)</span> <span class="title">interface</span></span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(f future)</span> <span class="title">Await</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">	<span class="keyword">return</span> f.await(context.Background())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exec executes the async function</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Exec</span><span class="params">(f <span class="keyword">func</span>()</span> <span class="title">interface</span></span>&#123;&#125;) Future &#123;</span><br><span class="line">	<span class="keyword">var</span> result <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">	c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> <span class="built_in">close</span>(c)</span><br><span class="line">		result = f()</span><br><span class="line">	&#125;()</span><br><span class="line">	<span class="keyword">return</span> future&#123;</span><br><span class="line">		await: <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context)</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">			<span class="keyword">select</span> &#123;</span><br><span class="line">			<span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">				<span class="keyword">return</span> ctx.Err()</span><br><span class="line">			<span class="keyword">case</span> &lt;-c:</span><br><span class="line">				<span class="keyword">return</span> result</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里发生的事情并不多，我们添加了一个具有 <code>Await</code> 方法标识的 <code>Future</code> 接口。接下来，我们添加一个 <code>future</code> 结构，它包含一个值，即 <code>await</code> 函数的函数标识。现在 <code>futute struct</code> 通过调用自己的 <code>await</code> 函数来实现 <code>Future</code> 接口的 <code>Await</code> 方法。</p>
<p>接下来在 <code>Exec</code> 函数中，我们在 <code>goroutine</code> 中异步执行传递的函数。然后返回 <code>await</code> 函数。它等待 <code>channel</code> 关闭或 <code>context</code> 读取。基于最先发生的情况，它要么返回错误，要么返回作为接口的结果。</p>
<p>现在，有了这个新的 <code>async</code> 包，让我们看看如何更改当前的 go 代码：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DoneAsync</span><span class="params">()</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">"Warming up ..."</span>)</span><br><span class="line">	time.Sleep(<span class="number">3</span> * time.Second)</span><br><span class="line">	fmt.Println(<span class="string">"Done ..."</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">"Let's start ..."</span>)</span><br><span class="line">	future := async.Exec(<span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">		<span class="keyword">return</span> DoneAsync()</span><br><span class="line">	&#125;)</span><br><span class="line">	fmt.Println(<span class="string">"Done is running ..."</span>)</span><br><span class="line">	val := future.Await()</span><br><span class="line">	fmt.Println(val)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>乍一看，它看起来干净得多，这里我们没有显式地使用 <code>goroutine</code> 或 <code>channels</code>。我们的 <code>DoneAsync</code> 函数已更改为完全同步的性质。在 <code>main</code> 函数中，我们使用 <code>async</code> 包的<code>Exec</code> 方法来处理 <code>DoneAsync</code>。在开始执行 <code>DoneAsync</code>。控制流返回到可以执行其他代码的 <code>main</code> 函数中。最后，我们对 <code>Await</code> 进行阻塞调用并回读数据。</p>
<p>现在，代码看起来更加简单易读。我们可以修改我们的 async 包从而能在 Golang 中合并许多其他类型的异步任务，但在本教程中，我们现在只坚持简单的实现。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们经历了 <code>async/await</code> 的过程，并在 Golang 中实现了一个简单的版本。我鼓励您进一步研究 <code>async/await</code>，看看它如何更好的让代码库便于易读。</p>
<blockquote>
<p>译自：<a href="https://hackernoon.com/asyncawait-in-golang-an-introductory-guide-ol1e34sg" target="_blank" rel="external">https://hackernoon.com/asyncawait-in-golang-an-introductory-guide-ol1e34sg</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[从 Go 分析 Struct 对齐如何影响内存使用量]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-struct.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>不知道大家在写 Go 时有没有注意过，<strong>一个 struct 所占的空间不见得等于各个 field 加起来的空间</strong>，甚至有时把 field 申明的顺序调换一下，又会得到不同的结果。</p>
<p>今天的文章就是要从 CPU 抓资料的原理开始介绍，然后再讲到 <strong>Data Structure Alignment</strong>（数据结构对齐），希望大家在看完之后能对 CPU 跟记忆体有更多认识～<br><a id="more"></a></p>
<h2 id="直接上例子"><a href="#直接上例子" class="headerlink" title="直接上例子"></a>直接上例子</h2><p>以 T1 为例，整个 <code>struct</code> 共有三个栏位，类型分别是 <code>int8</code>、<code>int64</code> 跟 <code>int32</code>，所以变数 <code>t1</code> 应该需要 <code>1+8+4=13 bytes</code> 的空间。但实际在 <a href="https://goplay.tools/snippet/6kzzmHddQgc" target="_blank" rel="external">Go Playground</a> 上跑，会发现 <code>t1</code> 竟然需要 <code>24 bytes</code>，真奇怪是吧？<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> T1 <span class="keyword">struct</span> &#123;</span><br><span class="line">    f1 <span class="keyword">int8</span>  <span class="comment">// 1 byte</span></span><br><span class="line">    f2 <span class="keyword">int64</span> <span class="comment">// 8 bytes</span></span><br><span class="line">    f3 <span class="keyword">int32</span> <span class="comment">// 4 bytes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    t1 := T1&#123;&#125;</span><br><span class="line">    fmt.Println(unsafe.Sizeof(t1)) <span class="comment">// 24 bytes</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果尝试把栏位的顺序调整一下，改成 <code>int8</code>、<code>int32</code>、<code>int64</code> 再跑一次，就只需要 16 bytes，但跟原本预期的 13 bytes 还是有差，那究竟为什么会这样的差异呢？<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> T2 <span class="keyword">struct</span> &#123;</span><br><span class="line">    f1 <span class="keyword">int8</span>  <span class="comment">// 1 byte</span></span><br><span class="line">    f3 <span class="keyword">int32</span> <span class="comment">// 4 bytes</span></span><br><span class="line">    f2 <span class="keyword">int64</span> <span class="comment">// 8 bytes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    t2 := T2&#123;&#125;</span><br><span class="line">    fmt.Println(unsafe.Sizeof(t2)) <span class="comment">// 16 bytes</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="从-CPU-如何抓资料开始讲起"><a href="#从-CPU-如何抓资料开始讲起" class="headerlink" title="从 CPU 如何抓资料开始讲起"></a>从 CPU 如何抓资料开始讲起</h2><p>如果买电脑时有在留意 CPU 规格的话（身为工程师一定要的吧XD），应该会发现近几年的 CPU 几乎都是 <code>64 bit</code> 的。而这边的 <code>64 bit</code>，指的就是 CPU 一次可以从记忆体里面抓 <code>64 bits</code> 的资料，换算一下也就是 <code>8 bytes</code>。</p>
<p>虽说是一次抓 <code>8 bytes</code>，但也不是想抓哪就抓哪，因为记忆体也会以 <strong><code>8 bytes</code> 分成一个一个 word</strong>（如下图），而 CPU 只能一次拿某一个 word。所以如果所需的资料刚好横跨两个 word，那就得花两个 <code>CPU cycle</code> 的时间去拿。<br><img src="/images/go/struct_1.png" alt="struct"></p>
<blockquote>
<p>注：在 <code>64 bit</code> 的系统中一个 word 是 <code>8 bytes</code>，<code>32 bit</code> 中则是 <code>4 bytes</code></p>
</blockquote>
<h2 id="所以为什么-struct-会变肥"><a href="#所以为什么-struct-会变肥" class="headerlink" title="所以为什么 struct 会变肥"></a>所以为什么 struct 会变肥</h2><p>了解 CPU 后我们再看一次 T1，他的栏位顺序是 <code>int8</code>、<code>int64</code>、<code>int32</code>，所以把 t1 的资料连续放在记忆体里面就长得像下图：因为第二个栏位 f2(<code>int64</code>) 需要 8 个 bytes，所以<strong>会有一个 byte 会被挤到第二个 word</strong>（第二排）<br><img src="/images/go/struct_2.png" alt="struct"></p>
<p>那这样有什么坏处呢？如果我的程式需要用到 <code>t1.f2</code>，譬如说把他 print 出来，那 CPU 就得花两个 cycle 的时间把 f2 从记忆体抓出来，<strong>因为 f2 分散在两个 word 里面</strong></p>
<p>所以为了让 CPU 可以更快存取到各个栏位，Go 编译器会帮你的 struct 做 <code>Data Structure Align</code>，也就是在 T1 的栏位间加上一些 padding，<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> T1 &#123;</span><br><span class="line">    f1 i8</span><br><span class="line">    _ [<span class="number">7</span>]<span class="keyword">byte</span> <span class="comment">// 7 bytes padding</span></span><br><span class="line">    f2 i64</span><br><span class="line">    f3 i32</span><br><span class="line">    _ [<span class="number">4</span>]<span class="keyword">byte</span> <span class="comment">// 4 bytes padding</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>画成图就长下面这样，有 13 bytes 用来储存 struct 的资料，而深色的 11 个 bytes 则是用来当 padding，<strong>确保每个 field 的所有内容都落在同一个 word 里面</strong>，所以 struct 才会从 13 bytes 肥到 24 bytes<br><img src="/images/go/struct_3.png" alt="struct"></p>
<h2 id="Padding-可以不要那么肥吗？"><a href="#Padding-可以不要那么肥吗？" class="headerlink" title="Padding 可以不要那么肥吗？"></a>Padding 可以不要那么肥吗？</h2><p>虽说 padding 是为了把每个 field 放到更好的位置，但 padding 的空间实际上就是浪费掉了。以 T1 来说，24 bytes 里面就浪费了将近一半，那有什么方法可以兼顾 Alignment 但又不浪费太多空间吗？</p>
<p>再看一次 T1 的记忆体分佈，就会发现最下面 4 bytes 的 f3 其实可以挪到上面的 padding，反正第一排的 padding 空间超大的，不用白不用，<strong>而且挪上去之后每个栏位都还是在同一个 word 里面</strong>。<br><img src="/images/go/struct_4.png" alt="struct"></p>
<p>一旦把 f3 移上去，就可以省掉最下面一整个 word(8 bytes) 的空间，所以 T2 整个 struct 就只需要 16 bytes，是原本 T1 24 bytes 的三分之二<br><img src="/images/go/struct_5.png" alt="struct"></p>
<p>写成程式码的话，因为 Go 会按照栏位的顺序来安排记忆体中的位置，所以要把 f2 跟 f3 的顺序交换，宣告的顺序变成 <code>int8</code>、<code>int32</code>、<code>int64</code>，这样才会顺利排成上面那个图哦～<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> T2 <span class="keyword">struct</span> &#123;</span><br><span class="line">    f1 <span class="keyword">int8</span>  <span class="comment">// 1 byte</span></span><br><span class="line">    f3 <span class="keyword">int32</span> <span class="comment">// 4 bytes</span></span><br><span class="line">    f2 <span class="keyword">int64</span> <span class="comment">// 8 bytes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    t2 := T2&#123;&#125;</span><br><span class="line">    fmt.Println(unsafe.Sizeof(t2)) <span class="comment">// 16 bytes</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="编译器没办法自动最佳化吗？"><a href="#编译器没办法自动最佳化吗？" class="headerlink" title="编译器没办法自动最佳化吗？"></a>编译器没办法自动最佳化吗？</h2><p>看到这你一定觉得很麻烦，难不成每次用 Struct 都要自己拼拼凑凑、算算看怎么样的顺序最省空间？这种底层的鸟事应该<a href="https://medium.com/starbugs/see-what-compiler-optimization-do-from-llvm-ir-dfd3774292cb" target="_blank" rel="external">由编译器来最佳化</a>才对啊！</p>
<p>遗憾的是，目前 Go 编译器不会自动做这些最佳化（但 <a href="https://camlorn.net/posts/April%202017/rust-struct-field-reordering/" target="_blank" rel="external">Rust 三年前就支援了</a>，希望 Go 也能赶快跟进XD），所以如果很在意 struct 有没有充分利用记忆体空间，可以自己画图排排看，或是用 <a href="https://github.com/orijtech/structslop" target="_blank" rel="external">structslop</a> 进行分析。</p>
<h3 id="structslop"><a href="#structslop" class="headerlink" title="structslop"></a>structslop</h3><p><code>structslop</code> 是一个用 Go 写成的开源工具，他的功能就是帮你调整 struct 的栏位顺序，<strong>以达到最好的空间利用率</strong>。像下面的例子 Student 里面包含了学号、姓名、班级、成绩等等资讯。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Student <span class="keyword">struct</span> &#123;</span><br><span class="line">    id       <span class="keyword">int8</span>     <span class="comment">// 1 byte</span></span><br><span class="line">    name     <span class="keyword">string</span>   <span class="comment">// 16 bytes</span></span><br><span class="line">    classID  <span class="keyword">int8</span>     <span class="comment">// 1 byte</span></span><br><span class="line">    phone    [<span class="number">10</span>]<span class="keyword">byte</span> <span class="comment">// 10 bytes</span></span><br><span class="line">    address  <span class="keyword">string</span>   <span class="comment">// 16 bytes</span></span><br><span class="line">    grade    <span class="keyword">int32</span>    <span class="comment">// 4 bytes</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果画成图就长这样，可以看到里面还有很多深色的 padding，算一算总共浪费了 16 bytes，感觉不是那么优。<br><img src="/images/go/struct_6.png" alt="struct"></p>
<p>这时就可以用 <code>structslop</code> 帮你分析并且算出一个最佳解，只要把栏位顺序改成他建议的，Student 占用的空间就可以从 64 bytes 最佳化到 48 bytes，共<strong>省下 25% 的空间</strong>。<br><img src="/images/go/struct_7.png" alt="struct"></p>
<p>如果把 <code>structslop</code> 推荐的 field 顺序画成图就长这样，全部排得满满的，没有任何一点 padding，看了心情都好了起来XD<br><img src="/images/go/struct_8.png" alt="struct"></p>
<h2 id="有必要省空间省成这样吗"><a href="#有必要省空间省成这样吗" class="headerlink" title="有必要省空间省成这样吗"></a>有必要省空间省成这样吗</h2><p>讲完怎么省空间后，接著我们来想想，虽然重新排列栏位可以让 struct 更省空间，但真的有必要这样吗？</p>
<p>以 Student 的例子来说，经过重新排列后，一个 struct 可以省下 16 bytes。</p>
<p>如果你要写个程式来排序全校同学的成绩，需要宣告长度十万的 <code>Student array</code>，那省下的记忆体也不过 16 MB，跟现在个人电脑配备的 4GB 到 8GB 比起来根本是零头。</p>
<p>而且笔者我觉得栏位在经过重新排序之后，可读性可能会稍微降低，像 Student 原本的栏位依序是学号、姓名、班级…，满符合直觉的。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Student <span class="keyword">struct</span> &#123;</span><br><span class="line">    id       <span class="keyword">int8</span></span><br><span class="line">    name     <span class="keyword">string</span></span><br><span class="line">    classID  <span class="keyword">int8</span></span><br><span class="line">    phone    [<span class="number">10</span>]<span class="keyword">byte</span></span><br><span class="line">    address  <span class="keyword">string</span></span><br><span class="line">    grade    <span class="keyword">int32</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但重新排序后顺序就变成姓名、地址、成绩…一直到最后才是学号跟班级，总觉得越重要的栏位应该要放在越前面才是（我自己觉得啦XD）。</p>
<p>所以我的观点是不需要太早进行最佳化，除非你一开始就知道你的程式瓶颈会卡在这（也许程式要跑在嵌入式装置），否则就照平常的方式写 Go 就好，也不用去算这些有的没的，也许 Go 在哪一次更新之后就像 Rust 默默支援 <code>struct field reordering</code> 了</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，我想跟大家分享一个忘记在哪看到的句子：<strong>「Understanding the Hardware Makes You a Better Developer」</strong>，这边的 <strong>Hardware</strong> 我认为不一定是指硬体，而是泛指你所依赖的底层工具。</p>
<p>譬如说我完全不懂浏览器的 Reflow 跟 Repaint 还是可以写前端，但要做动画可能就会遇到效能瓶颈；不懂 Go 的 GC 机制还是可以把 Go 写得不错，但流量大起来时可能就会花太多时间在 GC。</p>
<p>所以虽然这篇文的结论是不需要特别去注意 <code>Data Structure Alignment</code> ，只要知道程式内部是这样运作的，并且顺其自然即可，但如果有一天真的因为这样记忆体不够了，那记得要想到调整一下栏位顺序哦～。</p>
<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a href="https://stackoverflow.com/questions/6730664/why-doesnt-c-make-the-structure-tighter" target="_blank" rel="external">Why doesn’t C++ make the structure tighter? — Stack Overflow</a></li>
<li><a href="https://camlorn.net/posts/April%202017/rust-struct-field-reordering/" target="_blank" rel="external">Optimizing Rust Struct Size: A 6-month Compiler Development Project</a></li>
<li><a href="https://techterms.com/help/difference_between_32-bit_and_64-bit_systems" target="_blank" rel="external">What is the difference between a 32-bit and 64-bit system?</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
<blockquote>
<p>来源：<a href="https://medium.com/starbugs/illustrate-how-data-alignment-affects-memory-usage-d29bf9d5bf08" target="_blank" rel="external">https://medium.com/starbugs/illustrate-how-data-alignment-affects-memory-usage-d29bf9d5bf08</a></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Golang 切片综合指南]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-slices.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在本文中，我们将讨论 “切片” 的概念，它是 Golang 中使用的一种重要数据结构。这一数据结构为你提供了处理与管理数据集合的方法。切片是围绕动态数组的概念构建的，它与动态数组相似，可以根据你的需要而伸缩。</p>
<ul>
<li>切片在增长方面是动态的，因为它们有自己的内置函数 <code>append</code>，可以快速高效地增长切片。</li>
<li>您还可以通过切割底层内存来减小切片的大小。</li>
<li>在底层内存中切片是在连续的块上分配的，因此切片为你提供的便利之处包括：索引、迭代与垃圾回收优化。<a id="more"></a>
<h2 id="切片表示"><a href="#切片表示" class="headerlink" title="切片表示"></a>切片表示</h2></li>
<li>切片不存储任何数据；它只描述底层数组的一部分。</li>
<li>切片使用一个包含三个字段的结构表示：指向底层数组的指针（pointer）、长度（length）与容量（capacity）。</li>
<li>这个数据结构类似于切片的描述符。</li>
</ul>
<p><img src="/images/go/slice_1.png" alt="Slice representation"></p>
<ul>
<li><strong>Pointer</strong>：指针用于指向数组的第一个元素，这个元素可以通过切片进行访问。在这里，指向的元素不必是数组的第一个元素。</li>
<li><strong>Length</strong>：长度代表数组中所有元素的总数。</li>
<li><strong>Capacity</strong>：容量表示切片可扩展的最大大小。</li>
</ul>
<h2 id="使用长度申明切片"><a href="#使用长度申明切片" class="headerlink" title="使用长度申明切片"></a>使用长度申明切片</h2><p>在声明切片过程中，当你仅指定长度（Length）时，容量（Capacity）值与长度（Length）值相同。<br><img src="/images/go/slice_2.png" alt="Declare a slice using the length"><br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Declaring a slice by length. Create a slice of int. </span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 5 elements. </span></span><br><span class="line">slice := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">5</span>)</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice)) <span class="comment">// Print 5</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice)) <span class="comment">// Print 5</span></span><br></pre></td></tr></table></figure></p>
<h2 id="使用长度和容量申明切片"><a href="#使用长度和容量申明切片" class="headerlink" title="使用长度和容量申明切片"></a>使用长度和容量申明切片</h2><p>在声明切片过程中，当你分别指定长度（Length）和容量（Capacity）时，这将初始化一段无法访问的底层数组来创建一个具有可用容量的切片。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span><br><span class="line"> Declaring a slice by length and capacity</span><br><span class="line"> Create a slice of integers. </span><br><span class="line"> Contains a length of 3 and has a capacity of 5 elements.</span><br><span class="line">*/</span></span><br><span class="line">slice := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice)) <span class="comment">// Print 3</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice)) <span class="comment">// Print 5</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/images/go/slice_3.png" alt="Declare a slice with length and capacity"></p>
<p>但是请注意，尝试创建容量小于长度的切片是不允许的。</p>
<h2 id="使用切片字面量创建切片"><a href="#使用切片字面量创建切片" class="headerlink" title="使用切片字面量创建切片"></a>使用切片字面量创建切片</h2><p>创建切片的惯用方法是使用切片字面量。它与创建数组相似，只是它不需要在 [ ] 操作符中指定值。你初始化切片时所用元素的数量将决定切片的初始长度与容量。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of strings. </span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 5 elements. </span></span><br><span class="line">slice := []<span class="keyword">string</span>&#123;<span class="string">"Red"</span>, <span class="string">"Blue"</span>, <span class="string">"Green"</span>, <span class="string">"Yellow"</span>, <span class="string">"Pink"</span>&#125; </span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice)) <span class="comment">//Print 5</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice)) <span class="comment">//Print 5</span></span><br><span class="line"><span class="comment">// Create a slice of integers. </span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 3 elements. </span></span><br><span class="line">intSlice:= []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>&#125;</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(intSlice)) <span class="comment">//Print 3</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(intSlice)) <span class="comment">//Print 3</span></span><br></pre></td></tr></table></figure></p>
<h2 id="声明一个带有索引位置的切片"><a href="#声明一个带有索引位置的切片" class="headerlink" title="声明一个带有索引位置的切片"></a>声明一个带有索引位置的切片</h2><p>当使用切片字面量时，你可以初始化切片的长度与容量。你所需要做的就是初始化表示所需长度和容量的索引。下面的语法将创建一个长度和容量均为 100 的切片。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of strings.</span></span><br><span class="line"><span class="comment">// Initialize the 100th element with an empty string.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">99</span>: <span class="number">88</span>&#125;</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice)) </span><br><span class="line"><span class="comment">// Print 100</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice)) </span><br><span class="line"><span class="comment">// Print 100</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/images/go/slice_4.png" alt="Declare a slice with index positions"></p>
<p>声明数组与切片的区别：</p>
<ul>
<li>如果你使用[]操作符中指定一个值，那么你在创建一个数组。</li>
<li>如果你不在[]中指定值，则创建一个切片。</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create an array of three integers. </span></span><br><span class="line">array := [<span class="number">3</span>]<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment">//Create a slice of integers with a length and capacity of three.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>&#125;</span><br></pre></td></tr></table></figure>
<h2 id="声明一个-nil-切片"><a href="#声明一个-nil-切片" class="headerlink" title="声明一个 nil 切片"></a>声明一个 nil 切片</h2><ul>
<li>切片用 <code>nil</code> 代表零值。</li>
<li>一个 nil 切片的长度和容量等于 0，且没有底层数组。</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a nil slice of integers. </span></span><br><span class="line"><span class="keyword">var</span> slice []<span class="keyword">int32</span></span><br><span class="line">fmt.Println(slice == <span class="literal">nil</span>) </span><br><span class="line"><span class="comment">//This line will print true</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice))   </span><br><span class="line"><span class="comment">// This line will print 0</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice))</span><br><span class="line"><span class="comment">// This line will print 0</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/go/slice_5.png" alt="Declare a nil slice"></p>
<h2 id="声明一个空切片"><a href="#声明一个空切片" class="headerlink" title="声明一个空切片"></a>声明一个空切片</h2><p>还可以通过初始化声明切片创建一个空切片。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Use make to create an empty slice of integers.</span></span><br><span class="line">sliceOne := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment">// Use a slice literal to create an empty slice of integers.</span></span><br><span class="line">sliceTwo := []<span class="keyword">int</span>&#123;&#125;</span><br><span class="line">fmt.Println(sliceOne == <span class="literal">nil</span>) <span class="comment">// This will print false</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(sliceOne))   <span class="comment">// This will print 0 </span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(sliceOne))   <span class="comment">// This will print 0</span></span><br><span class="line">fmt.Println(sliceTwo == <span class="literal">nil</span>) <span class="comment">// This will print false</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(sliceTwo))   <span class="comment">// This will print 0</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(sliceTwo))   <span class="comment">// This will print 0</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/images/go/slice_6.png" alt="Declare an empty slice"></p>
<h2 id="为任何特定索引赋值"><a href="#为任何特定索引赋值" class="headerlink" title="为任何特定索引赋值"></a>为任何特定索引赋值</h2><p>要修改单个元素的值，请使用[]操作符。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 4 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>&#125;</span><br><span class="line">fmt.Println(slice) <span class="comment">//This will print [10 20 30 40]</span></span><br><span class="line">slice[<span class="number">1</span>] = <span class="number">25</span> <span class="comment">// Change the value of index 1.</span></span><br><span class="line">fmt.Println(slice) <span class="comment">// This will print [10 25 30 40]</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/images/go/slice_7.png" alt="Assign a value to any specific index"></p>
<h2 id="对切片进行切片"><a href="#对切片进行切片" class="headerlink" title="对切片进行切片"></a>对切片进行切片</h2><p>我们之所以称呼切片为切片，是因为你可以通过对底层数组的一部分进行切片来创建一个新的切片。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Create a slice of integers. Contains a </span><br><span class="line">length and capacity of 5 elements.*/</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;</span><br><span class="line">fmt.Println(slice)  <span class="comment">// Print [10 20 30 40 50]</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice)) <span class="comment">// Print  5</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice)) <span class="comment">// Print  5</span></span><br><span class="line"><span class="comment">/* Create a new slice.Contains a length </span><br><span class="line">of 2 and capacity of 4 elements.*/</span></span><br><span class="line">newSlice := slice[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">fmt.Println(slice)  <span class="comment">//Print [10 20 30 40 50]</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(newSlice))  <span class="comment">//Print 2</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(newSlice))  <span class="comment">//Print 4</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/images/go/slice_9.png" alt="Take a slice of a slice"></p>
<p>在执行切片操作之后，我们拥有两个共享同一底层数组的切片。然而，这两个切片以不同的方式查看底层数组。原始切片认为底层数组的容量为 5，但 newSlice 与之不同，对 newSlice 而言，底层数组的容量为 4。newSlice 无法访问位于其指针之前的底层数组元素。就 newSlice 而言，这些元素甚至并不存在。使用下面的方式可以为任意切片后的 newSlice 计算长度和容量。</p>
<h3 id="切片的长度与容量如何计算？"><a href="#切片的长度与容量如何计算？" class="headerlink" title="切片的长度与容量如何计算？"></a>切片的长度与容量如何计算？</h3><blockquote>
<p>切片 slice[i:j] 的底层数组容量为 k 长度（Length）：j - i 容量（Capacity）：k - i</p>
</blockquote>
<h3 id="计算新的长度和容量"><a href="#计算新的长度和容量" class="headerlink" title="计算新的长度和容量"></a>计算新的长度和容量</h3><blockquote>
<p>切片 slice[1:3] 的底层数组容量为 5 长度（Length）：3 - 1 = 2 容量（Capacity）：5 - 1 = 4</p>
</blockquote>
<h3 id="对一个切片进行更改的结果"><a href="#对一个切片进行更改的结果" class="headerlink" title="对一个切片进行更改的结果"></a>对一个切片进行更改的结果</h3><p>一个切片对底层数组的共享部分所做的更改可以被另一个切片看到。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 5 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;</span><br><span class="line"><span class="comment">// Create a new slice.</span></span><br><span class="line"><span class="comment">// Contains a length of 2 and capacity of 4 elements.</span></span><br><span class="line">newSlice := slice[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"><span class="comment">// Change index 1 of newSlice.</span></span><br><span class="line"><span class="comment">// Change index 2 of the original slice.</span></span><br><span class="line">newSlice[<span class="number">1</span>] = <span class="number">35</span></span><br></pre></td></tr></table></figure></p>
<p>将数值 35 分配给 newSlice 的第二个元素后，该更改也可以在原始切片的元素中被看到。</p>
<h2 id="运行时错误显示索引超出范围"><a href="#运行时错误显示索引超出范围" class="headerlink" title="运行时错误显示索引超出范围"></a>运行时错误显示索引超出范围</h2><p>一个切片只能访问它长度以内的索引位。尝试访问超出长度的索引位元素将引发一个运行时错误。与切片容量相关联的元素只能用于切片增长。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 5 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;</span><br><span class="line"><span class="comment">// Create a new slice.</span></span><br><span class="line"><span class="comment">// Contains a length of 2 and capacity of 4 elements.</span></span><br><span class="line">newSlice := slice[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"><span class="comment">// Change index 3 of newSlice.</span></span><br><span class="line"><span class="comment">// This element does not exist for newSlice.</span></span><br><span class="line">newSlice[<span class="number">3</span>] = <span class="number">45</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span><br><span class="line">Runtime Exception:</span><br><span class="line">panic: runtime error: index out of range</span><br><span class="line">*/</span></span><br></pre></td></tr></table></figure></p>
<h2 id="切片增长"><a href="#切片增长" class="headerlink" title="切片增长"></a>切片增长</h2><p>与使用数组相比，使用切片的优势之一是：你可以根据需要增加切片的容量。当你使用内置函数 「append」 时，Golang 会负责处理所有操作细节。</p>
<ul>
<li>使用 append 前，你需要一个源切片和一个要追加的值。</li>
<li>当你的 append 调用并返回时，它将为你提供一个更改后的新切片。</li>
<li><strong>append</strong> 函数总会增加新切片的长度。</li>
<li>另一方面，容量可能会受到影响，也可能不会受到影响，这取决于源切片的可用容量。</li>
</ul>
<h2 id="使用-append-向切片追加元素"><a href="#使用-append-向切片追加元素" class="headerlink" title="使用 append 向切片追加元素"></a>使用 append 向切片追加元素</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*  Create a slice of integers.</span><br><span class="line">  Contains a length and capacity of 5 elements.*/</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Create a new slice.</span><br><span class="line"> Contains a length of 2 and capacity of 4 elements.*/</span></span><br><span class="line">newSlice := slice[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(newSlice)) <span class="comment">// Print 2</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(newSlice)) <span class="comment">// Print 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Allocate a new element from capacity.</span><br><span class="line"> Assign the value of 60 to the new element.*/</span></span><br><span class="line">newSlice = <span class="built_in">append</span>(newSlice, <span class="number">60</span>)</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(newSlice)) <span class="comment">// Print 3</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(newSlice)) <span class="comment">// Print 4</span></span><br></pre></td></tr></table></figure>
<p>当切片的底层数组没有可用容量时，append 函数将创建一个新的底层数组，拷贝正在引用的现有值，然后再分配新值。</p>
<h2 id="使用-append-增加切片的长度和容量"><a href="#使用-append-增加切片的长度和容量" class="headerlink" title="使用 append 增加切片的长度和容量"></a>使用 append 增加切片的长度和容量</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 4 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>&#125;</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice)) <span class="comment">// Print 4</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(slice)) <span class="comment">// Print 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Append a new value to the slice.</span></span><br><span class="line"><span class="comment">// Assign the value of 50 to the new element.</span></span><br><span class="line">newSlice= <span class="built_in">append</span>(slice, <span class="number">50</span>)</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(newSlice)) <span class="comment">//Print 5</span></span><br><span class="line">fmt.Println(<span class="built_in">cap</span>(newSlice)) <span class="comment">//Print 8</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/go/slice_9.png" alt="Increase the length and capacity of a slice"></p>
<p>在 append 操作后，newSlice 被给予一个自有的底层数组，该底层数组的容量是原底层数组容量的两倍。在增加底层数组容量时，append 操作十分聪明。举个例子，当切片的容量低于 1,000 个元素时，容量增长总是翻倍的。一旦元素的数量超过 1,000 个，容量就会增长 1.25 倍，即 25%。随着时间的推移，这种增长算法可能会在 Golang 中发生变化。</p>
<p>更改新切片不会对旧切片产生任何影响，因为新切片现在有一个不同的底层数组，它的指针指向一个新分配的数组。</p>
<h2 id="将一个切片追加到另一个切片中"><a href="#将一个切片追加到另一个切片中" class="headerlink" title="将一个切片追加到另一个切片中"></a>将一个切片追加到另一个切片中</h2><p>内置函数 <strong>append</strong> 还是一个<strong>可变参数</strong>函数。这意味着你可以传递多个值来追加到单个切片中。如果你使用 … 运算符，可以将一个切片的所有元素追加到另一个切片中。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create two slices each initialized with two integers.</span></span><br><span class="line">slice1:= []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>&#125;</span><br><span class="line">slice2 := []<span class="keyword">int</span>&#123;<span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"><span class="comment">// Append the two slices together and display the results.</span></span><br><span class="line">fmt.Println(<span class="built_in">append</span>(slice1, slice2...))</span><br><span class="line"><span class="comment">//Output: [1 2 3 4]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="对切片执行索引"><a href="#对切片执行索引" class="headerlink" title="对切片执行索引"></a>对切片执行索引</h2><ul>
<li>通过指定一个下限和一个上限来形成切片，例如：<code>a[low:high]</code>。这将选择一个半开范围，其中包含切片的第一个元素，但不包含切片的最后一个元素。</li>
<li>你可以省略上限或下限，这将使用它们的默认值。下限的默认值是 0，上限的默认值是切片的长度。</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a := [...]<span class="keyword">int</span>&#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125; </span><br><span class="line"><span class="comment">// an array</span></span><br><span class="line">s := a[<span class="number">1</span>:<span class="number">3</span>]               </span><br><span class="line"><span class="comment">// s == []int&#123;1, 2&#125;        </span></span><br><span class="line"><span class="comment">// cap(s) == 3</span></span><br><span class="line">s = a[:<span class="number">2</span>]                 </span><br><span class="line"><span class="comment">// s == []int&#123;0, 1&#125;        </span></span><br><span class="line"><span class="comment">// cap(s) == 4</span></span><br><span class="line">s = a[<span class="number">2</span>:]                 </span><br><span class="line"><span class="comment">// s == []int&#123;2, 3&#125;        </span></span><br><span class="line"><span class="comment">// cap(s) == 2</span></span><br><span class="line">s = a[:]                  </span><br><span class="line"><span class="comment">// s == []int&#123;0, 1, 2, 3&#125;  </span></span><br><span class="line"><span class="comment">// cap(s) == 4</span></span><br></pre></td></tr></table></figure>
<h2 id="遍历切片"><a href="#遍历切片" class="headerlink" title="遍历切片"></a>遍历切片</h2><p>Go 有一个特殊的关键字 <code>range</code>，你可以使用该关键字对切片进行遍历。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 4 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>&#125;</span><br><span class="line"><span class="comment">// Iterate over each element and display each value.</span></span><br><span class="line"><span class="keyword">for</span> index, value := <span class="keyword">range</span> slice &#123;</span><br><span class="line">   fmt.Printf(<span class="string">"Index: %d Value: %d\n"</span>, index, value)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span><br><span class="line">Output:</span><br><span class="line">Index: 0 Value: 10</span><br><span class="line">Index: 1 Value: 20</span><br><span class="line">Index: 2 Value: 30</span><br><span class="line">Index: 3 Value: 40</span><br><span class="line">*/</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>在遍历切片时，关键字 range 将返回两个值。</li>
<li>第一个值是索引下标，第二个值是索引位中值的副本。</li>
<li>一定要知道 range 是在复制值，而不是返回值的引用。</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span><br><span class="line"> Create a slice of integers.Contains </span><br><span class="line"> a length and capacity of 4 elements.</span><br><span class="line">*/</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>&#125;</span><br><span class="line"><span class="comment">/*</span><br><span class="line"> Iterate over each element and display </span><br><span class="line"> the value and addresses.</span><br><span class="line">*/</span></span><br><span class="line"><span class="keyword">for</span> index, value := <span class="keyword">range</span> slice &#123;</span><br><span class="line">   fmt.Printf(<span class="string">"Value: %d Value-Addr: %X ElemAddr: %X\n"</span>,</span><br><span class="line">   value, &amp;value, &amp;slice[index])</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span><br><span class="line">Output:</span><br><span class="line">Value: 10 Value-Addr: 10500168 ElemAddr: 1052E100</span><br><span class="line">Value: 20 Value-Addr: 10500168 ElemAddr: 1052E104</span><br><span class="line">Value: 30 Value-Addr: 10500168 ElemAddr: 1052E108</span><br><span class="line">Value: 40 Value-Addr: 10500168 ElemAddr: 1052E10C</span><br><span class="line">*/</span></span><br></pre></td></tr></table></figure>
<p><strong>range</strong> 关键字提供元素的拷贝。</p>
<p>如果你不需要下标值，你可以使用下划线字符丢弃该值。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 4 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>&#125;</span><br><span class="line"><span class="comment">// Iterate over each element and display each value.</span></span><br><span class="line"><span class="keyword">for</span> _, value := <span class="keyword">range</span> slice &#123;</span><br><span class="line">   fmt.Printf(<span class="string">"Value: %d\n"</span>, value)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span><br><span class="line">Output:</span><br><span class="line">Value: 10</span><br><span class="line">Value: 20</span><br><span class="line">Value: 30</span><br><span class="line">Value: 40</span><br><span class="line">*/</span></span><br></pre></td></tr></table></figure></p>
<p>关键字 <strong>range</strong> 总是从开始处遍历一个切片。如果你需要对切片的迭代进行更多的控制，你可以使用传统的 <strong>for</strong> 循环。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a slice of integers.</span></span><br><span class="line"><span class="comment">// Contains a length and capacity of 4 elements.</span></span><br><span class="line">slice := []<span class="keyword">int</span>&#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>&#125;</span><br><span class="line"><span class="comment">// Iterate over each element starting at element 3.</span></span><br><span class="line"><span class="keyword">for</span> index := <span class="number">2</span>; index &lt; <span class="built_in">len</span>(slice); index++ &#123;</span><br><span class="line">   fmt.Printf(<span class="string">"Index: %d Value: %d\n"</span>, index, slice[index])</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* </span><br><span class="line">Output:</span><br><span class="line">Index: 2 Value: 30</span><br><span class="line">Index: 3 Value: 40</span><br><span class="line">*/</span></span><br></pre></td></tr></table></figure></p>
<p>##总结<br>在本文中，我们深入探讨了切片的概念。我们了解到，切片并不存储任何数据，而是描述了底层数组的一部分。我们还看到，切片可以在底层数组的范围内增长和收缩，并配合索引可作为数组使用；切片的零值是 nil；函数 <strong>len</strong>、<strong>cap</strong> 和 <strong>append</strong> 都将 nil 看作一个长度和容量都为 0 的<strong>空切片</strong>；你可以通过<strong>切片字面量</strong>或调用 <strong>make</strong> 函数（将长度和容量作为参数）来创建切片。希望这些对你有所帮助！</p>
<h2 id="免责声明"><a href="#免责声明" class="headerlink" title="免责声明"></a>免责声明</h2><p>我参考了各种博客、书籍和媒体故事来撰写这篇文章。如有任何疑问，请在评论中与我联系。</p>
<p>到此为止……开心编码……快乐学习😃</p>
<blockquote>
<p>译自：掘金翻译计划 原文：<a href="https://codeburst.io/a-comprehensive-guide-to-slices-in-golang-bacebfe46669" target="_blank" rel="external">https://codeburst.io/a-comprehensive-guide-to-slices-in-golang-bacebfe46669</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Cilium：基于 BPF/XDP 实现 K8s Service 负载均衡]]></title>
      <url>http://team.jiunile.com/blog/2020/11/k8s-cilium-service.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>文章介绍了 K8s 的一些核心网络模型和设计、<code>Cilium</code> 对 <code>K8s Service</code> 的实现、<code>BPF/XDP</code> 性能优化，以及他们从中得到的一些实践经验，全是干货。</p>
<p>去年我们也参加了这个大会（LPC），并做了题为 <a href="https://linuxplumbersconf.org/event/4/contributions/458/" target="_blank" rel="external">Making the Kubernetes Service Abstraction Scale using eBPF</a> 的分享。 今天的内容是去年内容的延续，具体分为三个部分：</p>
<ul>
<li>Kubernetes 网络模型</li>
<li><code>Cilium</code> 对 <code>K8s Service</code> 负载均衡的实现，以及我们的一些实践经验</li>
<li>一些新的 <code>BPF</code> 内核扩展</li>
</ul>
<a id="more"></a>
<h2 id="1-K8s-网络基础：访问集群内服务的几种方式"><a href="#1-K8s-网络基础：访问集群内服务的几种方式" class="headerlink" title="1 K8s 网络基础：访问集群内服务的几种方式"></a>1 K8s 网络基础：访问集群内服务的几种方式</h2><p>Kubernetes 是一个分布式容器调度器，最小调度单位是 Pod。从网络的角度来说，可以认为 一个 pod 就是<strong>网络命名空间的一个实例</strong>（an instance of network namespace）。 一个 pod 内可能会有多个容器，因此，<strong>多个容器可以共存于同一个网络命名空间</strong>。</p>
<p>需要注意的是：<strong>K8s 只定义了网络模型，具体实现则是交给所谓的 CNI 插件</strong>，后者完成 pod 网络的创建和销毁。本文接下来将以 <code>Cilium CNI</code> 插件作为例子。</p>
<p>K8s 规定了<strong>每个 pod 的 IP 在集群内要能访问</strong>，这是通过 CNI 来完成的：CNI 插件负责为 pod 分配 IP 地址，然后为其创建和打通网络。 <strong>除此之外，K8s 没有对 CNI 插件做任何限制</strong>。尤其是，K8s 没有对<strong>从集群外访问 pod 的行为做任何规定</strong>。</p>
<p>接下来我们就来看看如何访问 K8s 集群里的一个<strong>服务</strong>（通常会对应多个 backend pods）。</p>
<h3 id="1-1-PodIP（直连容器-IP）"><a href="#1-1-PodIP（直连容器-IP）" class="headerlink" title="1.1 PodIP（直连容器 IP）"></a>1.1 PodIP（直连容器 IP）</h3><p>第一种方式是<strong>通过 PodIP 直接访问</strong>，这是最简单的方式。<br><img src="/images/k8s/cilium_pod-ip.png" alt="pod-ip"></p>
<p>如上图所示，这个服务的 3 个 backend pods 分别位于两个 node 上。当集群外的客户端 访问这个服务时，它会<strong>直接通过某个具体的 PodIP 来访问</strong>。</p>
<p>假设客户端和 Pod 之间的网络是可达的，那这种访问是没问题的。</p>
<p>但这种方式有几个<strong>缺点</strong>：</p>
<ol>
<li>pod 会因为某些原因重建，而 K8s <strong>无法保证它每次都会分到同一个 IP 地址</strong>。例如，如果 node 重启了，pod 很可能就会分到不同的 IP 地址，这对客户端来说个 大麻烦。</li>
<li><strong>没有内置的负载均衡</strong>。即，客户端选择一个 PodIP 后，所有的请求都会发送到这个 pod，而不是分散到不同的后端 pod。</li>
</ol>
<h3 id="1-2-HostPort（宿主机端口映射）"><a href="#1-2-HostPort（宿主机端口映射）" class="headerlink" title="1.2 HostPort（宿主机端口映射）"></a>1.2 HostPort（宿主机端口映射）</h3><p>第二种方式是使用所谓的 HostPort。<br><img src="/images/k8s/cilium_host-port.png" alt="host-port"></p>
<p>如上图所示，<strong>在宿主机的 netns 分配一个端口</strong>，并将这个端口的所有流量转发到 后端 pod。</p>
<p>这种情况下，</p>
<ol>
<li>客户端通过 Pod 所在的宿主机的 <code>HostIP:HostPort</code> 访问服务，例如上图中访问 <code>10.0.0.1:10000</code>；</li>
<li>宿主机先对<strong>流量进行 DNAT</strong>，然后转发给 Pod。</li>
</ol>
<p>这种方式的<strong>缺点</strong>：</p>
<ol>
<li>宿主机的端口资源是所有 Pod 共享的，任何一个端口只能被一个 pod 使用 ，因此<strong>在每台 node 上，任何一个服务最多只能有一个 pod</strong>（每个 backend 都是一 致的，因此需要使用相同的 HostPort）。对用户非常不友好。</li>
<li>和 PodIP 方式一样，没有内置的负载均衡。</li>
</ol>
<h3 id="1-3-NodePort-Service"><a href="#1-3-NodePort-Service" class="headerlink" title="1.3 NodePort Service"></a>1.3 NodePort Service</h3><p>NodePort 和上面的 HostPort 有点像（可以认为是 HostPort 的增强版），也是将 Pod 暴 露到宿主机 netns 的某个端口，但此时，<strong>集群内的每个 Node 上都会为这个服务的 pods 预留这个端口，并且将流量负载均衡到这些 pods</strong>。</p>
<p>如下图所示，假设这里的 NodePort 是 <code>30001</code>。当客户端请求到达任意一台 node 的 <code>30001</code> 端口时，它可以对请求做 DNAT 然后转发给本节点内的 Pod，如下图所示：<br><img src="/images/k8s/cilium_node-port.png" alt="node-port"></p>
<p>也可以 DNAT 之后将请求转发给其他节点上的 pod，如下图所示：<br><img src="/images/k8s/cilium_node-port-2.png" alt="node-port"></p>
<p>注意在后面跨宿主机转发的情况下，<strong>除了做 DNAT 还需要做 SNAT</strong>。</p>
<p><strong>优点：</strong></p>
<ol>
<li><strong>已经有了服务（service）的概念</strong>，多个 pod 属于同一个 service，挂掉一个时其 他 pod 还能继续提供服务。</li>
<li><strong>客户端不用关心 pod 在哪个 node 上</strong>，因为集群内的所有 node 上都开了这个端 口并监听在那里，它们对全局的 backend 有一致的视图。</li>
<li>已经<strong>有了负载均衡，每个 node 都是 LB</strong>。</li>
<li>在宿主机 netns 内访问这些服务时，通过 <code>localhost:NodePort</code> 就行了，无需 DNS 解析。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li><strong>大部分实现都是基于 SNAT</strong>，当 pod 不在本节点时，导致 packet 中的<strong>真实客户端 IP 地址</strong>信息丢失，监控、排障等不方便。</li>
<li>Node 做转发使得<strong>转发路径多了一跳，延时变大</strong>。</li>
</ol>
<h3 id="1-4-ExternalIPs-Service"><a href="#1-4-ExternalIPs-Service" class="headerlink" title="1.4 ExternalIPs Service"></a>1.4 ExternalIPs Service</h3><p>第四种从集群外访问 service 的方式是 <code>external IP</code>。</p>
<p>如果有外部可达的 IP ，即<strong>集群外能通过这个 IP 访问到集群内特定的 nodes</strong>，那我 们就可以通过这些 nodes 将流量转发到 service 的后端 pods，并提供负载均衡。</p>
<p>如下图所示，<code>1.1.1.1</code> 是一个 <code>external IP</code>，所有目的 IP 地址是 <code>1.1.1.1</code> 的流量会被底层的网络（K8s 控制之外）转发到 node1。<code>1.1.1.1:8080</code> 在 K8s 里定义了一个 Service，如果它将流量转发到本机内的 backend pod，需要做一次 DNAT：<br><img src="/images/k8s/cilium_external-ip.png" alt="external-ip"></p>
<p>同样，这里的后端 Pod 也可以在其他 node 上，这时除了做 DNAT 还需要做一次 SNAT， 如下图所示：<br><img src="/images/k8s/cilium_external-ip-2.png" alt="external-ip"></p>
<p><strong>优点：可以使用任何外部可达的 IP 地址来定义 Service 入口</strong>，只要用这个 IP 地址能访问集群内的至少一台机器即可。</p>
<p><strong>缺点：</strong></p>
<ol>
<li><strong>External IP 在 k8s 的控制范围之外</strong>，是由底层的网络平台提供的。例如，底层网 络通过 BGP 宣告，使得 IP 能到达某些 nodes。</li>
<li>由于这个 IP 是在 k8s 的控制之外，对 k8s 来说就是黑盒，因此<strong>从集群内访问 external IP 是存在安全隐患的</strong>，例如 <code>external IP</code> 上可能运行了 恶意服务，能够进行中间人攻击。因此，<code>Cilium</code> 目前不支持在集群内通过 <code>external IP</code> 访问 Service。</li>
</ol>
<h3 id="1-5-LoadBalancer-Service"><a href="#1-5-LoadBalancer-Service" class="headerlink" title="1.5 LoadBalancer Service"></a>1.5 LoadBalancer Service</h3><p>第五种访问方式是所谓的 LoadBalancer 模式。针对公有云还是私有云，LoadBalancer 又分为两种。</p>
<h4 id="1-5-1-私有云"><a href="#1-5-1-私有云" class="headerlink" title="1.5.1 私有云"></a>1.5.1 私有云</h4><p>如果是私有云，可以考虑实现一个自己的 <code>cloud provider</code>，或者直接使用 <a href="https://github.com/metallb/metallb" target="_blank" rel="external">MetalLB</a>。</p>
<p>如下图所示，<strong>这种模式和 externalIPs 模式非常相似</strong>，local 转发：<br><img src="/images/k8s/cilium_load-balancer.png" alt="load-balancer"></p>
<p>remote 转发：<br><img src="/images/k8s/cilium_load-balancer-2.png" alt="load-balancer"></p>
<p>但是，二者有重要区别：</p>
<ol>
<li><strong>externalIPs 在 K8s 的控制之外</strong>，使用方式是从某个地方申请一个 external IP， 然后填到 Service 的 Spec 里；这个 <code>external IP</code> 是存在安全隐患的，因为并不是 K8s 分配和控制的；</li>
<li><strong>LoadBalancer 在 K8s 的控制之内</strong>，只需要声明 这是一个 LoadBalancer 类型的 Service，K8s 的 <code>cloud-provider</code> 组件就会自动给这个 Service 分配一个外部可达的 IP，本质上 <code>cloud-provider</code> 做的事情就是从某个 LB 分配一个受信任的 VIP 然后填到 Service 的 Spec 里。</li>
</ol>
<p><strong>优点</strong>：LoadBalancer 分配的 IP 是归 K8s 管的，<strong>用户无法直接配置这些 IP</strong>，因 此也就避免了前面 <code>external IP</code> 的流量欺骗（traffic spoofing）风险。</p>
<p>但<strong>注意这些 IP 不是由 CNI 分配的，而是由 LoadBalancer 实现分配</strong>。</p>
<p><a href="https://github.com/metallb/metallb" target="_blank" rel="external">MetalLB</a> 能完成 LoadBalancer IP 的分配，然后<strong>基于 ARP/NDP 或 BGP 宣告 IP 的可达性</strong>。 此外，<strong>MetalLB 本身并不在 critical fast path</strong> 上（可以认为它只是控制平面，完成 LoadBalancer IP 的生效，接下来的请求和响应流量，即数据平面，都不经过它），因此不 影响 XDP 的使用。</p>
<h4 id="1-5-2-公有云"><a href="#1-5-2-公有云" class="headerlink" title="1.5.2 公有云"></a>1.5.2 公有云</h4><p>主流的云厂商都实现了 LoadBalancer，在它们提供的托管 K8s 内可以直接使用。</p>
<p>特点：</p>
<ol>
<li>有专门的 LB 节点作为统一入口。</li>
<li>LB 节点再将流量转发到 NodePort。</li>
<li>NodePort 再将流量转发到 backend pods。</li>
</ol>
<p>如下图所示，local 转发：<br><img src="/images/k8s/cilium_load-balancer-cloud.png" alt="load-balancer-cloud"></p>
<p>remote 转发：<br><img src="/images/k8s/cilium_load-balancer-cloud-2.png" alt="load-balancer-cloud"></p>
<p><strong>优点：</strong></p>
<ol>
<li>LoadBalancer 由云厂商实现，无需用户安装 BGP 软件、配置 BGP 协议等来宣告 VIP 可达性。</li>
<li>开箱即用，主流云厂商都针对它们的托管 K8s 集群实现了这样的功能。</li>
</ol>
<p>在这种情况下，<strong>Cloud LB 负责检测后端 node（注意不是后端 pod）的健康状态。</strong></p>
<p><strong>缺点：</strong></p>
<ol>
<li>存在两层 LB：LB 节点转发和 node 转发。</li>
<li>使用方式因厂商而已，例如各厂商的 annotations 并没有标准化到 K8s 中，跨云使用会有一些麻烦。</li>
<li><strong>Cloud API 非常慢</strong>，调用厂商的 API 来做拉入拉出非常受影响。</li>
</ol>
<h3 id="1-6-ClusterIP-Service"><a href="#1-6-ClusterIP-Service" class="headerlink" title="1.6 ClusterIP Service"></a>1.6 ClusterIP Service</h3><p>最后一种是<strong>集群内访问 Service 的方式</strong>：ClusterIP 方式。<br><img src="/images/k8s/cilium_cluster-ip.png" alt="cluster-ip"></p>
<p>ClusterIP 也是 Service 的一种 VIP，但这种方式只适用于从集群内访问 Service，例如 从一个 Pod 访问相同集群内的一个 Service。</p>
<p>ClusterIP 的特点：</p>
<ol>
<li>ClusterIP 使用的 IP 地址段是<strong>在创建 K8s 集群之前就预留好的</strong>；</li>
<li>ClusterIP <strong>不可路由</strong>（会在出宿主机之前被拦截，然后 DNAT 成具体的 PodIP）；</li>
<li><strong>只能在集群内访问</strong>（For in-cluster access only）。</li>
</ol>
<p>实际上，<strong>当创建一个 LoadBalancer 类型的 Service 时，K8s 会为我们自动创建三种类 型的 Service</strong>：</p>
<ol>
<li>LoadBalancer</li>
<li>NodePort</li>
<li>ClusterIP</li>
</ol>
<p>这三种类型的 Service 对应着同一组 backend pods。</p>
<p>我们此次分享的第一部分，K8s 网络基础至此就要结束了，实际上还有很多与 Service 相 关的 K8s 特性，例如 <code>sessionAffinity</code> 和 <code>externalTrafficPolicy</code>，但这里就不展开了，有兴趣可以参考附录。</p>
<h2 id="2-K8s-Service-负载均衡：Cilium-基于-BPF-XDP-的实现"><a href="#2-K8s-Service-负载均衡：Cilium-基于-BPF-XDP-的实现" class="headerlink" title="2 K8s Service 负载均衡：Cilium 基于 BPF/XDP 的实现"></a>2 K8s Service 负载均衡：Cilium 基于 BPF/XDP 的实现</h2><p><strong>Cilium 基于 eBPF/XDP 实现了前面提到的所有类型的 K8s Service</strong>。实现方式是：</p>
<ol>
<li>在每个 node 上运行一个 <code>cilium-agent</code>；</li>
<li><code>cilium-agent</code> 监听 <code>K8s apiserver</code>，因此能够感知到 K8s 里 Service 的变化；</li>
<li>根据 Service 的变化动态更新 BPF 配置。</li>
</ol>
<p><img src="/images/k8s/cilium_bpf-lb-layers.png" alt="bpf-lb-layers"></p>
<p>如上图所示，Service 的实现由两个主要部分组成：</p>
<ol>
<li>运行在 socket 层的 BPF 程序</li>
<li>运行在 tc/XDP 层的 BPF 程序</li>
</ol>
<p>以上两者共享 <code>service map</code> 等资源，其中存储了 service 及其 backend pods 的映射关系。</p>
<h3 id="2-1-Socket-层负载均衡（东西向流量）"><a href="#2-1-Socket-层负载均衡（东西向流量）" class="headerlink" title="2.1 Socket 层负载均衡（东西向流量）"></a>2.1 Socket 层负载均衡（东西向流量）</h3><p>Socket 层 BPF 负载均衡负责处理<strong>集群内的东西向流量</strong>。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>实现方式是：<strong>将 BPF 程序 attach 到 socket 的系统调用 hooks，使客户端直接和后端 pod 建连和通信</strong>，如下图所示，这里能 hook 的系统调用包括 <code>connect()</code>、<code>sendmsg()</code>、 <code>recvmsg()</code>、<code>getpeername()</code>、<code>bind()</code> 等，<br><img src="/images/k8s/cilium_e-w-lb.png" alt="e-w-lb"></p>
<p>这里的一个问题是，<strong>K8s 使用的还是 cgroup v1，但这个功能需要使用 v2</strong>，而由于 兼容性问题，v2 完全替换 v1 还需要很长时间。所以我们目前所能做的就是 支持 v1 和 v2 的混合模式。这也是为什么 <code>Cilium</code> 会 mount 自己的 <code>cgroup v2 instance</code> 的原因。</p>
<blockquote>
<p>Cilium mounts cgroup v2, attaches BPF to root cgroup. Hybrid use works well for root v2.</p>
</blockquote>
<p>具体到实现上，</p>
<ul>
<li><code>connect + sendmsg</code> 做<strong>正向</strong>变换（translation）</li>
<li><code>recvmsg + getpeername</code> 做<strong>反向</strong>变换，</li>
</ul>
<p>这个变换或转换是<strong>基于 socket structure 的，此时还没有创建 packet</strong>，因此<strong>不存在 packet 级别的 NAT！</strong>目前已经支持 TCP/UDP v4/v6, v4-in-v6。<strong>应用对此是无感知的，它以为自己连接到的还是 Service IP，但其实是 PodIP</strong>。</p>
<h4 id="查找后端-pods"><a href="#查找后端-pods" class="headerlink" title="查找后端 pods"></a>查找后端 pods</h4><p>Service lookup <strong>不一定能选到所有的 backend pods</strong>（scoped lookup），我们将 backend pods 拆成不同的集合。</p>
<p><strong>这样设计的好处</strong>：可以根据<strong>流量类型</strong>，例如是来自集群内还是集群外（ internal/external），<strong>来选择不同的 backends</strong>。例如，如果是到达 node 的 external traffic，我们可以限制它只能选择本机上的 backend pods，这样相比于转发到其他 node 上的 backend 就少了一跳。</p>
<p>另外，还支持通配符（wildcard）匹配，这样就能将 Service 暴露到 localhost 或者 loopback 地址，能在宿主机 netns 访问 Service。但这种方式不会将 Service 暴露到宿 主机外面。</p>
<h4 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h4><p>显然，这种 <strong>socket 级别的转换是非常高效和实用的</strong>，它可以直接将客户端 pod 连 接到某个 backend pod，与 kube-proxy 这样的实现相比，转发路径少了好几跳。</p>
<p>此外，<code>bind</code> BPF 程序在 NodePort 冲突时会<strong>直接拒绝应用的请求</strong>，因此相比产生流 量（packet）然后在后面的协议栈中被拒绝，bind 这里要更加高效，<strong>因为此时 流量（packet）都还没有产生</strong>。</p>
<p>对这一功能至关重要的两个函数：</p>
<ul>
<li><p><code>bpf_get_socket_cookie()</code><br>主要用于 UDP sockets，我们希望每个 UDP flow 都能选中相同的 backend pods。</p>
</li>
<li><p><code>bpf_get_netns_cookie()</code><br>用在两个地方：</p>
<ul>
<li>用于区分 <code>host netns</code> 和 <code>pod netns</code>，例如检测到在 <code>host netns</code> 执行 bind 时，直接拒绝（reject）；</li>
<li>用于 <code>serviceSessionAffinity</code>，实现在某段时间内永远选择相同的 backend pods。</li>
</ul>
</li>
</ul>
<p>由于 <code>cgroup v2 不感知 netns</code>，因此在这个 context 中我们没用 Pod 源 IP 信 息，通过这个 helper 能让它感知到源 IP，并以此作为它的 <code>source identifier</code>。</p>
<h3 id="2-2-TC-amp-XDP-层负载均衡（南北向流量）"><a href="#2-2-TC-amp-XDP-层负载均衡（南北向流量）" class="headerlink" title="2.2 TC &amp; XDP 层负载均衡（南北向流量）"></a>2.2 TC &amp; XDP 层负载均衡（南北向流量）</h3><p>第二种是进出集群的流量，称为南北向流量，在宿主机 tc 或 XDP hook 里处理。<br><img src="/images/k8s/cilium_n-s-lb.png" alt="n-s-lb"></p>
<p>BPF 做的事情，将入向流量转发到后端 Pod，</p>
<ol>
<li>如果 Pod 在本节点，做 DNAT；</li>
<li>如果在其他节点，还需要做 SNAT 或者 DSR。</li>
</ol>
<p><strong>这些都是 packet 级别的操作</strong>。</p>
<h3 id="2-3-XDP-相关优化"><a href="#2-3-XDP-相关优化" class="headerlink" title="2.3 XDP 相关优化"></a>2.3 XDP 相关优化</h3><p>在引入 XDP 支持时，为了使 context 的抽象更加通用，我们做了很多事情。下面就其中的 一些展开讨论。</p>
<h4 id="BPF-XDP-context-通用化"><a href="#BPF-XDP-context-通用化" class="headerlink" title="BPF/XDP context 通用化"></a>BPF/XDP context 通用化</h4><p>DNAT/SNAT engine, DSR, conntrack 等等都是在 tc BPF 里实现的。 BPF 代码中用 context 结构体传递数据包信息。</p>
<p>支持 XDP 时遇到的一个问题是：到底是将 context 抽象地更通用一些，还是直接实现一个 支持 XDP 的最小子集。我们最后是花大力气重构了以前几乎所有的 BPF 代码，来使得它更 加通用。好处是共用一套代码，这样对代码的优化同时适用于 TC 和 XDP 逻辑。</p>
<p>下面是一个具体例子：</p>
<p><code>ctx</code> 是一个通用抽象，具体是什么类型和 include 的头文件有关，基于 cxt 可以同时处 理 tc BPF 和 XDP BPF 逻辑，<br><img src="/images/k8s/cilium_generic-code.png" alt="generic-code"></p>
<p>例如对于 XDP 场景，编译时这些宏会被相应的 XDP 实现替换掉：<br><img src="/images/k8s/cilium_context-specific-code.png" alt="context-specific-code"></p>
<h4 id="内联汇编：绕过编译器自动优化"><a href="#内联汇编：绕过编译器自动优化" class="headerlink" title="内联汇编：绕过编译器自动优化"></a>内联汇编：绕过编译器自动优化</h4><p>我们遇到的另一个问题是：tc BPF 中已经为 skb 实现了很多的 helper 函数，由于共用一 套抽象，因此现在需要为 XDP 实现对应的一套函数集。这些 helpers 都是 inline 函数， 而 LLVM 会对 inline 函数的自动优化会导致接下来校验器（BPF verifier）失败。</p>
<p>我们的解决方式是用 <strong>inline asm（内联汇编）来绕过这个问题</strong>。</p>
<p>下面是一个具体例子：<code>xdp_load_bytes()</code>，使用下面这段等价的汇编代码，才能让 verifier 认出来：<br><img src="/images/k8s/cilium_inline-asm.png" alt="inline-asm"></p>
<h4 id="避免在用户侧使用-generic-XDP"><a href="#避免在用户侧使用-generic-XDP" class="headerlink" title="避免在用户侧使用 generic XDP"></a>避免在用户侧使用 generic XDP</h4><p>5.6 内核对 XDP 来说是一个里程碑式的版本（但可能不会是一个 LTS 版本），这个版本使得 <strong>XDP 在公有云上大规模可用了</strong>，例如 AWS ENA 和 Azure <code>hv_netvsc</code> 驱动。 但如果想跨平台使用 XDP，那你只应该使用最基本的一些 API，例如 XDP_PASS/DROP/TX 等等。</p>
<p>Cilium 在用户侧只使用 native XDP（only supports native XDP on user side）， 我们也用 Generic XDP，但目前只限于 CI 等场景。</p>
<p><strong>为什么我们避免在用户侧使用 generic XDP 呢</strong>？因为这套 LB 逻辑会运行在集群内的 每个 node 上，目前 linearize skb 以及 bypass GRO 会增加太大的 overhead。</p>
<h4 id="自定义内存操作函数"><a href="#自定义内存操作函数" class="headerlink" title="自定义内存操作函数"></a>自定义内存操作函数</h4><p>现在回到加载和存储字节相关的辅助函数（load and store bytes helpers）。</p>
<p>查看 BPF 反汇编代码时，发现内置函数会执行字节级别（byte-wise）的一些操作，因此我们实现了<strong>自己优化过的 <code>mem{cpy,zero,cmp,move}()</code> 函数</strong>。这一点做起来还是比较容 易的，因为 <strong>LLVM 对栈外数据（non-stack data）没有上下文信息</strong>，例如 packet data 、map data，因而它无法准确地知道底层的架构是否支持高效的非对齐访问（unaligned access）。</p>
<p>另外，在基准测试中我们发现，<strong>大流量的场景下，<code>bpf_ktime_get_ns()</code> 在 XDP 中的开 销非常大</strong>，因此我们将 clock source 变成可选的，Cilium 启动时会执行检查，如果内 核支持，就<strong>自动切换到 <code>bpf_jiffies64()</code></strong>（精度更低，但 conntrack 不需要那么高的 精度），这使得转发性能增加了大约 <code>1.1Mpps</code>。</p>
<h4 id="cb-control-buffer"><a href="#cb-control-buffer" class="headerlink" title="cb (control buffer)"></a>cb (control buffer)</h4><p>tc BPF 中大量使用 <code>skb-&gt;cb[]</code> 来传递数据，显然，XDP 中也是没有这个东西的。</p>
<p>为了在 XDP 中传递数据，我们最开始使用的是 <code>xdp_adjust_meta()</code>，但有两个缺点：</p>
<ul>
<li>missing driver support</li>
<li>high rate of cache-misses</li>
</ul>
<p><strong>后来换成 per-CPU scratch map</strong>（每个 CPU 独立的、内容可随意修改的 map）, 增加了大约 <code>1.2Mpps</code>。</p>
<h4 id="bpf-map-update-elem"><a href="#bpf-map-update-elem" class="headerlink" title="bpf_map_update_elem()"></a>bpf_map_update_elem()</h4><p>在 fast path 中有很多 <code>bpf_map_update_elem()</code> 调用，触发了 bucket spinlock。</p>
<p>如果流量来自多个 CPU，这里可以优化的是：先检查一下是否需要更新（这一步不需要加锁 ），如果原来已经存在，并且需要更新的值并没有变，那就直接返回，<br><img src="/images/k8s/cilium_bpf_map_update_ele.png" alt="bpf_map_update_ele"></p>
<h4 id="bpf-fib-lookup"><a href="#bpf-fib-lookup" class="headerlink" title="bpf_fib_lookup()"></a>bpf_fib_lookup()</h4><p><code>bpf_fib_lookup()</code> 开销非常大，但在 XDP 中，例如 hairpin LB 场景，是不需要这个 函数的，可以在编译时去掉。我们在测试环境的结果显示可以提高 <code>1.5Mpps</code>。</p>
<h4 id="静态-key"><a href="#静态-key" class="headerlink" title="静态 key"></a>静态 key</h4><p>作为这次分享的最后一个例子，不要对不确定的 LLVM 行为做任何假设。</p>
<p>我们在 BPF map 的基础上有大量的尾调用，它们有静态的 keys，能够在编译期间确 定 key 的大小。我们还实现了一个内联汇编来做静态的尾递归调用，保证 LLVM 不会出现 尾调用相关的问题。<br><img src="/images/k8s/cilium_tail_call_static.png" alt="tail_call_static"></p>
<h3 id="2-4-XDP-转发性能"><a href="#2-4-XDP-转发性能" class="headerlink" title="2.4 XDP 转发性能"></a>2.4 XDP 转发性能</h3><p>我们在 K8s 集群测试了 <strong>XDP 对 K8s Service 的转发</strong>。用 pktgen 生成 <code>10Mpps</code> 的入向处理流量，然后让 node 转发到位于其他节点的 backend pods。来看下几种不同的 负载均衡实现分别能处理多少。<br><img src="/images/k8s/cilium_fwd-performance.png" alt="fwd-performance"></p>
<p>由上图可以看出，</p>
<ol>
<li><strong>Cilium XDP 模式</strong>：能够处理全部的 <code>10Mpps</code> 入向流量，将它们转发到其他节点上的 backend pods。</li>
<li><strong>Cilium TC 模式</strong>：可以处理大约 <code>2.8Mpps</code>，虽然它的处理逻辑和 Cilium XDP 是类似的（除了 BPF helpers）。</li>
<li><strong>kube-proxy iptables 模式</strong>：能处理 <code>2.4Mpps</code>，这是 K8s 的默认 Service 负载均衡实现。</li>
<li><strong>kube-proxy IPVS 模式</strong>：性能更差一些，因为它的 <strong>per-packet overhead 更大一些</strong>，这里测试的 Service 只对应一个 backend pod。当 Service 数量更多时， <strong>IPVS 的可扩展性更好</strong>，相比 <code>iptables</code> 模式的 <code>kube-proxy</code> 性能会更好，但仍然没 法跟我们基于 TC BPF 和 XDP 的实现相比（no comparison at all）。</li>
</ol>
<p><strong>softirq 开销</strong>也是类似的，如下图所示，流量从 1Mpps 到 2Mpps 再到 4Mpps 时， XDP 模式下的 softirq 开销都远小于其他几种模式。<br><img src="/images/k8s/cilium_fwd-performance-cpu.png" alt="fwd-performance-cpu"></p>
<p>特别是 pps 到达某个临界点时，TC 和 Netfilter 实现中 <strong>softirq 开销会大到饱和</strong> —— 占用几乎全部 CPU。</p>
<h2 id="3-新的-BPF-内核扩展"><a href="#3-新的-BPF-内核扩展" class="headerlink" title="3 新的 BPF 内核扩展"></a>3 新的 BPF 内核扩展</h2><p>下面介绍几个新的 BPF 内核扩展，主要是 Cilium 相关的场景。</p>
<h3 id="3-1-避免穿越内核协议栈"><a href="#3-1-避免穿越内核协议栈" class="headerlink" title="3.1 避免穿越内核协议栈"></a>3.1 避免穿越内核协议栈</h3><p>主机收到的包，当其 backend 是本机上的 pod 时，或者包是本机产生的，目的端是一个本 机端口，这个包需要跨越不同的 netns，例如从宿主机的 netns 进入到容 器的 netns，<strong>现在 Cilium 的做法是，将包送到内核协议栈</strong>，如下图所示：<br><img src="/images/k8s/cilium_new-bpf-ext.png" alt="new-bpf-ext"></p>
<p>将包送到内核协议栈有两个原因（需要）：</p>
<ol>
<li>TPROXY 需要由内核协议栈完成：我们目前的 L7 proxy 功能会用到这个功能，</li>
<li>K8s 默认安装了一些 iptables rule，用来检测<strong>从连接跟踪的角度看是非法的连接</strong>（‘invalid’ connections on asymmetric paths），然后 netfilter 会 drop 这些连接 的包。我们最开始时曾尝试将包从宿主机 tc 层直接 redirect 到 veth，但应答包却要 经过协议栈，因此形成了<strong>非对称路径</strong>，流量被 drop。因此目前进和出都都要经过协议栈。</li>
</ol>
<p>但这样带来两个问题，如下图所示：<br><img src="/images/k8s/cilium_new-bpf-ext-3.png" alt="new-bpf-ext-3"></p>
<ol>
<li>Pod 的出向流量在进入协议栈后，在 socket buffer 层会丢掉 socket 信息（<code>skb-&gt;sk</code> gets orphaned at <code>ip_rcv_core()</code>），这导致包从主机设备发出去时， 我们无法在 FQ leaf 获得 TCP 反压（TCP back-pressure）。</li>
<li>转发和处理都是 packet 级别的，因此有 per-packet overhead。</li>
</ol>
<p>不久之前，<strong>BPF TPROXY 已经合并到内核，因此最后一个真正依赖 Netfilter 的东西已经 解决了。因此我们现在可以在 TC 层做全部逻辑处理了，无需进入内核协议栈</strong>，如下图所示：<br><img src="/images/k8s/cilium_new-bpf-ext-2.png" alt="new-bpf-ext"></p>
<h3 id="3-2-Redirection-helpers"><a href="#3-2-Redirection-helpers" class="headerlink" title="3.2 Redirection helpers"></a>3.2 Redirection helpers</h3><p>两个用于 redirection 的 TC BPF helpers：</p>
<ul>
<li><code>bpf_redirect_neigh()</code></li>
<li><code>bpf_redirect_peer()</code></li>
</ul>
<p><strong>从 IPVLAN driver 中借鉴了一些理念，实现到了 veth 驱动中</strong>。</p>
<h4 id="3-2-1-Pod-egress：bpf-redirect-neigh"><a href="#3-2-1-Pod-egress：bpf-redirect-neigh" class="headerlink" title="3.2.1 Pod egress：bpf_redirect_neigh()"></a>3.2.1 Pod egress：<code>bpf_redirect_neigh()</code></h4><p><img src="/images/k8s/cilium_tc-redir-helper.png" alt="tc-redir-helper"></p>
<p>对于 pod egress 流量，我们会填充 src 和 dst mac 地址，这和原来 neighbor subsystem 做的事情相同；此外，我们还可以保留 skb 的 socket。这些都是由 <code>bpf_redirect_neigh()</code> 来完成的：<br><img src="/images/k8s/cilium_tc-redir-helper-2.png" alt="tc-redir-helper"></p>
<p>整个过程大致实现如下，在 veth 主机端的 ingress（对应 pod 的 egress）调用这 个方法的时候：</p>
<ol>
<li>首先会查找路由，<code>ip_route_output_flow()</code></li>
<li>将 skb 和匹配的路由条目（dst entry）关联起来，<code>skb_dst_set()</code></li>
<li>然后调用到 neighbor 子系统，<code>ip_finish_output2()</code><ol>
<li>填充 neighbor 信息，即 src/dst MAC 地址</li>
<li>保留 <code>skb-&gt;sk</code> 信息，因此物理网卡上的 qdisc 都能访问到这个字段</li>
</ol>
</li>
</ol>
<p>这就是 pod 出向的处理过程。</p>
<h4 id="3-2-2-Pod-ingress：bpf-redirect-peer"><a href="#3-2-2-Pod-ingress：bpf-redirect-peer" class="headerlink" title="3.2.2 Pod ingress：bpf_redirect_peer()"></a>3.2.2 Pod ingress：<code>bpf_redirect_peer()</code></h4><p>入向流量，<strong>会有快速 netns 切换</strong>，从宿主机 netns 直接进入容器的 netns。<br><img src="/images/k8s/cilium_tc-redir-helper-3.png" alt="tc-redir-helper"></p>
<p>这是由 <code>bpf_redirect_peer()</code> 完成的。<br><img src="/images/k8s/cilium_tc-redir-helper-4.png" alt="tc-redir-helper"></p>
<p>在主机设备的 ingress 执行这个 helper 的时候，</p>
<ol>
<li>首先会获取对应的 veth pair，<code>dev = ops-&gt;ndo_get_peer_dev(dev)</code>，然后获取 veth 的对端（在另一个 netns）</li>
<li>然后，<code>skb_scrub_packet()</code></li>
<li>设置包的 dev 为容器内的 dev，<code>skb-&gt;dev = dev</code></li>
<li>重新调度一次，<code>sch_handle_ingress()</code>，这不会进入 CPU 的 backlog queue:<ol>
<li>goto another_round</li>
<li>no CPU backlog queue</li>
</ol>
</li>
</ol>
<h4 id="3-2-3-veth-to-veth"><a href="#3-2-3-veth-to-veth" class="headerlink" title="3.2.3 veth to veth"></a>3.2.3 veth to veth</h4><p>同宿主机上的两个 Pod 之间通信时，这两个 helper 也非常有用。 因为我们已经在主机 netns 的 TC ingress 层了，因此能直接将其 redirect 到另一个容 器的 ingress 路径。<br><img src="/images/k8s/cilium_tc-redir-helper-5.png" alt="tc-redir-helper"></p>
<p>这里比较好的一点是，需要针对老版本内核所做的兼容性非常少；因此，我们只需要在启动的 时候检测内核是否有相应的 helper，</p>
<ul>
<li>如果有，就用 redirection 功能；</li>
<li>如果没有，就直接返回 TC_OK，走传统的内核协议栈方式，经过内核邻居子系统。</li>
</ul>
<p>支持这些功能无需对原有的 BPF datapath 进行大规模重构。<br><img src="/images/k8s/cilium_tc-redir-helper-6.png" alt="tc-redir-helper"></p>
<h4 id="3-2-4-BPF-redirection-性能"><a href="#3-2-4-BPF-redirection-性能" class="headerlink" title="3.2.4 BPF redirection 性能"></a>3.2.4 BPF redirection 性能</h4><p>下面看下性能。</p>
<p>TCP stream 场景，相比 Cilium baseline，转发带宽增加了 <code>1.3Gbps</code>，接近线速：<br><img src="/images/k8s/cilium_new-ext-perf.png" alt="new-ext-perf"></p>
<p>更有趣的是 TCP_RR 的场景，以 transactions/second 衡量，提升了 <code>2.9</code> 倍，接近最 大性能：<br><img src="/images/k8s/cilium_new-ext-perf-2.png" alt="new-ext-perf"></p>
<h2 id="4-结束语"><a href="#4-结束语" class="headerlink" title="4 结束语"></a>4 结束语</h2><p><img src="/images/k8s/cilium_try-out.png" alt="try-out"></p>
<p>附录: <a href="/images/k8s/plumbers_2020_cilium_load_balancer.pdf">plumbers_2020_cilium_load_balancer.pdf</a></p>
<blockquote>
<p>译者：ArthurChiao 原文：<a href="https://linuxplumbersconf.org/event/7/contributions/674/" target="_blank" rel="external">https://linuxplumbersconf.org/event/7/contributions/674/</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 策略引擎工具 - Kyverno]]></title>
      <url>http://team.jiunile.com/blog/2020/11/k8s-kyverno.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kubernetes 已经能够允许人们大规模地运行分布式应用程序来彻底改变云原生生态系统。虽然 Kubernetes 是一个功能丰富、健壮的容器编排平台，但它也有自己的一套复杂性。与多个团队一起大规模管理 Kubernetes 并不容易，而且要确保人们做正确的事情并且不越界是很难管理的。</p>
<p><a href="https://github.com/kyverno/kyverno" target="_blank" rel="external">Kyverno</a> 正是解决这个问题的合适工具。它是一个开源的 Kubernetes 原生策略引擎，可以帮助您使用简单的 Kubernetes manifests 定义策略。它可以验证、修改和生成 Kubernetes 资源。因此，它允许组织定义和执行策略，以便开发人员和管理员保持一定的标准。</p>
<a id="more"></a>
<h2 id="Kyverno-是如何工作的？"><a href="#Kyverno-是如何工作的？" class="headerlink" title="Kyverno 是如何工作的？"></a>Kyverno 是如何工作的？</h2><p><code>Kyverno</code> 通过使用动态准入控制器来工作，该控制器检查您通过 <code>Kubectl</code> 发送到 <code>Kube API</code> 服务端的每个请求。如果请求与策略匹配，<code>Kyverno</code> 就应用它。否则，它将使用已定义的消息拒绝请求。</p>
<p>所以这使得 <code>Kyverno</code> 能够提供如下特性：</p>
<ul>
<li>检查 CPU 和内存限制。</li>
<li>确保用户不更改默认的网络策略。</li>
<li>检查资源名称是否与特定模式匹配。</li>
<li>确保特定的资源总是包含特定的标签。</li>
<li>拒绝对特定资源的删除和更改。</li>
<li>如果镜像标签是 <code>latest</code> 将自动更改 <code>imagePullPolicy</code> 为 <code>Always</code></li>
<li>为每个新的命名空间生成一个默认的网络策略。</li>
</ul>
<p><code>Kyverno</code> 使用自定义资源定义来定义策略，编写策略就像使用 kubectl 应用它们一样简单。</p>
<p><code>Kyverno</code> 提供了三个主要功能：</p>
<ul>
<li><strong>验证（Validation）</strong></li>
<li><strong>变更（Mutation）</strong></li>
<li><strong>生成（Generation）</strong></li>
</ul>
<p>让我们看一下它们各自的示例清单。</p>
<h2 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h2><p>一个很好的用例是确保所有的 pods 都设置了资源请求和限制。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kyverno.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterPolicy</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> check-resources</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  validationFailureAction:</span> enforce</span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">    - name:</span> check-pod-resources</span><br><span class="line"><span class="attr">      match:</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          kinds:</span></span><br><span class="line"><span class="bullet">            -</span> Pod</span><br><span class="line"><span class="attr">      validate:</span></span><br><span class="line"><span class="attr">        message:</span> <span class="string">"CPU and memory resource requests and limits are required"</span></span><br><span class="line"><span class="attr">        pattern:</span></span><br><span class="line"><span class="attr">          spec:</span></span><br><span class="line"><span class="attr">            containers:</span></span><br><span class="line"><span class="attr">              - name:</span> <span class="string">"*"</span></span><br><span class="line"><span class="attr">                resources:</span></span><br><span class="line"><span class="attr">                  limits:</span></span><br><span class="line"><span class="attr">                    memory:</span> <span class="string">"?*"</span></span><br><span class="line"><span class="attr">                    cpu:</span> <span class="string">"?*"</span></span><br><span class="line"><span class="attr">                  requests:</span></span><br><span class="line"><span class="attr">                    memory:</span> <span class="string">"?*"</span></span><br><span class="line"><span class="attr">                    cpu:</span> <span class="string">"?*"</span></span><br></pre></td></tr></table></figure></p>
<p>大多数配置都是比较清楚明白的，<code>validationFailureAction</code> 申明是强制执行（通过使用 <code>enforcement</code> ）还是只审计它(通过 <code>audit</code> )并报告违规情况。</p>
<h2 id="Mutation"><a href="#Mutation" class="headerlink" title="Mutation"></a>Mutation</h2><p><strong>Mutation</strong> 意味着如果匹配到满足特定的场景就变更资源属性。一个很好的例子是，如果镜像标签是最新的，那么将 <code>imagePullPolicy</code> 更改为 <code>Always</code>。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kyverno.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterPolicy</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> image-pull-policy-always</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">    - name:</span> image-pull-policy-latest</span><br><span class="line"><span class="attr">      match:</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          kinds:</span></span><br><span class="line"><span class="bullet">            -</span> Pod</span><br><span class="line"><span class="attr">      mutate:</span></span><br><span class="line"><span class="attr">        overlay:</span></span><br><span class="line"><span class="attr">          spec:</span></span><br><span class="line"><span class="attr">            containers:</span></span><br><span class="line"><span class="bullet">              -</span> (image): <span class="string">"*:latest"</span></span><br><span class="line"><span class="attr">                imagePullPolicy:</span> <span class="string">"Always"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Generate"><a href="#Generate" class="headerlink" title="Generate"></a>Generate</h2><p>顾名思义，针对特定事件生成资源。例如，如果有人创建了一个新的名称空间，我们可能希望执行默认的网络策略。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kyverno.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterPolicy</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">"default-deny"</span></span><br><span class="line"><span class="attr">    match:</span></span><br><span class="line"><span class="attr">      resources:</span> </span><br><span class="line"><span class="attr">        kinds:</span></span><br><span class="line"><span class="bullet">        -</span> Namespace</span><br><span class="line"><span class="attr">        name:</span> <span class="string">"*"</span></span><br><span class="line"><span class="attr">    exclude:</span></span><br><span class="line"><span class="attr">      namespaces:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"kube-system"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"default"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"kube-public"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"kyverno"</span></span><br><span class="line"><span class="attr">    generate:</span> </span><br><span class="line"><span class="attr">      kind:</span> NetworkPolicy</span><br><span class="line"><span class="attr">      name:</span> default-deny-all-traffic</span><br><span class="line"><span class="attr">      namespace:</span> <span class="string">"<span class="template-variable">&#123;&#123;request.object.metadata.namespace&#125;&#125;</span>"</span> </span><br><span class="line"><span class="attr">      data:</span>  </span><br><span class="line"><span class="attr">        spec:</span></span><br><span class="line"><span class="attr">          podSelector:</span> &#123;&#125;</span><br><span class="line"><span class="attr">          policyTypes:</span> </span><br><span class="line"><span class="bullet">          -</span> Ingress</span><br><span class="line"><span class="bullet">          -</span> Egress</span><br></pre></td></tr></table></figure></p>
<h2 id="体验一把"><a href="#体验一把" class="headerlink" title="体验一把"></a>体验一把</h2><p>现在让我们亲自动手，看看 <code>Kyverno</code> 的行为。我们将安装 <code>Kyverno</code>，然后应用验证策略来检查特定的标签。如果标签不存在，<code>Kyverno</code> 将拒绝请求。否则，它将应用它。</p>
<h3 id="安装-Kyverno"><a href="#安装-Kyverno" class="headerlink" title="安装 Kyverno"></a>安装 Kyverno</h3><p>安装 <code>Kyverno</code> 很简单。你可以应用 GitHub 上的 <code>Kyverno Kubernetes manifest</code>，或者安装最新的 <code>helm chart</code>。</p>
<h4 id="使用-manifest"><a href="#使用-manifest" class="headerlink" title="使用 manifest"></a>使用 manifest</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create <span class="_">-f</span> https://raw.githubusercontent.com/kyverno/kyverno/master/definitions/release/install.yaml</span><br></pre></td></tr></table></figure>
<h4 id="使用-helm-chart"><a href="#使用-helm-chart" class="headerlink" title="使用 helm chart"></a>使用 helm chart</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm repo add kyverno https://kyverno.github.io/kyverno/</span><br><span class="line">kubectl create ns kyverno</span><br><span class="line">helm install kyverno --namespace kyverno kyverno/kyverno</span><br></pre></td></tr></table></figure>
<p>检查我们是否成功安装了 <code>Kyverno</code>，列出 <code>Kyverno</code> 命名空间中的所有资源：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get all -n kyverno</span></span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/kyverno-5f7769d697-x8lkj   0/1     Running   0          21s</span><br><span class="line">NAME                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/kyverno-svc   ClusterIP   10.96.167.8   &lt;none&gt;        443/TCP   21s</span><br><span class="line">NAME                      READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/kyverno   0/1     1            0           21s</span><br><span class="line">NAME                                 DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/kyverno-5f7769d697   1         1         0       21s</span><br></pre></td></tr></table></figure></p>
<h3 id="应用策略"><a href="#应用策略" class="headerlink" title="应用策略"></a>应用策略</h3><p>让我们应用一个策略来确保所有 <code>pods</code> 都应该包含一个名为 <code>app</code> 的标签。创建名为<code>require-app-label.yaml</code> 的文件，其内容如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kyverno.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterPolicy</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> require-app-label</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  validationFailureAction:</span> enforce</span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - name:</span> check-for-app-label</span><br><span class="line"><span class="attr">    match:</span></span><br><span class="line"><span class="attr">      resources:</span></span><br><span class="line"><span class="attr">        kinds:</span></span><br><span class="line"><span class="bullet">        -</span> Pod</span><br><span class="line"><span class="attr">    validate:</span></span><br><span class="line"><span class="attr">      message:</span> <span class="string">"label `app` is required"</span></span><br><span class="line"><span class="attr">      pattern:</span></span><br><span class="line"><span class="attr">        metadata:</span></span><br><span class="line"><span class="attr">          labels:</span></span><br><span class="line"><span class="attr">            app:</span> <span class="string">"?*"</span></span><br></pre></td></tr></table></figure></p>
<p>如果您查看 YAML，会看到有一个匹配部分，其中包含我们应该匹配的资源类型。在这个场景中，我们看到一个 pod。<code>validate</code> 部分定义了验证失败时应该输出的消息，以及定义需要匹配什么内容的模式。</p>
<p>由于这是一个 CRD，我们可以直接应用它，得到想要的结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> require-app-label.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>让我们创建一个没有标签的 pod，看看我们会得到什么：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image=nginx</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/kyverno_1.gif" alt="kyverno"></p>
<p>所以正如我们所看到的，验证失败的原因如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Error from server: admission webhook <span class="string">"nirmata.kyverno.resource.validating-webhook"</span> denied the request:</span><br><span class="line">resource Deployment/default/nginx was blocked due to the following policies</span><br><span class="line">require-app-label:</span><br><span class="line">  autogen-check-for-app-label: <span class="string">'Validation error: label `app` is required; Validation rule autogen-check-for-app-label failed at path /spec/template/metadata/labels/app/'</span></span><br></pre></td></tr></table></figure></p>
<p>这和预期的一样，因为我们还没有提供标签。现在让我们尝试使用标签 <code>name=nginx</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image=nginx --labels=<span class="string">"name=nginx"</span> --generator=run-pod/v1</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/kyverno_2.gif" alt="kyverno"></p>
<p>这个也失败了，因为 app 标签仍然缺失。让我们用 <code>app=NGINX</code> 标签创建一个 NGINX pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image=nginx --labels=<span class="string">"app=nginx"</span> --generator=run-pod/v1</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/kyverno_3.gif" alt="kyverno"></p>
<p>正如我们所看到的，pod 已经成功创建。现在，让我们使用 kubectl 来获取 pod 和标签:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod nginx --show-labels</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/kyverno_4.gif" alt="kyverno"></p>
<p>pod 正在运行，并包含 <code>app=nginx</code> 标签。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>Kyverno</code> 是一款优秀的 “policy-as-code” 工具，它在组织层执行最佳实践方面非常强大。由于它是 kubernets 原生的，所以编写和操作都很简单，不需要专门的开发人员进行维护。</p>
<p>感谢你的阅读！希望你喜欢这篇文章。</p>
<blockquote>
<p>译自：<a href="https://medium.com/better-programming/policy-as-code-on-kubernetes-with-kyverno-b144749f144" target="_blank" rel="external">https://medium.com/better-programming/policy-as-code-on-kubernetes-with-kyverno-b144749f144</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[探索 Go Trace 包]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-trace.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote>
<p>本文基于 Go 1.13</p>
</blockquote>
<p>Go 为我们提供了一个工具，可以在运行时进行跟踪，并获得程序执行的详细视图。这个工具可以通过在测试中使用标记 <code>-trace</code> 来启用，可以通过 <code>pprof</code> 来进行实时跟踪，也可以通过<code>trace</code> <a href="https://golang.org/pkg/runtime/trace/" target="_blank" rel="external">包</a>在代码中的任何位置启用。这个工具可以更加强大，因为您可以自定义 traces 来增强它。让我们回顾一下它是如何工作的。</p>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>该工具的流程非常简单。每个事件，如内存分配；垃圾回收器的所有阶段；goroutines 在运行、暂停等情况下会被 Go 标准库静态记录，并格式化后显示。然而，在录制开始之前，Go 首先“stops the world”，并对当前的 goroutines 及其状态进行快照。<br><a id="more"></a></p>
<p>这将在之后能让 Go 正确地构建每个 goroutine 的生命周期。流程如下:<br><img src="/images/go/trace_1.png" alt="Initialization phase before tracing"></p>
<p>然后，将收集的事件推送到缓冲区，当达到最大容量时，该缓冲区随后将刷新到完整缓冲区列表。这是此流程的图:<br><img src="/images/go/trace_2.png" alt="Tracing collect events per P"></p>
<p>跟踪器现在需要一种将这些跟踪转储到输出的方法。为此，当追踪开始时，Go 会产生一个专用于此的 goroutine。如果可用，该 goroutine 将转储数据，并将把 goroutine 停放到下一个。这是它的一个表示：<br><img src="/images/go/trace_3.png" alt="A dedicated goroutine reads and dump the traces"></p>
<p>现在流程非常清晰，所以让我们回顾一下记录的跟踪事件。</p>
<h2 id="追踪"><a href="#追踪" class="headerlink" title="追踪"></a>追踪</h2><p>生成跟踪后，就可以通过运行命令 <code>go tool trace my-output.out</code> 来实现可视化。让我们以一些跟踪事件为例：<br><img src="/images/go/trace_4.png" alt="Tracing from go tool"></p>
<p>大多数都很简单。与垃圾回收器相关的跟踪位于蓝色跟踪 <code>GC</code> 下:<br><img src="/images/go/trace_5.png" alt="Traces of the garbage collector"></p>
<p>快速回顾：</p>
<ul>
<li><code>STW</code> 是垃圾回收器中的两个 “Stop the World” 阶段。在这两个阶段，goroutines 被停止。</li>
<li><code>GC (空闲)</code> 是在没有工作要做时标记内存的 goroutine。</li>
<li><code>MARK ASSIST</code> 是在分配期间帮助标记内存的 goroutines。</li>
<li><code>GXX runtime.bgsweep</code> 是垃圾回收器完成后的内存扫描阶段。</li>
<li><code>GXX runtime.gcBgMarkWorker</code> 是帮助标记内存的专用后台 goroutines。</li>
</ul>
<p>然而，有些追踪事件并不容易理解。让我们回顾一下，以便更好地理解：</p>
<ul>
<li><p>当处理器与线程关联时，将调用 <code>proc start</code>。当启动新线程或从 syscall 恢复时，就会发生这种情况。<br><img src="/images/go/trace_6.png" alt="trace"></p>
</li>
<li><p>当线程与当前处理器解除关联时，将调用 <code>proc stop</code>。当线程在 syscall 中被阻塞或线程退出时，就会发生这种情况。<br><img src="/images/go/trace_7.png" alt="trace"></p>
</li>
<li><p>当 goroutine 进行系统调用时，将调用 <code>syscall</code>:<br><img src="/images/go/trace_8.png" alt="trace"></p>
</li>
<li><p>当 goroutine 从 syscall 解除阻止时，将调用 <code>unblock</code> – 在这种情况下，标签 (<code>sysexit</code>) 将从被阻塞的通道显示:<br><img src="/images/go/trace_9.png" alt="trace"></p>
</li>
</ul>
<p>跟踪可以增强，因为 Go 允许您定义和可视化自己的跟踪以及标准库中的跟踪。</p>
<h2 id="用户自定义追踪"><a href="#用户自定义追踪" class="headerlink" title="用户自定义追踪"></a>用户自定义追踪</h2><p>我们可以定义的跟踪有两个级别：</p>
<ul>
<li>在任务的顶层，有开始和结束。</li>
<li>在区域的子级别上。</li>
</ul>
<p>下面是一个简单的例子：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	ctx, task := trace.NewTask(context.Background(), <span class="string">"main start"</span>)</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">	wg.Add(<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> wg.Done()</span><br><span class="line">		r := trace.StartRegion(ctx, <span class="string">"reading file"</span>)</span><br><span class="line">		<span class="keyword">defer</span> r.End()</span><br><span class="line"> </span><br><span class="line">		ioutil.ReadFile(<span class="string">`n1.txt`</span>)</span><br><span class="line">	&#125;()</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> wg.Done()</span><br><span class="line">		r := trace.StartRegion(ctx, <span class="string">"writing file"</span>)</span><br><span class="line">		<span class="keyword">defer</span> r.End()</span><br><span class="line"> </span><br><span class="line">		ioutil.WriteFile(<span class="string">`n2.txt`</span>, []<span class="keyword">byte</span>(<span class="string">`42`</span>), <span class="number">0644</span>)</span><br><span class="line">	&#125;()</span><br><span class="line"> </span><br><span class="line">	wg.Wait()</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">defer</span> task.End()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这些新的跟踪可以通过菜单用户自定义直接从工具中可视化：<br><img src="/images/go/trace_10.png" alt="Custom task and regions"></p>
<p>还可以任意将一些日志记录到任务中:<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ctx, task := trace.NewTask(context.Background(), <span class="string">"main start"</span>)</span><br><span class="line">trace.Log(ctx, <span class="string">"category"</span>, <span class="string">"I/O file"</span>)</span><br><span class="line">trace.Log(ctx, <span class="string">"goroutine"</span>, <span class="string">"2"</span>)</span><br></pre></td></tr></table></figure></p>
<p>这些日志将在设置任务的 goroutine 下找到：<br><img src="/images/go/trace_11.png" alt="Custom logs in the tracing"></p>
<p>还可以通过派生父任务的上下文等将任务嵌入到其他任务中。</p>
<p>但是，由于 <code>pprof</code> 的存在，在生产中实时跟踪所有这些事件可能会在收集它们时略微降低性能。</p>
<h2 id="性能影响"><a href="#性能影响" class="headerlink" title="性能影响"></a>性能影响</h2><p>一个简单的基准测试可以帮助理解跟踪的影响。其中一个将带标志 <code>-trace</code> 运行，另一个则不带。下面是 <code>ioutil.ReadFile()</code> 函数的基准测试结果，该函数生成了很多事件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name         time/op</span><br><span class="line">ReadFiles-8  48.1µs ± 0%</span><br><span class="line">name         time/op</span><br><span class="line">ReadFiles-8  63.5µs ± 0%  // with tracing</span><br></pre></td></tr></table></figure></p>
<p>在这种情况下，影响约为 ~35%，并且可能因应用程序而异。但是有一些工具（如 StackDriver ），允许在生产环境中进行连续的分析，同时又对应用程序保持较小的开销。</p>
<blockquote>
<p>译自：<a href="https://medium.com/a-journey-with-go/go-discovery-of-the-trace-package-e5a821743c3c" target="_blank" rel="external">https://medium.com/a-journey-with-go/go-discovery-of-the-trace-package-e5a821743c3c</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在 Go 中我应该使用指针还是拷贝结构体？]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-copy-struct-or-pointer.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>对于许多 Go 开发人员来说，系统地使用指针来共享结构体而不是拷贝本身似乎是性能方面的最佳选择。</p>
<p>为了理解使用指针而不是拷贝结构体的影响，我们将回顾两个用例。<br><a id="more"></a></p>
<h2 id="用例1：数据密集分配"><a href="#用例1：数据密集分配" class="headerlink" title="用例1：数据密集分配"></a>用例1：数据密集分配</h2><p>让我们举一个简单的例子，当你想共享一个结构体的值：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> S <span class="keyword">struct</span> &#123;</span><br><span class="line">   a, b, c <span class="keyword">int64</span></span><br><span class="line">   d, e, f <span class="keyword">string</span></span><br><span class="line">   g, h, i <span class="keyword">float64</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这是一个基本的结构体，可以通过拷贝或指针共享：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">byCopy</span><span class="params">()</span> <span class="title">S</span></span> &#123;</span><br><span class="line">   <span class="keyword">return</span> S&#123;</span><br><span class="line">      a: <span class="number">1</span>, b: <span class="number">1</span>, c: <span class="number">1</span>,</span><br><span class="line">      e: <span class="string">"foo"</span>, f: <span class="string">"foo"</span>,</span><br><span class="line">      g: <span class="number">1.0</span>, h: <span class="number">1.0</span>, i: <span class="number">1.0</span>,</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">byPointer</span><span class="params">()</span> *<span class="title">S</span></span> &#123;</span><br><span class="line">   <span class="keyword">return</span> &amp;S&#123;</span><br><span class="line">      a: <span class="number">1</span>, b: <span class="number">1</span>, c: <span class="number">1</span>,</span><br><span class="line">      e: <span class="string">"foo"</span>, f: <span class="string">"foo"</span>,</span><br><span class="line">      g: <span class="number">1.0</span>, h: <span class="number">1.0</span>, i: <span class="number">1.0</span>,</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>基于这两种方法，我们现在可以编写两个基准测试，其中一个是通过拷贝结构体传递的：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkMemoryStack</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">   <span class="keyword">var</span> s S</span><br><span class="line"></span><br><span class="line">   f, err := os.Create(<span class="string">"stack.out"</span>)</span><br><span class="line">   <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="built_in">panic</span>(err)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">defer</span> f.Close()</span><br><span class="line"></span><br><span class="line">   err = trace.Start(f)</span><br><span class="line">   <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="built_in">panic</span>(err)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">      s = byCopy()</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   trace.Stop()</span><br><span class="line"></span><br><span class="line">   b.StopTimer()</span><br><span class="line"></span><br><span class="line">   _ = fmt.Sprintf(<span class="string">"%v"</span>, s.a)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>另一个，非常相似，通过指针传递：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkMemoryHeap</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">   <span class="keyword">var</span> s *S</span><br><span class="line"></span><br><span class="line">   f, err := os.Create(<span class="string">"heap.out"</span>)</span><br><span class="line">   <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="built_in">panic</span>(err)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">defer</span> f.Close()</span><br><span class="line"></span><br><span class="line">   err = trace.Start(f)</span><br><span class="line">   <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="built_in">panic</span>(err)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">      s = byPointer()</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   trace.Stop()</span><br><span class="line"></span><br><span class="line">   b.StopTimer()</span><br><span class="line"></span><br><span class="line">   _ = fmt.Sprintf(<span class="string">"%v"</span>, s.a)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>让我们运行一下基准测试：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> ./... -bench=BenchmarkMemoryHeap -benchmem -run=^$ -count=10 &gt; head.txt &amp;&amp; benchstat head.txt</span><br><span class="line">go <span class="built_in">test</span> ./... -bench=BenchmarkMemoryStack -benchmem -run=^$ -count=10 &gt; stack.txt &amp;&amp; benchstat stack.txt</span><br></pre></td></tr></table></figure></p>
<p>以下是统计数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">name          time/op</span><br><span class="line">MemoryHeap-4  75.0ns ± 5%</span><br><span class="line">name          alloc/op</span><br><span class="line">MemoryHeap-4   96.0B ± 0%</span><br><span class="line">name          allocs/op</span><br><span class="line">MemoryHeap-4    1.00 ± 0%</span><br><span class="line">------------------</span><br><span class="line">name           time/op</span><br><span class="line">MemoryStack-4  8.93ns ± 4%</span><br><span class="line">name           alloc/op</span><br><span class="line">MemoryStack-4   0.00B</span><br><span class="line">name           allocs/op</span><br><span class="line">MemoryStack-4    0.00</span><br></pre></td></tr></table></figure></p>
<p>在这里使用结构的拷贝而不是指针要快 <strong>8 倍</strong>。</p>
<p>为了理解其中的原因，让我们来看一下 trace 生成的图表：<br><img src="/images/go/struct_1.png" alt="graph for the struct passed by copy"><br><img src="/images/go/struct_2.png" alt="graph for the struct passed by pointer"></p>
<p>第一个图很简单。由于没有使用堆，因此没有垃圾回收器和额外的 <code>goroutine</code>。</p>
<p>对于第二个图，指针的使用迫使 go 编译器将<a href="https://golang.org/doc/faq#stack_or_heap" target="_blank" rel="external">变量转义到堆中</a>，并对垃圾回收器施加压力。如果我们放大这个图，我们可以看到垃圾回收器在这个过程中扮演了重要的角色:<br><img src="/images/go/struct_3.png" alt="trace struct"></p>
<p>从这个图中我们可以看到，垃圾回收器必须每 4ms 工作一次。</p>
<p>如果我们再次放大，我们可以得到正在发生的事情的详细信息:<br><img src="/images/go/struct_4.png" alt="trace struct"></p>
<p>蓝色、粉色和红色的是垃圾回收器的阶段，而棕色的阶段与堆上的分配有关(在图表上标记为 “<code>runtime.bgsweep</code>”):</p>
<blockquote>
<p>扫描是指回收与堆内存中未标记为正在使用的值相关联的内存。当应用程序 <code>Goroutines</code> 试图在堆内存中分配新值时，就会发生此活动。扫描的延迟会增加在堆内存中执行分配的成本，并且不会与垃圾回收相关的任何延迟相关联。<br><a href="https://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html" target="_blank" rel="external">https://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html</a></p>
</blockquote>
<p>即使这个示例有点极端，我们也可以看到在堆上分配变量而不是在栈上分配变量的代价有多大。在我们的示例中，代码在栈上分配结构体并拷贝它比在堆上分配结构体并共享其地址要快得多。</p>
<p>如果您不熟悉栈/堆，如果您想了解更多关于每个堆的内部细节，您可以在网上找到许多资源，比如 Paul Gribble 的这篇<a href="https://www.gribblelab.org/CBootCamp/7_Memory_Stack_vs_Heap.html" target="_blank" rel="external">文章</a>。</p>
<p>如果我们将 <code>GOMAXPROCS=1</code> 的处理器限制为 1，情况会更糟：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">name        time/op</span><br><span class="line">MemoryHeap  114ns ± 4%</span><br><span class="line">name        alloc/op</span><br><span class="line">MemoryHeap  96.0B ± 0%</span><br><span class="line">name        allocs/op</span><br><span class="line">MemoryHeap   1.00 ± 0%</span><br><span class="line">------------------</span><br><span class="line">name         time/op</span><br><span class="line">MemoryStack  8.77ns ± 5%</span><br><span class="line">name         alloc/op</span><br><span class="line">MemoryStack   0.00B</span><br><span class="line">name         allocs/op</span><br><span class="line">MemoryStack    0.00</span><br></pre></td></tr></table></figure></p>
<p>如果在栈上有分配的基准没有改变，那么在堆上的基准已经从 75ns/op 降低到 114ns/op。</p>
<h2 id="用例2：密集的函数调用"><a href="#用例2：密集的函数调用" class="headerlink" title="用例2：密集的函数调用"></a>用例2：密集的函数调用</h2><p>对于第二个用例，我们将在结构体中添加两个空方法，稍微调整一下我们的基准用例:<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s S)</span> <span class="title">stack</span><span class="params">(s1 S)</span></span> &#123;&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *S)</span> <span class="title">heap</span><span class="params">(s1 *S)</span></span> &#123;&#125;</span><br></pre></td></tr></table></figure></p>
<p>在栈上分配的基准测试将创建一个结构并通过拷贝传递它:<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkMemoryStack</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">   <span class="keyword">var</span> s S</span><br><span class="line">   <span class="keyword">var</span> s1 S</span><br><span class="line"></span><br><span class="line">   s = byCopy()</span><br><span class="line">   s1 = byCopy()</span><br><span class="line">   <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">      <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++  &#123;</span><br><span class="line">         s.stack(s1)</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>而堆的基准测试将通过指针传递结构体：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkMemoryHeap</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">   <span class="keyword">var</span> s *S</span><br><span class="line">   <span class="keyword">var</span> s1 *S</span><br><span class="line"></span><br><span class="line">   s = byPointer()</span><br><span class="line">   s1 = byPointer()</span><br><span class="line">   <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">      <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++ &#123;</span><br><span class="line">         s.heap(s1)</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>不出所料，结果现在大不相同：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">name          time/op</span><br><span class="line">MemoryHeap-4  301µs ± 4%</span><br><span class="line">name          alloc/op</span><br><span class="line">MemoryHeap-4  0.00B</span><br><span class="line">name          allocs/op</span><br><span class="line">MemoryHeap-4   0.00</span><br><span class="line">------------------</span><br><span class="line">name           time/op</span><br><span class="line">MemoryStack-4  595µs ± 2%</span><br><span class="line">name           alloc/op</span><br><span class="line">MemoryStack-4  0.00B</span><br><span class="line">name           allocs/op</span><br><span class="line">MemoryStack-4   0.00</span><br></pre></td></tr></table></figure></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在 go 中，使用指针而不是拷贝结构体并不总是一件好事。</p>
<p>为了你的数据选择好的语义，我强烈建议阅读 <a href="https://twitter.com/goinggodotnet" target="_blank" rel="external">Bill Kennedy</a> 写的<a href="https://www.ardanlabs.com/blog/2017/06/design-philosophy-on-data-and-semantics.html" target="_blank" rel="external">关于值/指针语义</a>的文章。它将让您更好地了解结构和内置类型可以使用的策略。</p>
<p>此外，对内存使用情况的分析肯定会帮助您了解在分配和堆上发生了什么。</p>
<blockquote>
<p>来源：<a href="https://medium.com/a-journey-with-go/go-should-i-use-a-pointer-instead-of-a-copy-of-my-struct-44b43b104963" target="_blank" rel="external">https://medium.com/a-journey-with-go/go-should-i-use-a-pointer-instead-of-a-copy-of-my-struct-44b43b104963</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Golang 六种错误处理技术，可帮助您编写优雅的代码]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-handle-error.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>当在 GO 中遇到错误时你会怎么做？</p>
<p>处理错误并不简单。在讨论功能性需求时，很少考虑错误处理需求，但是错误处理是软件开发的一个重要部分。</p>
<p>在 GO 中，错误条件以方法返回值的形式返回。在我看来，将错误条件作为主流程的一部分是很有用的 – 它让开发人员在编写功能代码时承担处理错误的责任。这种范例与其他编程语言(如 Java )所提供的非常不同 – 其中异常是完全不同的流程。虽然这种不同的风格使代码更具可读性，但也带来了新的挑战。</p>
<p>本文讨论了六种处理错误、重试和可服务性的技术。虽然很少有想法是琐碎的，但其他想法并不那么受欢迎。</p>
<p>因此，让我们从列表开始！<br><a id="more"></a></p>
<h2 id="1-向左对齐"><a href="#1-向左对齐" class="headerlink" title="1 向左对齐"></a>1 向左对齐</h2><p>处理错误的最佳策略是检查错误并立即从函数返回。在一个函数中有多个错误返回语句是可以的 – 事实上，这是明智的选择。[1]</p>
<p>例如，下面的代码片段展示了如何使用 <code>if err == nil</code> 来处理一个愉快的场景，从而导致嵌套 if 检查。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Handling Happy case first - leading to nested if checks...</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">example</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">     err := somethingThatReturnsError()</span><br><span class="line">     <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="comment">//Happy processing</span></span><br><span class="line">        err = somethingElseThatReturnsError()</span><br><span class="line">        <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">           <span class="comment">//More Happy processing</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上述逻辑可以通过向左对齐逻辑来处理：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ABetterExample</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">     err := somethingThatReturnsError()</span><br><span class="line">     <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">     &#125;</span><br><span class="line">     </span><br><span class="line">     <span class="comment">// Happy processing</span></span><br><span class="line">     err = somethingElseThatReturnsError()</span><br><span class="line">     <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// More Happy processing</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="2-重试可恢复错误"><a href="#2-重试可恢复错误" class="headerlink" title="2 重试可恢复错误"></a>2 重试可恢复错误</h2><p>很少有可恢复的错误值得重试 – 网络故障、IO 操作等都可以通过简单的重试恢复。</p>
<p>下面的包可以帮助解决重试带来的麻烦。<br><a href="https://godoc.org/github.com/cenkalti/backoff#example-Retry" target="_blank" rel="external">package backoff</a></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// An operation that may fail.</span></span><br><span class="line">operation := <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span> <span class="comment">// or an error</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">err := Retry(operation, NewExponentialBackOff())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="comment">// Handle error.</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>指数回退意味着重试间隔呈指数增长 – 对于大多数网络 /IO 故障来说，这是一个明智的选择。</p>
<h2 id="3-包装错误"><a href="#3-包装错误" class="headerlink" title="3 包装错误"></a>3 包装错误</h2><p>默认的错误包是有限的 – 错误上下文的详细信息经常会丢失。例如：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testingError2</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   <span class="keyword">return</span> errors.New(<span class="string">"New Error"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testingError</span><span class="params">(accountNumber <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   err := testingError2()</span><br><span class="line">   <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> err</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">   err := testingError(<span class="string">"Acct1"</span>)</span><br><span class="line">   logrus.Error(<span class="string">"Error occurred"</span>, fmt.Sprintf(<span class="string">"%+v"</span>, err))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在这种情况下，主函数收到的错误实例没有发生在帐户 <code>Acct1</code> 上的信息。可以在函数 <code>testingErrror</code> 中记录 <code>accountNumber</code>，但是由于当前包错误，无法将该信息传递给主函数。</p>
<p>这就是 <code>github.Com/pkg/errors</code> 的来源。该库与 <code>errors</code> 兼容并带来了一些很酷的功能。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testingError2</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   <span class="keyword">return</span> errors.New(<span class="string">"New Error"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testingError</span><span class="params">(accountNumber <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   err := testingError2()</span><br><span class="line">   <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> errors.Wrap(err, <span class="string">"Error occurred while processing Card Number "</span>+accoutNumber)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">   err := testingError(<span class="string">"Acct1"</span>)</span><br><span class="line">   logrus.Error(<span class="string">"Error occurred"</span>, fmt.Sprintf(<span class="string">"%+v"</span>, err))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>github.com/pkg/errors</code> 中，您还可以使用一些额外的有用功能 – <code>errors.Unwrap</code> 和 <code>errors.Is</code></p>
<h2 id="4-日志策略"><a href="#4-日志策略" class="headerlink" title="4 日志策略"></a>4 日志策略</h2><p>Golang 的默认包日志不提供使用日志记录级别进行日志记录功能。这里有一些其他的选择：</p>
<ul>
<li><code>Glog</code>: <a href="https://github.com/golang/glog" target="_blank" rel="external">https://github.com/golang/glog</a></li>
<li><code>Logrus</code>: <a href="https://github.com/sirupsen/logrus" target="_blank" rel="external">https://github.com/sirupsen/logrus</a></li>
<li><code>Zap</code>: <a href="https://github.com/uber-go/zap" target="_blank" rel="external">https://github.com/uber-go/zap</a></li>
</ul>
<p><code>Logrus</code> 和 <code>Zap</code> 还提供了<strong>结构化日志输出</strong>的功能 – 这是一个非常方便的功能，因为它为开发人员提供了向错误日志消息添加上下文的能力。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">example</span><span class="params">(accountNumber <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> logrus.SetFormatter(&amp;logrus.JSONFormatter&#123;&#125;)</span><br><span class="line">ctxFields := logrus.Fields&#123;</span><br><span class="line">  <span class="string">"accountNumber"</span>: accountNumber,</span><br><span class="line">  <span class="string">"appname"</span>:       <span class="string">"my-app"</span>,</span><br><span class="line"> &#125;</span><br><span class="line"><span class="comment">//Happy processing</span></span><br><span class="line"> err := errors.New(<span class="string">"Some test error while doing happy processing"</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  logrus.WithFields(ctxFields).WithError(err).Error(<span class="string">"ErrMsg"</span>)</span><br><span class="line">  <span class="keyword">return</span> err</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>结构化日志输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"accountNumber"</span>:<span class="string">"ABC"</span>,<span class="string">"appname"</span>:<span class="string">"my-app"</span>,<span class="string">"error"</span>:<span class="string">"Some test error while doing happy processing"</span>,<span class="string">"level"</span>:<span class="string">"error"</span>,<span class="string">"msg"</span>:<span class="string">"ErrMsg"</span>,<span class="string">"time"</span>:<span class="string">"2009-11-10T23:00:00Z"</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>日志的另一个关键方面是获得日志堆栈跟踪的能力。如果你使用 <code>github.com/pkg/errors</code>，你就可以<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logrus.Error(<span class="string">"Error occurred"</span>, fmt.Sprintf(<span class="string">"%+v"</span>, err))</span><br></pre></td></tr></table></figure></p>
<p>你会得到一个错误堆栈跟踪如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">main.testingError2</span><br><span class="line">        /home/nayars/go/src/github.com/nayarsn/temp.go:12</span><br><span class="line">main.testingError</span><br><span class="line">        /home/nayars/go/src/github.com/nayarsn/temp.go:25</span><br><span class="line">main.main</span><br><span class="line">        /home/nayars/go/src/github.com/nayarsn/temp.go:39</span><br><span class="line">runtime.main</span><br><span class="line">        /usr/lib/go-1.15/src/runtime/proc.go:204</span><br><span class="line">runtime.goexit</span><br><span class="line">        /usr/lib/go-1.15/src/runtime/asm_amd64.s:1374</span><br></pre></td></tr></table></figure></p>
<p><code>Zap</code> 为性能进行了缓冲和优化。[2]</p>
<h2 id="5-错误检查"><a href="#5-错误检查" class="headerlink" title="5 错误检查"></a>5 错误检查</h2><p>将错误视为值是好的 – 它是明确的，而明确的有很多意义。但它也可以为开发人员提供跳过的机会。例如：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testingError</span><span class="params">(accoutNumber <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> err error</span><br><span class="line">    _ = errors.New(<span class="string">"errors.New with _"</span></span><br><span class="line">    errors.New(<span class="string">"errors.New not capturing return"</span>)</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的示例显示应用程序程序员是由 <code>errors.New</code> 语句返回的两个错误。这可能是有意或无意发生的。</p>
<p>幸运的是，有一个 linter 实用程序可以帮助您。<br><a href="https://github.com/kisielk/errcheck" target="_blank" rel="external">kisielk/errcheck</a></p>
<p>一旦你安装了 linter，你可以简单地做以下事情：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">errcheck -blank ./...</span><br></pre></td></tr></table></figure></p>
<p>你会得到这样的输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp.go:16:2:   _ = errors.New(<span class="string">"Error capturing return using _"</span>)</span><br><span class="line">temp.go:18:12:  errors.New(<span class="string">"Error not capturing return"</span>)</span><br></pre></td></tr></table></figure></p>
<p>这可以作为 CI/CD 流程的一部分，以确保应用程序开发人员不会错过这一部分。</p>
<p><code>errchec</code> 是 Go linters 聚合器实用程序的一部分 – <a href="https://golangci-lint.run/" target="_blank" rel="external">https://golangci-lint.run/</a></p>
<h2 id="6-多个错误"><a href="#6-多个错误" class="headerlink" title="6 多个错误"></a>6 多个错误</h2><p>你有多个错误的场景 – 它们是同一个 <code>go routine</code> 的一部分，你不想停止处理 – 而是继续处理并记录所有错误。这里有一个专门的库：<br><a href="https://github.com/hashicorp/go-multierror" target="_blank" rel="external">hashicorp/go-multierror</a></p>
<p>这里有一个简单的例子：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">step1</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> errors.New(<span class="string">"Step1"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">step2</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> errors.New(<span class="string">"Step2"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> result error</span><br><span class="line">    <span class="keyword">if</span> err := step1(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        result = multierror.Append(result, err)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> err := step2(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        result = multierror.Append(result, err)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    fmt.Println(multierror.Flatten(result))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>同样，对于多个 <code>go routines</code>，可以使用以下库：<br><a href="https://pkg.go.dev/golang.org/x/sync/errgroup" target="_blank" rel="external">errgroup</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我知道上述列表并非全部。对于你们中的一些人来说，这可能是微不足道的 – 但希望对你们中的一些人来说，这有助于你们掌握错误处理技术。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>[1] <a href="https://medium.com/@matryer/line-of-sight-in-code-186dd7cdea88" target="_blank" rel="external">https://medium.com/@matryer/line-of-sight-in-code-186dd7cdea88</a></li>
<li>[2] <a href="https://medium.com/a-journey-with-go/go-how-zap-package-is-optimized-dbf72ef48f2d" target="_blank" rel="external">https://medium.com/a-journey-with-go/go-how-zap-package-is-optimized-dbf72ef48f2d</a></li>
</ul>
<blockquote>
<p>译自：<a href="https://medium.com/higher-order-functions/golang-six-error-handling-techniques-to-help-you-write-elegant-code-8e6363e6d2b" target="_blank" rel="external">https://medium.com/higher-order-functions/golang-six-error-handling-techniques-to-help-you-write-elegant-code-8e6363e6d2b</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[深入理解 kubernetes iptables proxy 模式]]></title>
      <url>http://team.jiunile.com/blog/2020/11/k8s-proxy-iptables.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>最近在面试的时候问了不少 <code>network request</code> 如何到 <code>k8s service backend</code> 的问题，觉得可以整合一下网络上的资料，这篇主要讨论 <code>iptables proxy mode</code>。大部分的情况没有在使用 <code>userspace proxy modes</code>， <code>ipvs proxy mode</code> 可能要等到下一次讨论。</p>
<a id="more"></a>
<h2 id="事先准备"><a href="#事先准备" class="headerlink" title="事先准备"></a>事先准备</h2><p>要先了解 <code>iptable</code> 工作机制，建议可以看这一篇：<a href="https://phoenixnap.com/kb/iptables-tutorial-linux-firewall" target="_blank" rel="external">https://phoenixnap.com/kb/iptables-tutorial-linux-firewall</a>，当然 wikipedia 也是写的不错，我下面的文字也大多数引用：<a href="https://zh.wikipedia.org/wiki/Iptables" target="_blank" rel="external">https://zh.wikipedia.org/wiki/Iptables</a></p>
<h2 id="快速带过-iptable"><a href="#快速带过-iptable" class="headerlink" title="快速带过 iptable"></a>快速带过 <code>iptable</code></h2><p>说到 <code>iptable</code> 要先了解 <strong><code>Tables</code></strong>, <strong><code>Chains</code></strong> 和 <strong><code>Rueles</code></strong>。</p>
<ul>
<li><strong><code>Table</code></strong> 指不同类型的封包处理流程，总共有五种，不同的 <strong><code>Tables</code></strong> 处理不同的行为<ul>
<li><code>raw</code>：处理异常，追踪状态 -&gt; <code>/proc/net/nf_conntrack</code></li>
<li><code>mangle</code>：处理封包，修改 headler 之类的</li>
<li><code>nat</code>：进行位址转换操作</li>
<li><code>filter</code>：进行封包过滤</li>
<li><code>security</code>：SElinux 相关</li>
</ul>
</li>
<li><strong><code>Chains</code></strong> 来对应进行不同的行为。像是 “filter” <strong><code>Tables</code></strong> 进行封包过滤的流程，而 “nat” 针对连接进行位址转换操作。<strong><code>Chains</code></strong> 里面包含许多规则，主要有五种类型的 <strong><code>Chains</code></strong><ul>
<li><code>PREROUTING</code>：处理路由规则前通过此 <code>Chains</code>，通常用于目的位址转换（DNAT）</li>
<li><code>INPUT</code>：发往本机的封包通过此 <strong><code>Chains</code></strong>。</li>
<li><code>FORWARD</code>：本机转发的封包通过此 <strong><code>Chains</code></strong>。</li>
<li><code>OUTPUT</code>：处理本机发出的封包。</li>
<li><code>POSTROUTING</code>：完成路由规则后通过此 <strong><code>Chains</code></strong>，通常用于源位址转换（SNAT）</li>
</ul>
</li>
<li><strong><code>Rules</code></strong> 规则会被逐一进行匹配，如果匹配，可以执行相应的动作  </li>
</ul>
<h2 id="大致的工作流向情况分两种："><a href="#大致的工作流向情况分两种：" class="headerlink" title="大致的工作流向情况分两种："></a>大致的工作流向情况分两种：</h2><ol>
<li><p>backend 为本机</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NIC → PREROUTING → INPUT → Local process </span><br><span class="line">Local process → OUTPUT → POSTROUTING → NIC</span><br></pre></td></tr></table></figure>
</li>
<li><p>backend 目的地非本机</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NIC→PREROUTING → FORWARD → POSTROUTING→NIC</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="/images/k8s/proxy_iptable_1.png" alt="Iptables Basics"></p>
<p>下面是比较详细的流程，有包含 <code>EBTABLES</code>，但这个看久头会昏，我这次会主要讨论 Network Layer 这一部分，然后用上面这张比较精简的图<br><img src="/images/k8s/proxy_iptable_2.png" alt="Netfilter pic of wikipedia"></p>
<p><code>Kube-proxy</code> 修改了 filter，nat 两个表，自定义了<br><code>KUBE-SERVICES</code>，<code>KUBE-NODEPORTS</code>，<code>KUBE-POSTROUTING</code>，<code>KUBE-FORWARD</code>，<code>KUBE-MARK-MASQ</code> 和 <code>KUBE-MARK-DROP</code>，所以我这次会 focus on filter ，nat 两个 Table</p>
<h3 id="1-filter-table-有三个-Chain-“INPUT”-“OUTPUT”-“FORWARD”"><a href="#1-filter-table-有三个-Chain-“INPUT”-“OUTPUT”-“FORWARD”" class="headerlink" title="1. filter table 有三个 Chain “INPUT” “OUTPUT” “FORWARD”"></a>1. filter table 有三个 Chain “<strong>INPUT</strong>” “<strong>OUTPUT</strong>” “<strong>FORWARD</strong>”</h3><p><img src="/images/k8s/proxy_iptable_3.png" alt="iptables-routing"></p>
<p><code>kube-proxy</code> 在 filter table 的 “<strong>INPUT</strong>” “<strong>OUTPUT</strong>” chain 增加了 <code>KUBE-FIREWALL</code> 在 “<strong>INPUT</strong>” “<strong>OUTPUT</strong>” “<strong>FORWARD</strong>” chain 增加了 <code>KUBE-SERVICES</code></p>
<p><code>KUBE_FIREWALL</code> 会丢弃所有被 <code>KUBE-MARK-DROP</code> 标记 0x8000 的封包，而标记的动作可以在其他的 table 中(像是第二部分提到的 NAT table 中)<br><img src="/images/k8s/proxy_iptable_4.png" alt="proxy iptable"><br><img src="/images/k8s/proxy_iptable_5.png" alt="proxy iptable"></p>
<p>而 filter table 的 <code>KUBE-SERVICES</code> 可以过滤封包，假如一个 service 没有对应的 endpoint，就会被 reject，这里我先要建立一个 service 和没有正确设定 endpoint。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> Service </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> test-error-endpoint </span><br><span class="line"><span class="attr">  namespace:</span> default </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  ports:</span> </span><br><span class="line"><span class="attr">    - protocol:</span> TCP </span><br><span class="line"><span class="attr">      port:</span> <span class="number">7777</span> </span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">7777</span> </span><br><span class="line"><span class="bullet">-</span>-- </span><br><span class="line"><span class="attr">kind:</span> Endpoints </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> test-error-endpoint </span><br><span class="line"><span class="attr">  namespace:</span> default</span><br></pre></td></tr></table></figure>
<p>service cluster ip 为 10.95.58.92</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> Service </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> test-error-endpoint </span><br><span class="line"><span class="attr">  namespace:</span> default </span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/default/services/test-error-endpoint </span><br><span class="line"><span class="attr">  uid:</span> <span class="number">5</span>d415d63<span class="bullet">-6</span>fc3<span class="bullet">-444e-8</span>b5a<span class="bullet">-29015</span>b436a83 </span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">' 73026369'</span> </span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="string">'2020-11-17T05:48:52Z'</span> </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  ports:</span> </span><br><span class="line"><span class="attr">    - protocol:</span> TCP </span><br><span class="line"><span class="attr">      port:</span> <span class="number">7777</span> </span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">7777</span> </span><br><span class="line"><span class="attr">  clusterIP:</span> <span class="number">10.95</span><span class="number">.58</span><span class="number">.92</span> </span><br><span class="line"><span class="attr">  type:</span> ClusterIP </span><br><span class="line"><span class="attr">  sessionAffinity:</span> None </span><br><span class="line"><span class="attr">status:</span> </span><br><span class="line"><span class="attr">  loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>再次检查 iptable，就可以看到 <code>default/test-error-endpoint: has no endpoints -&gt; tcp dpt:7777 reject-with icmp-port-unreachable</code><br><img src="/images/k8s/proxy_iptable_6.png" alt="proxy iptable"></p>
<h3 id="2-nat-table-有三个-Chain-“PREROUTING”-“OUTPUT”-“POSTROUTING”"><a href="#2-nat-table-有三个-Chain-“PREROUTING”-“OUTPUT”-“POSTROUTING”" class="headerlink" title="2. nat table 有三个 Chain “PREROUTING” “OUTPUT” “POSTROUTING”"></a>2. nat table 有三个 Chain “<strong>PREROUTING</strong>” “<strong>OUTPUT</strong>” “<strong>POSTROUTING</strong>”</h3><p>在前两个封包处理流程是比较相似和复杂的，大体来说是藉由客制化的规则，来处理符合条件封包，帮它们找到正确的 k8s endpoint (后面会细讲)，在 <code>POSTROUTING</code> 主要是针对 k8s 处理的封包(标记 0x4000 的封包)，在离开 node 的时候做 SNAT</p>
<ul>
<li>(inbound) 在 “<strong>PREROUTING</strong>” 将所有封包转发到 <code>KUBE-SERVICES</code></li>
<li>(outbound) 在 “<strong>OUTPUT</strong>” 将所有封包转发到 <code>KUBE-SERVICES</code></li>
<li>(outbound) 在 “<strong>POSTROUTING</strong>” 将所有封包转发到 <code>KUBE-POSTROUTING</code></li>
</ul>
<p><img src="/images/k8s/proxy_iptable_7.png" alt="iptables-routing"><br>当封包进入 “<strong>PREROUTING</strong>” 和 “<strong>OUTPUT</strong>”，会整个被 <code>KUBE-SERVICES</code> Chain 整个绑架走，开始逐一匹配 <code>KUBE-SERVICES</code> 中的 rule 和打上标签。<br><img src="/images/k8s/proxy_iptable_8.png" alt="nat tables"></p>
<p><code>kube-proxy</code> 的用法是一种 O(n) 算法，其中的 n 随 k8s cluster 的规模同步增加，更简单的说就是 service 和 endpoint 的数量。<br><img src="/images/k8s/proxy_iptable_9.png" alt="kube-services"></p>
<blockquote>
<p>我这里会准备三个最常见的 service type 的 <code>kube-proxy</code> 路由流程</p>
<ul>
<li>cluster IP</li>
<li>nodePort</li>
<li>load balancer</li>
</ul>
</blockquote>
<h4 id="clusterIP-流程"><a href="#clusterIP-流程" class="headerlink" title="clusterIP 流程"></a>clusterIP 流程</h4><p>这里我使用 <code>default/jeff-api(clusterIP: 10.95.57.19)</code> 举例，我下面图过滤掉不必要的资讯<br><img src="/images/k8s/proxy_iptable_10.png" alt="kube-services"></p>
<p>最后会到实际 pod 的位置，<code>podIP: 10.95.35.31，hostIP: 10.20.0.128</code> 是该 pod 所在 node 的 ip<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> Pod </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> jeff-api<span class="bullet">-746</span>f4c9985<span class="bullet">-5</span>qmw6 </span><br><span class="line"><span class="attr">  generateName:</span> jeff-api<span class="bullet">-746</span>f4c9985- </span><br><span class="line"><span class="attr">  namespace:</span> default </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  containers:</span> </span><br><span class="line"><span class="attr">    - name:</span> promotion-api </span><br><span class="line"><span class="attr">      image:</span> <span class="string">'gcr.io/jeff-project/jeff /jeff-api:202011161901'</span> </span><br><span class="line"><span class="attr">      ports:</span> </span><br><span class="line"><span class="attr">        - name:</span> <span class="number">80</span>tcp02 </span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span> </span><br><span class="line"><span class="attr">          protocol:</span> TCP </span><br><span class="line"><span class="attr">  nodeName:</span> gke-sit-jeff-k8s-tw<span class="bullet">-01</span>-default-pool<span class="bullet">-7983</span>af35-ug91 </span><br><span class="line"><span class="attr">status:</span> </span><br><span class="line"><span class="attr">  phase:</span> Running </span><br><span class="line"><span class="attr">  hostIP:</span> <span class="number">10.20</span><span class="number">.0</span><span class="number">.128</span> </span><br><span class="line"><span class="attr">  podIP:</span> <span class="number">10.95</span><span class="number">.35</span><span class="number">.31</span></span><br></pre></td></tr></table></figure></p>
<h4 id="nodePort-流程"><a href="#nodePort-流程" class="headerlink" title="nodePort 流程"></a>nodePort 流程</h4><p>这里有一个关键就是 <code>KUBE-NODEPORTS</code> 一定是在 <code>KUBE-SERVICES</code> 最后一项，iptables 在处理 packet 会先处理 ip 为 cluster ip 的 service，当全部的 <code>KUBE-SVC-XXXXXX</code> 都对应不到的时候就会使用 nodePort 去匹配。<br><img src="/images/k8s/proxy_iptable_11.png" alt="kube-services"></p>
<p>我们看实际 pod 的资讯，<code>podIP: 10.95.32.17，hostIP: 10.20.0.124</code> 是其中一台 node 的 ip<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> Service </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> jeff-frontend </span><br><span class="line"><span class="attr">  namespace:</span> jeff-frontend </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  ports:</span> </span><br><span class="line"><span class="attr">    - protocol:</span> TCP </span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span> </span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">80</span> </span><br><span class="line"><span class="attr">      nodePort:</span> <span class="number">31929</span> </span><br><span class="line"><span class="attr">  selector:</span> </span><br><span class="line"><span class="attr">    app:</span> jeff-frontend </span><br><span class="line"><span class="attr">  clusterIP:</span> <span class="number">10.95</span><span class="number">.58</span><span class="number">.51</span> </span><br><span class="line"><span class="attr">  type:</span> NodePort </span><br><span class="line"><span class="attr">  externalTrafficPolicy:</span> Cluster </span><br><span class="line"><span class="bullet">-</span>-- </span><br><span class="line"><span class="attr">kind:</span> Pod </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> jeff-frontend-c94bf68d9-bbmp8 </span><br><span class="line"><span class="attr">  generateName:</span> jeff-frontend-c94bf68d9- </span><br><span class="line"><span class="attr">  namespace:</span> jeff-frontend </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line"><span class="attr">  containers:</span> </span><br><span class="line"><span class="attr">    - name:</span> jeff-frontend</span><br><span class="line"><span class="attr">      image:</span> <span class="string">'gcr.io/jeff-project/jeff/jeff-image:jeff-1.0.6.5'</span> </span><br><span class="line"><span class="attr">      ports:</span> </span><br><span class="line"><span class="attr">        - name:</span> http </span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span> </span><br><span class="line"><span class="attr">          protocol:</span> TCP </span><br><span class="line"><span class="attr">  nodeName:</span> gke-sit-jeff-k8s-tw<span class="bullet">-01</span>-default -pool-b5692f8d-enk7 </span><br><span class="line"><span class="attr">status:</span> </span><br><span class="line"><span class="attr">  phase:</span> Running </span><br><span class="line"><span class="attr">  hostIP:</span> <span class="number">10.20</span><span class="number">.0</span><span class="number">.124</span> </span><br><span class="line"><span class="attr">  podIP:</span> <span class="number">10.95</span><span class="number">.32</span><span class="number">.17</span></span><br></pre></td></tr></table></figure></p>
<h4 id="load-balancer流程"><a href="#load-balancer流程" class="headerlink" title="load balancer流程"></a>load balancer流程</h4><p>假如目的地 IP 是 load balancer 就会使用 <code>KUBE-FW-XXXXXX</code>，我建立一个 internal load balancer service 和 endpoint 指到 google postgresql DB(10.28.193.9)<br><img src="/images/k8s/proxy_iptable_12.png" alt="kube-services"></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">kind:</span> Service </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  annotations:</span> </span><br><span class="line">    cloud.google.com/load-balancer-type: Internal </span><br><span class="line">    networking.gke.io/internal-load-balancer-allow-global-access: <span class="string">'true'</span> </span><br><span class="line"><span class="attr">  name:</span> external-postgresql </span><br><span class="line">spec : </span><br><span class="line"><span class="attr">  ports:</span> </span><br><span class="line"><span class="attr">    - protocol:</span> TCP </span><br><span class="line"><span class="attr">      port:</span> <span class="number">5432</span> </span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">5432</span> </span><br><span class="line"><span class="attr">  type:</span> LoadBalancer </span><br><span class="line"><span class="bullet">-</span>-- </span><br><span class="line"><span class="attr">apiVersion:</span> v1 </span><br><span class="line"><span class="attr">kind:</span> Endpoints </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> external-postgresql </span><br><span class="line"><span class="attr">subsets:</span> </span><br><span class="line"><span class="attr">- addresses:</span> </span><br><span class="line"><span class="attr">  - ip:</span> <span class="number">10.28</span><span class="number">.193</span><span class="number">.9</span> </span><br><span class="line"><span class="attr">  ports:</span> </span><br><span class="line"><span class="attr">  - port:</span> <span class="number">5432</span> </span><br><span class="line">    protocol : TCP</span><br></pre></td></tr></table></figure>
<p><img src="/images/k8s/proxy_iptable_13.png" alt="kube-services"><br>在 NAT table 看到 <code>KUBE-MARK-MASQ</code> 和 <code>KUBE-MARK-DROP</code> 这两个规则主要是经过的封包打上标签，打上标签的封包会做相应的处理。<code>KUBE-MARK-DROP</code> 和 <code>KUBE-MARK-MASQ</code> 本质上就是使用 iptables 的 <a href="https://serverfault.com/questions/514116/how-to-set-mark-on-packet-when-forwarding-it-in-nat-prerouting-table" target="_blank" rel="external">MARK 指令</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure></p>
<p>如果打上了 <code>0x8000</code> 到后面 filter table (上面提到 <code>KUBE_FIREWALL</code> )就会丢弃。</p>
<p>如果打上了 <code>0x4000</code> k8s 将会在 <code>PREROUTING</code> table 的 <code>KUBE-POSTROUTING</code> chain 对它进行 SNAT 转换。<br><img src="/images/k8s/proxy_iptable_14.png" alt="kube-services"><br><img src="/images/k8s/proxy_iptable_15.png" alt="POSTROUTING table"><br><img src="/images/k8s/proxy_iptable_16.png" alt="KUBE-POSTROUTING Chain"><br><img src="/images/k8s/proxy_iptable_17.png" alt="KUBE-SERVICES"></p>
<p>参考:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Netfilter" target="_blank" rel="external">https://en.wikipedia.org/wiki/Netfilter</a></li>
<li><a href="https://zh.wikipedia.org/wiki/Iptables" target="_blank" rel="external">https://zh.wikipedia.org/wiki/Iptables</a></li>
<li><a href="https://phoenixnap.com/kb/iptables-tutorial-linux-firewall" target="_blank" rel="external">https://phoenixnap.com/kb/iptables-tutorial-linux-firewall</a></li>
<li><a href="https://www" target="_blank" rel="external">https://www</a> .cnblogs.com/charlieroro/p/9588019.html</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go</a></li>
<li><a href="https://www.lijiaocn.com/%E9%" target="_blank" rel="external">https://www.lijiaocn.com/%E9%</a> A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html</li>
<li><a href="https://juejin.im/post/6844904098605563912" target="_blank" rel="external">https://juejin.im/post/6844904098605563912</a></li>
<li><a href="https://tizeen.github.io/2019/03/19/" target="_blank" rel="external">https://tizeen.github.io/2019/03/19/</a> kubernetes-service-iptables%E5%88%86%E6%9E%90/</li>
<li><a href="https://www.hwchiu.com/kubernetes-service-ii.html" target="_blank" rel="external">https://www.hwchiu.com/kubernetes-service-ii.html</a></li>
<li><a href="https://www.hwchiu.com/kubernetes-service-iii" target="_blank" rel="external">https://www.hwchiu.com/kubernetes-service-iii</a> .html</li>
<li><a href="https://www.itread01.com/content/1542712570.html" target="_blank" rel="external">https://www.itread01.com/content/1542712570.html</a></li>
</ul>
<blockquote>
<p>来源：<a href="https://jeff-yen.medium.com/iptables-proxy-mode-in-kube-proxy-6862bb4b329" target="_blank" rel="external">https://jeff-yen.medium.com/iptables-proxy-mode-in-kube-proxy-6862bb4b329</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 多重错误管理]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-multi-errors.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在<a href="https://blog.golang.org/survey2019-results" target="_blank" rel="external">年度调查</a>中，关于开发人员在使用 Go 时面临的最大挑战，Go 中的错误管理总是容易引起争论，并且是一个反复出现的话题。然而，当涉及到在并发环境中处理错误或为同一个 <code>goroutine</code> 合并多个错误时，Go 提供了很棒的包，使管理多个错误变得容易。让我们看看如何合并由单个 <code>goroutine</code> 生成的多个错误。<br><a id="more"></a></p>
<h2 id="单个-goroutine-多个错误"><a href="#单个-goroutine-多个错误" class="headerlink" title="单个 goroutine, 多个错误"></a>单个 goroutine, 多个错误</h2><p>例如，当您处理具有重试策略的代码时，将多个错误合并为一个错误会非常有用。这是我们需要收集生成的错误的基本示例：<br><img src="/images/go/manage_muti_error_1.png" alt="multiple errors"></p>
<p>这个程序读取和解析一个 CSV 文本，并显示发现的错误。将错误分组以获得完整的报告可能更方便。要将错误合并为一个，我们可以在两个很棒的包中进行选择：</p>
<ul>
<li>使用 <a href="https://github.com/hashicorp" target="_blank" rel="external">HashiCorp</a> 的 <a href="https://github.com/hashicorp/go-multierror" target="_blank" rel="external">go-multierror</a></li>
</ul>
<p><img src="/images/go/manage_muti_error_2.png" alt="multiple errors"></p>
<p>输出结果如下：<br><img src="/images/go/manage_muti_error_3.png" alt="multiple errors"></p>
<ul>
<li>使用 <a href="https://github.com/uber-go" target="_blank" rel="external">Uber</a> 的 <a href="https://github.com/uber-go/multierr" target="_blank" rel="external">multierr</a></li>
</ul>
<p>这里的实现类似，下面是输出:<br><img src="/images/go/manage_muti_error_4.png" alt="multiple errors"></p>
<p>错误通过分号连接起来，没有任何其他格式。</p>
<p>对于每个包的性能，下面是一个基准测试结论：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name                    time/op         alloc/op        allocs/op</span><br><span class="line">HashiCorpMultiErrors-4  6.01µs ± 1%     6.78kB ± 0%     77.0 ± 0%</span><br><span class="line">UberMultiErrors-4       9.26µs ± 1%     10.3kB ± 0%      126 ± 0%</span><br></pre></td></tr></table></figure></p>
<p>Uber 的实现稍微慢一些，消耗更多的内存。但是，这个包的设计目的是将收集到的错误分组在一起，而不是每次都附加它们。当对错误进行分组时，结果很接近，但是代码不够优雅，因为它需要额外的步骤。以下是最新的测试结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name                    time/op         alloc/op        allocs/op</span><br><span class="line">HashiCorpMultiErrors-4  6.01µs ± 1%     6.78kB ± 0%     77.0 ± 0%</span><br><span class="line">UberMultiErrors-4       6.02µs ± 1%     7.06kB ± 0%     77.0 ± 0%</span><br></pre></td></tr></table></figure></p>
<p>这两个包都利用了 Go <code>error</code> 接口，并在其自定义实现中实现了 <code>Error() string</code> 函数。</p>
<h2 id="单个错误-多个-goroutines"><a href="#单个错误-多个-goroutines" class="headerlink" title="单个错误, 多个 goroutines"></a>单个错误, 多个 goroutines</h2><p>当处理多个 <code>goroutines</code> 来执行一个任务时，有必要正确管理结果和将错误聚合，以确保程序的正确性。</p>
<p>让我们从一个使用多个 <code>goroutines</code> 执行一系列操作的程序开始; 每个操作持续一秒钟:<br><img src="/images/go/manage_muti_error_5.png" alt="multiple errors"></p>
<p>为了说明错误传播，第三个 <code>goroutine</code> 的第一个操作将失败。事情是这样的:<br><img src="/images/go/manage_muti_error_6.png" alt="multiple errors"></p>
<p>正如预期的那样，这个程序大约需要 3 秒，因为大多数 <code>goroutines</code> 需要经历三个动作，每个动作需要 1 秒:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go run .  0.30s user 0.19s system 14% cpu 3.274 total</span><br></pre></td></tr></table></figure></p>
<p>但是，我们可能希望使 <code>goroutines</code> 相互依赖，并在其中一个失败时取消它们。避免不必要工作的解决方案是添加上下文，一旦 <code>goroutine</code> 失败，它就会取消它:<br><img src="/images/go/manage_muti_error_7.png" alt="multiple errors"></p>
<p>这正是 <a href="https://pkg.go.dev/golang.org/x/sync/errgroup?tab=doc" target="_blank" rel="external">errgroup</a> 所提供的;处理一组 <code>goroutines</code> 时的错误和上下文传播。下面是使用包 <a href="https://pkg.go.dev/golang.org/x/sync/errgroup?tab=doc" target="_blank" rel="external">errgroup</a> 的新代码：<br><img src="/images/go/manage_muti_error_8.png" alt="multiple errors"></p>
<p>程序现在运行得更快，因为它通过错误传播取消的上下文：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go run .  0.30s user 0.19s system 38% cpu 1.269 total</span><br></pre></td></tr></table></figure></p>
<p>该包提供的另一个好处是，我们不需要再担心等待组添加和标记 <code>goroutines</code> 完成。包为我们管理这些，我们只需要说我们准备好等待过程的结束。</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
<p>译自：<a href="https://medium.com/a-journey-with-go/go-multiple-errors-management-a67477628cf1" target="_blank" rel="external">https://medium.com/a-journey-with-go/go-multiple-errors-management-a67477628cf1</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go chan 为啥没有判断 close 的接口]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-chan-close.html</url>
      <content type="html"><![CDATA[<h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><ul>
<li>Go 为什么没有判断 close 的接口？</li>
<li>Go 关闭 channel 究竟做了什么？<br>  -<code>closechan</code></li>
<li>一个判断 chan 是否 close 的函数<ul>
<li>思考方法一：通过“写”chan 实现</li>
<li>思考方法二：通过“读”chan 实现</li>
<li>chan close 原则</li>
<li>其实并不需要 <code>isChanClose</code> 函数 !!!</li>
</ul>
</li>
<li>怎么优雅关闭 chan ？<ul>
<li>方法一：panic-recover</li>
<li>方法二：sync.Once</li>
<li>方法三：事件同步来解决</li>
</ul>
</li>
<li>总结</li>
</ul>
<a id="more"></a>
<h2 id="Go-为什么没有判断-close-的接口？"><a href="#Go-为什么没有判断-close-的接口？" class="headerlink" title="Go 为什么没有判断 close 的接口？"></a>Go 为什么没有判断 close 的接口？</h2><p><img src="/images/go/chan_close_1.png" alt="go channel close"></p>
<p>相信大家初学 golang chan 的时候应该都遇到过 “<strong>send on closed channel</strong>“ 的 panic 。这个 panic 是当你意图往一个已经 close 的 channel 里面投递元素的时候触发。那么你当你第一次遇到这个问题是否想过 channel 是否能提供一个接口方法来判断是否已经 close 了？我想过这个问题，但是把 chan 的源代码翻了个遍没有找到。为什么？</p>
<p>我先 hold 这个问题，我们捋一下跟 channel close 相关的事情，主要思考到 3 个问题：</p>
<ol>
<li>关闭 channel 究竟做了什么 ？</li>
<li>怎么避免 close channel 导致的 panic 问题 ？</li>
<li>怎么优雅的关闭 channel ？</li>
</ol>
<h2 id="Go-关闭-channel-究竟做了什么？"><a href="#Go-关闭-channel-究竟做了什么？" class="headerlink" title="Go 关闭 channel 究竟做了什么？"></a>Go 关闭 channel 究竟做了什么？</h2><p>首先，用户可以 close channel，如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="built_in">close</span>(c)</span><br></pre></td></tr></table></figure></p>
<p>用 gdb 或者 delve 调试下就能发现 close 一个 channel，编译器会转换成 <code>closechan</code> 函数，在这个函数里是关闭 channel 的全部实现了，我们可以分析下。</p>
<h3 id="closechan"><a href="#closechan" class="headerlink" title="closechan"></a>closechan</h3><p>对应编译函数为 <code>closechan</code> ，该函数很简单，大概做 3 个事情：</p>
<ol>
<li>标志位置 1 ，也就是 <code>c.closed = 1</code>；</li>
<li>释放资源，唤醒所有等待取元素的协程；</li>
<li>释放资源，唤醒所有等待写元素的协程；</li>
</ol>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">closechan</span><span class="params">(c *hchan)</span></span> &#123;</span><br><span class="line"> <span class="comment">// 以下为锁内操作</span></span><br><span class="line"> lock(&amp;c.lock)</span><br><span class="line"> <span class="comment">// 不能重复 close 一个 channel，否则 panic</span></span><br><span class="line"> <span class="keyword">if</span> c.closed != <span class="number">0</span> &#123;</span><br><span class="line">  unlock(&amp;c.lock)</span><br><span class="line">  <span class="built_in">panic</span>(plainError(<span class="string">"close of closed channel"</span>))</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// closed 标志位置 1</span></span><br><span class="line"> c.closed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">var</span> glist gList</span><br><span class="line"> <span class="comment">// 释放所有等待取元素的 waiter 资源</span></span><br><span class="line"> <span class="keyword">for</span> &#123;</span><br><span class="line">  <span class="comment">// 等待读的 waiter 出队</span></span><br><span class="line">  sg := c.recvq.dequeue()</span><br><span class="line">  <span class="comment">// 资源一个个销毁</span></span><br><span class="line">  <span class="keyword">if</span> sg.elem != <span class="literal">nil</span> &#123;</span><br><span class="line">   typedmemclr(c.elemtype, sg.elem)</span><br><span class="line">   sg.elem = <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">  gp := sg.g</span><br><span class="line">  gp.param = <span class="literal">nil</span></span><br><span class="line">  <span class="comment">//  相应 goroutine 加到统一队列，下面会统一唤醒</span></span><br><span class="line"></span><br><span class="line">  glist.push(gp)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 释放所有等待写元素的 waiter 资源（他们之后将会 panic）</span></span><br><span class="line"> <span class="keyword">for</span> &#123;</span><br><span class="line">  <span class="comment">// 等待写的 waiter 出队</span></span><br><span class="line">  sg := c.sendq.dequeue()</span><br><span class="line">  <span class="comment">// 资源一个个销毁</span></span><br><span class="line">  sg.elem = <span class="literal">nil</span></span><br><span class="line">  gp := sg.g</span><br><span class="line">  gp.param = <span class="literal">nil</span></span><br><span class="line">  <span class="comment">// 对应 goroutine 加到统一队列，下面会统一唤醒</span></span><br><span class="line">  glist.push(gp)</span><br><span class="line"> &#125;</span><br><span class="line"> unlock(&amp;c.lock)</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 唤醒所有的 waiter 对应的 goroutine （这个协程列表是上面 push 进来的）</span></span><br><span class="line"> <span class="keyword">for</span> !glist.empty() &#123;</span><br><span class="line">  gp := glist.pop()</span><br><span class="line">  gp.schedlink = <span class="number">0</span></span><br><span class="line">  goready(gp, <span class="number">3</span>)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过上面的代码逻辑，我们窥视到两个重要的信息：</p>
<ol>
<li>close chan 是有标识位的；</li>
<li>close chan 是会唤醒哪些等待的人们的；</li>
</ol>
<p>但是很奇怪的是，我们 golang 官方没有提供一个接口用于判断 chan 是否关闭？那我们能不能实现一个判断 chan 是否 close 的方法呢？</p>
<h2 id="一个判断-chan-是否-close-的函数"><a href="#一个判断-chan-是否-close-的函数" class="headerlink" title="一个判断 chan 是否 close 的函数"></a>一个判断 chan 是否 close 的函数</h2><p>怎么实现？首先 <code>isChanClose</code> 函数有几点要求：</p>
<ol>
<li>能够指明确实是 close 的；</li>
<li>任何时候能够正常运行，且有返回的（非阻塞）；</li>
</ol>
<p>想想 <code>send</code>, <code>recv</code> 相关的函数，我们可以知道，当前 channel 给到用户的使用姿势本质上只有两种：读和写，我们实现的 <code>isChanClose</code> 也只能在这个基础上做。</p>
<ul>
<li>写：<code>c &lt;- x</code></li>
<li>读：<code>&lt;-c</code> 或 <code>v := &lt;-c</code> 或 <code>v, ok := &lt;-c</code></li>
</ul>
<h3 id="思考方法一：通过“写”chan-实现"><a href="#思考方法一：通过“写”chan-实现" class="headerlink" title="思考方法一：通过“写”chan 实现"></a>思考方法一：通过“写”chan 实现</h3><p>“写”肯定不能作为判断，总不能为了判断 chan 是否 close，我尝试往里面写数据吧？这个会导致 <code>chansend</code> 里面直接 panic 的，如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">chansend</span><span class="params">(c *hchan, ep unsafe.Pointer, block <span class="keyword">bool</span>, callerpc <span class="keyword">uintptr</span>)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">        <span class="comment">//  ...</span></span><br><span class="line">        <span class="comment">// 当 channel close 之后的处理逻辑</span></span><br><span class="line">        <span class="keyword">if</span> c.closed != <span class="number">0</span> &#123;</span><br><span class="line">            unlock(&amp;c.lock)</span><br><span class="line">            <span class="built_in">panic</span>(plainError(<span class="string">"send on closed channel"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//  ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当然了，你路子要是野一点，这样做技术上也能实现，因为 panic 是可以捕捉的，只不过这也太野了吧，不推荐。</p>
<h3 id="思考方法二：通过“读”chan-实现"><a href="#思考方法二：通过“读”chan-实现" class="headerlink" title="思考方法二：通过“读”chan 实现"></a>思考方法二：通过“读”chan 实现</h3><p>“读”来判断。分析函数 <code>chanrecv</code> 可以知道，当尝试从一个已经 close 的 chan 读数据的时候，返回 （selected=true, received=false），我们通过 received = false 即可知道 channel 是否 close 。<code>chanrecv</code> 有如下代码：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">chanrecv</span><span class="params">(c *hchan, ep unsafe.Pointer, block <span class="keyword">bool</span>)</span> <span class="params">(selected, received <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 当 channel close 之后的处理逻辑</span></span><br><span class="line"> <span class="keyword">if</span> c.closed != <span class="number">0</span> &amp;&amp; c.qcount == <span class="number">0</span> &#123;</span><br><span class="line">  unlock(&amp;c.lock)</span><br><span class="line">  <span class="keyword">if</span> ep != <span class="literal">nil</span> &#123;</span><br><span class="line">   typedmemclr(c.elemtype, ep)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>, <span class="literal">false</span></span><br><span class="line"> &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>所以，我们现在知道了，可以通过 “读”的效果来判断，但是我们不能直接写成这样：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误示例</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">isChanClose</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    _, ok := &lt;- c</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面是个<strong>错误示例</strong>，因为 <code>_, ok := &lt;-c</code> 编译出来的是 <code>chanrecv2</code> ，这个函数 block 赋值 true 传入的，所以当 c 是正常的时候，这里是阻塞的，所以这个不能用来作为一个正常的函数调用，因为会卡死协程，怎么解决这个问题？用 <code>select</code>  和 <code>&lt;-chan</code>  来结合可以解决这个问题，<code>select</code> 和 <code>&lt;-chan</code> 结合起来是对应 <code>selectnbrecv</code>  和 <code>selectnbrecv2</code> 这两个函数，这两个函数是非阻塞的（ <code>block = false</code> ）。</p>
<p>正确示例：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">isChanClose</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> _, received := &lt;- ch:</span><br><span class="line">        <span class="keyword">return</span> !received</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>网上很多人举了一个 <code>isChanClose</code> 错误的例子，错误示例：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">isChanClose</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span>  &lt;- ch:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>思考下：为什么第一个例子是对的，第二个例子是错的？</p>
<p>因为，第一个例子编译出来对应的函数是 <code>selectnbrecv2</code> ，第二个例子编译出来对应的是 <code>selectnbrecv1</code> ，这两个函数的区别在于 <code>selectnbrecv2</code> 多了一个返回参数 <code>received</code>，只有这个函数才能指明是否元素出队成功，而 <code>selected</code> 只是判断是否要进到 select case 分支。我们通过 <code>received</code> 这个返回值（其实是一个入参，只不过是指针类型，函数内可修改）来反向推断 chan 是否 close 了。</p>
<h4 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h4><ol>
<li>case 的代码必须是 <code>_, received := &lt;- ch</code> 的形式，如果仅仅是 <code>&lt;- ch</code> 来判断，是错的逻辑，因为我们关注的是 <code>received</code> 的值；</li>
<li>select 必须要有 default 分支，否则会阻塞函数，我们这个函数要保证一定能正常返回；</li>
</ol>
<h3 id="chan-close-原则"><a href="#chan-close-原则" class="headerlink" title="chan close 原则"></a>chan close 原则</h3><ol>
<li>永远不要尝试在读取端关闭 channel ，写入端无法知道 channel 是否已经关闭，往已关闭的 channel 写数据会 panic ；</li>
<li>一个写入端，在这个写入端可以放心关闭 channel；</li>
<li>多个写入端时，不要在写入端关闭 channel ，其他写入端无法知道 channel 是否已经关闭，关闭已经关闭的 channel 会发生 panic （你要想个办法保证只有一个人调用 close）；</li>
<li>channel 作为函数参数的时候，最好带方向；</li>
</ol>
<p>其实这些原则只有一点：一定要是安全的是否才能去 close channel 。</p>
<h3 id="其实并不需要-isChanClose-函数"><a href="#其实并不需要-isChanClose-函数" class="headerlink" title="其实并不需要 isChanClose 函数 !!!"></a>其实并不需要 isChanClose 函数 !!!</h3><p>上面实现的 <code>isChanClose</code> 是可以判断出 channel 是否 close，但是适用场景优先，因为可能等你 <code>isChanClose</code> 判断的时候返回值 false，你以为 channel 还是正常的，但是下一刻 channel 被关闭了，这个时候往里面“写”数据就又会 panic ，如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> isChanClose( c ) &#123;</span><br><span class="line">    <span class="comment">// 关闭的场景，exit  </span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 未关闭的场景，继续执行（可能还是会 panic）</span></span><br><span class="line">c &lt;- x</span><br></pre></td></tr></table></figure></p>
<p>因为判断之后还是有时间窗，所以 <code>isChanClose</code> 的适用还是有限，那么是否有更好的办法？</p>
<p>我们换一个思路，你其实并不是一定要判断 channel 是否 close，真正的目的是：<strong>安全的使用 channel，避免使用到已经关闭的 closed channel，从而导致 panic</strong> 。</p>
<p>这个问题的本质上是保证一个事件的时序，官方推荐通过 <code>context</code> 来配合使用，我们可以通过一个 ctx 变量来指明 close 事件，而不是直接去判断 channel 的一个状态。举个栗子：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">    <span class="comment">// ... exit</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"><span class="keyword">case</span> v, ok := &lt;-c:</span><br><span class="line">    <span class="comment">// do something....</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">    <span class="comment">// do default ....</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>ctx.Done()</code> 事件发生之后，我们就明确不去读 channel 的数据。</p>
<p>或者<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">    <span class="comment">// ... exit</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">    <span class="comment">// push </span></span><br><span class="line">    c &lt;- x</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>ctx.Done()</code> 事件发生之后，我们就明确不写数据到 channel ，或者不从 channel 里读数据，那么保证这个时序即可。就一定不会有问题。</p>
<p>我们只需要确保一点：</p>
<ol>
<li>触发时序保证：一定要先触发 ctx.Done() 事件，再去做 close channel 的操作，保证这个时序的才能保证 select 判断的时候没有问题；<br> a. 只有这个时序，才能保证在获悉到 Done 事件的时候，一切还是安全的；</li>
<li>条件判断顺序：select 的 case 先判断 ctx.Done() 事件，这个很重要哦，否则很有可能先执行了 chan 的操作从而导致 panic 问题；</li>
</ol>
<h2 id="怎么优雅关闭-chan-？"><a href="#怎么优雅关闭-chan-？" class="headerlink" title="怎么优雅关闭 chan ？"></a>怎么优雅关闭 chan ？</h2><h3 id="方法一：panic-recover"><a href="#方法一：panic-recover" class="headerlink" title="方法一：panic-recover"></a>方法一：panic-recover</h3><p>关闭一个 channel 直接调用 close 即可，但是关闭一个已经关闭的 channel 会导致 panic，怎么办？panic-recover 配合使用即可。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">SafeClose</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">int</span>)</span> <span class="params">(closed <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"> <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">recover</span>() != <span class="literal">nil</span> &#123;</span><br><span class="line">   closed = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"> &#125;()</span><br><span class="line"> <span class="comment">// 如果 ch 是一个已经关闭的，会 panic 的，然后被 recover 捕捉到；</span></span><br><span class="line"> <span class="built_in">close</span>(ch)</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这并不优雅。</p>
<h3 id="方法二：sync-Once"><a href="#方法二：sync-Once" class="headerlink" title="方法二：sync.Once"></a>方法二：sync.Once</h3><p>可以使用 <code>sync.Once</code> 来确保 <code>close</code> 只执行一次。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> ChanMgr <span class="keyword">struct</span> &#123;</span><br><span class="line"> C    <span class="keyword">chan</span> <span class="keyword">int</span></span><br><span class="line"> once sync.Once</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewChanMgr</span><span class="params">()</span> *<span class="title">ChanMgr</span></span> &#123;</span><br><span class="line"> <span class="keyword">return</span> &amp;ChanMgr&#123;C: <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cm *ChanMgr)</span> <span class="title">SafeClose</span><span class="params">()</span></span> &#123;</span><br><span class="line"> cm.once.Do(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; <span class="built_in">close</span>(cm.C) &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这看着还可以。</p>
<h3 id="方法三：事件同步来解决"><a href="#方法三：事件同步来解决" class="headerlink" title="方法三：事件同步来解决"></a>方法三：事件同步来解决</h3><p>对于关闭 channel 这个我们有两个简要的原则：</p>
<ol>
<li>永远不要尝试在读端关闭 channel ；</li>
<li>永远只允许一个 goroutine（比如，只用来执行关闭操作的一个 goroutine ）执行关闭操作；</li>
</ol>
<p>可以使用 <code>sync.WaitGroup</code> 来同步这个关闭事件，遵守以上的原则，举几个例子：</p>
<h4 id="第一个例子：一个-sender"><a href="#第一个例子：一个-sender" class="headerlink" title="第一个例子：一个 sender"></a>第一个例子：一个 sender</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"sync"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"> <span class="comment">// channel 初始化</span></span><br><span class="line"> c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">10</span>)</span><br><span class="line"> <span class="comment">// 用来 recevivers 同步事件的</span></span><br><span class="line"> wg := sync.WaitGroup&#123;&#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// sender（写端）</span></span><br><span class="line"> <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="comment">// 入队</span></span><br><span class="line">  c &lt;- <span class="number">1</span></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="comment">// 满足某些情况，则 close channel</span></span><br><span class="line">  <span class="built_in">close</span>(c)</span><br><span class="line"> &#125;()</span><br><span class="line"></span><br><span class="line"> <span class="comment">// receivers （读端）</span></span><br><span class="line"> <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">  wg.Add(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">   <span class="keyword">defer</span> wg.Done()</span><br><span class="line">   <span class="comment">// ... 处理 channel 里的数据</span></span><br><span class="line">   <span class="keyword">for</span> v := <span class="keyword">range</span> c &#123;</span><br><span class="line">    _ = v</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;()</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="comment">// 等待所有的 receivers 完成；</span></span><br><span class="line"> wg.Wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里例子里面，我们在 sender 的 goroutine 关闭 channel，因为只有一个 sender，所以关闭自然是安全的。receiver 使用 <code>WaitGroup</code> 来同步事件，receiver 的 for 循环只有在 channel close 之后才会退出，主协程的 <code>wg.Wait()</code> 语句只有所有的 receivers 都完成才会返回。所以，事件的顺序是：</p>
<ol>
<li>写端入队一个整形元素</li>
<li>关闭 channel</li>
<li>所有的读端安全退出</li>
<li>主协程返回</li>
</ol>
<p>一切都是安全的</p>
<h4 id="第二个例子：多个-sender"><a href="#第二个例子：多个-sender" class="headerlink" title="第二个例子：多个 sender"></a>第二个例子：多个 sender</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"> <span class="string">"context"</span></span><br><span class="line"> <span class="string">"sync"</span></span><br><span class="line"> <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"> <span class="comment">// channel 初始化</span></span><br><span class="line"> c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">10</span>)</span><br><span class="line"> <span class="comment">// 用来 recevivers 同步事件的</span></span><br><span class="line"> wg := sync.WaitGroup&#123;&#125;</span><br><span class="line"> <span class="comment">// 上下文</span></span><br><span class="line"> ctx, cancel := context.WithCancel(context.TODO())</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 专门关闭的协程</span></span><br><span class="line"> <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">  time.Sleep(<span class="number">2</span> * time.Second)</span><br><span class="line">  cancel()</span><br><span class="line">  <span class="comment">// ... 某种条件下，关闭 channel</span></span><br><span class="line">  <span class="built_in">close</span>(c)</span><br><span class="line"> &#125;()</span><br><span class="line"></span><br><span class="line"> <span class="comment">// senders（写端）</span></span><br><span class="line"> <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">  <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context, id <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">   <span class="keyword">select</span> &#123;</span><br><span class="line">   <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">   <span class="keyword">case</span> c &lt;- id: <span class="comment">// 入队</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">   &#125;</span><br><span class="line">  &#125;(ctx, i)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// receivers（读端）</span></span><br><span class="line"> <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">  wg.Add(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">   <span class="keyword">defer</span> wg.Done()</span><br><span class="line">   <span class="comment">// ... 处理 channel 里的数据</span></span><br><span class="line">   <span class="keyword">for</span> v := <span class="keyword">range</span> c &#123;</span><br><span class="line">    _ = v</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;()</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="comment">// 等待所有的 receivers 完成；</span></span><br><span class="line"> wg.Wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个例子我们看到有多个 sender 和 receiver ，这种情况我们还是要保证一点：close(ch) 操作的只能有一个人，我们单独抽出来一个 goroutine 来做这个事情，并且使用 context 来做事件同步，事件发生顺序是：</p>
<ol>
<li>10 个写端协程（sender）运行，投递元素；</li>
<li>10 个读端协程（receiver）运行，读取元素；</li>
<li>2 分钟超时之后，单独协程执行 <code>close(channel)</code> 操作；</li>
<li>主协程返回；</li>
</ol>
<p>一切都是安全的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>channel 并没有直接提供判断是否 close 的接口，官方推荐使用 context 和 select 语法配合使用，事件通知的方式，达到优雅判断 channel 关闭的效果；</li>
<li>channel 关闭姿势也有讲究，永远不要尝试在读端关闭，永远保持一个关闭入口处，使用 sync.WaitGroup 和 context 实现事件同步，达到优雅关闭效果；</li>
</ol>
<p>作者：奇伢   来源：奇伢云存储</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go Sync.Pool 背后的想法]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-sync-pool.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>我最近在我的一个项目中遇到了垃圾回收问题。大量对象被重复分配，并导致 GC 的巨大工作量。使用 <code>sync.Pool</code>，我能够减少分配和 GC 工作负载。</p>
<h2 id="什么是-sync-Pool？"><a href="#什么是-sync-Pool？" class="headerlink" title="什么是 sync.Pool？"></a>什么是 sync.Pool？</h2><p>Go 1.3 版本的亮点之一是同步池。它是 <code>sync</code> 包下的一个组件，用于创建自我管理的临时检索对象池。</p>
<h2 id="为什么要使用-sync-Pool？"><a href="#为什么要使用-sync-Pool？" class="headerlink" title="为什么要使用 sync.Pool？"></a>为什么要使用 sync.Pool？</h2><p>我们希望尽可能减少 GC 开销。频繁的内存分配和回收会给 GC 带来沉重的负担。<code>sync.Poll</code> 可以缓存暂时不使用的对象，并在下次需要时直接使用它们（无需重新分配）。这可能会减少 GC 工作负载并提高性能。<br><a id="more"></a></p>
<h2 id="怎么使用-sync-Pool？"><a href="#怎么使用-sync-Pool？" class="headerlink" title="怎么使用 sync.Pool？"></a>怎么使用 sync.Pool？</h2><p>首先，您需要设置新函数。当池中没有缓存对象时将使用此函数。之后，您只需要使用 <code>Get</code> 和 <code>Put</code> 方法来检索和返回对象。另外，池在第一次使用后绝对不能复制。</p>
<p>由于 <code>New</code> 函数类型是 <code>func() interface{}</code>，<code>Get</code> 方法返回一个 <code>interface{}</code>。为了得到具体对象，你需要做一个类型断言。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A dummy struct</span></span><br><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span> &#123;</span><br><span class="line">	Name <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Initializing pool</span></span><br><span class="line"><span class="keyword">var</span> personPool = sync.Pool&#123;</span><br><span class="line">	<span class="comment">// New optionally specifies a function to generate</span></span><br><span class="line">	<span class="comment">// a value when Get would otherwise return nil.</span></span><br><span class="line">	New: <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123; <span class="keyword">return</span> <span class="built_in">new</span>(Person) &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Main function</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="comment">// Get hold of an instance</span></span><br><span class="line">	newPerson := personPool.Get().(*Person)</span><br><span class="line">	<span class="comment">// Defer release function</span></span><br><span class="line">	<span class="comment">// After that the same instance is </span></span><br><span class="line">	<span class="comment">// reusable by another routine</span></span><br><span class="line">	<span class="keyword">defer</span> personPool.Put(newPerson)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Using the instance</span></span><br><span class="line">	newPerson.Name = <span class="string">"Jack"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span> &#123;</span><br><span class="line">	Age <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> personPool = sync.Pool&#123;</span><br><span class="line">	New: <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123; <span class="keyword">return</span> <span class="built_in">new</span>(Person) &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkWithoutPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> p *Person</span><br><span class="line">	b.ReportAllocs()</span><br><span class="line">	b.ResetTimer()</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">		<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; <span class="number">10000</span>; j++ &#123;</span><br><span class="line">			p = <span class="built_in">new</span>(Person)</span><br><span class="line">			p.Age = <span class="number">23</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkWithPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> p *Person</span><br><span class="line">	b.ReportAllocs()</span><br><span class="line">	b.ResetTimer()</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">		<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; <span class="number">10000</span>; j++ &#123;</span><br><span class="line">			p = personPool.Get().(*Person)</span><br><span class="line">			p.Age = <span class="number">23</span></span><br><span class="line">			personPool.Put(p)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkWithoutPool</span><br><span class="line">BenchmarkWithoutPool-8   160698 ns/op   80001 B/op   10000 allocs/op</span><br><span class="line">BenchmarkWithPool</span><br><span class="line">BenchmarkWithPool-8      191163 ns/op       0 B/op       0 allocs/op</span><br></pre></td></tr></table></figure></p>
<h2 id="权衡"><a href="#权衡" class="headerlink" title="权衡"></a>权衡</h2><p>生活中的一切都是一种权衡。池也有它的性能成本。使用 <code>sync.Pool</code> 比简单的初始化要慢得多。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> p sync.Pool</span><br><span class="line">	b.RunParallel(<span class="function"><span class="keyword">func</span><span class="params">(pb *testing.PB)</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> pb.Next() &#123;</span><br><span class="line">			p.Put(<span class="number">1</span>)</span><br><span class="line">			p.Get()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkAllocation</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">	b.RunParallel(<span class="function"><span class="keyword">func</span><span class="params">(pb *testing.PB)</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> pb.Next() &#123;</span><br><span class="line">			i := <span class="number">0</span></span><br><span class="line">			i = i</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>压测结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkPool</span><br><span class="line">BenchmarkPool-8           283395016          4.40 ns/op</span><br><span class="line">BenchmarkAllocation</span><br><span class="line">BenchmarkAllocation-8    1000000000         0.344 ns/op</span><br></pre></td></tr></table></figure></p>
<h2 id="sync-Pool-是如何工作的？"><a href="#sync-Pool-是如何工作的？" class="headerlink" title="sync.Pool 是如何工作的？"></a>sync.Pool 是如何工作的？</h2><p><code>sync.Pool</code> 有两个对象容器: 本地池 (活动) 和受害者缓存 (存档)。</p>
<p>根据 <code>sync/pool.go</code> ，包 <code>init</code> 函数作为清理池的方法<a href="https://golang.org/src/sync/pool.go?s=8003:8060#L271" target="_blank" rel="external">注册到运行时</a>。此方法将由 GC 触发。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">   runtime_registerPoolCleanup(poolCleanup)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当 GC 被触发时，受害者缓存中的对象将被收集，然后本地池中的对象将被移动到受害者缓存中。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">poolCleanup</span><span class="params">()</span></span> &#123;</span><br><span class="line">   <span class="comment">// Drop victim caches from all pools.</span></span><br><span class="line">   <span class="keyword">for</span> _, p := <span class="keyword">range</span> oldPools &#123;</span><br><span class="line">      p.victim = <span class="literal">nil</span></span><br><span class="line">      p.victimSize = <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Move primary cache to victim cache.</span></span><br><span class="line">   <span class="keyword">for</span> _, p := <span class="keyword">range</span> allPools &#123;</span><br><span class="line">      p.victim = p.local</span><br><span class="line">      p.victimSize = p.localSize</span><br><span class="line">      p.local = <span class="literal">nil</span></span><br><span class="line">      p.localSize = <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   oldPools, allPools = allPools, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>新对象被放入本地池中。调用 <code>Put</code> 方法也会将对象放入本地池中。调用 <code>Get</code> 方法将首先从受害者缓存中获取对象，如果受害者缓存为空，则对象将从本地池中获取。<br><img src="/images/go/syncpool_1.gif" alt="sync.Pool localPool and victimCache"></p>
<p>供你参考，Go 1.12 sync.pool 实现使用基于 <code>mutex</code> 的锁，用于来自多个 Goroutines 的线程安全操作。Go 1.13 <a href="https://github.com/golang/go/commit/d5fd2dd6a17a816b7dfd99d4df70a85f1bf0de31#diff-491b0013c82345bf6cfa937bd78b690d" target="_blank" rel="external">引入了一个双链表</a>作为共享池，它删除了 <code>mutex</code> 并改善了共享访问。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>当有一个昂贵的对象需要频繁创建时，使用 <code>sync.Pool</code> 是非常有益的。</p>
<p>译自：<a href="https://medium.com/swlh/go-the-idea-behind-sync-pool-32da5089df72" target="_blank" rel="external">https://medium.com/swlh/go-the-idea-behind-sync-pool-32da5089df72</a></p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[我在 Go 中犯了 5 个错误]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-5-mistakes.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>人皆犯错，宽恕是德   — Alexander Pope</p>
</blockquote>
<p>这些都是我在写 Go 中犯的错误。尽管这些可能不会导致任何类型的错误，但它们可能会潜在地影响软件。</p>
<h2 id="1-内循环"><a href="#1-内循环" class="headerlink" title="1 内循环"></a>1 内循环</h2><p>有几种方法可以造成循环内部的混乱，你需要注意。<br><a id="more"></a></p>
<h3 id="1-1-使用引用循环迭代变量"><a href="#1-1-使用引用循环迭代变量" class="headerlink" title="1.1 使用引用循环迭代变量"></a>1.1 使用引用循环迭代变量</h3><p>由于效率的原因，循环迭代变量是单个变量，在每次循环迭代中采用不同的值。这可能会导致不知情的行为。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">in := []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> out []*<span class="keyword">int</span></span><br><span class="line"><span class="keyword">for</span>  _, v := <span class="keyword">range</span> in &#123;</span><br><span class="line">	out = <span class="built_in">append</span>(out, &amp;v)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">"Values:"</span>, *out[<span class="number">0</span>], *out[<span class="number">1</span>], *out[<span class="number">2</span>])</span><br><span class="line">fmt.Println(<span class="string">"Addresses:"</span>, out[<span class="number">0</span>], out[<span class="number">1</span>], out[<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p>
<p>结果将是：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Values: 3 3 3</span><br><span class="line">Addresses: 0xc000014188 0xc000014188 0xc000014188</span><br></pre></td></tr></table></figure></p>
<p>正如你所看到的，<code>out</code> 切片中的所有元素都是 3。实际上，实际上很容易解释为什么会发生这种情况：在每次迭代中，我们都会将 <code>v</code> 的地址附加到 <code>out</code> 切片中。如前所述，<code>v</code> 是在每次迭代中接受新值的单个变量。因此，正如您在输出的第二行中看到的，地址是相同的，并且所有地址都指向相同的值。</p>
<p>简单的解决方法是将循环迭代器变量复制到新变量中：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">in := []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> out []*<span class="keyword">int</span></span><br><span class="line"><span class="keyword">for</span>  _, v := <span class="keyword">range</span> in &#123;</span><br><span class="line">	v := v</span><br><span class="line">	out = <span class="built_in">append</span>(out, &amp;v)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">"Values:"</span>, *out[<span class="number">0</span>], *out[<span class="number">1</span>], *out[<span class="number">2</span>])</span><br><span class="line">fmt.Println(<span class="string">"Addresses:"</span>, out[<span class="number">0</span>], out[<span class="number">1</span>], out[<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p>
<p>新的输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Values: 1 2 3</span><br><span class="line">Addresses: 0xc0000b6010 0xc0000b6018 0xc0000b6020</span><br></pre></td></tr></table></figure></p>
<p>同样的问题可以找到正在 Goroutine 中使用的循环迭代变量。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">list := []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, v := <span class="keyword">range</span> list &#123;</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"%d "</span>, v)</span><br><span class="line">	&#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>结果将是：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 3 3</span><br></pre></td></tr></table></figure></p>
<p>它可以使用上面提到的相同的解决方案来修复。注意，如果不使用 Goroutine 运行该函数，代码将按照预期运行。</p>
<h3 id="1-2-在循环中调用-WaitGroup-Wait"><a href="#1-2-在循环中调用-WaitGroup-Wait" class="headerlink" title="1.2 在循环中调用 WaitGroup.Wait"></a>1.2 在循环中调用 WaitGroup.Wait</h3><p>使用 <code>WaitGroup</code> 类型的共享变量会犯此错误，如下面的代码所示，当第 5 行的 <code>Done()</code> 被调用 <code>len(tasks)</code> 次数时，第 7 行的 <code>Wait()</code> 只能被解除阻塞，因为它被用作参数在第 2 行调用 <code>Add()</code>。但是，<code>Wait()</code> 在循环中被调用，因此在下一个迭代中，它会阻止在第 4 行创建 Goroutine。简单的解决方案是将 <code>Wait()</code> 的调用移出循环。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">wg.Add(<span class="built_in">len</span>(tasks))</span><br><span class="line"><span class="keyword">for</span> _, t := <span class="keyword">range</span> tasks &#123;</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(t *task)</span></span> &#123; </span><br><span class="line">		<span class="keyword">defer</span> group.Done()</span><br><span class="line">	&#125;(t)</span><br><span class="line">	<span class="comment">// group.Wait()</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group.Wait()</span><br></pre></td></tr></table></figure></p>
<h3 id="1-3-在循环中使用-defer"><a href="#1-3-在循环中使用-defer" class="headerlink" title="1.3 在循环中使用 defer"></a>1.3 在循环中使用 defer</h3><p><code>defer</code> 直到函数返回才执行。除非你确定你在做什么，否则你不应该在循环中使用 <code>defer</code>。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> mutex sync.Mutex</span><br><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span> &#123;</span><br><span class="line">	Age <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line">persons := <span class="built_in">make</span>([]Person, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> _, p := <span class="keyword">range</span> persons &#123;</span><br><span class="line">	mutex.Lock()</span><br><span class="line">	<span class="comment">// defer mutex.Unlock()</span></span><br><span class="line">	p.Age = <span class="number">13</span></span><br><span class="line">	mutex.Unlock()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在上面的例子中，如果你使用第 8 行而不是第 10 行，下一次迭代就不能持有互斥锁，因为锁已经在使用中，并且循环永远阻塞。</p>
<p>如果你真的需要使用 defer 内循环，你可能想委托另一个函数来做这项工作。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> mutex sync.Mutex</span><br><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span> &#123;</span><br><span class="line">	Age <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line">persons := <span class="built_in">make</span>([]Person, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> _, p := <span class="keyword">range</span> persons &#123;</span><br><span class="line">	<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		mutex.Lock()</span><br><span class="line">		<span class="keyword">defer</span> mutex.Unlock()</span><br><span class="line">		p.Age = <span class="number">13</span></span><br><span class="line">	&#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是，有时使用 <code>defer</code> 在循环可能会变得很方便。所以你真的需要知道你在做什么。</p>
<blockquote>
<p>Go 不能容忍愚蠢者</p>
</blockquote>
<h2 id="2-发送到一个无保证的-channel"><a href="#2-发送到一个无保证的-channel" class="headerlink" title="2 发送到一个无保证的 channel"></a>2 发送到一个无保证的 channel</h2><p>您可以将值从一个 Goroutine 发送到 channels，并将这些值接收到另一个 Goroutine。默认情况下，发送和接收，直到另一方准备好。这允许 Goroutines 在没有显式锁或条件变量的情况下进行同步。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">doReq</span><span class="params">(timeout time.Duration)</span> <span class="title">obj</span></span> &#123;</span><br><span class="line">	<span class="comment">// ch :=make(chan obj)</span></span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> obj, <span class="number">1</span>)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		obj := do()</span><br><span class="line">		ch &lt;- result</span><br><span class="line">	&#125; ()</span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> result = &lt;- ch :</span><br><span class="line">		<span class="keyword">return</span> result</span><br><span class="line">	<span class="keyword">case</span>&lt;- time.After(timeout):</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span> </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>让我们检查一下上面的代码。<code>doReq</code> 函数在第 4 行创建一个子 Goroutine 来处理请求，这在Go服务程序中是一种常见的做法。子 Goroutine 执行 <code>do</code> 函数并通过第 6 行通道 <code>ch</code> 将结果发送回父节点。子进程会在第 6 行阻塞，直到父进程在第 9 行接收到 <code>ch</code> 的结果。同时，父进程将阻塞 <code>select</code>，直到子进程将结果发送给 <code>ch</code>（第9行）或发生超时（第11行）。如果超时发生在更早的时候，父函数将从第 12 行 <code>doReq</code> 方法返回，并且没有人可以再接收 <code>ch</code> 的结果，这将导致子函数永远被阻塞。解决方案是将 <code>ch</code> 从无缓冲通道更改为缓冲通道，这样即使父及退出，子 Goroutine 也始终可以发送结果。另一个修复方法是在第 6 行使用默认为空的 <code>select</code> 语句，这样如果没有 Goroutine 接收 <code>ch</code>，就会发生默认情况。尽管这种解决方案可能并不总是有效。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">select</span> &#123; </span><br><span class="line"><span class="keyword">case</span> ch &lt;- result: </span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<h2 id="3-不使用接口"><a href="#3-不使用接口" class="headerlink" title="3 不使用接口"></a>3 不使用接口</h2><p>接口可以使代码更加灵活。这是在代码中引入多态的一种方法。接口允许您请求一组行为，而不是特定类型。不使用接口可能不会导致任何错误，但它会导致代码不简单、不灵活和不具有可扩展性。</p>
<p>在众多接口中，<code>io.Reader</code> 和 <code>io.Writer</code> 可能是最受欢迎的。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Reader <span class="keyword">interface</span> &#123;</span><br><span class="line">    Read(p []<span class="keyword">byte</span>) (n <span class="keyword">int</span>, err error)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> Writer <span class="keyword">interface</span> &#123;</span><br><span class="line">    Write(p []<span class="keyword">byte</span>) (n <span class="keyword">int</span>, err error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这些接口可以非常强大。假设您要将对象写入文件中，因此您定义了一个 Save 方法：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(o *obj)</span> <span class="title">Save</span><span class="params">(file os.File)</span> <span class="title">error</span></span></span><br></pre></td></tr></table></figure></p>
<p>如果您第二天需要写入 <code>http.ResponseWriter</code> 该怎么办？您不想定义新方法。是吧？所以使用 <code>io.Writer</code>。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(o *obj)</span> <span class="title">Save</span><span class="params">(w io.Writer)</span> <span class="title">error</span></span></span><br></pre></td></tr></table></figure></p>
<p>还有一个重要的注意事项，你应该知道，总是要求你要使用的行为。在上面的例子中，请求一个<code>io.ReadWriteCloser</code> 也可以工作得很好，但当你要使用的唯一方法是 <code>Write</code> 时，这不是一个最佳实践。接口越大，抽象就越弱。</p>
<p>所以大多数时候你最好专注于行为而不是具体的类型。</p>
<h2 id="4-不好的顺序结构"><a href="#4-不好的顺序结构" class="headerlink" title="4 不好的顺序结构"></a>4 不好的顺序结构</h2><p>这个错误也不会导致任何错误，但是它会导致更多的内存使用。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> BadOrderedPerson <span class="keyword">struct</span> &#123;</span><br><span class="line">	Veteran <span class="keyword">bool</span>   <span class="comment">// 1 byte</span></span><br><span class="line">	Name    <span class="keyword">string</span> <span class="comment">// 16 byte</span></span><br><span class="line">	Age     <span class="keyword">int32</span>  <span class="comment">// 4 byte</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> OrderedPerson <span class="keyword">struct</span> &#123;</span><br><span class="line">	Name    <span class="keyword">string</span></span><br><span class="line">	Age     <span class="keyword">int32</span></span><br><span class="line">	Veteran <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>似乎两种类型的大小都相同，为 21 个字节，但结果显示出完全不同。使用 <code>GOARCH=amd64</code> 编译代码，<code>BadOrderedPerson</code> 类型分配 32 字节，而 <code>OrderedPerson</code> 类型分配 24 字节。为什么？原因是<a href="https://en.wikipedia.org/wiki/Data_structure_alignment" target="_blank" rel="external">数据结构对齐</a>。在 64 位体系结构中，内存分配 8 字节的连续数据包。需要添加的填充可以通过以下方式计算：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">padding = (align - (offset mod align)) mod align</span><br><span class="line">aligned = offset + padding</span><br><span class="line">        = offset + ((align - (offset mod align)) mod align)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> BadOrderedPerson <span class="keyword">struct</span> &#123;</span><br><span class="line">	Veteran <span class="keyword">bool</span>     <span class="comment">// 1 byte</span></span><br><span class="line">	_       [<span class="number">7</span>]<span class="keyword">byte</span>  <span class="comment">// 7 byte: padding for alignment</span></span><br><span class="line">	Name    <span class="keyword">string</span>   <span class="comment">// 16 byte</span></span><br><span class="line">	Age     <span class="keyword">int32</span>    <span class="comment">// 4 byte</span></span><br><span class="line">	_       <span class="keyword">struct</span>&#123;&#125; <span class="comment">// to prevent unkeyed literals</span></span><br><span class="line">	<span class="comment">// zero sized values, like struct&#123;&#125; and [0]byte occurring at </span></span><br><span class="line">	<span class="comment">// the end of a structure are assumed to have a size of one byte.</span></span><br><span class="line">	<span class="comment">// so padding also will be addedd here as well.</span></span><br><span class="line">	</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> OrderedPerson <span class="keyword">struct</span> &#123;</span><br><span class="line">	Name    <span class="keyword">string</span></span><br><span class="line">	Age     <span class="keyword">int32</span></span><br><span class="line">	Veteran <span class="keyword">bool</span></span><br><span class="line">	_       <span class="keyword">struct</span>&#123;&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当您有一个大的常用类型时，它可能会导致性能问题。但是不要担心，您不必手动处理所有的结构。使用 <a href="https://github.com/mdempsky/maligned" target="_blank" rel="external">maligned</a> 你可以轻松检查代码以解决此问题。</p>
<h2 id="5-在测试中没有使用-race-detector"><a href="#5-在测试中没有使用-race-detector" class="headerlink" title="5 在测试中没有使用 race detector"></a>5 在测试中没有使用 race detector</h2><p>数据竞争会导致神秘的故障，通常是在代码部署到生产环境很久之后。正因为如此，它们是并发系统中最常见也是最难调试的 bug 类型。为了帮助区分这些 bug, Go 1.1 引入了一个内置的数据竞争检测器。它可以简单地添加 <code>-race</code> 标志。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ go <span class="built_in">test</span> -race pkg    // to <span class="built_in">test</span> the package</span><br><span class="line">$ go run -race pkg.go  // to run the <span class="built_in">source</span> file</span><br><span class="line">$ go build -race       // to build the package</span><br><span class="line">$ go install -race pkg // to install the package</span><br></pre></td></tr></table></figure></p>
<p>启用 race 检测器后，编译器将记录在代码中访问内存的时间和方式，而 <code>runtime</code> 监视对共享变量的不同步访问。</p>
<p>当发现数据竞争时，竞争检测器将打印一份报告，其中包含冲突访问的堆栈跟踪。下面是一个例子：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">WARNING: DATA RACE</span><br><span class="line">Read by goroutine 185:</span><br><span class="line">  net.(*pollServer).AddFD()</span><br><span class="line">      src/net/fd_unix.go:89 +0x398</span><br><span class="line">  net.(*pollServer).WaitWrite()</span><br><span class="line">      src/net/fd_unix.go:247 +0x45</span><br><span class="line">  net.(*netFD).Write()</span><br><span class="line">      src/net/fd_unix.go:540 +0x4d4</span><br><span class="line">  net.(*conn).Write()</span><br><span class="line">      src/net/net.go:129 +0x101</span><br><span class="line">  net.func·060()</span><br><span class="line">      src/net/timeout_test.go:603 +0xaf</span><br><span class="line">Previous write by goroutine 184:</span><br><span class="line">  net.setWriteDeadline()</span><br><span class="line">      src/net/sockopt_posix.go:135 +0xdf</span><br><span class="line">  net.setDeadline()</span><br><span class="line">      src/net/sockopt_posix.go:144 +0x9c</span><br><span class="line">  net.(*conn).SetDeadline()</span><br><span class="line">      src/net/net.go:161 +0xe3</span><br><span class="line">  net.func·061()</span><br><span class="line">      src/net/timeout_test.go:616 +0x3ed</span><br><span class="line">Goroutine 185 (running) created at:</span><br><span class="line">  net.func·061()</span><br><span class="line">      src/net/timeout_test.go:609 +0x288</span><br><span class="line">Goroutine 184 (running) created at:</span><br><span class="line">  net.TestProlongTimeout()</span><br><span class="line">      src/net/timeout_test.go:618 +0x298</span><br><span class="line">  testing.tRunner()</span><br><span class="line">      src/testing/testing.go:301 +0xe8</span><br></pre></td></tr></table></figure></p>
<h2 id="6-最后一句"><a href="#6-最后一句" class="headerlink" title="6 最后一句"></a>6 最后一句</h2><p>唯一真正的错误是我们什么也没学到。</p>
<blockquote>
<p>译自：<a href="https://medium.com/swlh/5-mistakes-ive-made-in-go-75fb64b943b8" target="_blank" rel="external">https://medium.com/swlh/5-mistakes-ive-made-in-go-75fb64b943b8</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[用 Go 从头开始构建容器（第1部分：命名空间）]]></title>
      <url>http://team.jiunile.com/blog/2020/11/go-build-container-ns.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在过去几年中，容器的使用显著增加。容器的概念已经出现好几年了，但是 Docker 易于使用的命令行才从 2013 年开始在开发人员中普及容器。</p>
<p>在这个系列中，我试图演示容器是如何在下面工作的，以及我是如何开发容器的。</p>
<a id="more"></a>
<h2 id="什么是-vessel？"><a href="#什么是-vessel？" class="headerlink" title="什么是 vessel？"></a>什么是 vessel？</h2><p><a href="https://github.com/0xc0d/vessel" target="_blank" rel="external">vessel</a> 是我的一个教学目的的项目，它实现了一个小版本的 Docker 来管理容器。它既不使用 <a href="https://containerd.io/" target="_blank" rel="external">containerd</a> 也不使用 <a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runc</a>，而是使用一组 Linux 特性来创建容器。</p>
<p>vessel 既不是生产就绪的，也没有经过良好测试的软件。这只是一个简单的项目来了解更多关于容器的知识。</p>
<h2 id="让我们开始：阅读-Docker！"><a href="#让我们开始：阅读-Docker！" class="headerlink" title="让我们开始：阅读 Docker！"></a>让我们开始：阅读 Docker！</h2><p>我发现，在开始编写代码之前，先看一下 <a href="https://docs.docker.com/get-started/overview/" target="_blank" rel="external">Docker 文档</a>，了解一下容器是很有用的。</p>
<p>Docker 就其<a href="https://docs.docker.com/get-started/overview/#the-underlying-technology" target="_blank" rel="external">文档</a>而言，利用了 linux 内核的几个特性，并将它们组合成一个称为容器格式的包装器。这些特性是:</p>
<ul>
<li><strong>Namespaces</strong></li>
<li><strong>Control groups</strong></li>
<li><strong>Union file systems</strong></li>
</ul>
<p>现在让我们浏览一下上面的列表，并简要地了解一下它们是什么。</p>
<h2 id="什么是命名空间（Namespace-）"><a href="#什么是命名空间（Namespace-）" class="headerlink" title="什么是命名空间（Namespace!）?"></a>什么是命名空间（Namespace!）?</h2><p>Linux 命名空间是最现代容器实现背后的基础技术。名称空间是进程对周围运行的其他事物的感知。命名空间允许隔离一组进程中的全局系统资源。例如，网络命名空间隔离网络堆栈，这意味着该网络命名空间中的进程可以拥有自己的独立路由、防火墙规则和网络设备。</p>
<p>因此，如果没有命名空间，容器中的进程可能（例如）卸载文件系统，或在另一个容器中设置网络接口。</p>
<h3 id="哪些资源可以使用命名空间进行隔离？"><a href="#哪些资源可以使用命名空间进行隔离？" class="headerlink" title="哪些资源可以使用命名空间进行隔离？"></a>哪些资源可以使用命名空间进行隔离？</h3><p>在当前的 linux 内核 (5.9) 中，有 8 种类型的不同命名空间。每个命名空间可以隔离某个全局系统资源。</p>
<ul>
<li><strong>Cgroup</strong>: 此命名空间隔离控制组根目录。我将在第 2 部分中解释什么是 cgroups。但简而言之，cgroup 允许系统为一组进程定义资源限制。但要注意的是，“cgroup namespce” 仅控制在命名空间中哪些 cgroup 可见。命名空间无法分配资源限制。我们稍后将对此进行深入解释。</li>
<li><strong>IPC</strong>: 此命名空间隔离进程间通信机制，如 System V 和 POSIX 消息队列。理解IPC 并不难，但这篇文章不会讨论这个主题。</li>
<li><strong>Network</strong>: 此名称空间隔离路由、防火墙规则和名称空间内的一组进程可以看到的网络设备。</li>
<li><strong>Mount</strong>：此名称空间隔离每个名称空间中的挂载点列表。在单独的挂载名称空间中运行的进程可以挂载和卸载，而不会影响其他名称空间。</li>
<li><strong>PID</strong>：这个命名空间隔离进程 ID 号空间。它支持在名称空间内挂起/恢复进程之类的函数。</li>
<li><strong>Time</strong>：这个命名空间隔离了 <code>CLOCK_MONOTONIC</code> 和 <code>CLOCK_BOOTTIME</code> 系统时钟，它们影响了针对这些时钟（如系统正常运行时间）测量的 API。</li>
<li><strong>User</strong>：此名称空间隔离用户 id、组 id、根目录、密钥和功能。这允许进程在名称空间内是根，但不在命名空间外（如在主机中）。</li>
<li><strong>UTS</strong>：这个命名空间隔离主机名和域名</li>
</ul>
<h3 id="关于命名空间的重要注意事项"><a href="#关于命名空间的重要注意事项" class="headerlink" title="关于命名空间的重要注意事项"></a>关于命名空间的重要注意事项</h3><p>命名空间除了隔离之外什么也没做，这意味着，例如，加入一个新的网络名称空间不会给您提供一组隔离的网络设备，您必须自己创建它们。UTS 命名空间也是如此，它不会改变您的主机名。它所做的唯一事情就是隔离与主机名相关的系统调用。我们将在这个系列中一起做这些事情。</p>
<h3 id="命名空间生命周期"><a href="#命名空间生命周期" class="headerlink" title="命名空间生命周期"></a>命名空间生命周期</h3><p>当命名空间中的最后一个进程离开命名空间时，命名空间将自动删除。然而，有许多例外情况使名称空间在没有任何成员进程的情况下保持活动。我们将在为 vessel 创建网络名称空间时解释其中一个例外。</p>
<h3 id="命名空间的系统调用"><a href="#命名空间的系统调用" class="headerlink" title="命名空间的系统调用"></a>命名空间的系统调用</h3><p>现在我们已经简要了解了命名空间是什么，接下来看看如何与命名空间交互。在 Linux 中，有一组允许创建、加入和发现命名空间的系统调用。</p>
<ul>
<li><strong><code>clone</code></strong>：此系统调用实际上创建了一个新进程。但是借助 flags 参数，新进程将创建自己的新命名空间。</li>
<li><strong><code>setns</code></strong>：此系统调用允许正在运行的进程加入现有命名空间。</li>
<li><strong><code>unshare</code></strong>：此系统调用实际上与克隆相同，但不同之处在于此系统调用将创建当前进程并将其移动到新的命名空间，而 <code>clone</code> 将创建具有新的命名空间的新进程。</li>
</ul>
<p>额外提示：<code>fork</code> 和 <code>vfork</code> 内部系统调用只是使用不同的参数调用 <code>clone()</code>。</p>
<h3 id="命名空间-Flags"><a href="#命名空间-Flags" class="headerlink" title="命名空间 Flags"></a>命名空间 Flags</h3><p>上面提到的系统调用需要一个能够指定所需命名空间的 flag。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CLONE_NEWCGROUP Cgroup namespaces</span><br><span class="line">CLONE_NEWIPC    IPC namespaces</span><br><span class="line">CLONE_NEWNET    Network namespaces</span><br><span class="line">CLONE_NEWNS     Mount namespaces$$ </span><br><span class="line">CLONE_NEWPID    PID namespaces</span><br><span class="line">CLONE_NEWTIME   Time namespaces</span><br><span class="line">CLONE_NEWUSER   User namespaces</span><br><span class="line">CLONE_NEWUTS    UTS namespaces</span><br></pre></td></tr></table></figure></p>
<p>例如，如果你想为当前进程创建一个新的网络命名空间，你应该用 <code>CLONE_NEWNET</code> 标记调用<code>unshare</code>，如果您想使用新用户和 UTS 命名空间创建新进程，你应该用<code>CLONE_NEWUSER|CLONE_NEWUTS</code> 调用 clone。竖线表示或按位组合两个标记。</p>
<h3 id="命名空间文件"><a href="#命名空间文件" class="headerlink" title="命名空间文件"></a>命名空间文件</h3><p>在上面我提到过 <code>setns</code> 系统调用将在名称空间之间移动一个正在运行的进程。但是，如何指定要移动到哪个名称空间呢？好的，在创建名称空间之后，成员进程将具有指向命名空间文件的符号链接。</p>
<blockquote>
<p>在 Unix 中，所有内容都是文件。</p>
</blockquote>
<p>例如，在您的 shell 中，通过列出 /proc/[pid]/ns 目录下的文件，您可以看到进程命名空间。在这里你可以看到正在运行的 shell 的当前命名空间（<code>self</code> 代表当前 shell pid）:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ls <span class="_">-l</span> /proc/self/ns | cut <span class="_">-d</span> <span class="string">' '</span> <span class="_">-f</span> 10-12</span><br><span class="line">cgroup            -&gt; cgroup:[4026531835]</span><br><span class="line">ipc               -&gt; ipc:[4026531839]</span><br><span class="line">mnt               -&gt; mnt:[4026531840]</span><br><span class="line">net               -&gt; net:[4026532008]</span><br><span class="line">pid               -&gt; pid:[4026531836]</span><br><span class="line">pid_<span class="keyword">for</span>_children  -&gt; pid:[4026531836]</span><br><span class="line">time              -&gt; time:[4026531834]</span><br><span class="line">time_<span class="keyword">for</span>_children -&gt; time:[4026531834]</span><br><span class="line">user              -&gt; user:[4026531837]</span><br><span class="line">uts               -&gt; uts:[4026531838]</span><br></pre></td></tr></table></figure></p>
<p>同样使用 <code>lsns</code> 命令，您也可以看到进程命名空间列表:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lsns</span></span><br><span class="line">        NS TYPE   NPROCS   PID USER    COMMAND</span><br><span class="line">4026531834 time      244     1 root    /sbin/init</span><br><span class="line">4026531835 cgroup    244     1 root    /sbin/init</span><br><span class="line">4026531836 pid       199     1 root    /sbin/init</span><br><span class="line">4026531837 user      198     1 root    /sbin/init</span><br><span class="line">4026531838 uts       241     1 root    /sbin/init</span><br><span class="line">4026531839 ipc       244     1 root    /sbin/init</span><br><span class="line">4026531840 mnt       234     1 root    /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>实际上 <code>setns</code> syscall 所做的是更改 <code>/proc/[pid]/ns</code> 目录下文件的链接。</p>
<h2 id="废话少说，让我们编码吧！"><a href="#废话少说，让我们编码吧！" class="headerlink" title="废话少说，让我们编码吧！"></a>废话少说，让我们编码吧！</h2><p>现在我们知道我们想要的一切。是时候编写第一个在单独命名空间上运行的代码了。首先让我们看看 <code>unshare</code> 是如何工作的。下面的代码，在第 1 行使用 <code>syscall</code> 包和 <code>Unshare</code> 方法为当前运行的 Go 程序创建一个新的名称空间，然后在第 5 行将主机名设置为“container”，然后在第 9 行，它创建一个新命令并运行它。<code>Run</code> 启动命令并等待其完成。</p>
<blockquote>
<p>除用户命名空间外，创建命名空间需要 <code>CAP_SYS_ADMIN</code> 功能。因此，您需要以 root 用户来运行该程序。</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">err := syscall.Unshare(syscall.CLONE_NEWPID|syscall.CLONE_NEWUTS)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	fmt.Fprintln(os.Stderr, err)</span><br><span class="line">&#125;</span><br><span class="line">err = syscall.Sethostname([]<span class="keyword">byte</span>(<span class="string">"container"</span>))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	fmt.Fprintln(os.Stderr, err)</span><br><span class="line">&#125;</span><br><span class="line">cmd := exec.Command(<span class="string">"/bin/sh"</span>)</span><br><span class="line">cmd.Stdin = os.Stdin</span><br><span class="line">cmd.Stdout = os.Stdout</span><br><span class="line">cmd.Stderr = os.Stderr</span><br><span class="line">cmd.Run()</span><br></pre></td></tr></table></figure>
<p>让我们构建程序并进行测试。对于 host 中的第一个命令，我运行 ps 来监视正在运行的进程，然后获取主机名和当前 shell PID（例如 self，$$ 代表当前进程 PID）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ps</span><br><span class="line">    PID TTY          TIME CMD</span><br><span class="line">  27973 pts/2    00:00:00 sh</span><br><span class="line">  27984 pts/2    00:00:00 ps</span><br><span class="line">$ hostname</span><br><span class="line">host</span><br><span class="line">$ <span class="built_in">echo</span> $$</span><br><span class="line">27973</span><br></pre></td></tr></table></figure></p>
<p>现在让我们看看运行程序后会发生什么。获取主机名它返回“container”。似乎有用！<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hostname</span><br><span class="line">container</span><br></pre></td></tr></table></figure></p>
<p>让我们看看进程 ID 是什么。是的！它是 1，可行。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> $$</span><br><span class="line">1</span><br></pre></td></tr></table></figure></p>
<p>让我们运行 ps 来查看在容器内运行的进程。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ps</span><br><span class="line">    PID TTY          TIME CMD</span><br><span class="line">  27973 pts/2    00:00:00 sh</span><br><span class="line">  27998 pts/2    00:00:00 unshare</span><br><span class="line">  28003 pts/2    00:00:00 sh</span><br><span class="line">  28011 pts/2    00:00:00 ps</span><br></pre></td></tr></table></figure></p>
<p>发生什么事了!？我们可以看到带有大型 pid 的容器内的主机进程没有意义。</p>
<p>我将终止其中一个进程，看看会发生什么:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">kill</span> 27998</span><br><span class="line">sh: <span class="built_in">kill</span>: (27998) - No such process</span><br></pre></td></tr></table></figure></p>
<p>没有这样的进程，它说。精彩吗？让我解释一下。代码实际上是有效的，我们在一个新的 PID 命名空间中，我们可以看到我们的进程 ID 是 1。问题是 ps 命令。下面的 ps 使用 proc 伪文件系统列出所有正在运行的进程。为了能够拥有我们自己的 proc 文件系统，我们需要一个新的挂载名称空间，以及一个新的根路径来将 proc 挂载到其中。我们将在下一部分深入讨论。</p>
<h3 id="Clone-in-Go"><a href="#Clone-in-Go" class="headerlink" title="Clone in Go"></a>Clone in Go</h3><p>在我看来，Go 没有 clone 功能。但是，有一个名为 <a href="https://github.com/liquidgecka/goclone" target="_blank" rel="external">goclone</a> 的包，它包装了 Go 的 clone 系统调用。但是我们将要使用的解决方案是不同的。在 vessel 中，我们使用一个叫做 <code>reexec</code> 的包，它是 Docker 团队开发的。</p>
<h3 id="reexec-是什么？"><a href="#reexec-是什么？" class="headerlink" title="reexec 是什么？"></a>reexec 是什么？</h3><p>Go 允许您使用一组新的名称空间运行命令。<code>reexec</code> 背后的思想是用新的名称空间重新执行正在运行的程序本身。<code>reexec</code> 包，后台的 <code>reexec</code> 包将从调用 <code>/proc/self/exe</code> 的 Go 标准库返回 <code>*exec.Cmd</code>。该文件基本上是指向正在运行的程序可执行文件的链接。</p>
<p>现在您已经了解了 <code>reexec</code> 是如何工作的，让我们从容器中深入研究一些代码。下面的代码，是在 vessel 的早期阶段。它实际上是使用一组新名称空间运行新进程的代码。这个过程就是我们的容器。在第 1 行到第 4 行，函数创建参数和新的 reexec 命令，然后为其设置标准的输入、输出和错误。</p>
<blockquote>
<p>注意: 容器的 <code>fork</code> 子命令（第一行）是容器模式。虽然它被隐藏在使用中。</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">args := []<span class="keyword">string</span>&#123;<span class="string">"fork"</span>&#125;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">cmd := reexec.Command(args...)</span><br><span class="line">cmd.Stdin, cmd.Stdout, cmd.Stderr = os.Stdin, os.Stdout, os.Stderr</span><br><span class="line">cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123;</span><br><span class="line">	Cloneflags: syscall.CLONE_NEWUTS |</span><br><span class="line">		syscall.CLONE_NEWIPC |</span><br><span class="line">		syscall.CLONE_NEWPID |</span><br><span class="line">		syscall.CLONE_NEWNS,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Go 中的  <code>SysProcAttr</code> 命令包含操作系统特定的属性。这些属性之一是 <code>Cloneflags</code>，通过将 flags 传递到这个值，该命令将使用新的特定名称空间运行。这样，我们的新进程就有了新的 IPC、UTS、PID 和 Mount (NS) 命名空间。但是网络命名空间呢？!</p>
<h3 id="深入研究网络命名空间"><a href="#深入研究网络命名空间" class="headerlink" title="深入研究网络命名空间"></a>深入研究网络命名空间</h3><p>正如我已经提到的，命名空间只能隔离资源和容器感知的边界。因此，使用新的网络命名空间运行容器不会有太大帮助。我们也应该做一些连接容器到外部网络的事情。但这怎么可能？!</p>
<h3 id="什么是虚拟以太网设备？"><a href="#什么是虚拟以太网设备？" class="headerlink" title="什么是虚拟以太网设备？"></a>什么是虚拟以太网设备？</h3><p><code>veth</code> 可以充当网络命名空间之间的隧道。这意味着它可以在另一个命名空间中创建与网络设备的连接。<br><img src="/images/go/docker_ns_1.png" alt="figure 1: Virtual Ethernet Devices"></p>
<blockquote>
<p>虚拟以太网设备总是成对地创建，并相互连接。在一对中的一个设备上传输的所有数据将立即在另一个设备上接收。当任一设备关闭时，这对设备的链路状态也关闭。</p>
</blockquote>
<p>例如，在图 1 中，有两个 veth 对。在每对设备中，一个对等设备位于主机网络命名空间内，另一个位于容器内。主机命名空间中的设备连接到网桥，该网桥被路由到名为 <code>eth0</code> 的物理互联网连接设备。</p>
<p>现在让我们来看看 vessel 是如何创建这样一个网络的。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Container)</span> <span class="title">SetupNetwork</span><span class="params">(bridge <span class="keyword">string</span>)</span> <span class="params">(filesystem.Unmounter, error)</span></span> &#123;</span><br><span class="line">	nsMountTarget := filepath.Join(netnsPath, c.Digest)</span><br><span class="line">	vethName := fmt.Sprintf(<span class="string">"veth%.7s"</span>, c.Digest)</span><br><span class="line">	peerName := fmt.Sprintf(<span class="string">"P%s"</span>, vethName)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> err := network.SetupVirtualEthernet(vethName, peerName); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkSetMaster(vethName, bridge); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line">	unmount, err := network.MountNewNetworkNamespace(nsMountTarget)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkSetNsByFile(nsMountTarget, peerName); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Change current network namespace to setup the veth</span></span><br><span class="line">	unset, err := network.SetNetNSByFile(nsMountTarget)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">defer</span> unset()</span><br><span class="line"></span><br><span class="line">	ctrEthName := <span class="string">"eth0"</span></span><br><span class="line">	ctrEthIPAddr := c.GetIP()</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkRename(peerName, ctrEthName); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkAddAddr(ctrEthName, ctrEthIPAddr); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkSetup(ctrEthName); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkAddGateway(ctrEthName, <span class="string">"172.30.0.1"</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> err := network.LinkSetup(<span class="string">"lo"</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> unmount, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的代码涵盖了容器包的 <code>SetupNetwork</code> 方法。这个方法的职责是创建一个如图 1 所示的网络。</p>
<p>在调用此方法之前，vessel 将创建其名为 <code>vessel0</code> 的桥梁。这是实际传递给 <code>SetupNetwork</code> 网桥值的名称。</p>
<p>从现在开始，事情可能会有点混乱，但别担心。请务必多阅读几次，并遵循代码。</p>
<p>在第 3-4 行，定义了 veth 设备对名称。然后在第 6 行，将使用关联的名称创建 veth。在第 9 行，veth 将指定 <code>vessel0</code> 作为其主服务器，以便进一步通信。<br><img src="/images/go/docker_ns_2.png" alt="docker_ns_2"></p>
<p>现在是时候创建一个新的网络名称空间，并将其中一个 veth 对移动到其中。我们的容器终究会加入这个网络命名空间。然而，问题是命名空间的生命周期！如前所述，当最后一个进程成员离开名称空间时，名称空间将被删除。我也提到了一些例外。其中一个例外是绑定挂载命名空间时。这就是为什么有一个名为 <code>MountNewNetworkNamespace</code> 的函数。这个函数创建一个新的名称空间，并将其绑定到一个文件以保持其活动。下面的代码涵盖了此功能。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MountNewNetworkNamespace</span><span class="params">(nsTarget <span class="keyword">string</span>)</span> <span class="params">(filesystem.Unmounter, error)</span></span> &#123;</span><br><span class="line">	_, err := os.OpenFile(nsTarget, syscall.O_RDONLY|syscall.O_CREAT|syscall.O_EXCL, <span class="number">0644</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, errors.Wrap(err, <span class="string">"unable to create target file"</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// store current network namespace</span></span><br><span class="line">	file, err = os.OpenFile(<span class="string">"/proc/self/ns/net"</span>, os.O_RDONLY, <span class="number">0</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">defer</span> file.Close()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err := syscall.Unshare(syscall.CLONE_NEWNET); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, errors.Wrap(err, <span class="string">"unshare syscall failed"</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	mountPoint := filesystem.MountOption&#123;</span><br><span class="line">		Source: <span class="string">"/proc/self/ns/net"</span>,</span><br><span class="line">		Target: nsTarget,</span><br><span class="line">		Type:   <span class="string">"bind"</span>,</span><br><span class="line">		Flag:   syscall.MS_BIND,</span><br><span class="line">	&#125;</span><br><span class="line">	unmount, err := filesystem.Mount(mountPoint)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// reset previous network namespace</span></span><br><span class="line">	<span class="keyword">if</span> err := unix.Setns(<span class="keyword">int</span>(file.Fd()), syscall.CLONE_NEWNET); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> unmount, errors.Wrap(err, <span class="string">"setns syscall failed: "</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> unmount, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在第 2 行，函数创建一个文件。此文件将用于绑定新的网络命名空间。然后在第 9 行，函数存储了当前的命名空间链接，以便能够返回到它。现在是时候创建一个新的网络命名空间，并在第 15 行使用 <code>unshare</code> 系统调用连接它。该函数现在将 <code>/proc/self/ns/net</code> 绑定到第 2 行创建的文件。记住，<code>/proc/self/ns/net</code> 将在 <code>unshare</code> 系统调用后改变。</p>
<p>这一切都很好，我们只需要离开当前的网络命名空间，然后使用第 29 行的 <code>setns</code> 系统调用返回到我们以前的命名空间。这就是为什么函数首先存储了进程网络名称空间（第 9 行）。</p>
<p>回到 <code>SetupNetwork</code> 函数，现在让我们将对等设备移动到我们刚刚在 <code>MountNewNetworkNamespace</code> 函数中创建的命名空间。由于 <code>nsMountTarget</code> 值绑定到网络名称空间，因此它表示命名空间本身。因此，我们可以使用该文件的描述符来指定命名空间。</p>
<p>好吧，毕竟我们有一个虚拟以太网设备对，其中一个设备在主机网络命名空间内，另一个在新的命名空间内。</p>
<p>现在剩下的唯一任务是在新命名空间内配置设备。问题是设备在主机网络命名空间中不再可见，因此，我们需要使用 <code>SetNetNsByFile</code> 函数（第21行）再次加入网络命名空间。此函数仅使用给定文件的描述符调用 <code>setns</code> 系统调用。注意，我们需要 <code>defer</code> <code>unset</code> 函数（第 25 行），以将容器网络命名空间保留在函数的末尾。</p>
<p>现在，代码的其余部分（第 22-43 行）在容器网络命名空间内运行。首先要做的是将容器设备重命名为 eth0（第 29行），然后关联一个新的 IP 地址（第 32 行），设置设备（第 35 行），添加设备的网关（第 38 行），最后设置回环（127.0.0.1）网络接口。现在我们完成了这里的工作，我们的网络命名空间已经完全准备好了。</p>
<p>还要提到 172.30.0.1 是 <code>vessel0</code> 网桥的默认 IP 地址，这并不是最好的方法，因为这个 IP 地址可能已经在使用了。我这样做是为了简单。现在你的任务是让它变得更好，并发送一个 Pull 请求。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们了解到命名空间是 Linux 特性之一，它为一组进程隔离全局系统资源，因此它是大多数容器中的基本技术。我们还学习了如何在 Go 中使用 <code>unshare</code>、<code>clone</code> 和 <code>setns</code> 系统调用与命名空间进行交互。</p>
<p>它还没有完成。我们将在下一部分中讨论 union 文件系统，但是现在让我们试着阅读容器代码来理解它。</p>
<p>另外，别忘了用谷歌搜索 “Liz Rice”，看她谈论容器。</p>
<p>感谢阅读！</p>
<p>作者：Ali Josie 来源：medium.com</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 网络模型来龙去脉]]></title>
      <url>http://team.jiunile.com/blog/2020/11/k8s-network-source.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/images/k8s/network_1.jpg" alt="network"><br>容器网络发端于 <code>Docker</code> 的网络。<code>Docker</code> 使用了一个比较简单的网络模型，即内部的网桥加内部的保留 IP。这种设计的好处在于容器的网络和外部世界是解耦的，无需占用宿主机的 IP 或者宿主机的资源，完全是虚拟的。<br><a id="more"></a><br>它的设计初衷是：当需要访问外部世界时，会采用 <code>SNAT</code> 这种方法来借用 Node 的 IP 去访问外面的服务。比如容器需要对外提供服务的时候，所用的是 <code>DNAT</code> 技术，也就是在 Node 上开一个端口，然后通过 <code>iptable</code> 或者别的某些机制，把流导入到容器的进程上以达到目的。</p>
<p>该模型的问题在于，外部网络无法区分哪些是容器的网络与流量、哪些是宿主机的网络与流量。比如，如果要做一个高可用的时候，172.16.1.1 和 172.16.1.2 是拥有同样功能的两个容器，此时我们需要将两者绑成一个 Group 对外提供服务，而这个时候我们发现从外部看来两者没有相同之处，它们的 IP 都是借用宿主机的端口，因此很难将两者归拢到一起。<br><img src="/images/k8s/network_2.jpg" alt="network"></p>
<p>在此基础上，<code>Kubernetes</code> 提出了这样一种机制：即每一个 Pod，也就是一个功能聚集小团伙应有自己的“身份证”，或者说 ID。在 TCP 协议栈上，这个 ID 就是 IP。</p>
<p>这个 IP 是真正属于该 Pod 的，外部世界不管通过什么方法一定要给它。对这个 Pod IP 的访问就是真正对它的服务的访问，中间拒绝任何的变化。比如以 10.1.1.1 的 IP 去访问 10.1.2.1 的 Pod，结果到了 10.1.2.1 上发现，它实际上借用的是宿主机的 IP，而不是源 IP，这样是不被允许的。Pod 内部会要求共享这个 IP，从而解决了一些功能内聚的容器如何变成一个部署的原子的问题。</p>
<p>剩下的问题是我们的部署手段。<code>Kubernetes</code> 对怎么实现这个模型其实是没有什么限制的，用 <code>underlay</code> 网络来控制外部路由器进行导流是可以的；如果希望解耦，用 <code>overlay</code> 网络在底层网络之上再加一层叠加网，这样也是可以的。总之，只要达到模型所要求的目的即可。</p>
<h2 id="Pod-究竟如何上网"><a href="#Pod-究竟如何上网" class="headerlink" title="Pod 究竟如何上网"></a>Pod 究竟如何上网</h2><p>容器网络的网络包究竟是怎么传送的？<br><img src="/images/k8s/network_3.jpg" alt="network"></p>
<p>我们可以从以下两个维度来看：</p>
<ul>
<li>协议层次</li>
<li>网络拓扑</li>
</ul>
<h3 id="2-1-协议层次"><a href="#2-1-协议层次" class="headerlink" title="2.1 协议层次"></a>2.1 协议层次</h3><p>它和 TCP 协议栈的概念是相同的，需要从两层、三层、四层一层层地摞上去，发包的时候从右往左，即先有应用数据，然后发到了 TCP 或者 UDP 的四层协议，继续向下传送，加上 IP 头，再加上 MAC 头就可以送出去了。收包的时候则按照相反的顺序，首先剥离 MAC 的头，再剥离 IP 的头，最后通过协议号在端口找到需要接收的进程。</p>
<h3 id="2-2-网络拓扑"><a href="#2-2-网络拓扑" class="headerlink" title="2.2 网络拓扑"></a>2.2 网络拓扑</h3><p>一个容器的包所要解决的问题分为两步：</p>
<ul>
<li>第一步，如何从容器的空间 (c1) 跳到宿主机的空间 (infra)；</li>
<li>第二步，如何从宿主机空间到达远端。</li>
</ul>
<p>我个人的理解是，容器网络的方案可以通过接入、流控、通道这三个层面来考虑。</p>
<ul>
<li>第一个是接入，就是说我们的容器和宿主机之间是使用哪一种机制做连接，比如 <code>Veth + bridge</code>、<code>Veth + pair</code> 这样的经典方式，也有利用高版本内核的新机制等其他方式（如 mac/IPvlan 等），来把包送入到宿主机空间；</li>
<li>第二个是流控，就是说我的这个方案要不要支持 <code>Network Policy</code>，如果支持的话又要用何种方式去实现。这里需要注意的是，我们的实现方式一定需要在数据路径必经的一个关节点上。如果数据路径不通过该 Hook 点，那就不会起作用；</li>
<li>第三个是通道，即两个主机之间通过什么方式完成包的传输。我们有很多种方式，比如以路由的方式，具体又可分为 BGP 路由或者直接路由。还有各种各样的隧道技术等等。最终我们实现的目的就是一个容器内的包通过容器，经过接入层传到宿主机，再穿越宿主机的流控模块（如果有）到达通道送到对端。</li>
</ul>
<h3 id="2-3-一个最简单的路由方案：Flannel-host-gw"><a href="#2-3-一个最简单的路由方案：Flannel-host-gw" class="headerlink" title="2.3 一个最简单的路由方案：Flannel-host-gw"></a>2.3 一个最简单的路由方案：Flannel-host-gw</h3><p>这个方案采用的是每个 Node 独占网段，每个 Subnet 会绑定在一个 Node 上，网关也设置在本地，或者说直接设在 cni0 这个网桥的内部端口上。该方案的好处是管理简单，坏处就是无法跨 Node 迁移 Pod。就是说这个 IP、网段已经是属于这个 Node 之后就无法迁移到别的 Node 上。<br><img src="/images/k8s/network_4.jpg" alt="network"></p>
<p>这个方案的精髓在于 route 表的设置，如上图所示。接下来为大家一一解读一下。</p>
<ul>
<li>第一条很简单，我们在设置网卡的时候都会加上这一行。就是指定我的默认路由是通过哪个 IP 走掉，默认设备又是什么；</li>
<li>第二条是对 Subnet 的一个规则反馈。就是说我的这个网段是 10.244.0.0，掩码是 24 位，它的网关地址就在网桥上，也就是 10.244.0.1。这就是说这个网段的每一个包都发到这个网桥的 IP 上；</li>
<li>第三条是对对端的一个反馈。如果你的网段是 10.244.1.0（上图右边的 Subnet），我们就把它的 Host 的网卡上的 IP (10.168.0.3) 作为网关。也就是说，如果数据包是往 10.244.1.0 这个网段发的，就请以 10.168.0.3 作为网关。</li>
</ul>
<p>再来看一下这个数据包到底是如何跑起来的？</p>
<p>假设容器 (10.244.0.2) 想要发一个包给 10.244.1.3，那么它在本地产生了 TCP 或者 UDP 包之后，再依次填好对端 IP 地址、本地以太网的 MAC 地址作为源 MAC 以及对端 MAC。一般来说本地会设定一条默认路由，默认路由会把 cni0 上的 IP 作为它的默认网关，对端的 MAC 就是这个网关的 MAC 地址。然后这个包就可以发到桥上去了。如果网段在本桥上，那么通过 MAC 层的交换即可解决。</p>
<p>这个例子中我们的 IP 并不属于本网段，因此网桥会将其上送到主机的协议栈去处理。主机协议栈恰好找到了对端的 MAC 地址。使用 10.168.0.3 作为它的网关，通过本地 ARP 探查后，我们得到了 10.168.0.3 的 MAC 地址。即通过协议栈层层组装，我们达到了目的，将 Dst-MAC 填为右图主机网卡的 MAC 地址，从而将包从主机的 eth0 发到对端的 eth0 上去。</p>
<p>所以大家可以发现，这里有一个隐含的限制，上图中的 MAC 地址填好之后一定是能到达对端的，但如果这两个宿主机之间不是二层连接的，中间经过了一些网关、一些复杂的路由，那么这个 MAC 就不能直达，这种方案就是不能用的。当包到达了对端的 MAC 地址之后，发现这个包确实是给它的，但是 IP 又不是它自己的，就开始 Forward 流程，包上送到协议栈，之后再走一遍路由，刚好会发现 10.244.1.0/24 需要发到 10.244.1.1 这个网关上，从而到达了 cni0 网桥，它会找到 10.244.1.3 对应的 MAC 地址，再通过桥接机制，这个包就到达了对端容器。</p>
<p>大家可以看到，整个过程总是二层、三层，发的时候又变成二层，再做路由，就是一个大环套小环。这是一个比较简单的方案，如果中间要走隧道，则可能会有一条 <code>vxlan tunnel</code> 的设备，此时就不填直接的路由，而填成对端的隧道号。</p>
<h2 id="Service-究竟如何工作"><a href="#Service-究竟如何工作" class="headerlink" title="Service 究竟如何工作"></a>Service 究竟如何工作</h2><p>Service 其实是一种负载均衡 (Load Balance) 的机制。</p>
<p>我们认为它是一种用户侧(Client Side) 的负载均衡，也就是说 VIP 到 RIP 的转换在用户侧就已经完成了，并不需要集中式地到达某一个 NGINX 或者是一个 ELB 这样的组件来进行决策。<br><img src="/images/k8s/network_5.jpg" alt="network"></p>
<p>它的实现是这样的：首先是由一群 Pod 组成一组功能后端，再在前端上定义一个虚 IP 作为访问入口。一般来说，由于 IP 不太好记，我们还会附赠一个 DNS 的域名，Client 先访问域名得到虚 IP 之后再转成实 IP。<code>Kube-proxy</code>则是整个机制的实现核心，它隐藏了大量的复杂性。它的工作机制是通过 <code>apiserver</code> 监控 Pod/Service 的变化（比如是不是新增了 Service、Pod）并将其反馈到本地的规则或者是用户态进程。</p>
<h2 id="一个-LVS-版的-Service"><a href="#一个-LVS-版的-Service" class="headerlink" title="一个 LVS 版的 Service"></a>一个 LVS 版的 Service</h2><p>我们来实际做一个 LVS 版的 Service。LVS 是一个专门用于负载均衡的内核机制。它工作在第四层，性能会比用 <code>iptable</code> 实现好一些。</p>
<p>假设我们是一个 <code>Kube-proxy</code>，拿到了一个 Service 的配置，如下图所示：它有一个 <code>Cluster IP</code>，在该 IP 上的端口是 9376，需要反馈到容器上的是 80 端口，还有三个可工作的 Pod，它们的 IP 分别是 10.1.2.3, 10.1.14.5, 10.1.3.8。<br><img src="/images/k8s/network_6.jpg" alt="network"></p>
<p>它要做的事情就是：<br><img src="/images/k8s/network_7.png" alt="network"></p>
<ul>
<li>第 1 步，绑定 VIP 到本地（欺骗内核）；</li>
</ul>
<p>首先需要让内核相信它拥有这样的一个虚 IP，这是 LVS 的工作机制所决定的，因为它工作在第四层，并不关心 IP 转发，只有它认为这个 IP 是自己的才会拆到 TCP 或 UDP 这一层。在第一步中，我们将该 IP 设到内核中，告诉内核它确实有这么一个 IP。实现的方法有很多，我们这里用的是 ip route 直接加 local 的方式，用 Dummy 设备上加 IP 的方式也是可以的。</p>
<ul>
<li>第 2 步，为这个虚 IP 创建一个 IPVS 的 <code>virtual server</code>；</li>
</ul>
<p>告诉它我需要为这个 IP 进行负载均衡分发，后面的参数就是一些分发策略等等。<code>virtual server</code> 的 IP 其实就是我们的 <code>Cluster IP</code>。</p>
<ul>
<li>第 3 步，为这个 <code>IPVS service</code> 创建相应的 <code>real server</code>。</li>
</ul>
<p>我们需要为 <code>virtual server</code> 配置相应的 <code>real server</code>，就是真正提供服务的后端是什么。比如说我们刚才看到有三个 Pod，于是就把这三个的 IP 配到 <code>virtual server</code> 上，完全一一对应过来就可以了。<code>Kube-proxy</code> 工作跟这个也是类似的。只是它还需要去监控一些 Pod 的变化，比如 Pod 的数量变成 5 个了，那么规则就应变成 5 条。如果这里面某一个 Pod 死掉了或者被杀死了，那么就要相应地减掉一条。又或者整个 Service 被撤销了，那么这些规则就要全部删掉。所以它其实做的是一些管理层面的工作。</p>
<h2 id="啥？负载均衡还分内部外部"><a href="#啥？负载均衡还分内部外部" class="headerlink" title="啥？负载均衡还分内部外部"></a>啥？负载均衡还分内部外部</h2><p>最后我们介绍一下 Service 的类型，可以分为以下 4 类。</p>
<h3 id="5-1-ClusterIP"><a href="#5-1-ClusterIP" class="headerlink" title="5.1 ClusterIP"></a>5.1 ClusterIP</h3><p>集群内部的一个虚拟 IP，这个 IP 会绑定到一堆服务的 Group Pod 上面，这也是默认的服务方式。它的缺点是这种方式只能在 Node 内部也就是集群内部使用。</p>
<h3 id="5-2-NodePort"><a href="#5-2-NodePort" class="headerlink" title="5.2 NodePort"></a>5.2 NodePort</h3><p>供集群外部调用。将 Service 承载在 Node 的静态端口上，端口号和 Service 一一对应，那么集群外的用户就可以通过 <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code> 的方式调用到 Service。</p>
<h3 id="5-3-LoadBalancer"><a href="#5-3-LoadBalancer" class="headerlink" title="5.3 LoadBalancer"></a>5.3 LoadBalancer</h3><p>给云厂商的扩展接口。像阿里云、亚马逊这样的云厂商都是有成熟的 LB 机制的，这些机制可能是由一个很大的集群实现的，为了不浪费这种能力，云厂商可通过这个接口进行扩展。它首先会自动创建 NodePort 和 ClusterIP 这两种机制，云厂商可以选择直接将 LB 挂到这两种机制上，或者两种都不用，直接把 Pod 的 RIP 挂到云厂商的 ELB 的后端也是可以的。</p>
<h3 id="5-4-ExternalName"><a href="#5-4-ExternalName" class="headerlink" title="5.4 ExternalName"></a>5.4 ExternalName</h3><p>摈弃内部机制，依赖外部设施，比如某个用户特别强，他觉得我们提供的都没什么用，就是要自己实现，此时一个 Service 会和一个域名一一对应起来，整个负载均衡的工作都是外部实现的。</p>
<p>下图是一个实例。它灵活地应用了 ClusterIP、NodePort 等多种服务方式，又结合了云厂商的 ELB，变成了一个很灵活、极度伸缩、生产上真正可用的一套系统。<br><img src="/images/k8s/network_8.png" alt="network"></p>
<p>首先我们用 ClusterIP 来做功能 Pod 的服务入口。大家可以看到，如果有三种 Pod 的话，就有三个 <code>Service Cluster IP</code> 作为它们的服务入口。这些方式都是 Client 端的，如何在 Server 端做一些控制呢？</p>
<p>首先会起一些 Ingress 的 Pod（Ingress 是 K8s 后来新增的一种服务，本质上还是一堆同质的 Pod），然后将这些 Pod 组织起来，暴露到一个 NodePort 的 IP，K8s 的工作到此就结束了。</p>
<p>任何一个用户访问 23456 端口的 Pod 就会访问到 Ingress 的服务，它的后面有一个 Controller，会把 Service IP 和 Ingress 的后端进行管理，最后会调到 ClusterIP，再调到我们的功能 Pod。前面提到我们去对接云厂商的 ELB，我们可以让 ELB 去监听所有集群节点上的 23456 端口，只要在 23456 端口上有服务的，就认为有一个 Ingress 的实例在跑。</p>
<p>整个的流量经过外部域名的一个解析跟分流到达了云厂商的 ELB，ELB 经过负载均衡并通过 NodePort 的方式到达 Ingress，Ingress 再通过 ClusterIP 调用到后台真正的 Pod。这种系统看起来比较丰富，健壮性也比较好。任何一个环节都不存在单点的问题，任何一个环节也都有管理与反馈。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的主要内容就到此为止了，这里为大家简单总结一下：</p>
<ul>
<li>大家要从根本上理解 Kubernetes 网络模型的演化来历，理解 PerPodPerIP 的用心在哪里；</li>
<li>网络的事情万变不离其宗，按照模型从 4 层向下就是发包过程，反正层层剥离就是收包过程，容器网络也是如此；</li>
<li>Ingress 等机制是在更高的层次上（服务&lt;-&gt;端口）方便大家部署集群对外服务，通过一个真正可用的部署实例，希望大家把 <code>Ingress + Cluster IP + PodIP</code> 等概念联合来看，理解社区出台新机制、新资源对象的思考。</li>
</ul>
<p>作者：叶磊 来源：阿里巴巴云原生</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 网络插件在超过 10Gbit/s 下的基准测试结果]]></title>
      <url>http://team.jiunile.com/blog/2020/11/k8s-cni-benchmark.html</url>
      <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><blockquote>
<p>在运行 Kubernetes 1.19 和 Ubuntu 18.04 上进行基准测试</p>
</blockquote>
<ol>
<li>在我们深入讨论度量之前</li>
<li>CNI 经过 MTU 调优</li>
<li>CNI 基准:原始数据</li>
<li>CNI 加密</li>
<li>总结</li>
<li>结论-我的结论</li>
</ol>
<h2 id="1-在我们深入讨论度量标准之前…"><a href="#1-在我们深入讨论度量标准之前…" class="headerlink" title="1 在我们深入讨论度量标准之前…"></a>1 在我们深入讨论度量标准之前…</h2><h3 id="1-1-自2019年4月以来有什么新鲜事吗"><a href="#1-1-自2019年4月以来有什么新鲜事吗" class="headerlink" title="1.1 自2019年4月以来有什么新鲜事吗?"></a>1.1 自2019年4月以来有什么新鲜事吗?</h3><ul>
<li><strong>测试您自己的群集</strong>：现在您可以使用我们发布的“Kubernetes网络基准测试”工具：knb (<a href="https://github.com/InfraBuilder/k8s-bench-suite" target="_blank" rel="external">https://github.com/InfraBuilder/k8s-bench-suite</a>)在自己的集群上运行基准测试。</li>
<li>在CNI 竞争中欢迎新的挑战者:<ul>
<li>来自 VMware Tanzu 的 “<a href="https://antrea.io/docs/master/getting-started/" target="_blank" rel="external">Antrea</a>”</li>
<li>来自 alauda.io 的 “<a href="https://github.com/alauda/kube-ovn" target="_blank" rel="external">Kube-OVN</a>”</li>
</ul>
</li>
<li><strong>新场景</strong>：这个基准测试涵盖了 “Pod-to-Pod” 的网络性能，还包括一个新的 “Pod-to-Service” 场景，该方案涉及真实的测试案例。实际上，您的 API 容器将通过服务而不是容器 IP 消耗服务中的数据库（当然，我们对这两种场景也会测试 TCP 和 UDP）</li>
<li><strong>资源消耗</strong>：现在每个测试都有自己的资源比较。</li>
<li><strong>删除应用程序测试</strong>：我们不再运行 HTTP、FTP 和 SCP 测试。我们与社区和 CNI 维护者卓有成效的合作突显了 iperf TCP 结果和 curl 结果之间的差距，这是由于 CNI 启动的延迟(Pod 启动时的最初几秒钟，与实际用例无关)。</li>
<li><strong>开源</strong>：所有基准测试的源代码（脚本、cni yml 和原始结果）都可以在 github 上获得：<a href="https://github.com/icyxp/benchmark-k8s-cni-2020-08" target="_blank" rel="external">https://github.com/icyxp/benchmark-k8s-cni-2020-08</a><a id="more"></a>
</li>
</ul>
<h3 id="1-2-基准协议"><a href="#1-2-基准协议" class="headerlink" title="1.2 基准协议"></a>1.2 基准协议</h3><p>整个协议详见：<a href="https://github.com/icyxp/benchmark-k8s-cni-2020-08/blob/master/PROTOCOL.md" target="_blank" rel="external">https://github.com/icyxp/benchmark-k8s-cni-2020-08/blob/master/PROTOCOL.md</a></p>
<p>请注意，当前的文章只关注 Ubuntu 18.04 的默认内核。</p>
<h3 id="1-3-选择-CNIs-做基准测试"><a href="#1-3-选择-CNIs-做基准测试" class="headerlink" title="1.3 选择 CNIs 做基准测试"></a>1.3 选择 CNIs 做基准测试</h3><p>这个基准测试旨在比较可以用单个 yaml 文件设置的 CNIs（因此排除所有基于脚本的安装，如基于 VPP 的 CNIs 等）。</p>
<p>我们将比较的 CNIs 列表如下：</p>
<ul>
<li><code>Antrea</code> v.0.9.1</li>
<li><code>Calico</code> v3.16</li>
<li><code>Canal</code> v3.16 (Flannel network + Calico Network Policies)</li>
<li><code>Cilium</code> 1.8.2</li>
<li><code>Flannel</code> 0.12.0</li>
<li><code>Kube-router</code> latest (2020–08–25)</li>
<li><code>WeaveNet</code> 2.7.0</li>
</ul>
<h2 id="2-CNI-MTU-调优"><a href="#2-CNI-MTU-调优" class="headerlink" title="2 CNI MTU 调优"></a>2 CNI MTU 调优</h2><p>首先，我们将检查 <code>MTU detection</code> 对 TCP 性能的影响：<br><img src="/images/k8s/cni_1.png" alt="MTU impact on TCP performance"></p>
<p>UDP 的缺点更加明显：<br><img src="/images/k8s/cni_2.png" alt="MTU Impact on UDP performance"></p>
<p>考虑到这里所揭示的对性能的巨大影响，我们希望向所有 CNI 维护者传达一个希望的消息：请在CNIs 中实现 <code>MTU auto-detection</code>。你将会拯救小猫，独角兽，甚至是最可爱的一个:  devops 小家伙！</p>
<p>然而，如果您确实必须选择一个没有实现 <code>auto-MTU</code> 的 CNI，那么您需要自己对它进行调优以保持性能。请注意，这适用于 <code>Calico</code>，<code>Canal</code> 和 <code>Weavenet</code>。<br><img src="/images/k8s/cni_3.png" alt="My little message to CNI maintainers …."></p>
<h2 id="3-CNI基准：原始数据"><a href="#3-CNI基准：原始数据" class="headerlink" title="3 CNI基准：原始数据"></a>3 CNI基准：原始数据</h2><p>在本节中，我们将比较 CNI 和正确的 MTU（<code>auto-detected</code> 或手动调优）。这里的主要目标是在图表中显示原始数据。</p>
<p>颜色代码：</p>
<ul>
<li>灰色 : 参考 (裸机)</li>
<li>绿色 : 带宽 &gt; 9500 Mbit/s</li>
<li>黄色 : 带宽 &gt; 9000 Mbit/s</li>
<li>橙色 : 带宽 &gt; 8000 Mbit/s</li>
<li>红色 : 带宽 &lt; 8000 Mbit/s</li>
<li>蓝色 : 中性 (与带宽无关)</li>
</ul>
<h3 id="3-1-Idle"><a href="#3-1-Idle" class="headerlink" title="3.1 Idle"></a>3.1 Idle</h3><p>首先要建立 CNI 消耗，当整个集群….像是在休眠？<br><img src="/images/k8s/cni_4.png" alt="Idle resource consumption"></p>
<h3 id="3-2-Pod-to-Pod"><a href="#3-2-Pod-to-Pod" class="headerlink" title="3.2 Pod-to-Pod"></a>3.2 Pod-to-Pod</h3><p>在这个场景中，客户端 Pod 直接连接到服务器 Pod 的 IP 地址。<br><img src="/images/k8s/cni_5.png" alt="Pod-to-Pod scenario"></p>
<h4 id="3-2-1-TCP"><a href="#3-2-1-TCP" class="headerlink" title="3.2.1 TCP"></a>3.2.1 TCP</h4><p>“<strong>Pod-to-Pod</strong>” <strong>TCP</strong> 相关资源消耗结果如下:<br><img src="/images/k8s/cni_6.png" alt="pod-to-pod tcp"><br><img src="/images/k8s/cni_7.png" alt="pod-to-pod tcp"></p>
<h4 id="3-2-2-UDP"><a href="#3-2-2-UDP" class="headerlink" title="3.2.2 UDP"></a>3.2.2 UDP</h4><p>“<strong>Pod-to-Pod</strong>” <strong>UDP</strong> 相关资源消耗结果如下:<br><img src="/images/k8s/cni_8.png" alt="pod-to-pod udp"><br><img src="/images/k8s/cni_9.png" alt="pod-to-pod udp"></p>
<h3 id="3-3-Pod-to-Service"><a href="#3-3-Pod-to-Service" class="headerlink" title="3.3 Pod-to-Service"></a>3.3 Pod-to-Service</h3><p>在本节中，客户端 Pod 通过 <code>ClusterIP Service</code> 连接到服务端 Pod。这与实际的用例更相关。<br><img src="/images/k8s/cni_10.png" alt="Pod-to-Service scenario"></p>
<h4 id="3-3-1-TCP"><a href="#3-3-1-TCP" class="headerlink" title="3.3.1 TCP"></a>3.3.1 TCP</h4><p>“<strong>Pod-to-Service</strong>” <strong>TCP</strong> 相关资源消耗结果如下:<br><img src="/images/k8s/cni_11.png" alt="pod-to-service tcp"><br><img src="/images/k8s/cni_12.png" alt="pod-to-service tcp"></p>
<h4 id="3-3-2-UDP"><a href="#3-3-2-UDP" class="headerlink" title="3.3.2 UDP"></a>3.3.2 UDP</h4><p>“<strong>Pod-to-Service</strong>” <strong>UDP</strong> 相关资源消耗结果如下:<br><img src="/images/k8s/cni_13.png" alt="pod-to-service udp"><br><img src="/images/k8s/cni_14.png" alt="pod-to-service udp"></p>
<h3 id="3-4-网络策略"><a href="#3-4-网络策略" class="headerlink" title="3.4 网络策略"></a>3.4 网络策略</h3><p>在此基准测试中所列出的所有 CNIs 中，唯一不完全支持网络策略的是 Flannel。所有其他的都正确地实现了网络策略，包括入口和出口。做得好!</p>
<h2 id="4-CNI-加密"><a href="#4-CNI-加密" class="headerlink" title="4 CNI 加密"></a>4 CNI 加密</h2><p>在我们测试的所有 CNIs 中，以下是能够加密 pod 间通信：</p>
<ul>
<li><code>Antrea</code> with IPsec</li>
<li><code>Calico</code> with wireguard</li>
<li><code>Cilium</code> with IPsec</li>
<li><code>WeaveNet</code> with IPsec</li>
</ul>
<h3 id="4-1-带宽"><a href="#4-1-带宽" class="headerlink" title="4.1 带宽"></a>4.1 带宽</h3><p>由于在这场对比中 CNIs 较少，因此让我们在一张图中概况所有场景：<br><img src="/images/k8s/cni_15.png" alt="encryption"></p>
<h3 id="4-2-资源消耗"><a href="#4-2-资源消耗" class="headerlink" title="4.2 资源消耗"></a>4.2 资源消耗</h3><p>在本节中，我们将研究 Pod-to-Pod 通信所使用的资源，包括 TCP 和 UDP。在这里显示 Pod-to-Service 的图没有意义，因为它没有提供更多信息。<br><img src="/images/k8s/cni_16.png" alt="encryption-tcp"><br><img src="/images/k8s/cni_17.png" alt="encryption-udp"></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><p>让我们尝试回顾一下这些图表。我们在这里引入了一点主观性，用修饰符替换了实际值，如“非常快”、“低”等。<br><img src="/images/k8s/cni_18.png" alt="Benchmark result summary (August 2020)"></p>
<h2 id="6-结论-我的结论"><a href="#6-结论-我的结论" class="headerlink" title="6 结论 - 我的结论"></a>6 结论 - 我的结论</h2><p>最后一部分是主观的，表达了我自己对结果的理解。</p>
<p>我很高兴看到 CNI 的新成员加入进来。<code>Antrea</code> 在游戏中表现得很好，它提供了许多特性，甚至在早期版本中也有：<code>auto-mtu</code>、加密选项和直接安装。</p>
<p>考虑到性能，除了 <code>Kube-OVN</code> 和 <code>Kube-Router</code> 之外，所有 CNIs 都表现的很好。关于 <code>Kube-Router</code>，它无法检测到 MTU，而且我在文档中找不到调优它的方法（<a href="https://github.com/cloudnativelabs/kube-router/issues/165" target="_blank" rel="external">这里</a>有一个关于 MTU 配置的 open issue）。</p>
<p>在资源消耗方面，<code>Cilium</code> 仍然比竞争对手使用更多的 RAM，但该公司公开目标是大规模集群，这与在这个 3 节点基准的情况不完全一样。<code>Kube-OVN</code> 也是 RAM 和 cpu 密集型的，它仍然是一个相当年轻的 CNI，它依赖于 <code>Open vSwitch</code> (<code>Antrea</code> 也是，但 <code>Antrea</code> 更轻，性能更好)。</p>
<p>网络策略由所有测试的 CNIs 实现，<code>Flannel</code> 除外。<code>Flannel</code> 很可能永远不会（永远）实现它，因为他们的目的很明确：越轻越好。</p>
<p>此外，加密性能在这里是真正的 “哇效果”。<code>Calico</code> 是最古老的 CNIs 之一，但他们直到几周前才提供加密服务。他们更喜欢 <code>wireguard</code> 而不是 <code>IPsec</code>，而且至少可以说，它在这一领域的表现是伟大的，惊人的，完全优秀的对于其他 CNIs。当然，由于加密负载，它会消耗大量 CPU，但它们所实现的带宽是完全值得的（记住，<code>Calico encrypted perf</code> 大约比<code>Cilium</code> 好 6 倍，后者排名第二）。此外，在集群上部署 <code>Calico</code> 之后，您还可以随时激活 <code>wireguard</code> 加密，您也可以暂时禁用它，或者永远禁用它。这是难以置信的对于用户友好度。但是！我们提醒您，<code>Calico</code> 目前还不能 <code>auto-detect MTU</code>（该特性计划很快发布），因此，如果您的网络支持巨型帧（MTU 9000），请不要忘记调整 MTU。</p>
<p>此外，请注意，<code>Cilium</code> 能够加密整个节点到节点的通信（不仅仅是 pod 通信），这对于面向公共的集群节点来说可能是一个非常有吸引力的特性。</p>
<p>总结一下，这是我对以下用例的建议：</p>
<ul>
<li><strong>我需要一个 CNI 来容纳额外的小型节点集群，或者我不关心安全性</strong></li>
</ul>
<p>选用 <code>Flannel</code>，这是最轻最稳定的 CNI。<br>(它也是最古老的之一。根据传说，它是由 Homo-Kubernautus 或 Homo-Containorus 发明的。您可能会对精彩的 <a href="https://k3s.io/" target="_blank" rel="external">k3s</a> 项目感兴趣！点击这里查看详情！</p>
<ul>
<li><strong>我的标准群集需要一个 CNI</strong></li>
</ul>
<p>好的，<code>Calico</code> 是你的选择，如果有必要的话，不要忘记调整 MTU。您可以使用网络策略，轻松启用/禁用加密，等等。</p>
<ul>
<li><strong>我的（非常）大型集群需要一个 CNI</strong></li>
</ul>
<p>好吧，该基准测试无法反映大型集群的行为。我很乐意为此工作，但是我们没有数百台具有 10Gbit/s 连接性的服务器。因此，最好的选择是至少使用 <code>Calico</code> 和 <code>Cilium</code> 在您的节点上运行自定义的基准测试。</p>
<p>感谢阅读！</p>
<p>作者：Alexis Ducastel 来源：medium.com</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Nginx proxy_pass 配置域名引发的故障]]></title>
      <url>http://team.jiunile.com/blog/2020/11/nginx-proxy-pass-domain.html</url>
      <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><p>业务场景：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户 ----&gt; waf ----&gt; 后端服务</span><br></pre></td></tr></table></figure></p>
<p><code>waf</code> 是采用 Nginx 做的二次开发，做了一些安全验证后将请求转发到后端服务，通过 nginx <code>proxy_pass</code> 转发。 <code>proxy_pass</code> 后面直接配置的是域名（如：xxxxx-1760550967.cn-northwest-1.elb.amazonaws.com.cn ）</p>
<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>有部分用户开始反馈访问站点出错 <code>504 Gateway Time-out</code>, 通过监控查到有部分请求打了一个下线的 IP 上。这里简单简述下故障场景：使用nginx做反向代理，将请求发送到一个域名(例如: <code>proxy_pass http://www.test.com</code> 该域名对应的 IP 是 A) ，刚开始运行一切正常，但是当运行了一段时间以后，域名对应的 IP 变了(例如 <a href="http://www.test.com" target="_blank" rel="external">http://www.test.com</a> 对应的 IP 由 A 变为 B)，nginx 的转发仍然还在向原先的 IP 发送请求，导致业务中断，此时<code>reload nginx</code> 后才会重新恢复正常，且日志显示数据转发到新的 IP B。<br><a id="more"></a></p>
<h2 id="故障分析"><a href="#故障分析" class="headerlink" title="故障分析"></a>故障分析</h2><blockquote>
<p>此处只针对 nginx 向后端做代理，且后端代理为域名形式的这种情况做分析</p>
</blockquote>
<ul>
<li>1、正常情况下启动 nginx 后(或者 -t / reload nginx 时)，nginx 会通过操作系统配置的 DNS 服务器去解析域名对应的 IP</li>
<li>2、当 nginx 配置文件中的所有涉及到的域名都可以被正常解析到以后，才能启动(或者检查/重新加载)通过</li>
<li>3、<strong>这里需要提醒一点，在 <code>nginx -t</code> 或者 <code>nginx -s reload</code> 只是检查域名是否可以解析通过，并不会在此时缓存域名对应 IP，只有在通过 nginx 第一次向 <code>proxy_pass</code> 后端对应的域名做代理数据转发时，这里 nginx 会通过操作系统配置的 DNS 服务器解析域名，此时才会缓存域名对应的 IP，且会缓存很长时间，甚至一个月(整个过程均有生产实例证明，且抓包验证)</strong></li>
</ul>
<h2 id="如何解决？"><a href="#如何解决？" class="headerlink" title="如何解决？"></a>如何解决？</h2><p>1、既然是因为 nginx 缓存域名对应 IP 的 DNS 记录造成的，那么怎么才能解决呢，方法有两种：</p>
<ul>
<li>(1)、手动 reload nginx，让 nginx 重新解析域名，这个时候解析到域名对应的 IP 是最新的，不会包含已经被废弃的 IP</li>
<li>(2)、设置 nginx 的 DNS 缓存时间，比如 600s 失效，然后重新去解析</li>
</ul>
<p>2、方法(2)当然是最好的，但是 nginx 的 DNS 缓存时间在哪里设置呢，我没有找到！</p>
<p>3、但是我找到另外一种方法 – nginx 的 <code>resolver</code></p>
<h3 id="nginx-的-resolver-解决方案"><a href="#nginx-的-resolver-解决方案" class="headerlink" title="nginx 的 resolver 解决方案"></a>nginx 的 resolver 解决方案</h3><p>1、默认 nginx 会通过操作系统设置的 DNS 服务器（/etc/resolv.conf）去解析域名</p>
<p>2、其实 nginx 还可以通过自身设置 DNS 服务器，而不用去找操作系统的 DNS</p>
<p>3、下面来讲一个这个 <code>resolver</code></p>
<p>示例配置如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">       listen      8080;</span><br><span class="line">       server_name localhost;</span><br><span class="line">       resolver 114.114.114.114 223.5.5.5 valid=3600s;</span><br><span class="line">       resolver_timeout 3s;</span><br><span class="line">       set $qq &quot;www.qq.com&quot;;</span><br><span class="line">       location / &#123;</span><br><span class="line">          proxy_pass http://$qq;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p>
<p>参数说明：</p>
<ul>
<li><code>resolver</code> 可以在 http 全局设定，也可在 server 里面设定</li>
<li><code>resolver</code> 后面指定 DNS 服务器，可以指定多个，空格隔开</li>
<li><code>valid</code> 设置 DNS 缓存失效时间，自己根据情况判断，建议 600 以上</li>
<li><code>resolver_timeout</code> 指定解析域名时，DNS 服务器的超时时间，建议 3 秒左右</li>
</ul>
<p><strong>注意：</strong>当 <code>resolver</code> 后面跟多个 DNS 服务器时，一定要保证这些 DNS 服务器都是有效的，因为这种是负载均衡模式的，当 DNS 记录失效了（超过 valid 时间），首先由第一个  DNS 服务器（114.114.114.114）去解析，下一次继续失效时由第二个 DNS 服务器（223.5.5.5）去解析，亲自测试的，如有任何一个 DNS 服务器是坏的，那么这一次的解析会一直持续到 <code>resolver_timeout</code> ，然后解析失败，且日志报错解析不了域名，通过页面抛出502错误。</p>
<p><strong>重点：</strong>如上例，在代理到后端域名 <a href="http://www.qq.com" target="_blank" rel="external">http://www.qq.com</a> 时，千万不要直接写在 <code>proxy_pass</code> 中，因为 server 中使用了 <code>resolver</code>，所以必须先把域名定义到一个变量里面，然后在 <code>proxy_pass http://$变量名</code>，否则 nginx 语法检测一直会报错，提示解析不了域名。</p>
<h2 id="延展阅读"><a href="#延展阅读" class="headerlink" title="延展阅读"></a>延展阅读</h2><h3 id="这里列举几个-proxy-pass、upstream-与-reslover-的应用场景"><a href="#这里列举几个-proxy-pass、upstream-与-reslover-的应用场景" class="headerlink" title="这里列举几个 proxy_pass、upstream 与 reslover 的应用场景"></a>这里列举几个 <code>proxy_pass</code>、<code>upstream</code> 与 <code>reslover</code> 的应用场景</h3><h4 id="1-proxy-pass-upstream"><a href="#1-proxy-pass-upstream" class="headerlink" title="1. proxy_pass + upstream"></a>1. proxy_pass + upstream</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">upstream foo.example.com &#123;</span><br><span class="line">    server 127.0.0.1:8001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    location /foo &#123;</span><br><span class="line">        proxy_pass http://foo.example.com;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://localhost/foo，proxy" target="_blank" rel="external">http://localhost/foo，proxy</a> 模块会将请求转发到 127.0.0.1 的 8001 端口上。</p>
<h4 id="2-只有-proxy-pass，没有-upstream-与-resolver"><a href="#2-只有-proxy-pass，没有-upstream-与-resolver" class="headerlink" title="2. 只有 proxy_pass，没有 upstream 与 resolver"></a>2. 只有 proxy_pass，没有 upstream 与 resolver</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    location /foo &#123;</span><br><span class="line">        proxy_pass http://foo.example.com;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际上是隐式创建了 <code>upstream</code>，<code>upstream</code> 名字就是 foo.example.com。<code>upstream</code> 模块利用本机设置的 DNS 服务器（或/etc/hosts），将 foo.example.com 解析成 IP，访问 <a href="http://localhost/foo，`proxy`" target="_blank" rel="external">http://localhost/foo，`proxy`</a> 模块会将请求转发到解析后的 IP 上。</p>
<p>如果本机未设置 DNS 服务器，或者 DNS 服务器无法解析域名，则 nginx 启动时会报类似如下错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx: [emerg] host not found <span class="keyword">in</span> upstream <span class="string">"foo.example.com"</span> <span class="keyword">in</span> /path/nginx/conf/nginx.conf:110</span><br></pre></td></tr></table></figure></p>
<h4 id="3-proxy-pass-resolver（变量设置域名）"><a href="#3-proxy-pass-resolver（变量设置域名）" class="headerlink" title="3. proxy_pass + resolver（变量设置域名）"></a>3. proxy_pass + resolver（变量设置域名）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    resolver 114.114.114.114;</span><br><span class="line">    location /foo &#123;</span><br><span class="line">        set $foo foo.example.com;</span><br><span class="line">        proxy_pass http://$foo;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://localhost/foo，nginx" target="_blank" rel="external">http://localhost/foo，nginx</a> 会动态利用 <code>resolver</code> 设置的 DNS 服务器（本机设置的 DNS 服务器或 /etc/hosts 无效），将域名解析成 IP，<code>proxy</code> 模块会将请求转发到解析后的 IP 上。</p>
<h4 id="4-proxy-pass-upstream（显式）-resolver（变量设置域名）"><a href="#4-proxy-pass-upstream（显式）-resolver（变量设置域名）" class="headerlink" title="4. proxy_pass + upstream（显式） + resolver（变量设置域名）"></a>4. proxy_pass + upstream（显式） + resolver（变量设置域名）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">upstream foo.example.com &#123;</span><br><span class="line">    server 127.0.0.1:8001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    resolver 114.114.114.114;</span><br><span class="line">    location /foo &#123;</span><br><span class="line">        set $foo foo.example.com;</span><br><span class="line">        proxy_pass http://$foo;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://localhost/foo" target="_blank" rel="external">http://localhost/foo</a> 时，<code>upstream</code> 模块会优先查找是否有定义 <code>upstream</code> 后端服务器，如果有定义则直接利用，不再走 DNS 解析。所以 <code>proxy</code> 模块会将请求转发到127.0.0.1 的 8001 端口上。</p>
<h4 id="5-proxy-pass-upstream（隐式）-resolver（变量设置域名）"><a href="#5-proxy-pass-upstream（隐式）-resolver（变量设置域名）" class="headerlink" title="5. proxy_pass + upstream（隐式） + resolver（变量设置域名）"></a>5. proxy_pass + upstream（隐式） + resolver（变量设置域名）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    resolver 114.114.114.114;</span><br><span class="line">    location /foo &#123;</span><br><span class="line">        set $foo foo.example.com;</span><br><span class="line">        proxy_pass http://$foo;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /foo2 &#123;</span><br><span class="line">        proxy_pass http://foo.example.com;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>location /foo2 实际上是隐式定义了 <code>upstream foo.example.com</code>，并由本地 DNS 服务器进行了域名解析，访问 <a href="http://localhost/foo" target="_blank" rel="external">http://localhost/foo</a> 时，<code>upstream</code> 模块会优先查找 <code>upstream</code>，即隐式定义的 foo.example.com，<code>proxy</code> 模块会将请求转发到解析后的 IP 上。</p>
<h4 id="6-proxy-pass-resolver（不用变量设置域名）"><a href="#6-proxy-pass-resolver（不用变量设置域名）" class="headerlink" title="6. proxy_pass + resolver（不用变量设置域名）"></a>6. proxy_pass + resolver（不用变量设置域名）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    resolver 114.114.114.114;</span><br><span class="line">    location /foo &#123;</span><br><span class="line">        proxy_pass http://foo.example.com;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不使用变量设置域名，则 <code>resolver</code> 的设置不起作用，此时相当于场景 2，只有 <code>proxy_pass</code> 的场景。</p>
<h4 id="7-proxy-pass-upstream-resolver（不用变量设置域名）"><a href="#7-proxy-pass-upstream-resolver（不用变量设置域名）" class="headerlink" title="7. proxy_pass + upstream + resolver（不用变量设置域名）"></a>7. proxy_pass + upstream + resolver（不用变量设置域名）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">upstream foo.example.com &#123;</span><br><span class="line">    server 127.0.0.1:8001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    resolver 114.114.114.114;</span><br><span class="line">    location /foo &#123;</span><br><span class="line">        proxy_pass http://foo.example.com;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不使用变量设置域名，则 <code>resolver</code> 的设置不起作用，此时相当于场景 1 <code>proxy_pass + upstream</code>。</p>
<h4 id="8-proxy-pass-直接指定-IP-加端口号"><a href="#8-proxy-pass-直接指定-IP-加端口号" class="headerlink" title="8. proxy_pass 直接指定 IP 加端口号"></a>8. proxy_pass 直接指定 IP 加端口号</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    location /foo &#123;</span><br><span class="line">        proxy_pass http://127.0.0.1:8001/;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际上是隐式创建了 <code>upstream</code>，<code>proxy_pass</code> 会将请求转发到 127.0.0.1 的 8001 端口上。</p>
<h3 id="主要代码"><a href="#主要代码" class="headerlink" title="主要代码"></a>主要代码</h3><p>解析 proxy_pass 指令的代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">char</span> *</span><br><span class="line"><span class="title">ngx_http_proxy_pass</span><span class="params">(<span class="keyword">ngx_conf_t</span> *cf, <span class="keyword">ngx_command_t</span> *cmd, <span class="keyword">void</span> *conf)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">ngx_http_proxy_loc_conf_t</span> *plcf = conf;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">size_t</span>                      add;</span><br><span class="line">    u_short                     port;</span><br><span class="line">    <span class="keyword">ngx_str_t</span>                  *value, *url;</span><br><span class="line">    <span class="keyword">ngx_url_t</span>                   u;</span><br><span class="line">    <span class="keyword">ngx_uint_t</span>                  n;</span><br><span class="line">    <span class="keyword">ngx_http_core_loc_conf_t</span>   *clcf;</span><br><span class="line">    <span class="keyword">ngx_http_script_compile_t</span>   sc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (plcf-&gt;upstream.upstream || plcf-&gt;proxy_lengths) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"is duplicate"</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);</span><br><span class="line"></span><br><span class="line">    clcf-&gt;handler = ngx_http_proxy_handler;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (clcf-&gt;name.data[clcf-&gt;name.len - <span class="number">1</span>] == <span class="string">'/'</span>) &#123;</span><br><span class="line">        clcf-&gt;auto_redirect = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    value = cf-&gt;args-&gt;elts;</span><br><span class="line"></span><br><span class="line">    url = &amp;value[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 查找指令中$符号的位置，判断是否使用了变量 */</span></span><br><span class="line">    n = ngx_http_script_variables_count(url);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n) &#123;</span><br><span class="line">        <span class="comment">/* 使用变量设置域名 */</span></span><br><span class="line">        ngx_memzero(&amp;sc, <span class="keyword">sizeof</span>(<span class="keyword">ngx_http_script_compile_t</span>));</span><br><span class="line"></span><br><span class="line">        sc.cf = cf;</span><br><span class="line">        sc.source = url;</span><br><span class="line">        sc.lengths = &amp;plcf-&gt;proxy_lengths;</span><br><span class="line">        sc.values = &amp;plcf-&gt;proxy_values;</span><br><span class="line">        sc.variables = n;</span><br><span class="line">        sc.complete_lengths = <span class="number">1</span>;</span><br><span class="line">        sc.complete_values = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (ngx_http_script_compile(&amp;sc) != NGX_OK) &#123;</span><br><span class="line">            <span class="keyword">return</span> NGX_CONF_ERROR;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> (NGX_HTTP_SSL)</span></span><br><span class="line">        plcf-&gt;ssl = <span class="number">1</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> NGX_CONF_OK;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ngx_strncasecmp(url-&gt;data, (u_char *) <span class="string">"http://"</span>, <span class="number">7</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">        add = <span class="number">7</span>;</span><br><span class="line">        port = <span class="number">80</span>;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ngx_strncasecmp(url-&gt;data, (u_char *) <span class="string">"https://"</span>, <span class="number">8</span>) == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">#<span class="keyword">if</span> (NGX_HTTP_SSL)</span><br><span class="line">        plcf-&gt;ssl = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        add = <span class="number">8</span>;</span><br><span class="line">        port = <span class="number">443</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">        ngx_conf_log_error(NGX_LOG_EMERG, cf, <span class="number">0</span>,</span><br><span class="line">                           <span class="string">"https protocol requires SSL support"</span>);</span><br><span class="line">        <span class="keyword">return</span> NGX_CONF_ERROR;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ngx_conf_log_error(NGX_LOG_EMERG, cf, <span class="number">0</span>, <span class="string">"invalid URL prefix"</span>);</span><br><span class="line">        <span class="keyword">return</span> NGX_CONF_ERROR;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ngx_memzero(&amp;u, <span class="keyword">sizeof</span>(<span class="keyword">ngx_url_t</span>));</span><br><span class="line"></span><br><span class="line">    u.url.len = url-&gt;len - add;</span><br><span class="line">    u.url.data = url-&gt;data + add;</span><br><span class="line">    u.default_port = port;</span><br><span class="line">    u.uri_part = <span class="number">1</span>;</span><br><span class="line">    u.no_resolve = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    plcf-&gt;upstream.upstream = ngx_http_upstream_add(cf, &amp;u, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (plcf-&gt;upstream.upstream == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> NGX_CONF_ERROR;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    plcf-&gt;vars.schema.len = add;</span><br><span class="line">    plcf-&gt;vars.schema.data = url-&gt;data;</span><br><span class="line">    plcf-&gt;vars.key_start = plcf-&gt;vars.schema;</span><br><span class="line"></span><br><span class="line">    ngx_http_proxy_set_vars(&amp;u, &amp;plcf-&gt;vars);</span><br><span class="line"></span><br><span class="line">    plcf-&gt;location = clcf-&gt;name;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (clcf-&gt;named</span><br><span class="line">#<span class="keyword">if</span> (NGX_PCRE)</span><br><span class="line">        || clcf-&gt;regex</span><br><span class="line">#endif</span><br><span class="line">        || clcf-&gt;noname)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (plcf-&gt;vars.uri.len) &#123;</span><br><span class="line">            ngx_conf_log_error(NGX_LOG_EMERG, cf, <span class="number">0</span>,</span><br><span class="line">                               <span class="string">"\"proxy_pass\" cannot have URI part in "</span></span><br><span class="line">                               <span class="string">"location given by regular expression, "</span></span><br><span class="line">                               <span class="string">"or inside named location, "</span></span><br><span class="line">                               <span class="string">"or inside \"if\" statement, "</span></span><br><span class="line">                               <span class="string">"or inside \"limit_except\" block"</span>);</span><br><span class="line">            <span class="keyword">return</span> NGX_CONF_ERROR;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        plcf-&gt;location.len = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    plcf-&gt;url = *url;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> NGX_CONF_OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>upstream 模块初始化请求时的逻辑：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line"><span class="title">ngx_http_upstream_init_request</span><span class="params">(<span class="keyword">ngx_http_request_t</span> *r)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">ngx_str_t</span>                      *host;</span><br><span class="line">    <span class="keyword">ngx_uint_t</span>                      i;</span><br><span class="line">    <span class="keyword">ngx_resolver_ctx_t</span>             *ctx, temp;</span><br><span class="line">    <span class="keyword">ngx_http_cleanup_t</span>             *cln;</span><br><span class="line">    <span class="keyword">ngx_http_upstream_t</span>            *u;</span><br><span class="line">    <span class="keyword">ngx_http_core_loc_conf_t</span>       *clcf;</span><br><span class="line">    <span class="keyword">ngx_http_upstream_srv_conf_t</span>   *uscf, **uscfp;</span><br><span class="line">    <span class="keyword">ngx_http_upstream_main_conf_t</span>  *umcf;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (r-&gt;aio) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    u = r-&gt;upstream;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* NGX_HTTP_CACHE 等其他处理 */</span></span><br><span class="line"></span><br><span class="line">    cln-&gt;handler = ngx_http_upstream_cleanup;</span><br><span class="line">    cln-&gt;data = r;</span><br><span class="line">    u-&gt;cleanup = &amp;cln-&gt;handler;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (u-&gt;resolved == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="comment">/* 如果没有使用resolver设置DNS，直接取upstream的设置 */</span></span><br><span class="line">        uscf = u-&gt;conf-&gt;upstream;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">#<span class="keyword">if</span> (NGX_HTTP_SSL)</span><br><span class="line">        u-&gt;ssl_name = u-&gt;resolved-&gt;host;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">        host = &amp;u-&gt;resolved-&gt;host;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (u-&gt;resolved-&gt;sockaddr) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (u-&gt;resolved-&gt;port == <span class="number">0</span></span><br><span class="line">                &amp;&amp; u-&gt;resolved-&gt;sockaddr-&gt;sa_family != AF_UNIX)</span><br><span class="line">            &#123;</span><br><span class="line">                ngx_log_error(NGX_LOG_ERR, r-&gt;connection-&gt;<span class="built_in">log</span>, <span class="number">0</span>,</span><br><span class="line">                              <span class="string">"no port in upstream \"%V\""</span>, host);</span><br><span class="line">                ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                               NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ngx_http_upstream_create_round_robin_peer(r, u-&gt;resolved)</span><br><span class="line">                != NGX_OK)</span><br><span class="line">            &#123;</span><br><span class="line">                ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                               NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ngx_http_upstream_connect(r, u);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        umcf = ngx_http_get_module_main_conf(r, ngx_http_upstream_module);</span><br><span class="line"></span><br><span class="line">        uscfp = umcf-&gt;upstreams.elts;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 在显式/隐式定义的upstream中查找 */</span></span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; umcf-&gt;upstreams.nelts; i++) &#123;</span><br><span class="line"></span><br><span class="line">            uscf = uscfp[i];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (uscf-&gt;host.len == host-&gt;len</span><br><span class="line">                &amp;&amp; ((uscf-&gt;port == <span class="number">0</span> &amp;&amp; u-&gt;resolved-&gt;no_port)</span><br><span class="line">                     || uscf-&gt;port == u-&gt;resolved-&gt;port)</span><br><span class="line">                &amp;&amp; ngx_strncasecmp(uscf-&gt;host.data, host-&gt;data, host-&gt;len) == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">goto</span> found;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (u-&gt;resolved-&gt;port == <span class="number">0</span>) &#123;</span><br><span class="line">            ngx_log_error(NGX_LOG_ERR, r-&gt;connection-&gt;<span class="built_in">log</span>, <span class="number">0</span>,</span><br><span class="line">                          <span class="string">"no port in upstream \"%V\""</span>, host);</span><br><span class="line">            ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                               NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        temp.name = *host;</span><br><span class="line"></span><br><span class="line">        ctx = ngx_resolve_start(clcf-&gt;resolver, &amp;temp);</span><br><span class="line">        <span class="keyword">if</span> (ctx == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                               NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (ctx == NGX_NO_RESOLVER) &#123;</span><br><span class="line">            ngx_log_error(NGX_LOG_ERR, r-&gt;connection-&gt;<span class="built_in">log</span>, <span class="number">0</span>,</span><br><span class="line">                          <span class="string">"no resolver defined to resolve %V"</span>, host);</span><br><span class="line"></span><br><span class="line">            ngx_http_upstream_finalize_request(r, u, NGX_HTTP_BAD_GATEWAY);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ctx-&gt;name = *host;</span><br><span class="line">        ctx-&gt;handler = ngx_http_upstream_resolve_handler;</span><br><span class="line">        ctx-&gt;data = r;</span><br><span class="line">        ctx-&gt;timeout = clcf-&gt;resolver_timeout;</span><br><span class="line"></span><br><span class="line">        u-&gt;resolved-&gt;ctx = ctx;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (ngx_resolve_name(ctx) != NGX_OK) &#123;</span><br><span class="line">            u-&gt;resolved-&gt;ctx = <span class="literal">NULL</span>;</span><br><span class="line">            ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                               NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">found:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (uscf == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        ngx_log_error(NGX_LOG_ALERT, r-&gt;connection-&gt;<span class="built_in">log</span>, <span class="number">0</span>,</span><br><span class="line">                      <span class="string">"no upstream configuration"</span>);</span><br><span class="line">        ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                           NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> (NGX_HTTP_SSL)</span></span><br><span class="line">    u-&gt;ssl_name = uscf-&gt;host;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (uscf-&gt;peer.init(r, uscf) != NGX_OK) &#123;</span><br><span class="line">        ngx_http_upstream_finalize_request(r, u,</span><br><span class="line">                                           NGX_HTTP_INTERNAL_SERVER_ERROR);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    u-&gt;peer.start_time = ngx_current_msec;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (u-&gt;conf-&gt;next_upstream_tries</span><br><span class="line">        &amp;&amp; u-&gt;peer.tries &gt; u-&gt;conf-&gt;next_upstream_tries)</span><br><span class="line">    &#123;</span><br><span class="line">        u-&gt;peer.tries = u-&gt;conf-&gt;next_upstream_tries;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ngx_http_upstream_connect(r, u);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="详细分析"><a href="#详细分析" class="headerlink" title="详细分析"></a>详细分析</h3><h4 id="场景1"><a href="#场景1" class="headerlink" title="场景1"></a>场景1</h4><p>解析 <code>proxy_pass</code> 的函数 <code>ngx_http_proxy_pass</code> 中，没有找到 $ 符号（即，变量设置域名），走 <code>ngx_http_proxy_pass</code> 后半部分的处理逻辑。<code>ngx_http_upstream_init_round_robin</code> 初始化 <code>upstream</code> 时，走显式定义 <code>upstream</code> 的逻辑。<code>proxy_pass</code> 转发请求初始化时，<code>ngx_http_upstream_init_request</code> 中直接使用 <code>upstream</code> 中的后端 server 建立连接。</p>
<h4 id="场景2"><a href="#场景2" class="headerlink" title="场景2"></a>场景2</h4><p><code>ngx_http_upstream_init_round_robin</code> 初始化 <code>upstream</code> 时，走隐式定义 upstream 的逻辑，会调用 <code>ngx_inet_resolve_host</code> 对 <code>proxy_pass</code> 中的域名进行解析，设置 <code>upstream</code>。<code>proxy_pass</code> 转发请求初始化时，<code>ngx_http_upstream_init_request</code> 中直接使用 <code>upstream</code> 中的设置，也就是利用本地设置的 DNS 服务器解析出的 IP，建立连接。</p>
<h4 id="场景3"><a href="#场景3" class="headerlink" title="场景3"></a>场景3</h4><p>解析 <code>proxy_pass</code> 指令时，找到了 $ 符号，设置 <code>ngx_http_script_compile_t</code>，并利用 <code>ngx_http_script_compile</code> 进行编译，不走后半部分逻辑。配置文件没有显式/隐式定义 <code>upstream</code>，所以不会调用 <code>ngx_http_upstream_init_round_robin</code> 方法。<code>proxy_pass</code> 转发请求初始化时，<code>ngx_http_upstream_init_request</code> 中发现没有显式也没有隐式定义的 <code>upstream</code>，随后调用 <code>ngx_resolve_start</code>，对域名进行解析，之后将请求转发过去。</p>
<h4 id="场景4"><a href="#场景4" class="headerlink" title="场景4"></a>场景4</h4><p>解析 <code>proxy_pass</code> 指令时，找到了 $ 符号，设置 <code>ngx_http_script_compile_t</code>，并利用 <code>ngx_http_script_compile</code> 进行编译，不走后半部分逻辑。显式调用了 <code>upstream</code>，所以调用 <code>ngx_http_upstream_init_round_robin</code> 方法中的显式 <code>upstream</code> 的处理逻辑。<code>proxy_pass</code> 转发请求初始化时，<code>ngx_http_upstream_init_request</code> 中优先查找 <code>upstream</code>，如果找到了，直接将请求转发到 <code>upstream</code> 中的后端 server 上。如果 <code>upstream</code> 中没有找到，则对域名进行解析，然后将请求转发到解析后的 IP 上。</p>
<h4 id="场景5"><a href="#场景5" class="headerlink" title="场景5"></a>场景5</h4><p>基本与场景 4 相同，不同之处在于调用 <code>ngx_http_upstream_init_round_robin</code> 方法时，走隐式 <code>upstream</code> 部分的处理逻辑。</p>
<h4 id="场景6"><a href="#场景6" class="headerlink" title="场景6"></a>场景6</h4><p>与场景 2 相同。</p>
<h4 id="场景7"><a href="#场景7" class="headerlink" title="场景7"></a>场景7</h4><p>与场景 1 相同。</p>
<h4 id="场景8"><a href="#场景8" class="headerlink" title="场景8"></a>场景8</h4><p>实际上是隐式创建了 <code>upstream</code>，但是因为 <code>proxy_pass</code> 中指定了 IP 和端口号，所以<code>ngx_http_upstream_init_round_robin</code> 初始化 <code>upstream</code> 时，<code>us-&gt;servers</code> 不为空，所以走该函数的上半部分逻辑。与场景 1 有些类似。</p>
<p>参考：</p>
<ul>
<li><a href="https://www.jianshu.com/p/5caa48664da5" target="_blank" rel="external">https://www.jianshu.com/p/5caa48664da5</a></li>
<li><a href="https://www.zhihu.com/question/61786355" target="_blank" rel="external">https://www.zhihu.com/question/61786355</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[简单直观的阐述 Kubernetes Istio]]></title>
      <url>http://team.jiunile.com/blog/2020/11/k8s-istio-intro.html</url>
      <content type="html"><![CDATA[<h2 id="什么是-Istio？"><a href="#什么是-Istio？" class="headerlink" title="什么是 Istio？"></a>什么是 Istio？</h2><p>Istio 是一个服务网格，它允许在集群中的 pods 和服务之间进行更详细、复杂和可观察的通信。</p>
<p>它通过使用 CRD 扩展 Kubernetes API 来进行管理。它将代理容器注入到所有 pods 中，然后由这些 pods 控制集群中的流量。<br><a id="more"></a></p>
<h2 id="Kubernetes-Services"><a href="#Kubernetes-Services" class="headerlink" title="Kubernetes Services"></a>Kubernetes Services</h2><p>从这里开始，您应该已经了解了 Kubernetes Services，可以阅读<a href="https://medium.com/swlh/kubernetes-services-simply-visually-explained-2d84e58d70e5" target="_blank" rel="external">本系列的第 1 部分</a>。我们现在将简短地探讨如何实现 Kubernetes Services。我认为这有助于理解 Istio 如何做相同和不同的事情。<br><img src="/images/k8s/istio_1.png" alt="Kubernetes native service request"></p>
<p>图 1 显示了一个 Kubernetes 集群，该集群有两个节点和 4 个 pod，每个 pod 都有一个容器。服务 <code>service-nginx</code> 指向 nginx pods，服务 <code>service-python</code> 指向python pods。红线显示了从 <code>pod1-nginx</code> 中的 nginx 容器向 <code>service-python</code> 服务发出的请求，该服务将请求重定向到 <code>pod2-python</code>。</p>
<p>默认情况下，ClusterIP 服务执行简单的随机或循环分发。Kubernetes 中的 Services 并不存在于特定的节点上，而是存在于整个集群中。我们可以在图 2 中看到更多细节:<br><img src="/images/k8s/istio_2.png" alt="Kubernetes native service request with kube-proxy"></p>
<p>图 2 显示了与图 1 相同的示例，只是更详细一些。Kubernetes 中的服务是由运行在每个节点上的 <code>kube-proxy</code> 组件实现的。该组件创建 iptables 规则，并将请求重定向到 Pod。因此，服务就是 iptables 规则。(还有其他不使用 iptables 的代理模式，但过程是相同的。)</p>
<p>在图 2 中，我们看到 Kubernetes API 对每个 <code>kube-proxy</code> 进行编程。每当服务配置或服务的pods 发生更改时，就会发生这种情况。通过这种方式，Kubernetes API (以及整个主节点或控制平面)可以下降，但服务仍然可以工作。</p>
<h2 id="Kubernetes-Istio"><a href="#Kubernetes-Istio" class="headerlink" title="Kubernetes Istio"></a>Kubernetes Istio</h2><p>现在我们来看一个配置了 Istio 的相同示例:<br><img src="/images/k8s/istio_3.png" alt="Istio Control Plane programs istio-proxy"></p>
<p>图 3 显示安装了 Istio，它随 Istio 控制平面一起提供。还常见的是，每个 pod 都有第二个称为 <code>istio-proxy</code> 的容器，该容器在创建期间自动将其注入到 pods 中。</p>
<p>Istio 最常见的代理是具有惊人能力的 <a href="https://www.envoyproxy.io/" target="_blank" rel="external">Envoy</a>。虽然可以使用其他代理（<a href="https://www.nginx.com/blog/nginmesh-nginx-as-a-proxy-in-an-istio-service-mesh/" target="_blank" rel="external">如 Nginx</a>），这就是为什么我们从现在开始只将代理称为<code>istio-proxy</code>。</p>
<p>我们可以看到不再显示 <code>kube-proxy</code> 组件，这样做是为了保持图像的整洁。这些组件仍然存在，但是拥有 <code>istio-proxy</code> 的 pods 将不再使用 <code>kube-proxy</code> 组件。</p>
<p>每当配置或服务发生变化时，Istio 控制平面就会对所有 <code>istio-proxy</code> sidecars 进行编程。类似于图 2 中 Kubernetes API 程序所有 <code>kube-proxy</code> 组件的方式。Istio 控制平面使用现有的 Kubernetes 服务来接收每个服务点所指向的所有 pods 。通过使用 pod IP 地址，Istio 实现了自己的路由。</p>
<p>在 Istio 控制平面对所有 <code>istio-proxy</code> sidecars 编程之后，它看起来是这样的:<br><img src="/images/k8s/istio_4.png" alt="Istio Control Plane programmed all istio-proxys"></p>
<p>在图 4 中，我们看到 Istio 控制平面如何将当前配置应用到集群中的所有 <code>istio-proxy</code> 容器。为了简单起见，还包括 “ClusterIP” 声明。虽然 ClusterIP 是 Kubernetes 的内部服务类型。Istio 将把 Kubernetes 服务声明转换成它自己的路由声明。但是想象一下图像中显示的情况会很有帮助。</p>
<p>让我们看看如何使用 Istio 发出请求:<br><img src="/images/k8s/istio_5.png" alt="Request made with Istio"></p>
<p>在图 5 中，所有的 <code>istio-proxy</code> 容器已经被 Istio 控制平面编程，并包含所有必要的路由信息，如图 3/4 所示。来自 <code>pod1-nginx</code> 的 nginx 容器向 <code>service-python</code> 发出请求。</p>
<p>请求被 <code>pod1-nginx</code> 的 <code>istio-proxy</code> 容器拦截，并被重定向到一个 python pod 的 <code>istio-proxy</code> 容器，该容器随后将请求重定向到 python 容器。</p>
<h2 id="这里发生了什么"><a href="#这里发生了什么" class="headerlink" title="这里发生了什么?"></a>这里发生了什么?</h2><p>图 1-5 显示了使用 nginx 和 python pod 的 Kubernetes 应用程序的相同示例。我们已经看到了使用默认的 Kubernetes 服务和使用 Istio 是如何发生请求的。</p>
<p><strong>重要的是</strong>：无论使用什么方法，结果都是相同的，并且不需要更改应用程序本身，只需要更改基础结构代码。</p>
<h2 id="为什么要这样，为什么要使用-Istio？"><a href="#为什么要这样，为什么要使用-Istio？" class="headerlink" title="为什么要这样，为什么要使用 Istio？"></a>为什么要这样，为什么要使用 Istio？</h2><p>如果在使用 Istio 的时候没有什么变化(nginx pod 仍然可以像以前一样连接到 python pod)，为什么要首先使用 Istio 呢？</p>
<p><strong>其惊人的优势是</strong>，现在所有流量都通过每个 Pod 中的 <code>istio-proxy</code> 容器进行路由。每当 <code>istio-proxy</code> 接收并重定向一个请求时，它还会将有关该请求的信息提交给 Istio 控制平面。</p>
<p>因此 Istio 控制平面可以准确地知道该请求来自哪个 pod、存在哪些 HTTP 头、从一个<code>istio-proxy</code> 到另一个 <code>istio-proxy</code> 的请求需要多长时间等等。在具有许多彼此通信的服务的群集中，这可以提高可观察性并更好地控制所有流量。</p>
<h3 id="先进的路由"><a href="#先进的路由" class="headerlink" title="先进的路由"></a>先进的路由</h3><p>Kubernetes 内部 Services 只能对 pods 执行轮询或随机分发请求。使用 Istio 可以实现更复杂的方式。比如，如果发生错误，根据请求头进行重定向，或者重定向到最少使用的服务。</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>它允许将一定比例的流量路由到特定的服务版本，因此允许绿色/蓝色和金丝雀部署。</p>
<h3 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h3><p>可以对 pods 之间从 <code>istio-proxy</code> 到 <code>istio-proxy</code> 的集群内部通信进行加密。</p>
<h3 id="监控-图形生成"><a href="#监控-图形生成" class="headerlink" title="监控/图形生成"></a>监控/图形生成</h3><p>Istio 连接到 Prometheus 等监控工具。它也可以与 Kiali 一起很好的显示所有的服务和他们的流量。<br><img src="/images/k8s/istio_6.png" alt="kiali"></p>
<h3 id="追踪"><a href="#追踪" class="headerlink" title="追踪"></a>追踪</h3><p>因为 Istio 控制平面拥有大量关于请求的数据，所以可以使用 Jaeger 等工具跟踪和检查这些数据。<br><img src="/images/k8s/istio_7.png" alt="jaeger"></p>
<h3 id="多集群-mesh"><a href="#多集群-mesh" class="headerlink" title="多集群 mesh"></a>多集群 mesh</h3><p>Istio 有一个内部服务注册中心，它可以使用现有的 Kubernetes i服务。但是也可以从集群外部添加资源，甚至将不同的集群连接到一个网格中。<br><img src="/images/k8s/istio_8.png" alt="multiple-clusters"></p>
<h2 id="Sidecar-注入"><a href="#Sidecar-注入" class="headerlink" title="Sidecar 注入"></a>Sidecar 注入</h2><p>为了使 Istio 工作，每一个作为网状结构一部分的 pod 都需要注入 <code>istio-proxy</code> sidecar。这可以在 pod 创建期间为整个名称空间自动完成(通过 <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/" target="_blank" rel="external">Admission Controller 钩子</a>，也可以手动完成。</p>
<h2 id="Istio-会取代-Kubernetes-的服务吗"><a href="#Istio-会取代-Kubernetes-的服务吗" class="headerlink" title="Istio 会取代 Kubernetes 的服务吗?"></a>Istio 会取代 Kubernetes 的服务吗?</h2><p>不。当我开始使用 Istio 时，我问自己的一个问题是它是否会取代现有的 Kubernetes 服务。答案是否定的。Istio 使用现有的 Kubernetes 服务获取它们的所有 endpoints/pod IP 地址。</p>
<h2 id="Istio-取代了-Kubernetes-的-Ingress-吗"><a href="#Istio-取代了-Kubernetes-的-Ingress-吗" class="headerlink" title="Istio 取代了 Kubernetes 的 Ingress 吗?"></a>Istio 取代了 Kubernetes 的 Ingress 吗?</h2><p>是的。Istio 提供了新的资源，比如网关和虚拟服务，甚至还附带了 ingress 转换器<code>istioctl convert-ingress</code>。一个很好的来源是 <a href="https://istio.io/docs/concepts/traffic-management" target="_blank" rel="external">https://istio.io/docs/concepts/traffic-management</a></p>
<p>图 6 显示了 Istio 网关如何处理进入流量。网关本身也是一个 <code>istio-proxy</code> 组件。<br><img src="/images/k8s/istio_9.png" alt="Istio Gateway"></p>
<h2 id="控制平面组件"><a href="#控制平面组件" class="headerlink" title="控制平面组件"></a>控制平面组件</h2><p>Istio 控制平面由几个较小的部件组成，如 <code>Pilot</code>、<code>Mixer</code>、<code>Citadel</code> 和 <code>Galley</code>。如果您想深入研究，我建议您访问 <a href="https://istio.io/docs/ops/deployment/architecture" target="_blank" rel="external">https://istio.io/docs/ops/deployment/architecture</a></p>
<h2 id="如果-Istio-控制平面关闭会发生什么？"><a href="#如果-Istio-控制平面关闭会发生什么？" class="headerlink" title="如果 Istio 控制平面关闭会发生什么？"></a>如果 Istio 控制平面关闭会发生什么？</h2><p>因为所有的 <code>istio-proxy</code>  sidecar 都已经编程好了，所以 istio 的控制平面可以关闭，流量也会像以前一样工作。但是配置更新或新创建的 pods 不会被应用。</p>
<p>但对于高级路由，如将流量发送到使用最少的 pod 或策略(<a href="https://istio.io/docs/tasks/policy-enforcement" target="_blank" rel="external">https://istio.io/docs/tasks/policy-enforcement</a>)，所有 <code>istio-proxys</code> 之间需要通过 istio 控制平面进行通信。然后，在允许请求之前，每个 <code>istio-proxy</code> 都需要检查 istio 控制平面。</p>
<p>为了使这些配置正常工作，我认为控制平面必须始终可用。如果您有与此相关的链接，请随时添加评论。</p>
<h2 id="下一步你能做什么？"><a href="#下一步你能做什么？" class="headerlink" title="下一步你能做什么？"></a>下一步你能做什么？</h2><p>我写了一篇关于 <a href="https://medium.com/@wuestkamp/kubernetes-istio-canary-deployment-5ecfd7920e1c?source=friends_link&amp;sk=2be48393ac175a2199bf5d486cb91acf" target="_blank" rel="external">Istio Canray</a> 部署的示例文章。</p>
<p>Istio 提供了一个很好的示例应用程序和一些微服务。如果你喜欢进入 Istio，这是一个很好的开始方法：<a href="https://istio.io/docs/setup/getting-started/" target="_blank" rel="external">https://istio.io/docs/setup/getting-started</a></p>
<p>如果你想更深入地研究，<a href="https://www.youtube.com/watch?v=cB611FtjHcQ" target="_blank" rel="external">这段视频</a>也是很棒的。</p>
<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>这是一个简单的介绍和广泛的概述，我希望对人们有所帮助。Istio 无疑在 Kubernetes 之上又增加了另一层次的复杂性。尽管对于现代微服务架构来说，它实际上提供了一种比必须在应用程序代码本身中实现跟踪或可观察性更简单的方法。</p>
<p>作者：Kim Wuestkamp 来源：medium.com</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[k8s ingress 在删除资源时 hang 住无法删除]]></title>
      <url>http://team.jiunile.com/blog/2020/10/k8s-finalizers.html</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近将 aws <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller" target="_blank" rel="external">ALBIgressController</a> 升级到 2.0 后发现在中国区有点问题，随后就将 <code>alb-ingress-controller</code> 回退到 1.x。具体 2.0 的问题已经解决，原因是中国区 aws 没有对应的 <code>waf</code> ，解决方法是将 <code>waf</code> 相关的配置关闭即可。当然，这不是重点，重点是当我回退到 1.x 后遇到了几个问题：</p>
<ul>
<li><ol>
<li>kube-controller-manager 报如下错误日志</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E1029 02:43:20.921678       1 reflector.go:153] k8s.io/client-go/metadata/metadatainformer/informer.go:89: Failed to list *v1.PartialObjectMetadata: the server could not find the requested resource</span><br></pre></td></tr></table></figure>
<ul>
<li><ol>
<li>k8s ingress 资源无法删除，hang 住了，查看对应的 ingress yaml 看到如下信息</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">creationTimestamp: <span class="string">"2020-10-28T10:22:42Z"</span></span><br><span class="line">deletionGracePeriodSeconds: 0</span><br><span class="line">deletionTimestamp: <span class="string">"2020-10-29T06:57:04Z"</span></span><br><span class="line">finalizers:</span><br><span class="line">- ingress.k8s.aws/resources</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>针对问题一这里就不做过多分析了，原因是：<a href="https://github.com/kubernetes/kubernetes/issues/79610" target="_blank" rel="external">Dynamic informers do not stop when custom resource definition is removed #79610</a> 对应详细的分析点击链接访问即可。目前根本原因还未解决，临时解决方法是将对应的 CRD 资源恢复即可。</p>
<p>针对问题二就比较棘手，一时也无法定位到根本原因，从 ingress yaml 中对比分析只有 <code>deletion</code> 和 <code>finalizers</code> 这两个字样有异常，后来查阅相关文档了解才明白，解决方案是将 ingress 中 <code>finalizers</code> 对应的内容清空即可强制删除 ingress 。</p>
<h2 id="Finalizers"><a href="#Finalizers" class="headerlink" title="Finalizers"></a>Finalizers</h2><p><code>Finalizers</code> 允许 Operator 控制器实现异步的 <code>pre-delete hook</code>。比如你给 API 类型中的每个对象都创建了对应的外部资源，你希望在 k8s 删除对应资源时同时删除关联的外部资源，那么可以通过 <code>Finalizers</code> 来实现。</p>
<p><code>Finalizers</code> 是由字符串组成的列表，当 <code>Finalizers</code> 字段存在时，相关资源不允许被强制删除。存在 <code>Finalizers</code> 字段的的资源对象接收的第一个删除请求设置 <code>metadata.deletionTimestamp</code> 字段的值， 但不删除具体资源，在该字段设置后， <code>finalizer</code> 列表中的对象只能被删除，不能做其他操作。</p>
<p>当 <code>metadata.deletionTimestamp</code> 字段非空时，<code>controller watch</code> 对象并执行对应 <code>finalizers</code> 的动作，当所有动作执行完后，需要清空 <code>finalizers</code> ，之后 k8s 会删除真正想要删除的资源。</p>
<h2 id="Operator-finalizers-使用"><a href="#Operator-finalizers-使用" class="headerlink" title="Operator finalizers 使用"></a>Operator finalizers 使用</h2><p>介绍了 <code>Finalizers</code> 概念，那么我们来看看在 Operator 中如何使用，在 Operator Controller 中，最重要的逻辑就是 <code>Reconcile</code> 方法，<code>finalizers</code> 也是在 <code>Reconcile</code> 中实现的。要注意的是，设置了 <code>Finalizers</code> 会导致 k8s 的 delete 动作转为设置 <code>metadata.deletionTimestamp</code> 字段，如果你通过 <code>kubectl get</code> 命令看到资源存在这个字段，则表示资源正在删除（deleting）。</p>
<p>有以下几点需要理解：</p>
<ol>
<li>如果资源对象未被删除且未设置 <code>finalizers</code>，则添加 <code>finalizer</code> 并更新 k8s 资源对象；</li>
<li>如果正在删除资源对象并且 <code>finalizers</code> 仍然存在于 <code>finalizers</code> 列表中，则执行 <code>pre-delete hook</code> 并删除 <code>finalizers</code> ，更新资源对象；</li>
<li>由于以上两点，需要确保 <code>pre-delete hook</code> 是幂等的。</li>
</ol>
<h3 id="kuberbuilder-示例"><a href="#kuberbuilder-示例" class="headerlink" title="kuberbuilder 示例"></a>kuberbuilder 示例</h3><p>我们来看一个 kubebuilder 官方示例：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *CronJobReconciler)</span> <span class="title">Reconcile</span><span class="params">(req ctrl.Request)</span> <span class="params">(ctrl.Result, error)</span></span> &#123;</span><br><span class="line">    ctx := context.Background()</span><br><span class="line">    log := r.Log.WithValues(<span class="string">"cronjob"</span>, req.NamespacedName)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> cronJob batch.CronJob</span><br><span class="line">    <span class="keyword">if</span> err := r.Get(ctx, req.NamespacedName, &amp;cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        log.Error(err, <span class="string">"unable to fetch CronJob"</span>)</span><br><span class="line">        <span class="keyword">return</span> ctrl.Result&#123;&#125;, ignoreNotFound(err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明 finalizer 字段，类型为字符串</span></span><br><span class="line">    myFinalizerName := <span class="string">"storage.finalizers.tutorial.kubebuilder.io"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过检查 DeletionTimestamp 字段是否为0 判断资源是否被删除</span></span><br><span class="line">    <span class="keyword">if</span> cronJob.ObjectMeta.DeletionTimestamp.IsZero() &#123;</span><br><span class="line">        <span class="comment">// 如果为0 ，则资源未被删除，我们需要检测是否存在 finalizer，如果不存在，则添加，并更新到资源对象中</span></span><br><span class="line">        <span class="keyword">if</span> !containsString(cronJob.ObjectMeta.Finalizers, myFinalizerName) &#123;</span><br><span class="line">            cronJob.ObjectMeta.Finalizers = <span class="built_in">append</span>(cronJob.ObjectMeta.Finalizers, myFinalizerName)</span><br><span class="line">            <span class="keyword">if</span> err := r.Update(context.Background(), cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果不为 0 ，则对象处于删除中</span></span><br><span class="line">        <span class="keyword">if</span> containsString(cronJob.ObjectMeta.Finalizers, myFinalizerName) &#123;</span><br><span class="line">            <span class="comment">// 如果存在 finalizer 且与上述声明的 finalizer 匹配，那么执行对应 hook 逻辑</span></span><br><span class="line">            <span class="keyword">if</span> err := r.deleteExternalResources(cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="comment">// 如果删除失败，则直接返回对应 err，controller 会自动执行重试逻辑</span></span><br><span class="line">                <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果对应 hook 执行成功，那么清空 finalizers， k8s 删除对应资源</span></span><br><span class="line">            cronJob.ObjectMeta.Finalizers = removeString(cronJob.ObjectMeta.Finalizers, myFinalizerName)</span><br><span class="line">            <span class="keyword">if</span> err := r.Update(context.Background(), cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *Reconciler)</span> <span class="title">deleteExternalResources</span><span class="params">(cronJob *batch.CronJob)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// 删除 crobJob关联的外部资源逻辑</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// 需要确保实现是幂等的</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">containsString</span><span class="params">(slice []<span class="keyword">string</span>, s <span class="keyword">string</span>)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> _, item := <span class="keyword">range</span> slice &#123;</span><br><span class="line">        <span class="keyword">if</span> item == s &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">removeString</span><span class="params">(slice []<span class="keyword">string</span>, s <span class="keyword">string</span>)</span> <span class="params">(result []<span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> _, item := <span class="keyword">range</span> slice &#123;</span><br><span class="line">        <span class="keyword">if</span> item == s &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line">        result = <span class="built_in">append</span>(result, item)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="cluster-api-provider-vsphere-实现"><a href="#cluster-api-provider-vsphere-实现" class="headerlink" title="cluster-api-provider-vsphere 实现"></a>cluster-api-provider-vsphere 实现</h3><p>看完了示例，我们来找一个具体项目看看，<code>cluster-api-provider-vsphere</code> 是 cluster-api 相关项目，用于提供 vsphere 相关资源创建的 <code>Operator</code>，采用 <code>kubebuilder</code> 来实现的。</p>
<p>vspheremachine_controller.go 中实现了 <code>Reconcile</code> 方法：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reconcile ensures the back-end state reflects the Kubernetes resource state intent.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *VSphereMachineReconciler)</span> <span class="title">Reconcile</span><span class="params">(req ctrl.Request)</span> <span class="params">(_ ctrl.Result, reterr error)</span></span> &#123;</span><br><span class="line">	...</span><br><span class="line">	<span class="comment">// Always close the context when exiting this function so we can persist any VSphereMachine changes.</span></span><br><span class="line">	<span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">if</span> err := machineContext.Patch(); err != <span class="literal">nil</span> &amp;&amp; reterr == <span class="literal">nil</span> &#123;</span><br><span class="line">			reterr = err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Handle deleted machines</span></span><br><span class="line">	<span class="keyword">if</span> !vsphereMachine.ObjectMeta.DeletionTimestamp.IsZero() &#123;</span><br><span class="line">		<span class="keyword">return</span> r.reconcileDelete(machineContext)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Handle non-deleted machines</span></span><br><span class="line">	<span class="keyword">return</span> r.reconcileNormal(machineContext)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在 <code>Reconcile</code> 中检测了 <code>DeletionTimestamp</code> 是否为 0 ，如果不为 0 ，则表示资源处于正在删除中，那么来看下 <code>reconcileDelete</code> 实现：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *VSphereMachineReconciler)</span> <span class="title">reconcileDelete</span><span class="params">(ctx *context.MachineContext)</span> <span class="params">(reconcile.Result, error)</span></span> &#123;</span><br><span class="line">	ctx.Logger.Info(<span class="string">"Handling deleted VSphereMachine"</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> vmService services.VirtualMachineService = &amp;govmomi.VMService&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行删除虚拟机逻辑</span></span><br><span class="line">	vm, err := vmService.DestroyVM(ctx)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="comment">// 如果删除失败，则直接返回错误，controller 会自动重试</span></span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;&#125;, errors.Wrapf(err, <span class="string">"failed to destroy VM"</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 重新调度删除虚拟机逻辑，直到虚拟机状态处于 notfound 状态</span></span><br><span class="line">	<span class="keyword">if</span> vm.State != infrav1.VirtualMachineStateNotFound &#123;</span><br><span class="line">		ctx.Logger.V(<span class="number">6</span>).Info(<span class="string">"requeuing operation until vm state is reconciled"</span>, <span class="string">"expected-vm-state"</span>, infrav1.VirtualMachineStateNotFound, <span class="string">"actual-vm-state"</span>, vm.State)</span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;RequeueAfter: config.DefaultRequeue&#125;, <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// pre-delete hook执行成功，也就是上面的删除虚拟机逻辑执行成功，则清空 Finalizers</span></span><br><span class="line">	ctx.VSphereMachine.Finalizers = clusterutilv1.Filter(ctx.VSphereMachine.Finalizers, infrav1.MachineFinalizer)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到整体逻辑与示例的使用是一致的，主要通过这种方式来达到 pre-delete hook 的效果。</p>
<h3 id="k8s-initializer-finalizer-practice"><a href="#k8s-initializer-finalizer-practice" class="headerlink" title="k8s-initializer-finalizer-practice"></a>k8s-initializer-finalizer-practice</h3><p>在搜索相关资料的时候，看到有人在 SO 上问了如何使用的<a href="https://stackoverflow.com/questions/53057185/kubernetes-crd-finalizer" target="_blank" rel="external">问题</a>，其中有个回答中附上了一个练习项目，项目很小，很适合了解 <code>Finalizers</code> 概念。</p>
<p>相关逻辑如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">	customdeployment:=obj.(*crdv1alpha1.CustomDeployment).DeepCopy()</span><br><span class="line">	fmt.Println(<span class="string">"Event.............................."</span>)</span><br><span class="line">	<span class="keyword">if</span> customdeployment.DeletionTimestamp != <span class="literal">nil</span>&#123;</span><br><span class="line">		<span class="comment">// check if it has finalizer</span></span><br><span class="line">		<span class="keyword">if</span> customdeployment.GetFinalizers()!=<span class="literal">nil</span>&#123;</span><br><span class="line">			finalizers:=customdeployment.GetFinalizers()</span><br><span class="line"></span><br><span class="line">			<span class="comment">// check if first finalizer match with deletepod.crd.emruz.com</span></span><br><span class="line">			<span class="keyword">if</span> finalizers[<span class="number">0</span>]==<span class="string">"deletepods.crd.emruz.com"</span>&#123;</span><br><span class="line">				<span class="comment">//</span></span><br><span class="line">				_,err:=myutil.PatchCustomDeployment(c.clientset,customdeployment, <span class="function"><span class="keyword">func</span><span class="params">(deployment *crdv1alpha1.CustomDeployment)</span> *<span class="title">crdv1alpha1</span>.<span class="title">CustomDeployment</span></span> &#123;</span><br><span class="line">					<span class="comment">// delete pods under this deployment</span></span><br><span class="line">					err:=myutil.DeletePods(c.kubeclient,c.podLabel)</span><br><span class="line">					<span class="keyword">if</span> err!=<span class="literal">nil</span>&#123;</span><br><span class="line">						fmt.Println(<span class="string">"Failed to remove all pods. Reason: "</span>,err)</span><br><span class="line">						<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">					&#125;</span><br><span class="line">					<span class="comment">// pods sucessfully removed. remove the finalizer</span></span><br><span class="line">					customdeployment.ObjectMeta=myutil.RemoveFinalizer(customdeployment.ObjectMeta)</span><br><span class="line">					<span class="keyword">return</span> customdeployment</span><br><span class="line">				&#125;)</span><br><span class="line">				<span class="keyword">if</span> err!=<span class="literal">nil</span>&#123;</span><br><span class="line">					<span class="keyword">return</span> err</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在开发 <code>Operator</code> 时，<code>pre-delete hook</code> 是一个很常见的需求，目前只发现了 <code>Finalizers</code> 适合实现这个功能，需要好好掌握。</p>
<p>参考：</p>
<ul>
<li>zdyxry.github.io</li>
<li><a href="https://stackoverflow.com/questions/53057185/kubernetes-crd-finalizer" target="_blank" rel="external">https://stackoverflow.com/questions/53057185/kubernetes-crd-finalizer</a></li>
<li><a href="https://book.kubebuilder.io/reference/using-finalizers.html" target="_blank" rel="external">https://book.kubebuilder.io/reference/using-finalizers.html</a></li>
<li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers" target="_blank" rel="external">https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如何为 Kubernetes 实现原地升级]]></title>
      <url>http://team.jiunile.com/blog/2020/10/k8s-pod-update-local.html</url>
      <content type="html"><![CDATA[<h2 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h2><p><strong>原地升级</strong>一词中，“升级”不难理解，是将应用实例的版本由旧版替换为新版。那么如何结合 Kubernetes 环境来理解“原地”呢？</p>
<p>我们先来看看 K8s 原生 workload 的发布方式。这里假设我们需要部署一个应用，包括 foo、bar 两个容器在 Pod 中。其中，foo 容器第一次部署时用的镜像版本是 v1，我们需要将其升级为 v2 版本镜像，该怎么做呢？<br><a id="more"></a></p>
<ul>
<li>如果这个应用使用 Deployment 部署，那么升级过程中 Deployment 会触发新版本 ReplicaSet 创建 Pod，并删除旧版本 Pod。如下图所示：</li>
</ul>
<p><img src="/images/k8s/update-1.png" alt="1.png"></p>
<p>在本次升级过程中，原 Pod 对象被删除，一个新 Pod 对象被创建。新 Pod 被调度到另一个 Node 上，分配到一个新的 IP，并把 foo、bar 两个容器在这个 Node 上重新拉取镜像、启动容器。</p>
<ul>
<li>如果这个应该使用 StatefulSet 部署，那么升级过程中 StatefulSet 会先删除旧 Pod 对象，等删除完成后用同样的名字在创建一个新的 Pod 对象。如下图所示：</li>
</ul>
<p><img src="/images/k8s/update-2.png" alt="2.png"></p>
<p>值得注意的是，尽管新旧两个 Pod 名字都叫 pod-0，但其实是两个完全不同的 Pod 对象（uid也变了）。StatefulSet 等到原先的 pod-0 对象完全从 Kubernetes 集群中被删除后，才会提交创建一个新的 pod-0 对象。而这个新的 Pod 也会被重新调度、分配IP、拉镜像、启动容器。</p>
<ul>
<li>而所谓原地升级模式，就是在应用升级过程中避免将整个 Pod 对象删除、新建，而是基于原有的 Pod 对象升级其中某一个或多个容器的镜像版本：</li>
</ul>
<p><img src="/images/k8s/update-3.png" alt="3.png"></p>
<p>在原地升级的过程中，我们仅仅更新了原 Pod 对象中 foo 容器的 image 字段来触发 foo 容器升级到新版本。而不管是 Pod 对象，还是 Node、IP 都没有发生变化，甚至 foo 容器升级的过程中 bar 容器还一直处于运行状态。</p>
<p><strong>总结：这种只更新 Pod 中某一个或多个容器版本、而不影响整个 Pod 对象、其余容器的升级方式，被我们称为 Kubernetes 中的原地升级</strong>。</p>
<h2 id="收益分析"><a href="#收益分析" class="headerlink" title="收益分析"></a>收益分析</h2><p>那么，我们为什么要在 Kubernetes 中引入这种原地升级的理念和设计呢？</p>
<p>首先，这种原地升级的模式极大地提升了应用发布的效率，根据非完全统计数据，在阿里环境下原地升级至少比完全重建升级提升了 80% 以上的发布速度。这其实很容易理解，原地升级为发布效率带来了以下优化点：</p>
<ol>
<li>节省了调度的耗时，Pod 的位置、资源都不发生变化；</li>
<li>节省了分配网络的耗时，Pod 还使用原有的 IP；</li>
<li>节省了分配、挂载远程盘的耗时，Pod 还使用原有的 PV（且都是已经在 Node 上挂载好的）；</li>
<li>节省了大部分拉取镜像的耗时，因为 Node 上已经存在了应用的旧镜像，当拉取新版本镜像时只需要下载很少的几层 layer。</li>
</ol>
<p>其次，当我们升级 Pod 中一些 sidecar 容器（如采集日志、监控等）时，其实并不希望干扰到业务容器的运行。但面对这种场景，Deployment 或 StatefulSet 的升级都会将整个 Pod 重建，势必会对业务造成一定的影响。而容器级别的原地升级变动的范围非常可控，只会将需要升级的容器做重建，其余容器包括网络、挂载盘都不会受到影响。</p>
<p>最后，原地升级也为我们带来了集群的稳定性和确定性。当一个 Kubernetes 集群中大量应用触发重建 Pod 升级时，可能造成大规模的 Pod 飘移，以及对 Node 上一些低优先级的任务 Pod 造成反复的抢占迁移。这些大规模的 Pod 重建，本身会对 apiserver、scheduler、网络/磁盘分配等中心组件造成较大的压力，而这些组件的延迟也会给 Pod 重建带来恶性循环。而采用原地升级后，整个升级过程只会涉及到 controller 对 Pod 对象的更新操作和 kubelet 重建对应的容器。</p>
<h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>支持原地升级的控制器就位于 <a href="https://github.com/openkruise/kruise?spm=a2c6h.12873639.0.0.4bc314b5SPKTQY" target="_blank" rel="external">OpenKruise</a> 开源项目中。</p>
<p>也就是说，云原生应用都是统一使用 <code>OpenKruise</code> 中的扩展 workload 做部署管理的，而并没有采用原生 <code>Deployment/StatefulSet</code> 等。</p>
<p>那么 <code>OpenKruise</code> 是如何实现原地升级能力的呢？在介绍原地升级实现原理之前，我们先来看一些原地升级功能所依赖的原生 Kubernetes 功能：</p>
<h3 id="背景-1：Kubelet-针对-Pod-容器的版本管理"><a href="#背景-1：Kubelet-针对-Pod-容器的版本管理" class="headerlink" title="背景 1：Kubelet 针对 Pod 容器的版本管理"></a>背景 1：Kubelet 针对 Pod 容器的版本管理</h3><p>每个 Node 上的 Kubelet，会针对本机上所有 Pod.spec.containers 中的每个 container 计算一个 hash 值，并记录到实际创建的容器中。</p>
<p>如果我们修改了 Pod 中某个 container 的 image 字段，kubelet 会发现 container 的 hash 发生了变化、与机器上过去创建的容器 hash 不一致，而后 kubelet 就会把旧容器停掉，然后根据最新 Pod spec 中的 container 来创建新的容器。</p>
<p><strong>这个功能，其实就是针对单个 Pod 的原地升级的核心原理</strong>。</p>
<h3 id="背景-2：Pod-更新限制"><a href="#背景-2：Pod-更新限制" class="headerlink" title="背景 2：Pod 更新限制"></a>背景 2：Pod 更新限制</h3><p>在原生 kube-apiserver 中，对 Pod 对象的更新请求有严格的 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/validation/validation.go?spm=a2c6h.12873639.0.0.4bc314b5SPKTQY#L3729" target="_blank" rel="external">validation 校验逻辑</a>：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// validate updateable fields:</span></span><br><span class="line"><span class="comment">// 1.  spec.containers[*].image</span></span><br><span class="line"><span class="comment">// 2.  spec.initContainers[*].image</span></span><br><span class="line"><span class="comment">// 3.  spec.activeDeadlineSeconds</span></span><br></pre></td></tr></table></figure></p>
<p>简单来说，对于一个已经创建出来的 Pod，在 Pod Spec 中只允许修改 containers/initContainers 中的 image 字段，以及 activeDeadlineSeconds 字段。对 Pod Spec 中所有其他字段的更新，都会被 kube-apiserver 拒绝。</p>
<h3 id="背景-3：containerStatuses-上报"><a href="#背景-3：containerStatuses-上报" class="headerlink" title="背景 3：containerStatuses 上报"></a>背景 3：containerStatuses 上报</h3><p>kubelet 会在 pod.status 中上报 containerStatuses，对应 Pod 中所有容器的实际运行状态：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:latest</span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  containerStatuses:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:mainline</span><br><span class="line"><span class="attr">    imageID:</span> docker-pullable://nginx@sha256:<span class="number">2</span>f68b99bc0d6d25d0c56876b924ec20418544ff28e1fb89a4c27679a40da811b</span><br></pre></td></tr></table></figure></p>
<p>绝大多数情况下，spec.containers[x].image 与 status.containerStatuses[x].image 两个镜像是一致的。</p>
<p>但是也有上述这种情况，kubelet 上报的与 spec 中的 image 不一致（spec 中是 nginx:latest，但 status 中上报的是 nginx:mainline）。</p>
<p>这是因为，kubelet 所上报的 image 其实是从 CRI 接口中拿到的容器对应的镜像名。而如果 Node 机器上存在多个镜像对应了一个 imageID，那么上报的可能是其中任意一个：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker images | grep nginx</span><br><span class="line">nginx            latest              2622e6cca7eb        2 days ago          132MB</span><br><span class="line">nginx            mainline            2622e6cca7eb        2 days ago</span><br></pre></td></tr></table></figure></p>
<p>因此，一个 Pod 中 spec 和 status 的 image 字段不一致，并不意味着宿主机上这个容器运行的镜像版本和期望的不一致。</p>
<h3 id="背景-4：ReadinessGate-控制-Pod-是否-Ready"><a href="#背景-4：ReadinessGate-控制-Pod-是否-Ready" class="headerlink" title="背景 4：ReadinessGate 控制 Pod 是否 Ready"></a>背景 4：ReadinessGate 控制 Pod 是否 Ready</h3><p>在 Kubernetes 1.12 版本之前，一个 Pod 是否处于 Ready 状态只是由 kubelet 根据容器状态来判定：如果 Pod 中容器全部 ready，那么 Pod 就处于 Ready 状态。</p>
<p>但事实上，很多时候上层 operator 或用户都需要能控制 Pod 是否 Ready 的能力。因此，Kubernetes 1.12 版本之后提供了一个 readinessGates 功能来满足这个场景。如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  readinessGates:</span></span><br><span class="line"><span class="attr">  - conditionType:</span> MyDemo</span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  conditions:</span></span><br><span class="line"><span class="attr">  - type:</span> MyDemo</span><br><span class="line"><span class="attr">    status:</span> <span class="string">"True"</span></span><br><span class="line"><span class="attr">  - type:</span> ContainersReady</span><br><span class="line"><span class="attr">    status:</span> <span class="string">"True"</span></span><br><span class="line"><span class="attr">  - type:</span> Ready</span><br><span class="line"><span class="attr">    status:</span> <span class="string">"True"</span></span><br></pre></td></tr></table></figure></p>
<p>目前 kubelet 判定一个 Pod 是否 Ready 的两个前提条件：</p>
<ol>
<li>Pod 中容器全部 Ready（其实对应了 ContainersReady condition 为 True）；</li>
<li>如果 pod.spec.readinessGates 中定义了一个或多个 conditionType，那么需要这些 conditionType 在 pod.status.conditions 中都有对应的 status: “true” 的状态。</li>
</ol>
<p>只有满足上述两个前提，kubelet 才会上报 Ready condition 为 True</p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>了解了上面的四个背景之后，接下来分析一下 OpenKruise 是如何在 Kubernetes 中实现原地升级的原理。</p>
<h3 id="1-单个-Pod-如何原地升级？"><a href="#1-单个-Pod-如何原地升级？" class="headerlink" title="1. 单个 Pod 如何原地升级？"></a>1. 单个 Pod 如何原地升级？</h3><p>由“背景 1”可知，其实我们对一个存量 Pod 的 spec.containers[x] 中字段做修改，kubelet 会感知到这个 container 的 hash 发生了变化，随即就会停掉对应的旧容器，并用新的 container 来拉镜像、创建和启动新容器。</p>
<p>由“背景 2”可知，当前我们对一个存量 Pod 的 spec.containers[x] 中的修改，仅限于 image 字段。</p>
<p>因此，得出第一个实现原理：**对于一个现有的 Pod 对象，我们能且只能修改其中的 spec.containers[x].image 字段，来触发 Pod 中对应容器升级到一个新的 image。</p>
<h3 id="2-如何判断-Pod-原地升级成功？"><a href="#2-如何判断-Pod-原地升级成功？" class="headerlink" title="2. 如何判断 Pod 原地升级成功？"></a>2. 如何判断 Pod 原地升级成功？</h3><p>接下来的问题是，当我们修改了 Pod 中的 spec.containers[x].image 字段后，如何判断 kubelet 已经将容器重建成功了呢？</p>
<p>由“背景 3”可知，比较 spec 和 status 中的 image 字段是不靠谱的，因为很有可能 status 中上报的是 Node 上存在的另一个镜像名（相同 imageID）。</p>
<p>因此，得出第二个实现原理：<strong>判断 Pod 原地升级是否成功，相对来说比较靠谱的办法，是在原地升级前先将 status.containerStatuses[x].imageID 记录下来。在更新了 spec 镜像之后，如果观察到 Pod 的 status.containerStatuses[x].imageID 变化了，我们就认为原地升级已经重建了容器</strong>。</p>
<p>但这样一来，我们对原地升级的 image 也有了一个要求：<strong>不能用 image 名字（tag）不同、但实际对应同一个 imageID 的镜像来做原地升级，否则可能一直都被判断为没有升级成功（因为 status 中 imageID 不会变化）</strong>。</p>
<p>当然，后续我们还可以继续优化。OpenKruise 即将开源镜像预热的能力，会通过 DaemonSet 在每个 Node 上部署一个 NodeImage Pod。通过 NodeImage 上报我们可以得知 pod spec 中的 image 所对应的 imageID，然后和 pod status 中的 imageID 比较即可准确判断原地升级是否成功。</p>
<h3 id="3-如何确保原地升级过程中流量无损？"><a href="#3-如何确保原地升级过程中流量无损？" class="headerlink" title="3. 如何确保原地升级过程中流量无损？"></a>3. 如何确保原地升级过程中流量无损？</h3><p>在 Kubernetes 中，一个 Pod 是否 Ready 就代表了它是否可以提供服务。因此，像 Service 这类的流量入口都会通过判断 Pod Ready 来选择是否能将这个 Pod 加入 endpoints 端点中。</p>
<p>由“背景 4”可知，从 Kubernetes 1.12+ 之后，operator/controller 这些组件也可以通过设置 readinessGates 和更新 pod.status.conditions 中的自定义 type 状态，来控制 Pod 是否可用。</p>
<p>因此，得出第三个实现原理：<strong>可以在 <code>pod.spec.readinessGates</code> 中定义一个叫 <code>InPlaceUpdateReady</code> 的 conditionType</strong>。</p>
<p>在原地升级时：</p>
<ol>
<li><strong>先将 pod.status.conditions 中的 InPlaceUpdateReady condition 设为 “False”，这样就会触发 kubelet 将 Pod 上报为 NotReady，从而使流量组件（如 endpoint controller）将这个 Pod 从服务端点摘除</strong>；</li>
<li><strong>再更新 pod spec 中的 image 触发原地升级</strong>。</li>
</ol>
<p>原地升级结束后，再将 InPlaceUpdateReady condition 设为 “True”，使 Pod 重新回到 Ready 状态。</p>
<p>另外在原地升级的两个步骤中，第一步将 Pod 改为 NotReady 后，流量组件异步 watch 到变化并摘除端点可能是需要一定时间的。因此我们也提供优雅原地升级的能力，即通过 gracePeriodSeconds 配置在修改 NotReady 状态和真正更新 image 触发原地升级两个步骤之间的静默期时间。</p>
<h3 id="4-组合发布策略"><a href="#4-组合发布策略" class="headerlink" title="4. 组合发布策略"></a>4. 组合发布策略</h3><p>原地升级和 Pod 重建升级一样，可以配合各种发布策略来执行：</p>
<ul>
<li><strong>partition</strong>：如果配置 partition 做灰度，那么只会将 replicas-partition 数量的 Pod 做原地升级；</li>
<li><strong>maxUnavailable</strong>：如果配置 maxUnavailable，那么只会将满足 unavailable 数量的 Pod 做原地升级；</li>
<li><strong>maxSurge</strong>：如果配置 maxSurge 做弹性，那么当先扩出来 maxSurge 数量的 Pod 之后，存量的 Pod 仍然使用原地升级；</li>
<li><strong>priority/scatter</strong>：如果配置了发布优先级/打散策略，会按照策略顺序对 Pod 做原地升级。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如上文所述，<a href="https://github.com/openkruise/kruise?spm=a2c6h.12873639.0.0.4bc314b5SPKTQY" target="_blank" rel="external">OpenKruise</a> 结合 Kubernetes 原生提供的 kubelet 容器版本管理、readinessGates 等功能，实现了针对 Pod 的原地升级能力。</p>
<p>而原地升级也为应用发布带来大幅的效率、稳定性提升。值得关注的是，随着集群、应用规模的增大，这种提升的收益越加明显。</p>
<p>来源：阿里云开发者社区 作者：王思宇</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[docker 卡死引起的 container runtime is down]]></title>
      <url>http://team.jiunile.com/blog/2020/10/docker-hang.html</url>
      <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>最近升级了一版 kubelet，修复因 kubelet 删除 Pod 慢导致平台删除集群超时的问题。在灰度 redis 隔离集群的时候，发现升级 kubelet 并重启服务后，少量宿主状态变成了 NotReady，并且回滚 kubelet 至之前版本，宿主状态仍然是 NotReady。查看宿主状态时提示 ‘container runtime is down’ ，根据经验，此时一般就是容器运行时出了问题。弹性云使用的容器运行时是 docker，我们就去检查 docker 的状态，检测结果如下：</p>
<ul>
<li><code>docker ps</code> 查看所有容器状态，执行正常</li>
<li><code>docker inspect</code> 查看某一容器详细状态，执行阻塞</li>
</ul>
<p>典型的 <code>docker hang</code> 死行为。因为我们最近在升级 docker 版本，存量宿主 docker 的版本为 1.13.1，并且在逐步升级至 18.06.3，新宿主的 docker 版本都是 18.06.3。<code>docker hang</code> 死问题在 1.13.1 版本上表现得更彻底，在执行 <code>docker ps</code> 的时候就已经 hang 死了，一旦某个容器出了问题，docker 就处于无响应状态；而 docker 18.06.3 做了一点小小的优化，在执行 docker ps 时去掉了针对容器级别的加锁操作，但是 <code>docker inspect</code> 依然会加容器锁，因此某一个容器出现问题，并不会造成 docker 服务不可响应，受影响的也仅仅是该容器，无法执行任何操作。</p>
<p>至于为什么以 <code>docker ps</code> 与 <code>docker inspect</code> 为指标检查 docker 状态，因为 kubelet 就是依赖这两个 docker API 获取容器状态。</p>
<p>所以，现在问题有二：</p>
<ol>
<li><code>docker hang</code> 死的根因是什么？</li>
<li><code>docker hang</code> 死时，为什么重启 kubelet，会导致宿主状态变为 NotReady？<a id="more"></a> 
</li>
</ol>
<h2 id="2-重启-kubelet-变更宿主状态"><a href="#2-重启-kubelet-变更宿主状态" class="headerlink" title="2 重启 kubelet 变更宿主状态"></a>2 重启 kubelet 变更宿主状态</h2><p>kubelet 重启后宿主状态从 Ready 变为 NotReady，这个问题相较 <code>docker hang</code> 死而言，没有那么复杂，所以我们先排查这个问题。</p>
<p>kubelet 针对宿主会设置多个 Condition，表明宿主当前所处的状态，比如宿主内存是否告急、线程数是否告急，以及宿主是否就绪。其中 ReadyCondition 表明宿主是否就绪，kubectl 查看宿主状态时，展示的 Statue 信息就是 ReadCondition 的内容，常见的状态及其含义定义如下：</p>
<ol>
<li><strong>Ready 状态</strong>：表明当前宿主状态一切 OK，能正常响应 Pod 事件</li>
<li><strong>NotReady 状态</strong>：表明宿主的 kubelet 仍在运行，但是此时已经无法处理 Pod 事件。NotReady 绝大多数情况都是容器运行时出了问题</li>
<li><strong>Unknown 状态</strong>：表明宿主 kubelet 已停止运行</li>
</ol>
<p>kubelet 定义的 ReadyCondition 的判定条件如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// defaultNodeStatusFuncs is a factory that generates the default set of</span></span><br><span class="line"><span class="comment">// setNodeStatus funcs</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">defaultNodeStatusFuncs</span><span class="params">()</span> []<span class="title">func</span><span class="params">(*v1.Node)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   ......</span><br><span class="line">   setters = <span class="built_in">append</span>(setters,</span><br><span class="line">      nodestatus.OutOfDiskCondition(kl.clock.Now, kl.recordNodeStatusEvent),</span><br><span class="line">      nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),</span><br><span class="line">      nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),</span><br><span class="line">      nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),</span><br><span class="line">      nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, validateHostFunc, kl.containerManager.Status, kl.recordNodeStatusEvent),</span><br><span class="line">      nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),</span><br><span class="line">      <span class="comment">// TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event</span></span><br><span class="line">      <span class="comment">// and record state back to the Kubelet runtime object. In the future, I'd like to isolate</span></span><br><span class="line">      <span class="comment">// these side-effects by decoupling the decisions to send events and partial status recording</span></span><br><span class="line">      <span class="comment">// from the Node setters.</span></span><br><span class="line">      kl.recordNodeSchedulableEvent,</span><br><span class="line">   )</span><br><span class="line">   <span class="keyword">return</span> setters</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>深入 nodestatus.ReadyCondition 的实现可以发现，宿主是否 Ready 取决于很多条件，包含运行时判定、网络判定、基本资源判定等。这里我们只需关注运行时判定即可：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *runtimeState)</span> <span class="title">runtimeErrors</span><span class="params">()</span> []<span class="title">string</span></span> &#123;</span><br><span class="line">   s.RLock()</span><br><span class="line">   <span class="keyword">defer</span> s.RUnlock()</span><br><span class="line">   <span class="keyword">var</span> ret []<span class="keyword">string</span></span><br><span class="line">   <span class="keyword">if</span> !s.lastBaseRuntimeSync.Add(s.baseRuntimeSyncThreshold).After(time.Now()) &#123;  <span class="comment">// 1</span></span><br><span class="line">      ret = <span class="built_in">append</span>(ret, <span class="string">"container runtime is down"</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> s.internalError != <span class="literal">nil</span> &#123;</span><br><span class="line">      ret = <span class="built_in">append</span>(ret, s.internalError.Error())</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">for</span> _, hc := <span class="keyword">range</span> s.healthChecks &#123;                                            <span class="comment">// 2</span></span><br><span class="line">      <span class="keyword">if</span> ok, err := hc.fn(); !ok &#123;</span><br><span class="line">         ret = <span class="built_in">append</span>(ret, fmt.Sprintf(<span class="string">"%s is not healthy: %v"</span>, hc.name, err))</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> ret</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当出现如下两种状况之一时，则判定运行时检查不通过：</p>
<ol>
<li>距最近一次运行时同步操作的时间间隔超过指定阈值（默认 30s）</li>
<li>运行时健康检查未通过</li>
</ol>
<p>那么，当时宿主的 NotReady 是由哪种状况引起的呢？结合 kubelet 日志分析，kubelet 每隔 5s 就输出一条日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">I0715 10:43:28.049240   16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down]</span><br><span class="line">I0715 10:43:33.049359   16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down]</span><br><span class="line">I0715 10:43:38.049492   16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down]</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>因此，状况 1 是宿主 NotReady 的元凶。</p>
<p>我们继续分析为什么 kubelet 没有按照预期设置 lastBaseRuntimeSync。kubelet 启动时会创建一个 goroutine，并在该 goroutine 中循环设置 lastBaseRuntimeSync，循环如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">Run</span><span class="params">(updates &lt;-<span class="keyword">chan</span> kubetypes.PodUpdate)</span></span> &#123;</span><br><span class="line">   ......</span><br><span class="line">   <span class="keyword">go</span> wait.Until(kl.updateRuntimeUp, <span class="number">5</span>*time.Second, wait.NeverStop)</span><br><span class="line">   ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">updateRuntimeUp</span><span class="params">()</span></span> &#123;</span><br><span class="line">   kl.updateRuntimeMux.Lock()</span><br><span class="line">   <span class="keyword">defer</span> kl.updateRuntimeMux.Unlock()</span><br><span class="line">   ......</span><br><span class="line">   kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)</span><br><span class="line">   kl.runtimeState.setRuntimeSync(kl.clock.Now())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>正常情况下，kubelet 每隔 5s 会将 lastBaseRuntimeSync 设置为当前时间，而宿主状态异常时，这个时间戳一直未被更新。也即 updateRuntimeUp 一直被阻塞在设置 lastBaseRuntimeSync 之前的某一步。我们只需逐个排查 updateRuntimeUp 内的函数调用即可，具体过程不再展示，最终的函数调用链路如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initializeRuntimeDependentModules -&gt; kl.cadvisor.Start -&gt; cc.Manager.Start -&gt; self.createContainer -&gt; m.createContainerLocked -&gt; container.NewContainerHandler -&gt; factory.CanHandleAndAccept -&gt; self.client.ContainerInspect</span><br></pre></td></tr></table></figure></p>
<p>由于某个容器状态异常，kubelet 执行 <code>docker inspect</code> 操作也被 hang 死。</p>
<p>因此，重启 kubelet 引起宿主状态从 Ready 变为 NotReady，其根因在于某个容器状态异常，执行 docker inspect 时被 hang 死。而如果 <code>docker inspect hang</code> 死发生在 kubelet 重启之后，则不会对宿主的 Ready 状态造成任何影响，因为 oneTimeInitializer 是 <code>sync.Once</code> 类型，也即仅仅会在 kebelet 启动时执行一次。那时 kubelet 仅仅是不能处理该 Pod 相关的任何事件，包含删除、变更等，但是仍然能够处理其他 Pod 的任意事件。</p>
<p>可能有人会问，为什么 kubelet 重启时访问 <code>docker inspect</code> 操作不加超时控制？确实，如果添加了超时控制，kubelet 重启不会引起宿主状态变更。待详细挖掘后再来补充，我们先继续分析 <code>docker hang</code> 死的问题。</p>
<h2 id="3-docker-hang-死"><a href="#3-docker-hang-死" class="headerlink" title="3 docker hang 死"></a>3 docker hang 死</h2><p>我们对 <code>docker hang</code> 死并不陌生，因为已经发生了好多起。其发生时的现象也多种多样。以往针对 docker 1.13.1 版本的排查都发现了一些线索，但是并没有定位到根因，最终绝大多数也是通过重启 docker 解决。而这一次发生在 docker 18.06.3 版本的 <code>docker hang</code> 死行为，经过我们 4 人小分队接近一周的望闻问切，终于确定了其病因。注意，<code>docker hang</code> 死的原因不止一种，因此本处方并非是个万能药。</p>
<p>现在，我们掌握的知识仅仅是 docker 异常了，无法响应特定容器的 <code>docker inspect</code> 操作，而对详细信息则一无所知。</p>
<h3 id="3-1-链路跟踪"><a href="#3-1-链路跟踪" class="headerlink" title="3.1 链路跟踪"></a>3.1 链路跟踪</h3><p>首先，我们希望对 docker 运行的全局状况有一个大致的了解，熟悉 go 语言开发的用户自然能联想到神器 pprof。我们借助 pprof 描绘出了 docker 当时运行的蓝图：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">goroutine profile: total 722373</span><br><span class="line">717594 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2132ef 0x7fe8bc238c1a 0x7fe8bd56f7fe 0x7fe8bd56f6bd 0x7fe8bcea8719 0x7fe8bcea938b 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711</span><br><span class="line"><span class="comment">#	0x7fe8bc2132ee	sync.runtime_SemacquireMutex+0x3e																/usr/local/go/src/runtime/sema.go:71</span></span><br><span class="line"><span class="comment">#	0x7fe8bc238c19	sync.(*Mutex).Lock+0x109																	/usr/local/go/src/sync/mutex.go:134</span></span><br><span class="line"><span class="comment">#	0x7fe8bd56f7fd	github.com/docker/docker/daemon.(*Daemon).ContainerInspectCurrent+0x8d												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:40</span></span><br><span class="line"><span class="comment">#	0x7fe8bd56f6bc	github.com/docker/docker/daemon.(*Daemon).ContainerInspect+0x11c												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:29</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea8718	github.com/docker/docker/api/server/router/container.(*containerRouter).getContainersByName+0x118								/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/inspect.go:15</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea938a	github.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.getContainersByName)-fm+0x6a	/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container.go:39</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb726c9	github.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1+0xd9									/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/experimental.go:26</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb72b00	github.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1+0x400									/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/version.go:62</span></span><br><span class="line"><span class="comment">#	0x7fe8bc71c26a	github.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1+0x7aa										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/pkg/authorization/middleware.go:59</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb85f49	github.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1+0x199											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/server.go:141</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b9895	net/http.HandlerFunc.ServeHTTP+0x45																/usr/local/go/src/net/http/server.go:1947</span></span><br><span class="line"><span class="comment">#	0x7fe8bc72a437	github.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP+0x227										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb849e1	github.com/docker/docker/api/server.(*routerSwapper).ServeHTTP+0x71												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router_swapper.go:29</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4bc67d	net/http.serverHandler.ServeHTTP+0xbd																/usr/local/go/src/net/http/server.go:2694</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b88a2	net/http.(*conn).serve+0x652																	/usr/local/go/src/net/http/server.go:1830</span></span><br><span class="line"></span><br><span class="line">4175 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2132ef 0x7fe8bc238c1a 0x7fe8bcc2eccf 0x7fe8bd597af4 0x7fe8bcea2456 0x7fe8bcea956b 0x7fe8bcb73dff 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711</span><br><span class="line"><span class="comment">#	0x7fe8bc2132ee	sync.runtime_SemacquireMutex+0x3e																/usr/local/go/src/runtime/sema.go:71</span></span><br><span class="line"><span class="comment">#	0x7fe8bc238c19	sync.(*Mutex).Lock+0x109																	/usr/local/go/src/sync/mutex.go:134</span></span><br><span class="line"><span class="comment">#	0x7fe8bcc2ecce	github.com/docker/docker/container.(*State).IsRunning+0x2e													/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/container/state.go:240</span></span><br><span class="line"><span class="comment">#	0x7fe8bd597af3	github.com/docker/docker/daemon.(*Daemon).ContainerStats+0xb3													/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/stats.go:30</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea2455	github.com/docker/docker/api/server/router/container.(*containerRouter).getContainersStats+0x1e5								/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container_routes.go:115</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea956a	github.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.getContainersStats)-fm+0x6a	/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container.go:42</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb73dfe	github.com/docker/docker/api/server/router.cancellableHandler.func1+0xce											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/local.go:92</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb726c9	github.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1+0xd9									/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/experimental.go:26</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb72b00	github.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1+0x400									/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/version.go:62</span></span><br><span class="line"><span class="comment">#	0x7fe8bc71c26a	github.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1+0x7aa										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/pkg/authorization/middleware.go:59</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb85f49	github.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1+0x199											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/server.go:141</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b9895	net/http.HandlerFunc.ServeHTTP+0x45																/usr/local/go/src/net/http/server.go:1947</span></span><br><span class="line"><span class="comment">#	0x7fe8bc72a437	github.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP+0x227										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb849e1	github.com/docker/docker/api/server.(*routerSwapper).ServeHTTP+0x71												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router_swapper.go:29</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4bc67d	net/http.serverHandler.ServeHTTP+0xbd																/usr/local/go/src/net/http/server.go:2694</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b88a2	net/http.(*conn).serve+0x652																	/usr/local/go/src/net/http/server.go:1830</span></span><br><span class="line"></span><br><span class="line">1 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2131fb 0x7fe8bc239a3b 0x7fe8bcbb679d 0x7fe8bcc26774 0x7fe8bd570b20 0x7fe8bd56f81c 0x7fe8bd56f6bd 0x7fe8bcea8719 0x7fe8bcea938b 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711</span><br><span class="line"><span class="comment">#	0x7fe8bc2131fa	sync.runtime_Semacquire+0x3a																	/usr/local/go/src/runtime/sema.go:56</span></span><br><span class="line"><span class="comment">#	0x7fe8bc239a3a	sync.(*RWMutex).RLock+0x4a																	/usr/local/go/src/sync/rwmutex.go:50</span></span><br><span class="line"><span class="comment">#	0x7fe8bcbb679c	github.com/docker/docker/daemon/exec.(*Store).List+0x4c														/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/exec/exec.go:140</span></span><br><span class="line"><span class="comment">#	0x7fe8bcc26773	github.com/docker/docker/container.(*Container).GetExecIDs+0x33													/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/container/container.go:423</span></span><br><span class="line"><span class="comment">#	0x7fe8bd570b1f	github.com/docker/docker/daemon.(*Daemon).getInspectData+0x5cf													/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:178</span></span><br><span class="line"><span class="comment">#	0x7fe8bd56f81b	github.com/docker/docker/daemon.(*Daemon).ContainerInspectCurrent+0xab												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:42</span></span><br><span class="line"><span class="comment">#	0x7fe8bd56f6bc	github.com/docker/docker/daemon.(*Daemon).ContainerInspect+0x11c												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:29</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea8718	github.com/docker/docker/api/server/router/container.(*containerRouter).getContainersByName+0x118								/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/inspect.go:15</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea938a	github.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.getContainersByName)-fm+0x6a	/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container.go:39</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb726c9	github.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1+0xd9									/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/experimental.go:26</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb72b00	github.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1+0x400									/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/version.go:62</span></span><br><span class="line"><span class="comment">#	0x7fe8bc71c26a	github.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1+0x7aa										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/pkg/authorization/middleware.go:59</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb85f49	github.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1+0x199											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/server.go:141</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b9895	net/http.HandlerFunc.ServeHTTP+0x45																/usr/local/go/src/net/http/server.go:1947</span></span><br><span class="line"><span class="comment">#	0x7fe8bc72a437	github.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP+0x227										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb849e1	github.com/docker/docker/api/server.(*routerSwapper).ServeHTTP+0x71												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router_swapper.go:29</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4bc67d	net/http.serverHandler.ServeHTTP+0xbd																/usr/local/go/src/net/http/server.go:2694</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b88a2	net/http.(*conn).serve+0x652																	/usr/local/go/src/net/http/server.go:1830</span></span><br><span class="line"></span><br><span class="line">1 @ 0x7fe8bc202980 0x7fe8bc212946 0x7fe8bc8b6881 0x7fe8bc8b699d 0x7fe8bc8e259b 0x7fe8bc8e1695 0x7fe8bc8c47d5 0x7fe8bd2e0c06 0x7fe8bd2eda96 0x7fe8bc8c42fb 0x7fe8bc8c4613 0x7fe8bd2a6474 0x7fe8bd2e6976 0x7fe8bd3661c5 0x7fe8bd56842f 0x7fe8bcea7bdb 0x7fe8bcea9f6b 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711</span><br><span class="line"><span class="comment">#	0x7fe8bc8b6880	github.com/docker/docker/vendor/google.golang.org/grpc/transport.(*Stream).waitOnHeader+0x100											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/transport/transport.go:222</span></span><br><span class="line"><span class="comment">#	0x7fe8bc8b699c	github.com/docker/docker/vendor/google.golang.org/grpc/transport.(*Stream).RecvCompress+0x2c											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/transport/transport.go:233</span></span><br><span class="line"><span class="comment">#	0x7fe8bc8e259a	github.com/docker/docker/vendor/google.golang.org/grpc.(*csAttempt).recvMsg+0x63a												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/stream.go:515</span></span><br><span class="line"><span class="comment">#	0x7fe8bc8e1694	github.com/docker/docker/vendor/google.golang.org/grpc.(*clientStream).RecvMsg+0x44												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/stream.go:395</span></span><br><span class="line"><span class="comment">#	0x7fe8bc8c47d4	github.com/docker/docker/vendor/google.golang.org/grpc.invoke+0x184														/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/call.go:83</span></span><br><span class="line"><span class="comment">#	0x7fe8bd2e0c05	github.com/docker/docker/vendor/github.com/containerd/containerd.namespaceInterceptor.unary+0xf5										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/containerd/containerd/grpc.go:35</span></span><br><span class="line"><span class="comment">#	0x7fe8bd2eda95	github.com/docker/docker/vendor/github.com/containerd/containerd.(namespaceInterceptor).(github.com/docker/docker/vendor/github.com/containerd/containerd.unary)-fm+0xf5	/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/containerd/containerd/grpc.go:51</span></span><br><span class="line"><span class="comment">#	0x7fe8bc8c42fa	github.com/docker/docker/vendor/google.golang.org/grpc.(*ClientConn).Invoke+0x10a												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/call.go:35</span></span><br><span class="line"><span class="comment">#	0x7fe8bc8c4612	github.com/docker/docker/vendor/google.golang.org/grpc.Invoke+0xc2														/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/google.golang.org/grpc/call.go:60</span></span><br><span class="line"><span class="comment">#	0x7fe8bd2a6473	github.com/docker/docker/vendor/github.com/containerd/containerd/api/services/tasks/v1.(*tasksClient).Start+0xd3								/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/containerd/containerd/api/services/tasks/v1/tasks.pb.go:421</span></span><br><span class="line"><span class="comment">#	0x7fe8bd2e6975	github.com/docker/docker/vendor/github.com/containerd/containerd.(*process).Start+0xf5												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/containerd/containerd/process.go:109</span></span><br><span class="line"><span class="comment">#	0x7fe8bd3661c4	github.com/docker/docker/libcontainerd.(*client).Exec+0x4b4															/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/libcontainerd/client_daemon.go:381</span></span><br><span class="line"><span class="comment">#	0x7fe8bd56842e	github.com/docker/docker/daemon.(*Daemon).ContainerExecStart+0xb4e														/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/exec.go:251</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea7bda	github.com/docker/docker/api/server/router/container.(*containerRouter).postContainerExecStart+0x34a										/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/exec.go:125</span></span><br><span class="line"><span class="comment">#	0x7fe8bcea9f6a	github.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.postContainerExecStart)-fm+0x6a			/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container.go:59</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb726c9	github.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1+0xd9											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/experimental.go:26</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb72b00	github.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1+0x400											/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/version.go:62</span></span><br><span class="line"><span class="comment">#	0x7fe8bc71c26a	github.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1+0x7aa												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/pkg/authorization/middleware.go:59</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb85f49	github.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1+0x199													/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/server.go:141</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b9895	net/http.HandlerFunc.ServeHTTP+0x45																		/usr/local/go/src/net/http/server.go:1947</span></span><br><span class="line"><span class="comment">#	0x7fe8bc72a437	github.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP+0x227												/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103</span></span><br><span class="line"><span class="comment">#	0x7fe8bcb849e1	github.com/docker/docker/api/server.(*routerSwapper).ServeHTTP+0x71														/root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router_swapper.go:29</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4bc67d	net/http.serverHandler.ServeHTTP+0xbd																		/usr/local/go/src/net/http/server.go:2694</span></span><br><span class="line"><span class="comment">#	0x7fe8bc4b88a2	net/http.(*conn).serve+0x652																			/usr/local/go/src/net/http/server.go:1830</span></span><br></pre></td></tr></table></figure></p>
<p>注意，这是一份精简后的 docker 协程栈信息。从上面的蓝图，我们可以总结出如下结论：</p>
<ol>
<li>有 717594 个协程被阻塞在 <code>docker inspect</code></li>
<li>有 4175 个协程被阻塞在 <code>docker stats</code></li>
<li>有 1 个协程被阻塞在获取 <code>docker exec</code> 的任务 ID</li>
<li>有 1 个协程被阻塞在 <code>docker exec</code> 的执行过程</li>
</ol>
<p>从上面的结论，我们基本了解了异常容器 hang 死的原因，在于该容器执行 docker exec 后未返回 (4)，进而导致获取 <code>docker exec</code> 的任务 ID 阻塞 (3)，由于 (3) 实现获取了容器锁，进而导致了 <code>docker inspect</code> (1) 与 <code>docker stats</code> (2) 卡死。所以病因并非是 <code>docker inspect</code>，而是 <code>docker exec</code>。</p>
<p>要想继续往下挖掘，我们现在有必要补充一下背景知识。kubelet 启动容器或者在容器内执行命令的完整调用路径如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+--------------------------------------------------------------+</span><br><span class="line">|                                                              |</span><br><span class="line">|   +------------+                                             |</span><br><span class="line">|   |            |                                             |</span><br><span class="line">|   |   kubelet  |                                             |</span><br><span class="line">|   |            |                                             |</span><br><span class="line">|   +------|-----+                                             |</span><br><span class="line">|          |                                                   |</span><br><span class="line">|   +------v-----+       +---------------+                     |</span><br><span class="line">|   |            |       |               |                     |</span><br><span class="line">|   |   dockerd  -------&gt;|  containerd   |                     |</span><br><span class="line">|   |            |       |               |                     |</span><br><span class="line">|   +------------+       +-------|-------+                     |</span><br><span class="line">|                                |                             |</span><br><span class="line">|                        +-------v-------+     +-----------+   |</span><br><span class="line">|                        |               |     |           |   |</span><br><span class="line">|                        |containerd-shim-----&gt;|   runc    |   |</span><br><span class="line">|                        |               |     |           |   |</span><br><span class="line">|                        +---------------+     +-----------+   |</span><br><span class="line">|                                                              |</span><br><span class="line">+--------------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>dockerd 与 containerd 可以当做两层 nginx 代理，containerd-shim 是容器的监护人，而 runc 则是容器启动与命令执行的真正工具人。runc 干的事情也非常简单：按照用户指定的配置创建 NS，或者进入特定 NS，然后执行用户命令。说白了，创建容器就是新建 NS，然后在该 NS 内执行用户指定的命令。</p>
<p>按照上面介绍的背景知识，我们继续往下探索 containerd。幸运的是，借助 pprof，我们也可以描绘出 containerd 此时的运行蓝图：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">goroutine profile: total 430</span><br><span class="line">1 @ 0x7f6e55f82740 0x7f6e55f92616 0x7f6e56a8412c 0x7f6e56a83d6d 0x7f6e56a911bf 0x7f6e56ac6e3b 0x7f6e565093de 0x7f6e5650dd3b 0x7f6e5650392b 0x7f6e56b51216 0x7f6e564e5909 0x7f6e563ec76a 0x7f6e563f000a 0x7f6e563f6791 0x7f6e55fb0151</span><br><span class="line"><span class="comment">#	0x7f6e56a8412b	github.com/containerd/containerd/vendor/github.com/stevvooe/ttrpc.(*Client).dispatch+0x24b				/go/src/github.com/containerd/containerd/vendor/github.com/stevvooe/ttrpc/client.go:102</span></span><br><span class="line"><span class="comment">#	0x7f6e56a83d6c	github.com/containerd/containerd/vendor/github.com/stevvooe/ttrpc.(*Client).Call+0x15c					/go/src/github.com/containerd/containerd/vendor/github.com/stevvooe/ttrpc/client.go:73</span></span><br><span class="line"><span class="comment">#	0x7f6e56a911be	github.com/containerd/containerd/linux/shim/v1.(*shimClient).Start+0xbe							/go/src/github.com/containerd/containerd/linux/shim/v1/shim.pb.go:1745</span></span><br><span class="line"><span class="comment">#	0x7f6e56ac6e3a	github.com/containerd/containerd/linux.(*Process).Start+0x8a								/go/src/github.com/containerd/containerd/linux/process.go:125</span></span><br><span class="line"><span class="comment">#	0x7f6e565093dd	github.com/containerd/containerd/services/tasks.(*local).Start+0x14d							/go/src/github.com/containerd/containerd/services/tasks/local.go:187</span></span><br><span class="line"><span class="comment">#	0x7f6e5650dd3a	github.com/containerd/containerd/services/tasks.(*service).Start+0x6a							/go/src/github.com/containerd/containerd/services/tasks/service.go:72</span></span><br><span class="line"><span class="comment">#	0x7f6e5650392a	github.com/containerd/containerd/api/services/tasks/v1._Tasks_Start_Handler.func1+0x8a					/go/src/github.com/containerd/containerd/api/services/tasks/v1/tasks.pb.go:624</span></span><br><span class="line"><span class="comment">#	0x7f6e56b51215	github.com/containerd/containerd/vendor/github.com/grpc-ecosystem/go-grpc-prometheus.UnaryServerInterceptor+0xa5	/go/src/github.com/containerd/containerd/vendor/github.com/grpc-ecosystem/go-grpc-prometheus/server.go:29</span></span><br><span class="line"><span class="comment">#	0x7f6e564e5908	github.com/containerd/containerd/api/services/tasks/v1._Tasks_Start_Handler+0x168					/go/src/github.com/containerd/containerd/api/services/tasks/v1/tasks.pb.go:626</span></span><br><span class="line"><span class="comment">#	0x7f6e563ec769	github.com/containerd/containerd/vendor/google.golang.org/grpc.(*Server).processUnaryRPC+0x849				/go/src/github.com/containerd/containerd/vendor/google.golang.org/grpc/server.go:920</span></span><br><span class="line"><span class="comment">#	0x7f6e563f0009	github.com/containerd/containerd/vendor/google.golang.org/grpc.(*Server).handleStream+0x1319				/go/src/github.com/containerd/containerd/vendor/google.golang.org/grpc/server.go:1142</span></span><br><span class="line"><span class="comment">#	0x7f6e563f6790	github.com/containerd/containerd/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1+0xa0			/go/src/github.com/containerd/containerd/vendor/google.golang.org/grpc/server.go:637</span></span><br></pre></td></tr></table></figure></p>
<p>同样，我们仅保留了关键的协程信息，从上面的协程栈可以看出，containerd 阻塞在接收 exec 返回结果处，附上关键代码佐证：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Client)</span> <span class="title">dispatch</span><span class="params">(ctx context.Context, req *Request, resp *Response)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   errs := <span class="built_in">make</span>(<span class="keyword">chan</span> error, <span class="number">1</span>)</span><br><span class="line">   call := &amp;callRequest&#123;</span><br><span class="line">      req:  req,</span><br><span class="line">      resp: resp,</span><br><span class="line">      errs: errs,</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">select</span> &#123;</span><br><span class="line">   <span class="keyword">case</span> c.calls &lt;- call:</span><br><span class="line">   <span class="keyword">case</span> &lt;-c.done:</span><br><span class="line">      <span class="keyword">return</span> c.err</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">select</span> &#123;        <span class="comment">// 此处对应上面协程栈 /go/src/github.com/containerd/containerd/vendor/github.com/stevvooe/ttrpc/client.go:102</span></span><br><span class="line">   <span class="keyword">case</span> err := &lt;-errs:</span><br><span class="line">      <span class="keyword">return</span> filterCloseErr(err)</span><br><span class="line">   <span class="keyword">case</span> &lt;-c.done:</span><br><span class="line">      <span class="keyword">return</span> c.err</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>containerd 将请求传递至 containerd-shim 之后，一直在等待 containerd-shim 的返回。</p>
<p>正常情况下，如果我们能够按照调用链路逐个分析每个组件的协程调用栈信息，我们能够很快的定位问题所在。不幸的是，由于线上 docker 没有开启 debug 模式，我们无法收集 containerd-shim 的 pprof 信息，并且 runc 也没有开启 pprof。因此单纯依赖协程调用链路定位问题这条路被堵死了。</p>
<p>截至目前，我们已经收集了部分关键信息，同时也将问题排查范围更进一步地缩小在 containerd-shim 与 runc 之间。接下来我们换一种思路继续排查。</p>
<h3 id="3-2-进程排查"><a href="#3-2-进程排查" class="headerlink" title="3.2 进程排查"></a>3.2 进程排查</h3><p>当组件的运行状态无法继续获取时，我们转换一下思维，获取容器的运行状态，也即异常容器此时的进程状态。</p>
<p>既然 <code>docker ps</code> 执行正常，而 <code>docker inspect hang</code> 死，首先我们定位异常容器，命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps | grep -v NAME | awk <span class="string">'&#123;print $1&#125;'</span> | <span class="keyword">while</span> <span class="built_in">read</span> cid; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$cid</span>; docker inspect <span class="_">-f</span> &#123;&#123;.State.Pid&#125;&#125; <span class="variable">$cid</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>拿到异常容器的 ID 之后，我们就能扫描与该容器相关的所有进程：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root     11646  6655  0 Jun17 ?        00:01:04 docker-containerd-shim -namespace moby -workdir /home/docker_rt/containerd/daemon/io.containerd.runtime.v1.linux/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5 -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line">root     11680 11646  0 Jun17 ?        00:00:00 /dockerinit</span><br><span class="line">root     15581 11646  0 Jun17 ?        00:00:00 docker-runc --root /var/run/docker/runtime-runc/moby --log /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/log.json --log-format json <span class="built_in">exec</span> --process /tmp/runc-process616674997 --detach --pid-file /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/0594c5897a41d401e4d1d7ddd44dacdd316c7e7d53bfdae7f16b0f6b26fcbcda.pid bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5</span><br><span class="line">root     15638 15581  0 Jun17 ?        00:00:00 docker-runc init</span><br></pre></td></tr></table></figure></p>
<p>核心进程列表如上，简单备注下：</p>
<ul>
<li>6655：containerd 进程</li>
<li>11646：异常容器的 containerd-shim 进程</li>
<li>11680：异常容器的容器启动进程。在容器内查看，因 PID NS 的隔离，该进程 ID 是 1</li>
<li>15581：在异常容器内执行用户命令的进程</li>
<li>15638：在异常容器内执行用户命令时，进入容器 NS 的进程</li>
</ul>
<p>这里再补充一个背景知识：当我们启动容器时，首先会创建 <code>runc init</code> 进程，创建并进入新的容器 NS；而当我们在容器内执行命令时，首先也会创建 <code>runc init</code> 进程，进入容器的 NS。进入容器的隔离 NS 中，才会执行用户指定的命令。</p>
<p>面对上面的进程列表，我们无法直观地感受问题究竟由哪个进程引起。因此，我们还需要了解进程当前所处的状态。借助 strace，我们逐一展示进程的活动状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">// 11646 (container-shim)</span><br><span class="line">Process 11646 attached with 10 threads</span><br><span class="line">[pid 37342] epoll_pwait(5,  &lt;unfinished ...&gt;</span><br><span class="line">[pid 11656] futex(0x818cc0, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 11655] restart_syscall(&lt;... resuming interrupted call ...&gt; &lt;unfinished ...&gt;</span><br><span class="line">[pid 11654] futex(0x818bd8, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 11653] futex(0x7<span class="built_in">fc</span>730, FUTEX_WAKE, 1 &lt;unfinished ...&gt;</span><br><span class="line">[pid 11651] futex(0xc4200b4148, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 11650] futex(0xc420082948, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 11649] futex(0xc420082548, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 11647] restart_syscall(&lt;... resuming interrupted call ...&gt; &lt;unfinished ...&gt;</span><br><span class="line">[pid 11646] futex(0x7fd008, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 11653] &lt;... futex resumed&gt; )       = 0</span><br><span class="line">[pid 11647] &lt;... restart_syscall resumed&gt; ) = -1 EAGAIN (Resource temporarily unavailable)</span><br><span class="line">[pid 11653] epoll_<span class="built_in">wait</span>(4,  &lt;unfinished ...&gt;</span><br><span class="line">[pid 11647] pselect6(0, NULL, NULL, NULL, &#123;0, 20000&#125;, 0) = 0 (Timeout)</span><br><span class="line">[pid 11647] futex(0x7<span class="built_in">fc</span>730, FUTEX_WAIT, 0, &#123;60, 0&#125;</span><br><span class="line"></span><br><span class="line">// 11581 (runc <span class="built_in">exec</span>)</span><br><span class="line">Process 15581 attached with 7 threads</span><br><span class="line">[pid 15619] <span class="built_in">read</span>(6,  &lt;unfinished ...&gt;</span><br><span class="line">[pid 15592] futex(0xc4200be148, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15591] futex(0x7fd6d25f6238, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15590] futex(0xc420084d48, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15586] futex(0x7fd6d25f6320, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15584] restart_syscall(&lt;... resuming interrupted call ...&gt; &lt;unfinished ...&gt;</span><br><span class="line">[pid 15581] futex(0x7fd6d25d9b28, FUTEX_WAIT, 0, NULL</span><br><span class="line"></span><br><span class="line">// 11638 (runc init)</span><br><span class="line">Process 15638 attached with 7 threads</span><br><span class="line">[pid 15648] futex(0x7f512cea5320, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15647] futex(0x7f512cea5238, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15645] futex(0xc4200bc148, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15643] futex(0xc420082d48, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15642] futex(0xc420082948, FUTEX_WAIT, 0, NULL &lt;unfinished ...&gt;</span><br><span class="line">[pid 15639] restart_syscall(&lt;... resuming interrupted call ...&gt; &lt;unfinished ...&gt;</span><br><span class="line">[pid 15638] write(2, <span class="string">"/usr/local/go/src/runtime/proc.g"</span>..., 33</span><br></pre></td></tr></table></figure></p>
<p>从关联进程的活动状态，我们可以得出如下结论：</p>
<ul>
<li><code>runc exec</code> 在等待从 6 号 FD 读取数据</li>
<li><code>runc init</code> 在等待从 2 号 FD 写入数据</li>
</ul>
<p>这些 FD 究竟对应的是什么文件呢？我们借助 lsof 可以查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">// 11638 (runc init)</span><br><span class="line">COMMAND     PID USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME</span><br><span class="line">runc:[2:I 15638 root  cwd       DIR               0,41      192 1066743071 /</span><br><span class="line">runc:[2:I 15638 root  rtd       DIR               0,41      192 1066743071 /</span><br><span class="line">runc:[2:I 15638 root  txt       REG                0,4  7644224 1070360467 /memfd:runc_cloned:/proc/self/exe (deleted)</span><br><span class="line">runc:[2:I 15638 root  mem       REG                8,3  2107816    1053962 /usr/lib64/libc-2.17.so</span><br><span class="line">runc:[2:I 15638 root  mem       REG                8,3    19512    1054285 /usr/lib64/libdl-2.17.so</span><br><span class="line">runc:[2:I 15638 root  mem       REG                8,3   266688    1050626 /usr/lib64/libseccomp.so.2.3.1</span><br><span class="line">runc:[2:I 15638 root  mem       REG                8,3   142296    1055698 /usr/lib64/libpthread-2.17.so</span><br><span class="line">runc:[2:I 15638 root  mem       REG                8,3    27168    3024893 /usr/<span class="built_in">local</span>/gundam/gundam_client/preload/lib64/gundam_preload.so</span><br><span class="line">runc:[2:I 15638 root  mem       REG                8,3   164432    1054515 /usr/lib64/ld-2.17.so</span><br><span class="line">runc:[2:I 15638 root    0r     FIFO                0,8      0t0 1070361745 pipe</span><br><span class="line">runc:[2:I 15638 root    1w     FIFO                0,8      0t0 1070361746 pipe</span><br><span class="line">runc:[2:I 15638 root    2w     FIFO                0,8      0t0 1070361747 pipe</span><br><span class="line">runc:[2:I 15638 root    3u     unix 0xffff881ff8273000      0t0 1070361341 socket</span><br><span class="line">runc:[2:I 15638 root    5u  a_inode                0,9        0       7180 [eventpoll]</span><br><span class="line"></span><br><span class="line">// 11581 (runc <span class="built_in">exec</span>)</span><br><span class="line">COMMAND     PID USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME</span><br><span class="line">docker-ru 15581 root  cwd       DIR               0,18      120 1066743076 /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5</span><br><span class="line">docker-ru 15581 root  rtd       DIR                8,3     4096          2 /</span><br><span class="line">docker-ru 15581 root  txt       REG                8,3  7644224     919775 /usr/bin/docker-runc</span><br><span class="line">docker-ru 15581 root  mem       REG                8,3  2107816    1053962 /usr/lib64/libc-2.17.so</span><br><span class="line">docker-ru 15581 root  mem       REG                8,3    19512    1054285 /usr/lib64/libdl-2.17.so</span><br><span class="line">docker-ru 15581 root  mem       REG                8,3   266688    1050626 /usr/lib64/libseccomp.so.2.3.1</span><br><span class="line">docker-ru 15581 root  mem       REG                8,3   142296    1055698 /usr/lib64/libpthread-2.17.so</span><br><span class="line">docker-ru 15581 root  mem       REG                8,3    27168    3024893 /usr/<span class="built_in">local</span>/gundam/gundam_client/preload/lib64/gundam_preload.so</span><br><span class="line">docker-ru 15581 root  mem       REG                8,3   164432    1054515 /usr/lib64/ld-2.17.so</span><br><span class="line">docker-ru 15581 root    0r     FIFO                0,8      0t0 1070361745 pipe</span><br><span class="line">docker-ru 15581 root    1w     FIFO                0,8      0t0 1070361746 pipe</span><br><span class="line">docker-ru 15581 root    2w     FIFO                0,8      0t0 1070361747 pipe</span><br><span class="line">docker-ru 15581 root    3w      REG               0,18     5456 1066709902 /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/log.json</span><br><span class="line">docker-ru 15581 root    4u  a_inode                0,9        0       7180 [eventpoll]</span><br><span class="line">docker-ru 15581 root    6u     unix 0xffff881ff8275400      0t0 1070361342 socket</span><br><span class="line"></span><br><span class="line">// 11646 (container-shim)</span><br><span class="line">COMMAND     PID USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME</span><br><span class="line">docker-co 11646 root  cwd       DIR               0,18      120 1066743076 /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5</span><br><span class="line">docker-co 11646 root  rtd       DIR                8,3     4096          2 /</span><br><span class="line">docker-co 11646 root  txt       REG                8,3  4173632     919772 /usr/bin/docker-containerd-shim</span><br><span class="line">docker-co 11646 root    0r      CHR                1,3      0t0       2052 /dev/null</span><br><span class="line">docker-co 11646 root    1w      CHR                1,3      0t0       2052 /dev/null</span><br><span class="line">docker-co 11646 root    2w      CHR                1,3      0t0       2052 /dev/null</span><br><span class="line">docker-co 11646 root    3r     FIFO                0,8      0t0 1070361745 pipe</span><br><span class="line">docker-co 11646 root    4u  a_inode                0,9        0       7180 [eventpoll]</span><br><span class="line">docker-co 11646 root    5u  a_inode                0,9        0       7180 [eventpoll]</span><br><span class="line">docker-co 11646 root    6u     unix 0xffff881e8cac2800      0t0 1066743079 @/containerd-shim/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/shim.sock</span><br><span class="line">docker-co 11646 root    7u     unix 0xffff881e8cac3400      0t0 1066743968 @/containerd-shim/moby/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/shim.sock</span><br><span class="line">docker-co 11646 root    8r     FIFO                0,8      0t0 1066743970 pipe</span><br><span class="line">docker-co 11646 root    9w     FIFO                0,8      0t0 1070361745 pipe</span><br><span class="line">docker-co 11646 root   10r     FIFO                0,8      0t0 1066743971 pipe</span><br><span class="line">docker-co 11646 root   11u     FIFO               0,18      0t0 1066700778 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stdout</span><br><span class="line">docker-co 11646 root   12r     FIFO                0,8      0t0 1066743972 pipe</span><br><span class="line">docker-co 11646 root   13w     FIFO               0,18      0t0 1066700778 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stdout</span><br><span class="line">docker-co 11646 root   14u     FIFO               0,18      0t0 1066700778 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stdout</span><br><span class="line">docker-co 11646 root   15r     FIFO               0,18      0t0 1066700778 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stdout</span><br><span class="line">docker-co 11646 root   16u     FIFO               0,18      0t0 1066700779 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stderr</span><br><span class="line">docker-co 11646 root   17w     FIFO               0,18      0t0 1066700779 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stderr</span><br><span class="line">docker-co 11646 root   18u     FIFO               0,18      0t0 1066700779 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stderr</span><br><span class="line">docker-co 11646 root   19r     FIFO               0,18      0t0 1066700779 /run/docker/containerd/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/init-stderr</span><br><span class="line">docker-co 11646 root   20r     FIFO                0,8      0t0 1070361746 pipe</span><br><span class="line">docker-co 11646 root   26r     FIFO                0,8      0t0 1070361747 pipe</span><br></pre></td></tr></table></figure></p>
<p>有心人结合 strace 与 lsof 的结果，已经能够自己得出结论：</p>
<p><code>runc init</code> 往 2 号 FD 内写数据时阻塞，2 号 FD 对应的类型是 pipe 类型。而 linux pipe 有一个默认的数据大小，当写入的数据超过该 Size（这个 Size 可以借助 ulimit 获取）时，同时读端并未读取数据，写段就会被阻塞。总结一下：containerd-shim 启动 runc exec 去容器内执行用户命令，<code>runc exec</code> 启动 <code>runc init</code> 进入容器时，由于往 2 号 FD 写数据超过限制大小而被阻塞。当最底层的 <code>runc init</code> 被阻塞时，造成了调用链路上所有进程都被阻塞：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runc init → runc <span class="built_in">exec</span> → containerd-shim <span class="built_in">exec</span> → containerd <span class="built_in">exec</span> → dockerd <span class="built_in">exec</span></span><br></pre></td></tr></table></figure></p>
<p>问题定位至此，我们已经了解了 <code>docker hang</code> 死的原因。但是，现在我们还有如下问题并未解决：</p>
<ul>
<li>为什么 <code>runc init</code> 会往 2 号 FD （对应 go 语言的 os.Stderr) 中写入超过 linux pipe 大小限制的数据？</li>
<li>为什么 <code>runc init</code> 出现问题只发生在特定容器？</li>
</ul>
<p>如果常态下 <code>runc init</code> 就需要往 os.Stdout 或者 os.Stderr 中写入很多数据，那么所有容器的创建都应该有问题。所以，我们可以确定是该异常容器出现了什么未知原因，导致 <code>runc init</code> 非预期往 os.Stderr 写入了大量数据。而这被写入的数据就很有可能揭示非预期的异常。</p>
<p>所以，我们需要获取 <code>runc init</code> 当前正在写入的数据。由于 <code>runc init</code> 的 2 号 FD 是个匿名 pipe，我们无法使用常规文件读取的方式获取 pipe 内的数据。这里感谢鹤哥趟坑，找到了一种读取匿名 pipe 内容的方法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /proc/15638/fd/2</span></span><br><span class="line">runtime/cgo: pthread_create failed: Resource temporarily unavailable</span><br><span class="line">SIGABRT: abort</span><br><span class="line">PC=0x7f512b7365f7 m=0 sigcode=18446744073709551610</span><br><span class="line"></span><br><span class="line">goroutine 0 [idle]:</span><br><span class="line">runtime: unknown pc 0x7f512b7365f7</span><br><span class="line">stack: frame=&#123;sp:0x7ffe1121a658, fp:0x0&#125; stack=[0x7ffe0ae1bb28,0x7ffe1121ab50)</span><br><span class="line">00007ffe1121a558:  00007ffe1121a6d8  00007ffe1121a6b0</span><br><span class="line">00007ffe1121a568:  0000000000000001  00007f512c527660</span><br><span class="line">00007ffe1121a578:  00007f512c54d560  00007f512c54d208</span><br><span class="line">00007ffe1121a588:  00007f512c333e6f  0000000000000000</span><br><span class="line">00007ffe1121a598:  00007f512c527660  0000000000000005</span><br><span class="line">00007ffe1121a5a8:  0000000000000000  0000000000000001</span><br><span class="line">00007ffe1121a5b8:  00007f512c54d208  00007f512c528000</span><br><span class="line">00007ffe1121a5c8:  00007ffe1121a600  00007f512b704b0c</span><br><span class="line">00007ffe1121a5d8:  00007f512b7110c0  0000000000000000</span><br><span class="line">00007ffe1121a5e8:  00007f512c54d560  00007ffe1121a620</span><br><span class="line">00007ffe1121a5f8:  00007ffe1121a610  000000000f11ed7d</span><br><span class="line">00007ffe1121a608:  00007f512c550153  00000000ffffffff</span><br><span class="line">00007ffe1121a618:  00007f512c550a9b  00007f512b707d00</span><br><span class="line">00007ffe1121a628:  00007f512babc868  00007f512c9e9e5e</span><br><span class="line">00007ffe1121a638:  00007f512d3bb080  00000000000000f1</span><br><span class="line">00007ffe1121a648:  0000000000000011  0000000000000000</span><br><span class="line">00007ffe1121a658: &lt;00007f512b737ce8  0000000000000020</span><br><span class="line">00007ffe1121a668:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a678:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a688:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a698:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6a8:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6b8:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6c8:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6d8:  0000000000000000  00007f512babc868</span><br><span class="line">00007ffe1121a6e8:  00007f512c9e9e5e  00007f512d3bb080</span><br><span class="line">00007ffe1121a6f8:  00007f512c33f260  00007f512babc1c0</span><br><span class="line">00007ffe1121a708:  00007f512babc1c0  0000000000000001</span><br><span class="line">00007ffe1121a718:  00007f512babc243  00000000000000f1</span><br><span class="line">00007ffe1121a728:  00007f512b7787ec  0000000000000001</span><br><span class="line">00007ffe1121a738:  00007f512babc1c0  000000000000000a</span><br><span class="line">00007ffe1121a748:  00007f512b7e8a4d  000000000000000a</span><br><span class="line">runtime: unknown pc 0x7f512b7365f7</span><br><span class="line">stack: frame=&#123;sp:0x7ffe1121a658, fp:0x0&#125; stack=[0x7ffe0ae1bb28,0x7ffe1121ab50)</span><br><span class="line">00007ffe1121a558:  00007ffe1121a6d8  00007ffe1121a6b0</span><br><span class="line">00007ffe1121a568:  0000000000000001  00007f512c527660</span><br><span class="line">00007ffe1121a578:  00007f512c54d560  00007f512c54d208</span><br><span class="line">00007ffe1121a588:  00007f512c333e6f  0000000000000000</span><br><span class="line">00007ffe1121a598:  00007f512c527660  0000000000000005</span><br><span class="line">00007ffe1121a5a8:  0000000000000000  0000000000000001</span><br><span class="line">00007ffe1121a5b8:  00007f512c54d208  00007f512c528000</span><br><span class="line">00007ffe1121a5c8:  00007ffe1121a600  00007f512b704b0c</span><br><span class="line">00007ffe1121a5d8:  00007f512b7110c0  0000000000000000</span><br><span class="line">00007ffe1121a5e8:  00007f512c54d560  00007ffe1121a620</span><br><span class="line">00007ffe1121a5f8:  00007ffe1121a610  000000000f11ed7d</span><br><span class="line">00007ffe1121a608:  00007f512c550153  00000000ffffffff</span><br><span class="line">00007ffe1121a618:  00007f512c550a9b  00007f512b707d00</span><br><span class="line">00007ffe1121a628:  00007f512babc868  00007f512c9e9e5e</span><br><span class="line">00007ffe1121a638:  00007f512d3bb080  00000000000000f1</span><br><span class="line">00007ffe1121a648:  0000000000000011  0000000000000000</span><br><span class="line">00007ffe1121a658: &lt;00007f512b737ce8  0000000000000020</span><br><span class="line">00007ffe1121a668:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a678:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a688:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a698:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6a8:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6b8:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6c8:  0000000000000000  0000000000000000</span><br><span class="line">00007ffe1121a6d8:  0000000000000000  00007f512babc868</span><br><span class="line">00007ffe1121a6e8:  00007f512c9e9e5e  00007f512d3bb080</span><br><span class="line">00007ffe1121a6f8:  00007f512c33f260  00007f512babc1c0</span><br><span class="line">00007ffe1121a708:  00007f512babc1c0  0000000000000001</span><br><span class="line">00007ffe1121a718:  00007f512babc243  00000000000000f1</span><br><span class="line">00007ffe1121a728:  00007f512b7787ec  0000000000000001</span><br><span class="line">00007ffe1121a738:  00007f512babc1c0  000000000000000a</span><br><span class="line">00007ffe1121a748:  00007f512b7e8a4d  000000000000000a</span><br><span class="line"></span><br><span class="line">goroutine 1 [running, locked to thread]:</span><br><span class="line">runtime.systemstack_switch()</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/runtime/asm_amd64.s:363 fp=0xc4200a3ed0 sp=0xc4200a3ec8 pc=0x7f512c7281d0</span><br><span class="line">runtime.startTheWorld()</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/runtime/proc.go:978 +0x2f fp=0xc4200a3ee8 sp=0xc4200a3ed0 pc=0x7f512c70221f</span><br><span class="line">runtime.GOMAXPROCS(0x1, 0xc42013d9a0)</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/runtime/debug.go:30 +0xa0 fp=0xc4200a3f10 sp=0xc4200a3ee8 pc=0x7f512c6d9810</span><br><span class="line">main.init.0()</span><br><span class="line">	/go/src/github.com/opencontainers/runc/init.go:14 +0x61 fp=0xc4200a3f30 sp=0xc4200a3f10 pc=0x7f512c992801</span><br><span class="line">main.init()</span><br><span class="line">	&lt;autogenerated&gt;:1 +0x624 fp=0xc4200a3f88 sp=0xc4200a3f30 pc=0x7f512c9a1014</span><br><span class="line">runtime.main()</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/runtime/proc.go:186 +0x1d2 fp=0xc4200a3fe0 sp=0xc4200a3f88 pc=0x7f512c6ff962</span><br><span class="line">runtime.goexit()</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/runtime/asm_amd64.s:2361 +0x1 fp=0xc4200a3fe8 sp=0xc4200a3fe0 pc=0x7f512c72ad71</span><br><span class="line"></span><br><span class="line">goroutine 6 [syscall]:</span><br><span class="line">os/signal.signal_recv(0x0)</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/runtime/sigqueue.go:139 +0xa8</span><br><span class="line">os/signal.loop()</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/os/signal/signal_unix.go:22 +0x24</span><br><span class="line">created by os/signal.init.0</span><br><span class="line">	/usr/<span class="built_in">local</span>/go/src/os/signal/signal_unix.go:28 +0x43</span><br><span class="line"></span><br><span class="line">rax    0x0</span><br><span class="line">rbx    0x7f512babc868</span><br><span class="line">rcx    0xffffffffffffffff</span><br><span class="line">rdx    0x6</span><br><span class="line">rdi    0x271</span><br><span class="line">rsi    0x271</span><br><span class="line">rbp    0x7f512c9e9e5e</span><br><span class="line">rsp    0x7ffe1121a658</span><br><span class="line">r8     0xa</span><br><span class="line">r9     0x7f512c524740</span><br><span class="line">r10    0x8</span><br><span class="line">r11    0x206</span><br><span class="line">r12    0x7f512d3bb080</span><br><span class="line">r13    0xf1</span><br><span class="line">r14    0x11</span><br><span class="line">r15    0x0</span><br><span class="line">rip    0x7f512b7365f7</span><br><span class="line">rflags 0x206</span><br><span class="line">cs     0x33</span><br><span class="line">fs     0x0</span><br><span class="line">gs     0x0</span><br><span class="line"><span class="built_in">exec</span> failed: container_linux.go:348: starting container process caused <span class="string">"read init-p: connection reset by peer"</span></span><br></pre></td></tr></table></figure></p>
<p>额，<code>runc init</code> 因资源不足创建线程失败？？？这种输出显然不是 runc 的输出，而是 go runtime 非预期的输出内容。那么资源不足，究竟是什么资源类型资源不足呢？我们在结合 <code>/var/log/message</code> 日志分析：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Jun 17 03:18:17 host-xx kernel: runc:[2:INIT] invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=997</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: CPU: 14 PID: 12788 Comm: runc:[2:INIT] Tainted: G        W  OE  ------------ T 3.10.0-514.16.1.el7.stable.v1.4.x86_64 <span class="comment">#1</span></span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Hardware name: Inspur SA5212M4/YZMB-00370-107, BIOS 4.1.10 11/14/2016</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: ffff88103841dee0 00000000c4394691 ffff880263e4bcb8 ffffffff8168863d</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: ffff880263e4bd50 ffffffff81683585 ffff88203cc5e300 ffff880ee02b2380</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: 0000000000000001 0000000000000000 0000000000000000 0000000000000046</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Call Trace:</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff8168863d&gt;] dump_stack+0x19/0x1b</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81683585&gt;] dump_header+0x85/0x27f</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81185b06&gt;] ? find_lock_task_mm+0x56/0xc0</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81185fbe&gt;] oom_<span class="built_in">kill</span>_process+0x24e/0x3c0</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81093c2e&gt;] ? has_capability_noaudit+0x1e/0x30</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff811f4d91&gt;] mem_cgroup_oom_synchronize+0x551/0x580</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff811f41b0&gt;] ? mem_cgroup_charge_common+0xc0/0xc0</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81186844&gt;] pagefault_out_of_memory+0x14/0x90</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff816813fa&gt;] mm_fault_error+0x68/0x12b</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81694405&gt;] __<span class="keyword">do</span>_page_fault+0x395/0x450</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff816944f5&gt;] <span class="keyword">do</span>_page_fault+0x35/0x90</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [&lt;ffffffff81690708&gt;] page_fault+0x28/0x30</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: memory: usage 3145728kB, <span class="built_in">limit</span> 3145728kB, failcnt 14406932</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: memory+swap: usage 3145728kB, <span class="built_in">limit</span> 9007199254740988kB, failcnt 0</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: kmem: usage 3143468kB, <span class="built_in">limit</span> 9007199254740988kB, failcnt 0</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Memory cgroup stats <span class="keyword">for</span> /kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Memory cgroup stats <span class="keyword">for</span> /kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda/b761e05249245695278b3f409d2d6e5c6a5bff6995ff0cf44d03af4aa9764a30: cache:0KB rss:40KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:40KB inactive_file:0KB active_file:0KB unevictable:0KB</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Memory cgroup stats <span class="keyword">for</span> /kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda/1d1750ecc627cc5d60d80c071b2eb4d515ee8880c5b5136883164f08319869b0: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Memory cgroup stats <span class="keyword">for</span> /kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5: cache:0KB rss:2220KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2140KB inactive_file:0KB active_file:0KB unevictable:0KB</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Memory cgroup stats <span class="keyword">for</span> /kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5/super-agent: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [30598]     0 30598      255        1       4        0          -998 pause</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [11680]     0 11680   164833     1118      20        0           997 dockerinit</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: [12788]     0 12788   150184     1146      23        0           997 runc:[2:INIT]</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: oom-kill:,cpuset=bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5,mems_allowed=0-1,oom_memcg=/kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda,task_memcg=/kubepods/burstable/pod6c4333b3<span class="_">-a</span>663-11ea-b39f-6c92bf85beda/bbd5e4b5f9c13666dd0ec7ff7afb2c4c2b0ede40a4adf1de43cc31c606f283f5,task=runc:[2:INIT],pid=12800,uid=0</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Memory cgroup out of memory: Kill process 12800 (runc:[2:INIT]) score 997 or sacrifice child</span><br><span class="line">Jun 17 03:18:17 host-xx kernel: Killed process 12788 (runc:[2:INIT]) total-vm:600736kB, anon-rss:3296kB, file-rss:276kB, shmem-rss:1012kB</span><br></pre></td></tr></table></figure></p>
<p>在 <code>/var/log/message</code> 中可以找到该容器在大约 1 个月前大量的 OOM 日志记录，同时时间也基本匹配。</p>
<p>所以总结下，在一个非常关键的时间节点，<code>runc init</code> 由于内存资源不足，创建线程失败，触发 go runtime 的非预期输出，进而造成 <code>runc init</code> 阻塞在写 pipe 操作。</p>
<p>定位至此，问题的全貌已经基本描述清楚。但是我们还有一个疑问，既然 <code>runc init</code> 再往 pipe 中写数据，难道没有其他进程来读取这个内容吗？</p>
<p>大家还记得上面 lsof 执行的结果吗？有心人一定发现了该 pipe 的读端是谁了，对，就是 containerd-shim，对应的 pipe 的 inode 编号为 1070361747。那么，为什么 containerd-shim 没有来读 pipe 里面的内容呢？我们结合代码来分析：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *execProcess)</span> <span class="title">start</span><span class="params">(ctx context.Context)</span> <span class="params">(err error)</span></span> &#123;</span><br><span class="line">   ......</span><br><span class="line">   <span class="keyword">if</span> err := e.parent.runtime.Exec(ctx, e.parent.id, e.spec, opts); err != <span class="literal">nil</span> &#123;   <span class="comment">// 执行 runc init</span></span><br><span class="line">      <span class="built_in">close</span>(e.waitBlock)</span><br><span class="line">      <span class="keyword">return</span> e.parent.runtimeError(err, <span class="string">"OCI runtime exec failed"</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   ......</span><br><span class="line">   <span class="keyword">else</span> <span class="keyword">if</span> !e.stdio.IsNull() &#123;</span><br><span class="line">      fifoCtx, cancel := context.WithTimeout(ctx, <span class="number">15</span>*time.Second)</span><br><span class="line">      <span class="keyword">defer</span> cancel()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> err := copyPipes(fifoCtx, e.io, e.stdio.Stdin, e.stdio.Stdout, e.stdio.Stderr, &amp;e.wg, &amp;copyWaitGroup); err != <span class="literal">nil</span> &#123;   <span class="comment">// 读 pipe</span></span><br><span class="line">         <span class="keyword">return</span> errors.Wrap(err, <span class="string">"failed to start io pipe copy"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *Runc)</span> <span class="title">Exec</span><span class="params">(context context.Context, id <span class="keyword">string</span>, spec specs.Process, opts *ExecOpts)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">   ......</span><br><span class="line">   cmd := r.command(context, <span class="built_in">append</span>(args, id)...)</span><br><span class="line">   <span class="keyword">if</span> opts != <span class="literal">nil</span> &amp;&amp; opts.IO != <span class="literal">nil</span> &#123;</span><br><span class="line">      opts.Set(cmd)</span><br><span class="line">   &#125;</span><br><span class="line">   ......</span><br><span class="line">   ec, err := Monitor.Start(cmd)</span><br><span class="line">   ......</span><br><span class="line">   status, err := Monitor.Wait(cmd, ec)</span><br><span class="line">   ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>额，containerd-shim 的设计是，等待 <code>runc init</code> 执行完成之后，再来读取 pipe 中的内容。但是此时的 <code>runc init</code> 由于非预期的写入数据量比较大，被阻塞在了写 pipe 操作处。完美的死锁。</p>
<p>终于，本次 <code>docker hang</code> 死问题的核心脉络都已清楚。接下来我们聊聊怎么解决方案。</p>
<h2 id="4-解决方案"><a href="#4-解决方案" class="headerlink" title="4 解决方案"></a>4 解决方案</h2><p>当大家了解了 <code>docker hang</code> 死的成因之后，我们可以针对性的提出如下解决办法。</p>
<h3 id="4-1-最直观的办法"><a href="#4-1-最直观的办法" class="headerlink" title="4.1 最直观的办法"></a>4.1 最直观的办法</h3><p>既然 <code>docker exec</code> 可能会引起 <code>docker hang</code> 死，那么我们禁用系统中所有的 <code>docker exec</code> 操作即可。最典型的是 kubelet 的 probe，当前我们默认给所有 Pod 添加了 ReadinessProbe，并且是以 exec 的形式进入容器内执行命令。我们调整 kubelet 的探测行为，修改为 tcp 或者 http probe 即可。</p>
<p>这里虽然改动不大，但是涉及业务容器的改造成本太大了，如何迁移存量集群是个大问题。</p>
<h3 id="4-2-最根本的办法"><a href="#4-2-最根本的办法" class="headerlink" title="4.2 最根本的办法"></a>4.2 最根本的办法</h3><p>既然当前 containerd-shim 读 pipe 需要等待 <code>runc exec</code> 执行完毕，如果我们将读 pipe 的操作提前至 <code>runc exec</code> 命令执行之前，理论上也可以避免死锁。</p>
<p>同样。这种方案的升级成本太高了，升级 containerd-shim 时需要重启存量的所有容器，这个方案基本不可能通过。</p>
<h3 id="4-3-最简单的办法"><a href="#4-3-最简单的办法" class="headerlink" title="4.3 最简单的办法"></a>4.3 最简单的办法</h3><p>既然 <code>runc init</code> 阻塞在写 pipe，我们主动读取 pipe 内的内容，也能让 <code>runc init</code> 顺利退出。</p>
<p>在将本解决方案自动化的过程中，如何能够识别如 <code>docker hang</code> 死是由于写 pipe 导致的，是一个小小的挑战。但是相对于以上两种解决方案，我认为还是值得一试，毕竟影响面微乎其微。</p>
<p>来源：www.likakuli.com</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用 Linux tracepoint、perf 和 eBPF 跟踪数据包]]></title>
      <url>http://team.jiunile.com/blog/2020/10/trace-packet-with-perf-ebpf.html</url>
      <content type="html"><![CDATA[<p>一段时间以来，我一直在寻找 Linux 上的底层网络调试（debug）工具。</p>
<p>Linux 允许在主机上用<strong>虚拟网卡</strong>（virtual interface）和<strong>网络命名空间</strong>（network namespace）构建复杂的网络。但出现故障时，排障（troubleshooting）相当痛苦。如果是 3 层路由问题，<code>mtr</code> 可以排上用场。但如果是更底层的问题，我通常只能手动检查每个网 卡/网桥/网络命名空间/iptables 规则，用 <code>tcpdump</code> 抓一些包，以确定到底是什么状况。如 果不了解故障之前的网络设置，那感觉就像在走迷宫。<br><a id="more"></a></p>
<h2 id="1-开篇"><a href="#1-开篇" class="headerlink" title="1 开篇"></a>1 开篇</h2><h3 id="1-1-逃离迷宫：上帝视角"><a href="#1-1-逃离迷宫：上帝视角" class="headerlink" title="1.1 逃离迷宫：上帝视角"></a>1.1 逃离迷宫：上帝视角</h3><p>逃离迷宫的一种方式是在<strong>迷宫内</strong>不断左右尝试（exploring），寻找通往出口的路 。当你在玩迷宫游戏（置身迷宫内）时，你只能如此。不过，如果不是在游戏内，那还有另 一种方式：<strong>转换视角，高空俯视</strong>。</p>
<p>用 Linux 术语来说，就是转换到<strong>内核视角</strong>（the kernel point of view）。在这种视 角下，<strong>网络命名空间不再是容器（”containers”），而只是一些标签（labels）。内核、 数据包、网卡等此时都是“肉眼可见”的对象（objects）</strong>。</p>
<blockquote>
<p>原文注：上面的 “containers” 我加了引号，因为从技术上说，网络命名空间是 构成 Linux 容器的核心部件之一。</p>
</blockquote>
<h3 id="1-2-网络跟踪：渴求利器"><a href="#1-2-网络跟踪：渴求利器" class="headerlink" title="1.2 网络跟踪：渴求利器"></a>1.2 网络跟踪：渴求利器</h3><p>所以我想要的是这样一个工具，它可以直接告诉我 “嗨，我看到你的包了：它从<strong>属于这个网络命名空间的这个网卡</strong>上发出来，然后<strong>依次经过这些函数</strong>”。</p>
<p>本质上，我想要的是一个 <strong>2 层的 <code>mtr</code></strong>。这样的工具存在吗？不存在我们就造一个！</p>
<p>本文结束时，我们将拥有一个简单、易于使用的底层<strong>网络包跟踪器</strong>（packet tracker ）。如果你 ping 本机上的一个 Docker 容器，它会显示类似如下信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping -4 172.17.0.2</span></span><br><span class="line">[  4026531957]          docker0 request <span class="comment">#17146.001 172.17.0.1 -&gt; 172.17.0.2</span></span><br><span class="line">[  4026531957]      vetha373ab6 request <span class="comment">#17146.001 172.17.0.1 -&gt; 172.17.0.2</span></span><br><span class="line">[  4026532258]             eth0 request <span class="comment">#17146.001 172.17.0.1 -&gt; 172.17.0.2</span></span><br><span class="line">[  4026532258]             eth0   reply <span class="comment">#17146.001 172.17.0.2 -&gt; 172.17.0.1</span></span><br><span class="line">[  4026531957]      vetha373ab6   reply <span class="comment">#17146.001 172.17.0.2 -&gt; 172.17.0.1</span></span><br><span class="line">[  4026531957]          docker0   reply <span class="comment">#17146.001 172.17.0.2 -&gt; 172.17.0.1</span></span><br></pre></td></tr></table></figure></p>
<h3 id="1-3-巨人肩膀：perf-eBPF"><a href="#1-3-巨人肩膀：perf-eBPF" class="headerlink" title="1.3 巨人肩膀：perf/eBPF"></a>1.3 巨人肩膀：perf/eBPF</h3><p>在本文中，我将聚焦两个跟踪工具：<code>perf</code> 和 <code>eBPF</code>。</p>
<p><code>perf</code> 是 Linux 上的最重要的性能分析工具之一。它和内核出自同一个源码树（source tree），但编译需要针对指定的内核版本。<code>perf</code> 可以跟踪内核，也可以跟踪用户程序， 还可用于采样或者设置跟踪点。<strong>可以把它想象成开销更低，但功能更强大的 <code>strace</code></strong>。 本文只会使用非常简单的 <code>perf</code> 命令。想了解更多，强烈建议访问 <a href="http://www.brendangregg.com/perf.html" target="_blank" rel="external">Brendan Gregg</a> 的博客。</p>
<p><code>eBPF</code> 是 Linux 内核新近加入的，其中 e 是 <code>extended</code> 的缩写。从名字可以看出，它 是 BPF（Berkeley Packet Filter）字节码过滤器的增强版，后者是 BSD family 的网络包 过滤工具。在 Linux 上，eBPF 可以在运行中的内核（live kernel）中安全地执行任何平 台无关（platform independent）代码，只要这些代码满足一些安全前提。例如，在程序执 行之前必须验证内存访问合法性，而且要能证明程序会在有限时间内退出。如果内核无法验 证这些条件，那即使 eBPF 代码是安全的并且确定会退出，它也仍然会被拒绝。</p>
<p>eBPF 程序可用于 <strong>QOS 网络分类器</strong>（network classifier）、<strong>XDP</strong>（eXpress Data Plane） 很底层的网络功能和过滤功能组件、<strong>跟踪代理</strong>（tracing agent），以及其他很多方面。 <strong>任何在 <code>/proc/kallsyms</code> 导出的符号（内核函数）或者跟踪点（tracepoints）， 都可以插入 eBPF 跟踪点</strong>（tracing probes）。</p>
<p>本文将主要关注 attach 到 tracepoints 的跟踪代理（tracing agents attached to tracepoints）。如果想看一些在内核函数埋点进行跟踪的例子，或者入门级介绍，建议阅读这篇 <a href="https://blog.yadutaf.fr/2016/03/30/turn-any-syscall-into-event-introducing-ebpf-kernel-probes/" target="_blank" rel="external">eBPF 文章</a>。</p>
<h2 id="Perf"><a href="#Perf" class="headerlink" title="Perf"></a>Perf</h2><p>本文只会使用 perf 做非常简单的内核跟踪。</p>
<h3 id="2-1-安装-perf"><a href="#2-1-安装-perf" class="headerlink" title="2.1 安装 perf"></a>2.1 安装 perf</h3><p>我的环境基于 Ubuntu 17.04 （Zesty）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install linux-tools-generic</span><br><span class="line">$ perf <span class="comment"># test perf</span></span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-测试环境"><a href="#2-2-测试环境" class="headerlink" title="2.2 测试环境"></a>2.2 测试环境</h3><p>我们将使用 4 个 IP，其中 2 个为外部可路由网段（<code>192.168</code>）：</p>
<ol>
<li>localhost，IP <code>127.0.0.1</code></li>
<li>一个干净的容器，IP <code>172.17.0.2</code></li>
<li>我的手机，通过 USB 连接，IP <code>192.168.42.129</code></li>
<li>我的手机，通过 WiFi 连接，IP <code>192.168.43.1</code></li>
</ol>
<h3 id="2-3-初体验：跟踪-ping-包"><a href="#2-3-初体验：跟踪-ping-包" class="headerlink" title="2.3 初体验：跟踪 ping 包"></a>2.3 初体验：跟踪 ping 包</h3><p><code>perf trace</code> 是 <code>perf</code> 子命令，能够跟踪 packet 路径，默认输出类似于 <code>strace</code>（头 信息少很多）。</p>
<p>跟踪 ping 向 <code>172.17.0.2</code> 容器的包，这里我们只关心 <code>net</code> 事件，忽略系统调用信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ sudo perf trace --no-syscalls --event <span class="string">'net:*'</span> ping 172.17.0.2 -c1 &gt; /dev/null</span><br><span class="line">     0.000 net:net_dev_queue:dev=docker0 skbaddr=0xffff96d481988700 len=98)</span><br><span class="line">     0.008 net:net_dev_start_xmit:dev=docker0 queue_mapping=0 skbaddr=0xffff96d481988700 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=0 len=98 data_len=0 network_offset=14 transport_offset_valid=1 transport_offset=34 tx_flags=0 gso_size=0 gso_segs=0 gso_<span class="built_in">type</span>=0)</span><br><span class="line">     0.014 net:net_dev_queue:dev=veth79215ff skbaddr=0xffff96d481988700 len=98)</span><br><span class="line">     0.016 net:net_dev_start_xmit:dev=veth79215ff queue_mapping=0 skbaddr=0xffff96d481988700 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=0 len=98 data_len=0 network_offset=14 transport_offset_valid=1 transport_offset=34 tx_flags=0 gso_size=0 gso_segs=0 gso_<span class="built_in">type</span>=0)</span><br><span class="line">     0.020 net:netif_rx:dev=eth0 skbaddr=0xffff96d481988700 len=84)</span><br><span class="line">     0.022 net:net_dev_xmit:dev=veth79215ff skbaddr=0xffff96d481988700 len=98 rc=0)</span><br><span class="line">     0.024 net:net_dev_xmit:dev=docker0 skbaddr=0xffff96d481988700 len=98 rc=0)</span><br><span class="line">     0.027 net:netif_receive_skb:dev=eth0 skbaddr=0xffff96d481988700 len=84)</span><br><span class="line">     0.044 net:net_dev_queue:dev=eth0 skbaddr=0xffff96d481988b00 len=98)</span><br><span class="line">     0.046 net:net_dev_start_xmit:dev=eth0 queue_mapping=0 skbaddr=0xffff96d481988b00 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=0 len=98 data_len=0 network_offset=14 transport_offset_valid=1 transport_offset=34 tx_flags=0 gso_size=0 gso_segs=0 gso_<span class="built_in">type</span>=0)</span><br><span class="line">     0.048 net:netif_rx:dev=veth79215ff skbaddr=0xffff96d481988b00 len=84)</span><br><span class="line">     0.050 net:net_dev_xmit:dev=eth0 skbaddr=0xffff96d481988b00 len=98 rc=0)</span><br><span class="line">     0.053 net:netif_receive_skb:dev=veth79215ff skbaddr=0xffff96d481988b00 len=84)</span><br><span class="line">     0.060 net:netif_receive_skb_entry:dev=docker0 napi_id=0x3 queue_mapping=0 skbaddr=0xffff96d481988b00 vlan_tagged=0 vlan_proto=0x0000 vlan_tci=0x0000 protocol=0x0800 ip_summed=2 <span class="built_in">hash</span>=0x00000000 l4_<span class="built_in">hash</span>=0 len=84 data_len=0 truesize=768 mac_header_valid=1 mac_header=-14 nr_frags=0 gso_size=0 gso_<span class="built_in">type</span>=0)</span><br><span class="line">     0.061 net:netif_receive_skb:dev=docker0 skbaddr=0xffff96d481988b00 len=84)</span><br></pre></td></tr></table></figure></p>
<p>只保留事件名和 <code>skbaddr</code>，看起来清晰很多：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net_dev_queue           dev=docker0     skbaddr=0xffff96d481988700</span><br><span class="line">net_dev_start_xmit      dev=docker0     skbaddr=0xffff96d481988700</span><br><span class="line">net_dev_queue           dev=veth79215ff skbaddr=0xffff96d481988700</span><br><span class="line">net_dev_start_xmit      dev=veth79215ff skbaddr=0xffff96d481988700</span><br><span class="line">netif_rx                dev=eth0        skbaddr=0xffff96d481988700</span><br><span class="line">net_dev_xmit            dev=veth79215ff skbaddr=0xffff96d481988700</span><br><span class="line">net_dev_xmit            dev=docker0     skbaddr=0xffff96d481988700</span><br><span class="line">netif_receive_skb       dev=eth0        skbaddr=0xffff96d481988700</span><br><span class="line"></span><br><span class="line">net_dev_queue           dev=eth0        skbaddr=0xffff96d481988b00</span><br><span class="line">net_dev_start_xmit      dev=eth0        skbaddr=0xffff96d481988b00</span><br><span class="line">netif_rx                dev=veth79215ff skbaddr=0xffff96d481988b00</span><br><span class="line">net_dev_xmit            dev=eth0        skbaddr=0xffff96d481988b00</span><br><span class="line">netif_receive_skb       dev=veth79215ff skbaddr=0xffff96d481988b00</span><br><span class="line">netif_receive_skb_entry dev=docker0     skbaddr=0xffff96d481988b00</span><br><span class="line">netif_receive_skb       dev=docker0     skbaddr=0xffff96d481988b00</span><br></pre></td></tr></table></figure></p>
<p>这里面有很多信息。</p>
<p>首先注意，<strong><code>skbaddr</code> 在中间变了（0xffff96d481988700 -&gt; 0xffff96d481988b00）</strong>。变的这里，就是<strong>生成了 ICMP echo reply 包</strong>，并作为应答包发送的地方。接下来的 时间，这个包的 <code>skbaddr</code> 保持不变，说明没有 copy。copy 非常耗时。</p>
<p>其次，我们可以清楚地看到 <strong>packet 在内核的传输路径</strong>：</p>
<ol>
<li><code>docker0</code> 网桥</li>
<li>veth pair 的宿主机端（<code>veth79215ff</code>)</li>
<li>veth pair 的容器端（容器里的 <code>eth0</code>）</li>
<li>接下来是相反的返回路径</li>
</ol>
<p><strong>至此，虽然我们还没有看到网络命名空间，但已经得到了一个不错的全局视图</strong>。</p>
<h3 id="2-4-进阶：选择跟踪点"><a href="#2-4-进阶：选择跟踪点" class="headerlink" title="2.4 进阶：选择跟踪点"></a>2.4 进阶：选择跟踪点</h3><p>上面的信息有些杂，还有很多重复。我们可以选择几个最合适的跟踪点，使得输出看起来 更清爽。要查看所有可用的网络跟踪点，执行 <code>perf list</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo perf list <span class="string">'net:*'</span></span><br></pre></td></tr></table></figure></p>
<p>这个命令会列出 <code>tracepoint</code> 列表，名字类似于 <code>net:netif_rx</code>。<strong>冒号前面是事件类型 ，后面是事件名字</strong>。这里我选择了 4 个：</p>
<ul>
<li>net_dev_queue</li>
<li>netif_receive_skb_entry</li>
<li>netif_rx</li>
<li>napi_gro_receive_entry</li>
</ul>
<p>效果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo perf trace --no-syscalls           \</span><br><span class="line">    --event <span class="string">'net:net_dev_queue'</span>           \</span><br><span class="line">    --event <span class="string">'net:netif_receive_skb_entry'</span> \</span><br><span class="line">    --event <span class="string">'net:netif_rx'</span>                \</span><br><span class="line">    --event <span class="string">'net:napi_gro_receive_entry'</span>  \</span><br><span class="line">    ping 172.17.0.2 -c1 &gt; /dev/null</span><br><span class="line">       0.000 net:net_dev_queue:dev=docker0 skbaddr=0xffff8e847720a900 len=98)</span><br><span class="line">       0.010 net:net_dev_queue:dev=veth7781d5c skbaddr=0xffff8e847720a900 len=98)</span><br><span class="line">       0.014 net:netif_rx:dev=eth0 skbaddr=0xffff8e847720a900 len=84)</span><br><span class="line">       0.034 net:net_dev_queue:dev=eth0 skbaddr=0xffff8e849cb8<span class="built_in">cd</span>00 len=98)</span><br><span class="line">       0.036 net:netif_rx:dev=veth7781d5c skbaddr=0xffff8e849cb8<span class="built_in">cd</span>00 len=84)</span><br><span class="line">       0.045 net:netif_receive_skb_entry:dev=docker0 napi_id=0x1 queue_mapping=0</span><br></pre></td></tr></table></figure></p>
<p>漂亮！</p>
<h2 id="3-eBPF"><a href="#3-eBPF" class="headerlink" title="3 eBPF"></a>3 eBPF</h2><p>前面介绍的内容已经可以满足大部分 tracing 场景的需求了。如果你只是想学习如何在 Linux 上跟踪一个 packet 的传输路径，那到此已经足够了。但如果想跟更进一步，学习如 何写一个自定义的过滤器，跟踪网络命名空间、源 IP、目的 IP 等信息，请继续往下读。</p>
<h3 id="3-1-eBPF-和-kprobes"><a href="#3-1-eBPF-和-kprobes" class="headerlink" title="3.1 eBPF 和 kprobes"></a>3.1 eBPF 和 kprobes</h3><p>从 Linux 内核 4.7 开始，eBPF 程序可以 attach 到内核跟踪点（kernel tracepoints） 。在此之前，要完成类似的工作，只能用 kprobes 之类的工具 attach 到<strong>导出的内核函数</strong>（exported kernel sysbols）。后者虽然可以完成工作，但存在很多不足：</p>
<ol>
<li>内核的内部（internal）API 不稳定</li>
<li>出于性能考虑，大部分网络相关的内层函数（inner functions）都是内联或者静态的（ inlined or static），两者都不可探测</li>
<li>找出调用某个函数的所有地方是相当乏味的，有时所需的字段数据不全具备</li>
</ol>
<p>这篇博客的早期版本使用了 kprobes，但结果并不是太好。 现在，诚实地说，通过内核 tracepoints 访问数据比通过 kprobe 要更加乏味。我尽量保持本文简洁，如果你想了解本文稍老的版本，可以访问这里<a href="https://blog.yadutaf.fr/2016/03/30/turn-any-syscall-into-event-introducing-ebpf-kernel-probes/" target="_blank" rel="external">英文</a>。</p>
<h3 id="3-2-安装"><a href="#3-2-安装" class="headerlink" title="3.2 安装"></a>3.2 安装</h3><p>我不是徒手汇编控（fans of handwritten assembly），因此接下来将使用 <code>bcc</code>。<code>bcc</code> 是一个灵活强大的工具，允许用受限的 C 语法（restricted C）写内核探测代码，然后用 Python 在用户态做控制。这种方式对于生产环境算是重量级，但对开发来说非常完美。</p>
<p><strong>注意：eBPF 需要 Linux Kernel 4.7+</strong>。</p>
<p>Ubuntu 17.04 <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md" target="_blank" rel="external">安装 (GitHub)</a> <code>bcc</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install dependencies</span></span><br><span class="line">$ sudo apt install bison build-essential cmake flex git libedit-dev python zlib1g-dev libelf-dev libllvm4.0 llvm-dev libclang-dev luajit luajit-5.1-dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># Grab the sources</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/iovisor/bcc.git</span><br><span class="line">$ mkdir bcc/build</span><br><span class="line">$ <span class="built_in">cd</span> bcc/build</span><br><span class="line">$ cmake .. -DCMAKE_INSTALL_PREFIX=/usr</span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure></p>
<h3 id="3-3-自定义跟踪器：Hello-World"><a href="#3-3-自定义跟踪器：Hello-World" class="headerlink" title="3.3 自定义跟踪器：Hello World"></a>3.3 自定义跟踪器：Hello World</h3><p>接下来我们从一个简单的 hello world 例子展示如何在底层打点。我们还是用上一篇 文章里选择的四个点：</p>
<ul>
<li><code>net_dev_queue</code></li>
<li><code>netif_receive_skb_entry</code></li>
<li><code>netif_rx</code></li>
<li><code>napi_gro_receive_entry</code></li>
</ul>
<p>每当网络包经过这些点，我们的处理逻辑就会触发。为保持简单，我们的处理逻辑只是将程 序的 <code>comm</code> 字段（16 字节）发送出来（到用户空间程序），这个字段里存的是发 送相应的网络包的程序的名字。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bcc/proto.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/sched.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Event structure</span></span><br><span class="line"><span class="keyword">struct</span> <span class="keyword">route_evt_t</span> &#123;</span><br><span class="line">        <span class="keyword">char</span> comm[TASK_COMM_LEN];</span><br><span class="line">&#125;;</span><br><span class="line">BPF_PERF_OUTPUT(route_evt);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">do_trace</span><span class="params">(<span class="keyword">void</span>* ctx, <span class="keyword">struct</span> sk_buff* skb)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="comment">// Built event for userland</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="keyword">route_evt_t</span> evt = &#123;&#125;;</span><br><span class="line">    bpf_get_current_comm(evt.comm, TASK_COMM_LEN);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Send event to userland</span></span><br><span class="line">    route_evt.perf_submit(ctx, &amp;evt, <span class="keyword">sizeof</span>(evt));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line">  * Attach to Kernel Tracepoints</span><br><span class="line">  */</span></span><br><span class="line">TRACEPOINT_PROBE(net, netif_rx) &#123;</span><br><span class="line">    <span class="keyword">return</span> do_trace(args, (<span class="keyword">struct</span> sk_buff*)args-&gt;skbaddr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(net, net_dev_queue) &#123;</span><br><span class="line">    <span class="keyword">return</span> do_trace(args, (<span class="keyword">struct</span> sk_buff*)args-&gt;skbaddr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(net, napi_gro_receive_entry) &#123;</span><br><span class="line">    <span class="keyword">return</span> do_trace(args, (<span class="keyword">struct</span> sk_buff*)args-&gt;skbaddr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(net, netif_receive_skb_entry) &#123;</span><br><span class="line">    <span class="keyword">return</span> do_trace(args, (<span class="keyword">struct</span> sk_buff*)args-&gt;skbaddr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，程序 attach 到 4 个 tracepoint，并会访问 <code>skbaddr</code> 字段，将其传给处理 逻辑函数，这个函数现在只是将程序名字发送出来。你可能会有疑问，<code>args-&gt;skbaddr</code> 是 哪里来的？答案是，每次用 <code>TRACEPONT_PROBE</code> 定义一个 tracepoint，<code>bcc</code> 就会为其自 动生成 <code>args</code> 参数，由于它是动态生成的，因此要查看它的定义不太容易。</p>
<p>不过，有另外一种简单的方式可以查看。在 Linux 上每个 tracepoint 都对应一个 <code>/sys/kernel/debug/tracing/events</code> 条目。例如，查看 <code>net:netif_rx</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/kernel/debug/tracing/events/net/netif_rx/format</span><br><span class="line">name: netif_rx</span><br><span class="line">ID: 1183</span><br><span class="line">format:</span><br><span class="line">	field:unsigned short common_<span class="built_in">type</span>;         offset:0; size:2; signed:0;</span><br><span class="line">	field:unsigned char common_flags;         offset:2; size:1; signed:0;</span><br><span class="line">	field:unsigned char common_preempt_count; offset:3; size:1; signed:0;</span><br><span class="line">	field:int common_pid;                     offset:4; size:4; signed:1;</span><br><span class="line"></span><br><span class="line">	field:void * skbaddr;         offset:8;  size:8; signed:0;</span><br><span class="line">	field:unsigned int len;       offset:16; size:4; signed:0;</span><br><span class="line">	field:__data_loc char[] name; offset:20; size:4; signed:1;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> fmt: <span class="string">"dev=%s skbaddr=%p len=%u"</span>, __get_str(name), REC-&gt;skbaddr, REC-&gt;len</span><br></pre></td></tr></table></figure></p>
<p>注意最后一行 <code>print fmt</code>，这正是 <code>perf trace</code> 打印相应消息的格式。</p>
<p>在底层插入这样的探测点之后，我们再写个 Python 脚本，接收内核发出来的消息，每个 eBPF 发出的数据都打印一行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> inet_ntop</span><br><span class="line"><span class="keyword">from</span> bcc <span class="keyword">import</span> BPF</span><br><span class="line"><span class="keyword">import</span> ctypes <span class="keyword">as</span> ct</span><br><span class="line"></span><br><span class="line">bpf_text = <span class="string">'''&lt;SEE CODE SNIPPET ABOVE&gt;'''</span></span><br><span class="line"></span><br><span class="line">TASK_COMM_LEN = <span class="number">16</span> <span class="comment"># linux/sched.h</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RouteEvt</span><span class="params">(ct.Structure)</span>:</span></span><br><span class="line">    _fields_ = [</span><br><span class="line">        (<span class="string">"comm"</span>,    ct.c_char * TASK_COMM_LEN),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">event_printer</span><span class="params">(cpu, data, size)</span>:</span></span><br><span class="line">    <span class="comment"># Decode event</span></span><br><span class="line">    event = ct.cast(data, ct.POINTER(RouteEvt)).contents</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print event</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Just got a packet from %s"</span> % (event.comm)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    b = BPF(text=bpf_text)</span><br><span class="line">    b[<span class="string">"route_evt"</span>].open_perf_buffer(event_printer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        b.kprobe_poll()</span><br></pre></td></tr></table></figure></p>
<p>现在可以测试了，注意需要 root 权限。</p>
<p><strong>注意：现在的代码没有对包做任何过滤，因此即便你的机器网络流量很小，输出也很可能刷屏</strong>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$&gt; sudo python ./tracepkt.py</span><br><span class="line">...</span><br><span class="line">Just got a packet from ping6</span><br><span class="line">Just got a packet from ping6</span><br><span class="line">Just got a packet from ping</span><br><span class="line">Just got a packet from irq/46-iwlwifi</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>上面的输出显示，我正在使用 ping 和 ping6，另外 WiFi 驱动也收到了一些包。</p>
<h3 id="3-4-自定义跟踪器：改进"><a href="#3-4-自定义跟踪器：改进" class="headerlink" title="3.4 自定义跟踪器：改进"></a>3.4 自定义跟踪器：改进</h3><p>接下来添加一些有用的数据/过滤条件。</p>
<h4 id="3-4-1-添加网卡信息"><a href="#3-4-1-添加网卡信息" class="headerlink" title="3.4.1 添加网卡信息"></a>3.4.1 添加网卡信息</h4><p>首先，可以安全地删除前面代码中的 <code>comm</code> 字段，它在这里没什么用处。然后，include <code>net/inet_sock.h</code> 头文件，这里有我们所需要的函数声明。最后给 <code>event</code> 结构体添加 <code>char ifname[IFNAMSIZ]</code> 字段。</p>
<p>现在可以从 <code>device</code> 结构体中访问 device name 字段。这里开始展示出<strong>代码的强大之处：我们可以访问任何受控范围内的字段</strong>。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Get device pointer, we'll need it to get the name and network namespace</span></span><br><span class="line"><span class="keyword">struct</span> net_device *dev;</span><br><span class="line">bpf_probe_read(&amp;dev, <span class="keyword">sizeof</span>(skb-&gt;dev), ((<span class="keyword">char</span>*)skb) + offsetof(typeof(*skb), dev));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Load interface name</span></span><br><span class="line">bpf_probe_read(&amp;evt.ifname, IFNAMSIZ, dev-&gt;name);</span><br></pre></td></tr></table></figure></p>
<p>现在你可以测试一下，这样是能工作的。注意相应地修改一下 Python 部分。那么，它是怎 么工作的呢？</p>
<p>我们引入了 <code>net_device</code> 结构体来访问<strong>网卡名字</strong>字段。第一个 <code>bpf_probe_read</code> 从内核 的网络包中将网卡名字拷贝到 <code>dev</code>，第二个将其接力复制到 <code>evt.ifname</code>。</p>
<p>不要忘了，eBPF 的目标是允许安全地编写在内核运行的脚本。这意味着，随机内存访问是绝 对不允许的。所有的内存访问都要经过验证。除非你要访问的内存在协议栈，否则你需要通 过 <code>bpf_probe_read</code> 读取数据。这会使得代码看起来很繁琐，但非常安全。<code>bpf_probe_read</code> 像是 <code>memcpy</code> 的一个更安全的版本，它定义在内核源文件 <a href="http://elixir.free-electrons.com/linux/v4.10.17/source/kernel/trace/bpf_trace.c#L64" target="_blank" rel="external">bpf_trace.c</a> 中:</p>
<ol>
<li>它和 memcpy 类似，因此注意内存拷贝的代价</li>
<li>如果遇到错误，它会返回一个错误和一个初始化为 0 的缓冲区，而不会造成程序崩溃或停 止运行</li>
</ol>
<p>接下来为使代码看起来更加简洁，我将使用如下宏：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> member_read(destination, source_struct, source_member)                 \</span><br><span class="line">  do&#123;                                                                          \</span><br><span class="line">    bpf_probe_read(                                                            \</span><br><span class="line">      destination,                                                             \</span><br><span class="line">      sizeof(source_struct-&gt;source_member),                                    \</span><br><span class="line">      ((char*)source_struct) + offsetof(typeof(*source_struct), source_member) \</span><br><span class="line">    );                                                                         \</span><br><span class="line">  &#125; while(0)</span></span><br></pre></td></tr></table></figure></p>
<p>这样上面的例子就可以写成：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">member_<span class="built_in">read</span>(&amp;dev, skb, dev);</span><br></pre></td></tr></table></figure></p>
<h4 id="3-4-2-添加网络命名空间-ID"><a href="#3-4-2-添加网络命名空间-ID" class="headerlink" title="3.4.2 添加网络命名空间 ID"></a>3.4.2 添加网络命名空间 ID</h4><p>采集网络命名空间信息非常有用，但是实现起来要复杂一些。原理上可以从两个地方访问：</p>
<ol>
<li>socket 结构体 <code>sk</code></li>
<li>device 结构体 <code>dev</code></li>
</ol>
<p>当我在写 <a href="https://github.com/iovisor/bcc/blob/master/tools/solisten.py" target="_blank" rel="external">solisten.py</a> 时 ，我使用的时 socket 结构体。不幸的是，不知道为什么，网络命名空间 ID 在跨命名空间的地 方消失了。这个字段全是 0，很明显是有非法内存访问时的返回值（回忆前面介绍的 <code>bpf_probe_read</code> 如何处理错误）。</p>
<p>幸好，device 结构体工作正常。想象一下，我们可以问一个 <code>packet</code> 它在哪个<code>网卡</code>，进而 问这个网卡它在哪个<code>网络命名空间</code>。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> net* net;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get netns id. Equivalent to: evt.netns = dev-&gt;nd_net.net-&gt;ns.inum</span></span><br><span class="line"><span class="keyword">possible_net_t</span> *skc_net = &amp;dev-&gt;nd_net;</span><br><span class="line">member_read(&amp;net, skc_net, net);</span><br><span class="line"><span class="keyword">struct</span> ns_common* ns = member_address(net, ns);</span><br><span class="line">member_read(&amp;evt.netns, ns, inum);</span><br></pre></td></tr></table></figure></p>
<p>其中的宏定义如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> member_address(source_struct, source_member) \</span><br><span class="line">(&#123;                                                   \</span><br><span class="line">  void* __ret;                                       \</span><br><span class="line">  __ret = (void*) (((char*)source_struct) + offsetof(typeof(*source_struct), source_member)); \</span><br><span class="line">  __ret;                                             \</span><br><span class="line">&#125;)</span></span><br></pre></td></tr></table></figure></p>
<p>这个宏还可以用于简化 <code>member_read</code>，这个就留给读者作为练习了。</p>
<p>好了，有了以上实现，我们再运行的效果就是：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$&gt; sudo python ./tracepkt.py</span><br><span class="line">[  4026531957]          docker0</span><br><span class="line">[  4026531957]      vetha373ab6</span><br><span class="line">[  4026532258]             eth0</span><br><span class="line">[  4026532258]             eth0</span><br><span class="line">[  4026531957]      vetha373ab6</span><br><span class="line">[  4026531957]          docker0</span><br></pre></td></tr></table></figure></p>
<p>如果 ping 一个容器，你看到的就是类似上面的输出。packet 首先经过本地的 docker0 网桥， 然后经 veth pair 跨过网络命名空间，最后到达容器的 eth0 网卡。应答包沿着相反的路径回 到宿主机。</p>
<p>至此，功能是实现了，不过还太粗糙，继续改进。</p>
<h4 id="3-4-3-只跟踪-ICMP-echo-request-reply-包"><a href="#3-4-3-只跟踪-ICMP-echo-request-reply-包" class="headerlink" title="3.4.3 只跟踪 ICMP echo request/reply 包"></a>3.4.3 只跟踪 ICMP echo request/reply 包</h4><p>这次我们将读取包的 IP 信息，这里我只展示 IPv4 的例子，IPv6 的与此类似。</p>
<p>不过，事情也并没有那么简单。我们是在和 kernel 的网络部分打交道。一些包可能还没被打 开，这意味着，变量的很多字段是没有初始化的。我们只能从 MAC 头开始，用 offset 的方式 计算 IP 头和 ICMP 头的位置。</p>
<p>首先从 MAC 头地址推导 IP 头地址。这里我们不(从 <code>skb</code> 的相应字段)加载 MAC 头长度信息，就认为 它是固定的 14 字节。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Compute MAC header address</span></span><br><span class="line"><span class="keyword">char</span>* head;</span><br><span class="line">u16 mac_header;</span><br><span class="line"></span><br><span class="line">member_read(&amp;head,       skb, head);</span><br><span class="line">member_read(&amp;mac_header, skb, mac_header);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compute IP Header address</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAC_HEADER_SIZE 14;</span></span><br><span class="line"><span class="keyword">char</span>* ip_header_address = head + mac_header + MAC_HEADER_SIZE;</span><br></pre></td></tr></table></figure></p>
<p>这表示我们假设 IP 头开始的地方在：<code>skb-&gt;head + skb-&gt;mac_header + MAC_HEADER_SIZE</code> 。 现在，我们可以解析 IP 头第一个字节的前 4 个 bit：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// Load IP protocol version</span><br><span class="line">u8 ip_version;</span><br><span class="line">bpf_probe_read(&amp;ip_version, sizeof(u8), ip_header_address);</span><br><span class="line">ip_version = ip_version &gt;&gt; <span class="number">4</span> &amp; <span class="number">0xf</span>;</span><br><span class="line"></span><br><span class="line">// Filter IPv4 packets</span><br><span class="line"><span class="keyword">if</span> (ip_version != <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后加载整个 IP 头，获取 IP 地址，以使得 Python 程序的输出看起来更有意义。另外注意，IP 包内的下一个头就是 ICMP 头。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// Load IP Header</span><br><span class="line">struct iphdr iphdr;</span><br><span class="line">bpf_probe_read(&amp;iphdr, sizeof(iphdr), ip_header_address);</span><br><span class="line"></span><br><span class="line">// Load protocol <span class="keyword">and</span> address</span><br><span class="line">u8 icmp_offset_from_ip_header = iphdr.ihl * <span class="number">4</span>;</span><br><span class="line">evt.saddr[<span class="number">0</span>] = iphdr.saddr;</span><br><span class="line">evt.daddr[<span class="number">0</span>] = iphdr.daddr;</span><br><span class="line"></span><br><span class="line">// Filter ICMP packets</span><br><span class="line"><span class="keyword">if</span> (iphdr.protocol != IPPROTO_ICMP) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>最后，我们加载 ICMP 头，如果是 ICMP echo request 或 reply，就读取序列号：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// Compute ICMP header address <span class="keyword">and</span> load ICMP header</span><br><span class="line">char* icmp_header_address = ip_header_address + icmp_offset_from_ip_header;</span><br><span class="line">struct icmphdr icmphdr;</span><br><span class="line">bpf_probe_read(&amp;icmphdr, sizeof(icmphdr), icmp_header_address);</span><br><span class="line"></span><br><span class="line">// Filter ICMP echo request <span class="keyword">and</span> echo reply</span><br><span class="line"><span class="keyword">if</span> (icmphdr.type != ICMP_ECHO &amp;&amp; icmphdr.type != ICMP_ECHOREPLY) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Get ICMP info</span><br><span class="line">evt.icmptype = icmphdr.type;</span><br><span class="line">evt.icmpid   = icmphdr.un.echo.id;</span><br><span class="line">evt.icmpseq  = icmphdr.un.echo.sequence;</span><br><span class="line"></span><br><span class="line">// Fix endian</span><br><span class="line">evt.icmpid  = be16_to_cpu(evt.icmpid);</span><br><span class="line">evt.icmpseq = be16_to_cpu(evt.icmpseq);</span><br></pre></td></tr></table></figure></p>
<p>这就是全部工作了。</p>
<p>如果你想过滤特定的 ping 进程的包，你可以认为 <code>evt.icmpid</code> 就是相应 ping 进程的进程号， 至少 Linux 上如此。</p>
<h3 id="3-5-最终效果"><a href="#3-5-最终效果" class="headerlink" title="3.5 最终效果"></a>3.5 最终效果</h3><p>再写一些比较简单的 Python 程序配合，我们就可以测试我们的跟踪器在多种场景下的用途。 以 root 权限启动这个程序，在不同终端发起几个 ping 进程，就会看到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping -4 localhost</span></span><br><span class="line">[  4026531957]               lo request <span class="comment">#20212.001 127.0.0.1 -&gt; 127.0.0.1</span></span><br><span class="line">[  4026531957]               lo request <span class="comment">#20212.001 127.0.0.1 -&gt; 127.0.0.1</span></span><br><span class="line">[  4026531957]               lo   reply <span class="comment">#20212.001 127.0.0.1 -&gt; 127.0.0.1</span></span><br><span class="line">[  4026531957]               lo   reply <span class="comment">#20212.001 127.0.0.1 -&gt; 127.0.0.1</span></span><br></pre></td></tr></table></figure></p>
<p>这个 ICMP 请求是进程 20212（Linux ping 的 ICMP ID）在 loopback 网卡发出的，最后的 reply 原路回到了这个 loopback。这个环回接口既是发送网卡又是接收网卡。</p>
<p>如果是我的 WiFi 网关会是什么样子内？<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping -4 192.168.43.1</span></span><br><span class="line">[  4026531957]           wlp2s0 request <span class="comment">#20710.001 192.168.43.191 -&gt; 192.168.43.1</span></span><br><span class="line">[  4026531957]           wlp2s0   reply <span class="comment">#20710.001 192.168.43.1 -&gt; 192.168.43.191</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，这种情况下走的是 WiFi 网卡，也没问题。</p>
<p>另外，让我们的话题稍微偏一下，还记得刚开始我们只打印程序名字的版本吗？在 上面这种情况下，ICMP 请求的程序名字会是 ping，而应答包的程序的名字会是 WiFi 驱动，因 为是驱动发的应答包，至少 Linux 上是如此。</p>
<p>最后还是拿我最喜欢的例子：ping 容器。之所以最喜欢并不是因为 Docker，而是它展示了 eBPF 的强大，<strong>就像给 ping 过程做了一次 X 射线检查</strong>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping -4 172.17.0.2</span></span><br><span class="line">[  4026531957]          docker0 request <span class="comment">#17146.001 172.17.0.1 -&gt; 172.17.0.2</span></span><br><span class="line">[  4026531957]      vetha373ab6 request <span class="comment">#17146.001 172.17.0.1 -&gt; 172.17.0.2</span></span><br><span class="line">[  4026532258]             eth0 request <span class="comment">#17146.001 172.17.0.1 -&gt; 172.17.0.2</span></span><br><span class="line">[  4026532258]             eth0   reply <span class="comment">#17146.001 172.17.0.2 -&gt; 172.17.0.1</span></span><br><span class="line">[  4026531957]      vetha373ab6   reply <span class="comment">#17146.001 172.17.0.2 -&gt; 172.17.0.1</span></span><br><span class="line">[  4026531957]          docker0   reply <span class="comment">#17146.001 172.17.0.2 -&gt; 172.17.0.1</span></span><br></pre></td></tr></table></figure></p>
<p>来点 ASCII 艺术，就变成：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">       Host netns           | Container netns</span><br><span class="line">+---------------------------+-----------------+</span><br><span class="line">| docker0 ---&gt; veth0e65931 ---&gt; eth0          |</span><br><span class="line">+---------------------------+-----------------+</span><br></pre></td></tr></table></figure></p>
<h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>在 eBPF/bcc 出现之前，要深入的排查和追踪很多网络问题，只能靠给内核打补丁。现在，我 们可以比较方便地用 eBPF/bcc 编写一些工具来完成这些事情。跟踪点(tracepoint)也很方便 ，它们提示了我们可以在哪些地方进行探测，避免了去看繁杂的内核代码。kprobe 无法探测 的一些地方，例如一些内联函数和静态函数，eBPF/bcc 也可以探测。</p>
<p>本文的例子要添加对 IPv6 的支持也非常简单，我就留给读者作为练习。</p>
<p>如果要使本文更加完善的话，我需要对我们的程序做性能测试。但考虑到文章本身已经非常 长，这里就不做了。</p>
<p>对我们的代码进行改进，用在跟踪路由和 iptables 判决，或是 ARP 包，也是很有意思的。 这将会把它变成一个完美的 X 射线跟踪器，对像我这样需要经常处理复杂网络问题的 人来说将非常有用。</p>
<p>完整的（包含 IPv6 支持）代码可以访问：<a href="https://github.com/icyxp/tracepkt" target="_blank" rel="external">https://github.com/icyxp/tracepkt</a>。</p>
<p>本文翻译自：Tracing a packet’s journey using Linux tracepoints, perf and eBPF 来源：arthurchiao.art 作者：ArthurChiao</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Docker 中 Storage-driver 启用 Overlay2 并限制单个容器的磁盘空间]]></title>
      <url>http://team.jiunile.com/blog/2020/10/docker-storage-driver.html</url>
      <content type="html"><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>目前 docker 中常见的 Storage-driver 主要有 AUFS、Devicemapper 以及 Overlay2，这三种文件存储驱动这里简单介绍下。同时着重介绍 Overlay2 的使用事项。<br><a id="more"></a></p>
<h2 id="2-AUFS"><a href="#2-AUFS" class="headerlink" title="2 AUFS"></a>2 AUFS</h2><h3 id="2-1-如何存储文件？"><a href="#2-1-如何存储文件？" class="headerlink" title="2.1 如何存储文件？"></a>2.1 如何存储文件？</h3><p>AUFS 使用多层目录存储，每一次目录在 Docker 中称之为层（layer），最终呈现给用户的则是一个普通的单层文件系统，我们把多层以单一层的方式呈现出来的过程叫做联合挂载。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/var/lib/docker/aufs/mnt       联合挂载点</span><br><span class="line">      ↓     ↓     ↓</span><br><span class="line">/var/lib/docker/aufs/diff      容器层（可读写）</span><br><span class="line">      ↓     ↓     ↓</span><br><span class="line">/var/lib/docker/aufs/diff</span><br><span class="line">/var/lib/docker/aufs/diff      镜像层（只读）</span><br><span class="line">/var/lib/docker/aufs/diff</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>diff 文件夹</code>：存储镜像内容，每一层都存储在以镜像层 ID 命名的子文件夹中</li>
<li><code>layers 文件夹</code>：存储镜像层关系的元数据，在 diff 文件夹下的每个镜像层在这里都会有一个文件，文件的内容为该层镜像的父级镜像的 ID。</li>
<li><code>mnt 文件夹</code>：联合挂载点目录，未生成容器时，该目录为空。</li>
</ul>
<h3 id="2-2-读写文件"><a href="#2-2-读写文件" class="headerlink" title="2.2 读写文件"></a>2.2 读写文件</h3><h4 id="2-2-1-读文件"><a href="#2-2-1-读文件" class="headerlink" title="2.2.1 读文件"></a>2.2.1 读文件</h4><p>当读取的文件在容器层时，直接从容器层读取，当文件不存在容器层时，则从镜像层中读取，当文件既在镜像层又在容器层时，则从容器层读取</p>
<h4 id="2-2-2-修改或删除文件"><a href="#2-2-2-修改或删除文件" class="headerlink" title="2.2.2 修改或删除文件"></a>2.2.2 修改或删除文件</h4><p>当第一次修改文件时，AUFS 会触发写时复制，先从镜像层复制文件到容器层，然后在执行修改操作。当修改文件或目录时，AUFS 并不会真正从镜像中删除，而是创建一个特殊的文件或文件夹（whiteout），这种特殊的文件或文件夹会组织容器访问。</p>
<h3 id="2-3-优缺点"><a href="#2-3-优缺点" class="headerlink" title="2.3 优缺点"></a>2.3 优缺点</h3><p>在容器密度比较高的场景下，AUFS 是非常好的选择，因为 AUFS 的容器间共享镜像层的特性使其磁盘利用率很高，容器的启动时间很短。</p>
<p>AUFS 的写时复制策略会带来很高的性能开销，因为 AUFS 对文件的第一次更改需要将整个文件复制到读写层，当容器层数很多或文件所在目录很深时尤其明显。其次 AUFS 未能进入 Linux 内核主线。</p>
<h2 id="3-Devicemapper"><a href="#3-Devicemapper" class="headerlink" title="3 Devicemapper"></a>3 Devicemapper</h2><h3 id="3-1-如何存储文件？"><a href="#3-1-如何存储文件？" class="headerlink" title="3.1 如何存储文件？"></a>3.1 如何存储文件？</h3><p>Devicemapper 使用专门的块设备来实现镜像的存储，并且像 AUFS 一样使用了写时复制的技术来保障最大程度节省存储空间，Devicemapper 的镜像分层使用快照的方式实现。</p>
<p>Devicemapper 创建镜像的过程如下：</p>
<ul>
<li>创建一个精简配置池，精简配置池由块设备或稀疏文件创建。</li>
<li>接下来创建一个基础设备。</li>
<li>每个镜像和镜像层都是基础设备的快照；在写快照支持写时复制策略，这意味着它们起始都是空的，当有数据写入时才耗费空间。</li>
</ul>
<p><img src="/images/sd-dm.jpg" alt="device mapper"></p>
<h3 id="3-2-读写文件"><a href="#3-2-读写文件" class="headerlink" title="3.2 读写文件"></a>3.2 读写文件</h3><h4 id="3-2-1-读文件"><a href="#3-2-1-读文件" class="headerlink" title="3.2.1 读文件"></a>3.2.1 读文件</h4><p><img src="/images/sd-dm-read.jpg" alt="device mapper read"></p>
<ul>
<li>某个进程发出读取文件的请求；由于容器只是镜像的精简快照 (thin snapshot)，它并没有这个文件。但它有指向这个文件在下面层中存储位置的指针。</li>
<li>Devicemapper 由指针找到在镜像层号为 a005e 中的块号为 0xf33 的数据；</li>
<li>Devicemapper 将这个位置的文件复制到容器的存储区内；</li>
<li>Devicemapper 将数据返回给应用进程；</li>
</ul>
<h4 id="3-2-2-写文件"><a href="#3-2-2-写文件" class="headerlink" title="3.2.2 写文件"></a>3.2.2 写文件</h4><p>在 Devicemapper 中，对容器的写操作由“需要时分配”策略完成。更新已有数据由“写时复制”策略完成，这些操作都在块的层次上完成。当我们需要写数据时，则向瘦供给池（thinpool）动态申请存储空间生成读写层，然后把数据复制到读写层进行修改。Devicemapper 默认每次申请的大小是 64KB 或者 64KB 的倍数，因此每次新生成的读写层的大小都是 64KB 或者 64KB 的倍数。</p>
<h3 id="3-3-优缺点"><a href="#3-3-优缺点" class="headerlink" title="3.3 优缺点"></a>3.3 优缺点</h3><p>Devicemapper 的写时复制策略以 64KB 作为粒度，意味着无论是对 32KB 的文件还是对 1GB 大小的文件的修改都仅复制 64KB 大小的文件。这相对于在文件层面进行的读操作具有很明显的性能优势。</p>
<p>但是，如果容器频繁对小于 64KB 的文件进行改写，Devicemapper 的性能是低于 AUFS 的。同时 Devicemapper 不是最有效使用存储空间的 storage driver，启动 n 个相同的容器就复制了 n 份文件在内存中，这对内存的影响很大。所以 Devicemapper 并不适合容器密度高的场景。</p>
<h2 id="4-Overlay2"><a href="#4-Overlay2" class="headerlink" title="4 Overlay2"></a>4 Overlay2</h2><h3 id="4-1-如何存储文件？"><a href="#4-1-如何存储文件？" class="headerlink" title="4.1 如何存储文件？"></a>4.1 如何存储文件？</h3><p>Overlay2 将一个 Linux 主机中的两个目录组合起来，一个在上，一个在下，对外提供统一的视图。这两个目录就是层 layer，将两个层组合在一起的技术被称为联合挂载（union mount）。在 Overlay2 中，上层的目录被称作 <code>upperdir</code>，下层的，目录被称作 <code>lowerdir</code>，对外提供的统一视图被称作 <code>merged</code>。<br><img src="/images/sd-ol2.jpg" alt="overlay2"></p>
<p>注意到，镜像层和容器层可以有相同的文件，这种情况下，<code>upperdir</code> 中的文件覆盖 <code>lowerdir</code> 中的文件。</p>
<blockquote>
<p>Overlay2 文件系统最多支持 128 个层数叠加，也就是说你的 Dockerfile 最多只能写 128 行。</p>
</blockquote>
<h3 id="4-2-读写文件"><a href="#4-2-读写文件" class="headerlink" title="4.2 读写文件"></a>4.2 读写文件</h3><h4 id="4-2-1-读文件"><a href="#4-2-1-读文件" class="headerlink" title="4.2.1 读文件"></a>4.2.1 读文件</h4><p>要读的文件不在 container layer 中，那就从 <code>lowerdir</code> 中读，会耗费一点性能；要读的文件存在于 container layer 中，直接从 <code>upperdir</code> 中读；要读的文件在container layer 和 image layer 中都存在, 从 <code>upperdir</code> 中读文件</p>
<h4 id="4-2-2-写文件"><a href="#4-2-2-写文件" class="headerlink" title="4.2.2 写文件"></a>4.2.2 写文件</h4><p>在第一次修改时，文件不在 container layer(upperdir) 中，overlay driver 调用写时复制将文件从 <code>lowerdir</code> 读到 <code>upperdir</code> 中，然后对文件的副本做出修改。文件被删除时，和 AUFS 一样，相应的 whiteout 文件被创建在 <code>upperdir</code>。并不删除容器层(lowerdir) 中的文件，<code>whiteout</code> 文件屏蔽了它的存在。</p>
<h3 id="4-3-优缺点"><a href="#4-3-优缺点" class="headerlink" title="4.3 优缺点"></a>4.3 优缺点</h3><p>Overlay2 的拷贝操作工作在文件层面上，也就是对文件的第一次修改需要复制整个文件，会带来一些性能开销，在修改大文件时尤其明显。但 Overlay2 的拷贝操作比 AUFS 还是快一点，因为 AUFS 有很多层，而 Overlay2 只有两层，所以 Overlay2 在文件的搜索方面相对于 AUFS 具有优势。Overlay2 支持页缓存的共享，这意味着多个使用同一文件的容器可以共享同一页缓存，这使得 Overlay2 具有很高的内存使用效率。</p>
<h2 id="5-如何使用-Overlay2"><a href="#5-如何使用-Overlay2" class="headerlink" title="5 如何使用 Overlay2"></a>5 如何使用 Overlay2</h2><h3 id="5-1-先决条件"><a href="#5-1-先决条件" class="headerlink" title="5.1 先决条件"></a>5.1 先决条件</h3><ol>
<li>Docker 版本必须高于 17.06.02</li>
<li>如果操作系统是 RHEL 或 CentOS，内核版本必须高于 3.10.0-514，其他 Linux 内核版本必须高于 4.0</li>
<li><p>Overlay2 最好搭配 xfs 文件系统使用，并且使用 xfs 作为底层文件系统时，d_type 必须开启</p>
<p>验证 d_type 是否开启：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xfs_info /var/lib/docker | grep ftype</span><br></pre></td></tr></table></figure>
<p>当输出结果中有 <code>ftype=1</code> 时，表示 d_type 已经开启。如果为 0 时，则需要重新格式化磁盘。命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkfs.xfs <span class="_">-f</span> -n ftype=1 /path/to/disk</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5-2-限制单个容器可占用磁盘空间"><a href="#5-2-限制单个容器可占用磁盘空间" class="headerlink" title="5.2 限制单个容器可占用磁盘空间"></a>5.2 限制单个容器可占用磁盘空间</h3><h4 id="5-2-1-开启-xfs-的-quota-特性"><a href="#5-2-1-开启-xfs-的-quota-特性" class="headerlink" title="5.2.1 开启 xfs 的 quota 特性"></a>5.2.1 开启 xfs 的 quota 特性</h4><p>在 <code>/etc/fstab</code> 中设置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UUID=4cbf4a19-1fba-4027-bf92-9aa969683fa9     /var/lib/docker   xfs    defaults,pquota  0   0</span><br></pre></td></tr></table></figure></p>
<p>将 <code>/var/lib/docker</code> 卸载后重新挂载即可。</p>
<h4 id="5-2-2-配置-docker-daemon"><a href="#5-2-2-配置-docker-daemon" class="headerlink" title="5.2.2 配置 docker daemon"></a>5.2.2 配置 docker daemon</h4><p><code>/etc/docker/daemon.json</code> 配置文件如下，这里将每个容器可以使用的磁盘空间设置为1G：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"storage-driver"</span>: <span class="string">"overlay2"</span>,</span><br><span class="line">    <span class="attr">"storage-opts"</span>: [</span><br><span class="line">      <span class="string">"overlay2.override_kernel_check=true"</span>,</span><br><span class="line">      <span class="string">"overlay2.size=1G"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="5-2-3-写入文件测试"><a href="#5-2-3-写入文件测试" class="headerlink" title="5.2.3 写入文件测试"></a>5.2.3 写入文件测试</h4><p>重启docker后，启动一个容器，在容器中创建文件。</p>
<p>先创建一个1000M的文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># dd if=/dev/zero of=/a bs=100M count=10</span></span><br><span class="line">10+0 records <span class="keyword">in</span></span><br><span class="line">10+0 records out</span><br></pre></td></tr></table></figure></p>
<p>然后创建第二个1000M的文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># dd if=/dev/zero of=/b bs=100M count=10</span></span><br><span class="line">dd: writing <span class="string">'/b'</span>: No space left on device</span><br><span class="line">2+0 records <span class="keyword">in</span></span><br><span class="line">0+1 records out</span><br></pre></td></tr></table></figure></p>
<p>可以看到第二个 1000M 文件因为空间不足创建失败。</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[带你了解 Kube-proxy 工作原理]]></title>
      <url>http://team.jiunile.com/blog/2020/10/k8s-kube-proxy.html</url>
      <content type="html"><![CDATA[<h2 id="kube-proxy介绍"><a href="#kube-proxy介绍" class="headerlink" title="kube-proxy介绍"></a>kube-proxy介绍</h2><h3 id="为什么需要kube-proxy"><a href="#为什么需要kube-proxy" class="headerlink" title="为什么需要kube-proxy"></a>为什么需要kube-proxy</h3><p>我们知道容器的特点是快速创建、快速销毁，Kubernetes Pod 和容器一样只具有临时的生命周期，一个 Pod 随时有可能被终止或者漂移，随着集群的状态变化而变化，一旦Pod 变化，则该 Pod 提供的服务也就无法访问，如果直接访问 Pod 则无法实现服务的连续性和高可用性，因此显然不能使用 Pod 地址作为服务暴露端口。</p>
<p>解决这个问题的办法和传统数据中心解决无状态服务高可用的思路完全一样，通过负载均衡和 VIP 实现后端真实服务的自动转发、故障转移。</p>
<p>这个负载均衡在 Kubernetes 中称为 Service，VIP 即 Service ClusterIP，因此可以认为Kubernetes 的 Service 就是一个四层负载均衡，Kubernetes 对应的还有七层负载均衡 Ingress，本文仅介绍 Kubernetes Service。</p>
<p>这个 Service 就是由 kube-proxy 实现的，ClusterIP 不会因为 Pod 状态改变而变，需要注意的是 VIP 即 ClusterIP 是个假的 IP，这个 IP 在整个集群中根本不存在，当然也就无法通过IP协议栈无法路由，底层 underlay 设备更无法感知这个 IP 的存在，因此 ClusterIP 只能是单主机（Host Only）作用域可见，这个IP在其他节点以及集群外均无法访问。</p>
<p>Kubernetes 为了实现在集群所有的节点都能够访问 Service，kube-proxy 默认会在所有的 Node 节点都创建这个 VIP 并且实现负载，所以在部署 Kubernetes 后发现 kube-proxy 是一个 DaemonSet。<br><a id="more"></a></p>
<p>而 Service 负载之所以能够在 Node 节点上实现是因为无论 Kubernetes 使用哪个网络模型，均需要保证满足如下三个条件：</p>
<ol>
<li>容器之间要求不需要任何NAT能直接通信；</li>
<li>容器与Node之间要求不需要任何NAT能直接通信；</li>
<li>容器看到自身的IP和外面看到它的IP必须是一样的，即不存在IP转化的问题。</li>
</ol>
<p>至少第 2 点是必须满足的，有了如上几个假设，Kubernetes Service 才能在 Node 上实现，否则 Node 不通 Pod IP 也就实现不了了。</p>
<p>有人说既然 kube-proxy 是四层负载均衡，那 kube-proxy 应该可以使用 haproxy、nginx 等作为负载后端啊？</p>
<p>事实上确实没有问题，不过唯一需要考虑的就是性能问题，如上这些负载均衡功能都强大，但毕竟还是基于用户态转发或者反向代理实现的，性能必然不如在内核态直接转发处理好。</p>
<p>因此 kube-proxy 默认会优先选择基于内核态的负载作为后端实现机制，目前 kube-proxy 默认是通过 iptables 实现负载的，在此之前还有一种称为 userspace 模式，其实也是基于 iptables 实现，可以认为当前的 iptables 模式是对之前 userspace 模式的优化。</p>
<p>本节接下来将详细介绍kube-proxy iptables模式的实现原理。</p>
<h3 id="kube-proxy-iptables-模式实现原理"><a href="#kube-proxy-iptables-模式实现原理" class="headerlink" title="kube-proxy iptables 模式实现原理"></a>kube-proxy iptables 模式实现原理</h3><h4 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h4><p>首先创建了一个 ClusterIP 类型的 Service:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc -l owner=int32bit</span></span><br><span class="line">NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">kubernetes-bootcamp-v1   ClusterIP   10.106.224.41   &lt;none&gt;        8080/TCP   163m</span><br></pre></td></tr></table></figure></p>
<p>其中 ClusterIP 为 10.106.224.41，我们可以验证这个IP在本地是不存在的:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-172:~<span class="comment"># ping -c 2 -w 2 10.106.224.41</span></span><br><span class="line">PING 10.106.224.41 (10.106.224.41) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line">--- 10.106.224.41 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 received, 100% packet loss, time 1025ms</span><br><span class="line"></span><br><span class="line">root@ip-192-168-193-172:~<span class="comment"># ip a | grep 10.106.224.41</span></span><br><span class="line">root@ip-192-168-193-172:~<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>所以<strong>不要尝试去 ping ClusterIP，它不可能通的</strong>。</p>
<p>此时在 Node 节点 192.168.193.172 上访问该 Service 服务，首先流量到达的是 OUTPUT 链，这里我们只关心 nat 表的 OUTPUT 链：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A OUTPUT'</span></span><br><span class="line">-A OUTPUT -m comment --comment <span class="string">"kubernetes service portals"</span> -j KUBE-SERVICES</span><br></pre></td></tr></table></figure></p>
<p>该链跳转到 <code>KUBE-SERVICES</code> 子链中:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A KUBE-SERVICES'</span></span><br><span class="line">...</span><br><span class="line">-A KUBE-SERVICES ! <span class="_">-s</span> 10.244.0.0/16 <span class="_">-d</span> 10.106.224.41/32 -p tcp -m comment --comment <span class="string">"default/kubernetes-bootcamp-v1: cluster IP"</span> -m tcp --dport 8080 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES <span class="_">-d</span> 10.106.224.41/32 -p tcp -m comment --comment <span class="string">"default/kubernetes-bootcamp-v1: cluster IP"</span> -m tcp --dport 8080 -j KUBE-SVC-RPP7DHNHMGOIIFDC</span><br></pre></td></tr></table></figure></p>
<p>我们发现与之相关的有两条规则：</p>
<ul>
<li>第一条负责打标记 <code>MARK 0x4000/0x4000</code>，后面会用到这个标记。</li>
<li>第二条规则跳到 <code>KUBE-SVC-RPP7DHNHMGOIIFDC</code> 子链。</li>
</ul>
<p>其中 <code>KUBE-SVC-RPP7DHNHMGOIIFDC</code> 子链规则如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A KUBE-SVC-RPP7DHNHMGOIIFDC'</span></span><br><span class="line">-A KUBE-SVC-RPP7DHNHMGOIIFDC -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-FTIQ6MSD3LWO5HZX</span><br><span class="line">-A KUBE-SVC-RPP7DHNHMGOIIFDC -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-SQBK6CVV7ZCKBTVI</span><br><span class="line">-A KUBE-SVC-RPP7DHNHMGOIIFDC -j KUBE-SEP-IAZPHGLZVO2SWOVD</span><br></pre></td></tr></table></figure></p>
<p>这几条规则看起来复杂，其实实现的功能很简单:</p>
<ul>
<li>1/3 的概率跳到子链 <code>KUBE-SEP-FTIQ6MSD3LWO5HZX</code>,</li>
<li>剩下概率的 1/2，(1 - 1/3) * 1/2 == 1/3，即 1/3 的概率跳到子链 <code>KUBE-SEP-SQBK6CVV7ZCKBTVI</code>，</li>
<li>剩下 1/3 的概率跳到 <code>KUBE-SEP-IAZPHGLZVO2SWOVD</code>。</li>
</ul>
<p>我们查看其中一个子链 KUBE-SEP-FTIQ6马上到！3LWO5HZX规则:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A KUBE-SEP-FTIQ6MSD3LWO5HZX'</span></span><br><span class="line">...</span><br><span class="line">-A KUBE-SEP-FTIQ6MSD3LWO5HZX -p tcp -m tcp -j DNAT --to-destination 10.244.1.2:8080</span><br></pre></td></tr></table></figure></p>
<p>可见这条规则的目的是做了一次 DNAT，DNAT 目标为其中一个 Endpoint，即 Pod 服务。</p>
<p>由此可见子链 <code>KUBE-SVC-RPP7DHNHMGOIIFDC</code> 的功能就是按照概率均等的原则DNAT 到其中一个 Endpoint IP，即 Pod IP，假设为 10.244.1.2，</p>
<p>此时相当于:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.172:xxxx -&gt; 10.106.224.41:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.172:xxxX -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure></p>
<p>接着来到 POSTROUTING 链:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A POSTROUTING'</span></span><br><span class="line">-A POSTROUTING -m comment --comment <span class="string">"kubernetes postrouting rules"</span> -j KUBE-POSTROUTING</span><br><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A KUBE-POSTROUTING'</span></span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment <span class="string">"kubernetes service traffic requiring SNAT"</span> -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure></p>
<p>这两条规则只做一件事就是只要标记了 <code>0x4000/0x4000</code> 的包就一律做 MASQUERADE（SNAT)，由于 10.244.1.2 默认是从 flannel.1 转发出去的，因此会把源IP改为 flannel.1 的IP <code>10.244.0.0</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.172:xxxx -&gt; 10.106.224.41:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.172:xxxx -&gt; 10.244.1.2:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  SNAT</span><br><span class="line">                      V</span><br><span class="line">10.244.0.0:xxxx -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure></p>
<p>剩下的就是常规的走 Vxlan 隧道转发流程了，这里不再赘述。</p>
<h4 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h4><p>接下来研究下 NodePort 过程，首先创建如下 Service:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc -l owner=int32bit</span></span><br><span class="line">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes-bootcamp-v1   NodePort   10.106.224.41   &lt;none&gt;        8080:30419/TCP   3h30m</span><br></pre></td></tr></table></figure></p>
<p>其中 Service 的 NodePort 端口为 30419。</p>
<p>假设有一个外部IP 192.168.193.197，通过 <code>192.168.193.172:30419</code> 访问服务。</p>
<p>首先到达 PREROUTING 链:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A PREROUTING'</span></span><br><span class="line">-A PREROUTING -m comment --comment <span class="string">"kubernetes service portals"</span> -j KUBE-SERVICES</span><br><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A KUBE-SERVICES'</span></span><br><span class="line">...</span><br><span class="line">-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span><br></pre></td></tr></table></figure></p>
<p>PREROUTING 的规则非常简单，凡是发给自己的包，则交给子链 <code>KUBE-NODEPORTS</code> 处理。注意前面省略了判断 ClusterIP 的部分规则。</p>
<p><code>KUBE-NODEPORTS</code> 规则如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t nat | grep -- '-A KUBE-NODEPORTS'</span></span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="string">"default/kubernetes-bootcamp-v1:"</span> -m tcp --dport 30419 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="string">"default/kubernetes-bootcamp-v1:"</span> -m tcp --dport 30419 -j KUBE-SVC-RPP7DHNHMGOIIFDC</span><br></pre></td></tr></table></figure></p>
<p>这个规则首先给包打上标记 <code>0x4000/0x4000</code>，然后交给子链 <code>KUBE-SVC-RPP7DHNHMGOIIFDC</code> 处理， <code>KUBE-SVC-RPP7DHNHMGOIIFDC</code> 刚刚已经见面过了，其功能就是按照概率均等的原则 DNAT 到其中一个 Endpoint IP，即 Pod IP，假设为10.244.1.2。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:30419</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.197:xxxx -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure></p>
<p>此时发现 10.244.1.2 不是自己的IP，于是经过路由判断目标为 10.244.1.2 需要从 flannel.1 发出去。</p>
<p>接着到了 <code>FORWARD</code> 链，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables-save -t filter | grep -- '-A FORWARD'</span></span><br><span class="line">-A FORWARD -m comment --comment <span class="string">"kubernetes forwarding rules"</span> -j KUBE-FORWARD</span><br><span class="line"><span class="comment"># iptables-save -t filter | grep -- '-A KUBE-FORWARD'</span></span><br><span class="line">-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP</span><br><span class="line">-A KUBE-FORWARD -m comment --comment <span class="string">"kubernetes forwarding rules"</span> -m mark --mark 0x4000/0x4000 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<p>FORWARD 表在这里只是判断下，只允许打了标记 <code>0x4000/0x4000</code> 的包才允许转发。</p>
<p>最后来到 <code>POSTROUTING</code> 链，这里和 ClusterIP 就完全一样了，在 <code>KUBE-POSTROUTING</code> 中做一次 <code>MASQUERADE</code>(SNAT)，最后结果:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:30419</span><br><span class="line">                      |</span><br><span class="line">                      |  DNAT</span><br><span class="line">                      V</span><br><span class="line">192.168.193.197:xxxx -&gt; 10.244.1.2:8080</span><br><span class="line">                      |</span><br><span class="line">                      |  SNAT</span><br><span class="line">                      V</span><br><span class="line">10.244.0.0:xxxx -&gt; 10.244.1.2:8080</span><br></pre></td></tr></table></figure></p>
<h3 id="kube-proxy-使用-iptables-存在的问题"><a href="#kube-proxy-使用-iptables-存在的问题" class="headerlink" title="kube-proxy 使用 iptables 存在的问题"></a>kube-proxy 使用 iptables 存在的问题</h3><p>我们发现基于 iptables 模式的 kube-proxy ClusterIP 和 NodePort 都是基于 iptables 规则实现的，我们至少发现存在如下几个问题：</p>
<ul>
<li>iptables 规则复杂零乱，真要出现什么问题，排查 iptables 规则必然得掉层皮。 <code>LOG+TRACE</code> 大法也不好使。</li>
<li>iptables 规则多了之后性能下降，这是因为 iptables 规则是基于链表实现，查找复杂度为 O(n)，当规模非常大时，查找和处理的开销就特别大。据<a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/" target="_blank" rel="external">官方说法</a>，当节点到达5000 个时，假设有 2000 个 NodePort Service，每个 Service 有 10 个 Pod，那么在每个 Node 节点中至少有 20000 条规则，内核根本支撑不住，iptables 将成为最主要的性能瓶颈。</li>
<li>iptables 主要是专门用来做主机防火墙的，而不是专长做负载均衡的。虽然通过 iptables 的 <code>statistic</code> 模块以及 DNAT 能够实现最简单的只支持概率轮询的负载均衡，但是往往我们还需要更多更灵活的算法，比如基于最少连接算法、源地址 HASH 算法等。而同样基于 netfilter 的 ipvs 却是专门做负载均衡的，配置简单，基于散列查找 O(1) 复杂度性能好，支持数十种调度算法。因此显然 ipvs 比 iptables 更适合做 kube-proxy 的后端，毕竟专业的人做专业的事，物尽其美。</li>
</ul>
<p>本文接下来将介绍 kube-proxy 的 ipvs 实现，由于本人之前也是对 ipvs 很陌生，没有用过，专门学习了下 ipvs，因此在第二章简易介绍了下 ipvs，如果已经很熟悉 ipvs 了，可以直接跳过，这一章和 Kubernetes 几乎没有任何关系。</p>
<h2 id="IPVS-简易入门"><a href="#IPVS-简易入门" class="headerlink" title="IPVS 简易入门"></a>IPVS 简易入门</h2><h3 id="IPVS-简介"><a href="#IPVS-简介" class="headerlink" title="IPVS 简介"></a>IPVS 简介</h3><p>我们接触比较多的是应用层负载均衡，比如 haproxy、nginx、F5 等，这些负载均衡工作在用户态，因此会有对应的进程和监听 socket，一般能同时支持 4 层负载和 7 层负载，使用起来也比较方便。</p>
<p>LVS 是国内章文嵩博士开发并贡献给社区的（<a href="http://jm.taobao.org/2016/06/02/zhangwensong-and-load-balance/" target="_blank" rel="external">章文嵩博士和他背后的负载均衡帝国</a>)，主要由 ipvs 和 ipvsadm 组成，ipvs 是工作在内核态的 4 层负载均衡，和 iptables 一样都是基于内核底层 netfilter 实现，netfilter 主要通过各个链的钩子实现包处理和转发。ipvsadm 和 ipvs 的关系，就好比 netfilter 和 iptables 的关系，它运行在用户态，提供简单的 CLI 接口进行 ipvs 配置。</p>
<p>由于 ipvs 工作在内核态，直接基于内核处理包转发，所以最大的特点就是性能非常好。又由于它工作在 4 层，因此不会处理应用层数据，经常有人问 ipvs 能不能做 SSL 证书卸载、或者修改 HTTP 头部数据，显然这些都不可能做的。</p>
<p>我们知道应用层负载均衡大多数都是基于反向代理实现负载的，工作在应用层，当用户的包到达负载均衡监听器 listening 后，基于一定的算法从后端服务列表中选择其中一个后端服务进行转发。当然中间可能还会有一些额外操作，最常见的如 SSL 证书卸载。</p>
<p>而 ipvs 工作在内核态，只处理四层协议，因此只能基于路由或者 NAT 进行数据转发，可以把 ipvs 当作一个特殊的路由器网关，这个网关可以根据一定的算法自动选择下一跳，或者把 ipvs 当作一个多重 DNAT，按照一定的算法把 ip 包的目标地址 DNAT 到其中真实服务的目标 IP。针对如上两种情况分别对应 ipvs 的两种模式–网关模式和 NAT 模式，另外 ipip 模式则是对网关模式的扩展，本文下面会针对这几种模式的实现原理进行详细介绍。</p>
<h3 id="IPVS-用法"><a href="#IPVS-用法" class="headerlink" title="IPVS 用法"></a>IPVS 用法</h3><p>ipvsadm 命令行用法和 iptables 命令行用法非常相似，毕竟是兄弟，比如 <code>-L</code> 列举， <code>-A</code> 添加， <code>-D</code> 删除。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.193.172:32016 <span class="_">-s</span> rr</span><br></pre></td></tr></table></figure></p>
<p>但是其实 ipvsadm 相对 iptables 命令简直太简单了，因为没有像 iptables 那样存在各种table，table 嵌套各种链，链里串着一堆规则，ipvsadm 就只有两个核心实体，分别为service 和 server，service 就是一个负载均衡实例，而 server 就是后端 member，ipvs术语中叫做 real server，简称 RS。</p>
<p>如下命令创建一个 service 实例 <code>172.17.0.1:32016</code>， -t 指定监听的为 TCP 端口， -s 指定算法为轮询算法 rr(Round Robin)，ipvs 支持简单轮询(rr)、加权轮询(wrr)、最少连接(lc)、源地址或者目标地址散列(sh、dh)等 10 种调度算法。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 172.17.0.1:32016 <span class="_">-s</span> rr</span><br></pre></td></tr></table></figure></p>
<p>然后把 10.244.1.2:8080、10.244.1.3:8080、10.244.3.2:8080 添加到 service 后端 member 中。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm <span class="_">-a</span> -t 172.17.0.1:32016 -r 10.244.1.2:8080 -m -w 1</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 172.17.0.1:32016 -r 10.244.1.3:8080 -m -w 1</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 172.17.0.1:32016 -r 10.244.3.2:8080 -m -w 1</span><br></pre></td></tr></table></figure></p>
<p>其中 <code>-t</code> 指定 service 实例， <code>-r</code> 指定 server 地址， <code>-w</code> 指定权值， <code>-m</code> 即前面说的转发模式，其中 <code>-m</code> 表示为 <code>masquerading</code>，即 NAT 模式， <code>-g</code> 为 <code>gatewaying</code>，即直连路由模式， <code>-i</code> 为 ipip，即 <code>IPIP</code> 隧道模式。</p>
<p>与 iptables-save、iptables-restore 对应的工具 ipvs 也有 ipvsadm-save、ipvsadm-restore。</p>
<h3 id="NAT-network-access-translation-模式"><a href="#NAT-network-access-translation-模式" class="headerlink" title="NAT(network access translation) 模式"></a>NAT(network access translation) 模式</h3><p>​NAT 模式由字面意思理解就是通过 NAT 实现的，但究竟是如何 NAT 转发的，我们通过实验环境验证下。</p>
<p>现环境中 LB 节点 IP 为 192.168.193.197，三个 RS 节点如下:</p>
<ul>
<li>192.168.193.172:30620</li>
<li>192.168.193.194:30620</li>
<li>192.168.193.226:30620</li>
</ul>
<p>为了模拟 LB 节点 IP 和 RS 不在同一个网络的情况，在 LB 节点中添加一个虚拟 IP 地址:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 10.222.0.1/24 dev ens5</span><br></pre></td></tr></table></figure></p>
<p>创建负载均衡 Service 并把 RS 添加到 Service 中:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 10.222.0.1:8080 <span class="_">-s</span> rr</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 10.222.0.1:8080 -r 192.168.193.194:30620 -m</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 10.222.0.1:8080 -r 192.168.193.226:30620 -m</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 10.222.0.1:8080 -r 192.168.193.172:30620 -m</span><br></pre></td></tr></table></figure></p>
<p>这里需要注意的是，和应用层负载均衡如 haproxy、nginx 不一样的是，haproxy、nginx进程是运行在用户态，因此会创建 socket，本地会监听端口，而 <strong>ipvs 的负载是直接运行在内核态的，因此不会出现监听端口</strong>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-197:/var/<span class="built_in">log</span><span class="comment"># netstat -lnpt</span></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      674/systemd-resolve</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      950/sshd</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      950/sshd</span><br></pre></td></tr></table></figure></p>
<p><strong>可见并没有监听 10.222.0.1:8080 Socket</strong>。</p>
<p>Client 节点IP为 192.168.193.226，为了和 LB 节点的虚拟 IP 10.222.0.1通，我们手动添加静态路由如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip r add 10.222.0.1 via 192.168.193.197 dev ens5</span><br></pre></td></tr></table></figure></p>
<p>此时 Client 节点能够 ping 通 LB 节点 VIP:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># ping -c 2 -w 2 10.222.0.1</span></span><br><span class="line">PING 10.222.0.1 (10.222.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.222.0.1: icmp_seq=1 ttl=64 time=0.345 ms</span><br><span class="line">64 bytes from 10.222.0.1: icmp_seq=2 ttl=64 time=0.249 ms</span><br><span class="line"></span><br><span class="line">--- 10.222.0.1 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1022ms</span><br><span class="line">rtt min/avg/max/mdev = 0.249/0.297/0.345/0.048 ms</span><br></pre></td></tr></table></figure></p>
<p>可见 Client 节点到 VIP 的链路没有问题，那是否能够访问我们的 Service 呢？</p>
<p>我们验证下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -m 2 --retry 1 -sSL 10.222.0.1:8080</span></span><br><span class="line">curl: (28) Connection timed out after 2001 milliseconds</span><br></pre></td></tr></table></figure></p>
<p>非常意外的结果是并不通。</p>
<p>在 RS 节点抓包如下:<br><img src="/images/k8s/kp-1.png" alt="tcpdump ipvs"></p>
<p>我们发现数据包的源 IP 为 Client IP，目标 IP 为 RS IP，换句话说，LB 节点 IPVS 只做了 DNAT，把目标 IP 改成 RS IP了，而没有修改源 IP。此时虽然 RS 和 Client 在同一个子网，链路连通性没有问题，<strong>但是由于 Client 节点发出去的包的目标 IP 和收到的包源 IP 不一致，因此会被直接丢弃，相当于给张三发信，李四回的信，显然不受信任</strong>。</p>
<p>既然 IPVS 没有给我们做 SNAT，那自然想到的是我们手动做 SNAT，在 LB 节点添加如下 iptables 规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -m ipvs  --vaddr 10.222.0.1 --vport 8080 -j LOG --log-prefix <span class="string">'[int32bit ipvs]'</span></span><br><span class="line">iptables -t nat -A POSTROUTING -m ipvs  --vaddr 10.222.0.1 --vport 8080 -j MASQUERADE</span><br></pre></td></tr></table></figure></p>
<p>再次检查 Service 是否可以访问:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -m 2 --retry 1 -sSL 10.222.0.1:8080</span></span><br><span class="line">curl: (28) Connection timed out after 2001 milliseconds</span><br></pre></td></tr></table></figure></p>
<p>服务依然不通。并且在 LB 节点的 iptables 日志为空:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-197:~<span class="comment"># cat /var/log/syslog | grep 'int32bit ipvs'</span></span><br><span class="line">root@ip-192-168-193-197:~<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>也就是说，ipvs 的包根本不会经过 iptables nat 表 POSTROUTING 链？</p>
<p>那 mangle 表呢？我们打开 LOG 查看下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A POSTROUTING -m ipvs --vaddr 10.222.0.1 --vport 8080 -j LOG --log-prefix <span class="string">"[int32bit ipvs]"</span></span><br></pre></td></tr></table></figure></p>
<p>此时查看日志如下：<br><img src="/images/k8s/kp-iptables_ipvs.png" alt="iptables_ipvs"></p>
<p>我们发现在 mangle 表中可以看到 DNAT 后的包。</p>
<p>只是 mangle 表的 POSTROUTING 并不支持 NAT 功能:<br><img src="/images/k8s/kp-dmesg_log.png" alt="dmesg_log"></p>
<p>对比 Kubernetes 配置发现需要设置如下系统参数:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.ipv4.vs.conntrack=1</span><br></pre></td></tr></table></figure></p>
<p>再次验证<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -i 10.222.0.1:8080</span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Type: text/plain</span><br><span class="line">Date: Wed, 27 Nov 2019 15:28:06 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line"></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-g9bkx | v=1</span><br></pre></td></tr></table></figure></p>
<p>终于通了，查看 RS 抓包：<br><img src="/images/k8s/kp-tcpdump_5.png" alt="tcpdump_5"></p>
<p>如期望，修改了源 IP 为 LB IP。</p>
<p>原来需要配置 net.ipv4.vs.conntrack = 1 参数，这个问题折腾了一个晚上，不得不说目前 ipvs 的文档都太老了。</p>
<p>前面是通过手动 iptables 实现 SNAT 的，性能可能会有损耗，于是如下开源项目通过修改 lvs 直接做 SNAT:</p>
<ul>
<li>小米运维部在 LVS 的 FULLNAT 基础上，增加了 SNAT 网关功能，参考 <a href="https://github.com/xiaomi-sa/dsnat" target="_blank" rel="external">xiaomi-sa/dsnat</a></li>
<li><a href="https://github.com/jlijian3/lvs-snat" target="_blank" rel="external">lvs-snat</a></li>
</ul>
<p>除了 SNAT 的办法，是否还有其他办法呢？想想我们最初的问题，Client 节点发出去的包的目标 IP 和收到的包源 IP 不一致导致包被丢弃，那解决问题的办法就是把包重新引到 LB 节点上，只需要在所有的 RS 节点增加如下路由即可:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip r add 192.168.193.226 via 192.168.193.197 dev ens5</span><br></pre></td></tr></table></figure></p>
<p>此时我们再次检查我们的 Service 是否可连接:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -i -m 2 --retry 1 -sSL 10.222.0.1:8080</span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Type: text/plain</span><br><span class="line">Date: Wed, 27 Nov 2019 03:21:47 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line"></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-4v9z4 | v=1</span><br></pre></td></tr></table></figure></p>
<p>结果没有问题。</p>
<p>不过我们是通过手动添加 Client IP 到所有 RS 的明细路由实现的，如果 Client 不固定，这种方案仍然不太可行，所以通常做法是干脆把所有 RS 默认路由指向 LB 节点，即把LB 节点当作所有 RS 的默认网关。</p>
<p>由此可知，用户通过 LB 地址访问服务，LB 节点 IPVS 会把用户的目标 IP 由 LB IP 改为 RS IP，源 IP 不变，包不经过 iptables 的 OUTPUT 直接到达 POSTROUTING 转发出去，包回来的时候也必须先到 LB 节点，LB 节点把目标 IP 再改成用户的源 IP，最后转发给用户。</p>
<p>显然这种模式来回都需要经过 LB 节点，因此又称为双臂模式。</p>
<h3 id="网关-Gatewaying-模式"><a href="#网关-Gatewaying-模式" class="headerlink" title="网关(Gatewaying)模式"></a>网关(Gatewaying)模式</h3><p>网关模式（Gatewaying）又称为直连路由模式（Direct Routing）、透传模式，<strong>所谓透传即 LB 节点不会修改数据包的源 IP、端口以及目标 IP、端口</strong>，LB 节点做的仅仅是路由转发出去，可以把 LB 节点看作一个特殊的路由器网关，而 RS 节点则是网关的下一跳，这就相当于对于同一个目标地址，会有多个下一跳，这个路由器网关的特殊之处在于能够根据一定的算法选择其中一个 RS 作为下一跳，达到负载均衡和冗余的效果。</p>
<p>既然是通过直连路由的方式转发，那显然 LB 节点必须与所有的 RS 节点在同一个子网，不能跨子网，否则路由不可达。换句话说，<strong>这种模式只支持内部负载均衡(Internal LoadBalancer)</strong>。</p>
<p>另外如前面所述，LB 节点不会修改源端口和目标端口，因此这种模式也无法支持端口映射，换句话说 <strong>LB 节点监听的端口和所有 RS 节点监听的端口必须一致</strong>。</p>
<p>现在假设有LB节点IP为 192.168.193.197，有三个 RS 节点如下:</p>
<ul>
<li>192.168.193.172:30620</li>
<li>192.168.193.194:30620</li>
<li>192.168.193.226:30620</li>
</ul>
<p>创建负载均衡Service并把RS添加到Service中:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.193.197:30620 <span class="_">-s</span> rr</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 192.168.193.197:30620 -r 192.168.193.194:30620 -g</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 192.168.193.197:30620 -r 192.168.193.226:30620 -g</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 192.168.193.197:30620 -r 192.168.193.172:30620 -g</span><br></pre></td></tr></table></figure></p>
<p>注意到我们的 Service 监听的端口 30620 和 RS 的端口是一样的，并且通过 <code>-g</code> 参数指定为直连路由模式(网关模式)。</p>
<p>Client 节点 IP 为 192.168.193.226，我们验证 Service 是否可连接：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -m 5 -sSL 192.168.193.197:30620</span></span><br><span class="line">curl: (28) Connection timed out after 5001 milliseconds</span><br></pre></td></tr></table></figure></p>
<p>我们发现并不通，在其中一个 RS 节点 192.168.193.172 上抓包:<br><img src="/images/k8s/kp-tcpdump_1.png" alt="tcpdump_1"></p>
<p>正如前面所说，LB 是通过路由转发的，根据路由的原理，源 MAC 地址修改为 LB 的MAC 地址，而目标 MAC 地址修改为 RS MAC 地址，相当于 RS 是 LB 的下一跳。</p>
<p>并且源 IP 和目标 IP 都不会修改。问题就来了，我们 Client 期望访问的是 RS，但 RS 收到的目标 IP 却是 LB 的 IP，发现这个目标 IP 并不是自己的 IP，因此不会通过 INPUT链转发到用户空间，这时要不直接丢弃这个包，要不根据路由再次转发到其他地方，总之两种情况都不是我们期望的结果。</p>
<p>那怎么办呢？为了让 RS 接收这个包，必须得让 R S有这个目标 IP 才行。于是不妨在 lo上添加个虚拟 IP，IP 地址伪装成 LB IP 192.168.193.197:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig lo:0 192.168.193.197/32</span><br></pre></td></tr></table></figure></p>
<p>问题又来了，这就相当于有两个相同的 IP，IP 重复了怎么办？办法是隐藏这个虚拟网卡，不让它回复 ARP，其他主机的 neigh 也就不可能知道有这么个网卡的存在了，参考 <a href="http://kb.linuxvirtualserver.org/wiki/Using_arp_announce/arp_ignore_to_disable_ARP" target="_blank" rel="external">Using arp announce/arp ignore to disable ARP</a>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.ipv4.conf.lo.arp_ignore=1</span><br><span class="line">sysctl net.ipv4.conf.lo.arp_announce=2</span><br></pre></td></tr></table></figure></p>
<p>此时再次从客户端curl:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -m 2 --retry 1 -sSL 192.168.193.197:30620</span></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-4v9z4 | v=1</span><br></pre></td></tr></table></figure></p>
<p>终于通了。</p>
<p>我们从前面的抓包中知道，源 IP 为 Client IP 192.168.193.226，因此直接回包给 Client即可，不可能也不需要再回到 LB 节点了，即 A-&gt;B,B-&gt;C，C-&gt;A，流量方向是三角形状的，因此这种模式又称为三角模式。</p>
<p>我们从原理中不难得出如下结论：</p>
<ul>
<li>Client、LB 以及所有的 RS 必须在同一个子网。</li>
<li>LB 节点直接通过路由转发，因此性能非常高。</li>
<li>不能做端口映射。</li>
</ul>
<h3 id="ipip-隧道模式"><a href="#ipip-隧道模式" class="headerlink" title="ipip 隧道模式"></a>ipip 隧道模式</h3><p>前面介绍了网关直连路由模式，要求所有的节点在同一个子网，而 ipip 隧道模式则主要解决这种限制，LB 节点 IP 和 RS 可以不在同一个子网，此时需要通过 ipip 隧道进行传输。</p>
<p>现在假设有 LB 节点 IP为 <code>192.168.193.77/25</code>，在该节点上增加一个VIP地址:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 192.168.193.48/25 dev eth0</span><br></pre></td></tr></table></figure></p>
<p>有三个 RS 节点如下:</p>
<ul>
<li>192.168.193.172:30620</li>
<li>192.168.193.194:30620</li>
<li>192.168.193.226:30620</li>
</ul>
<p>如上三个 RS 节点子网掩码均为 255.255.255.128，即 25 位子网，显然和 VIP 192.168.193.48/25 不在同一个子网。</p>
<p>创建负载均衡 Service 并把 RS 添加到 Service 中:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.193.48:30620 <span class="_">-s</span> rr</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 192.168.193.48:30620 -r 192.168.193.194:30620 -i</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 192.168.193.48:30620 -r 192.168.193.226:30620 -i</span><br><span class="line">ipvsadm <span class="_">-a</span> -t 192.168.193.48:30620 -r 192.168.193.172:30620 -i</span><br></pre></td></tr></table></figure></p>
<p>注意到我们的 Service 监听的端口 30620 和 RS 的端口是一样的，并且通过 -i 参数指定为 ipip 隧道模式。</p>
<p>在所有的 RS 节点上加载 ipip 模块以及添加 VIP (和直连路由类型）:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">modprobe ipip</span><br><span class="line">ifconfig tunl0  192.168.193.48/32</span><br><span class="line">sysctl net.ipv4.conf.tunl0.arp_ignore=1</span><br><span class="line">sysctl net.ipv4.conf.tunl0.arp_announce=2</span><br></pre></td></tr></table></figure></p>
<p>Client节点IP为192.168.193.226/25，我们验证 Service 是否可连接：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-226:~<span class="comment"># curl -i -sSL 192.168.193.48:30620</span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Type: text/plain</span><br><span class="line">Date: Wed, 27 Nov 2019 07:05:40 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line"></span><br><span class="line">Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-v1-c5ccf9784-dgn74 | v=1</span><br><span class="line">root@ip-192-168-193-226:~<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>Service 可访问，我们在 RS 节点上抓包如下：<br><img src="/images/k8s/kp-tcpdump_3.png" alt="tcpdump_3"></p>
<p>我们发现和直连路由一样，源 IP 和目标 IP 没有修改。</p>
<p>所以 IPIP 模式和网关 (Gatewaying) 模式原理基本一样，唯一不同的是网关 (Gatewaying) 模式要求所有的 RS 节点和 LB 节点在同一个子网，而 IPIP 模式则可以支持跨子网的情况，为了解决跨子网通信问题，使用了 ipip 隧道进行数据传输。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>ipvs 是一个内核态的四层负载均衡，支持 NAT、Gateway 以及 IPIP 隧道模式，Gateway 模式性能最好，但 LB 和 RS 不能跨子网，IPIP 性能次之，通过 ipip 隧道解决跨网段传输问题，因此能够支持跨子网。而 NAT 模式没有限制，这也是唯一一种支持端口映射的模式。</p>
<p>我们不难猜想，由于 Kubernetes Service 需要使用端口映射功能，因此 kube-proxy 必然只能使用 ipvs 的 NAT 模式。</p>
<h2 id="kube-proxy-使用-ipvs-模式"><a href="#kube-proxy-使用-ipvs-模式" class="headerlink" title="kube-proxy 使用 ipvs 模式"></a>kube-proxy 使用 ipvs 模式</h2><h3 id="配置-kube-proxy-使用-ipvs-模式"><a href="#配置-kube-proxy-使用-ipvs-模式" class="headerlink" title="配置 kube-proxy 使用 ipvs 模式"></a>配置 kube-proxy 使用 ipvs 模式</h3><p>使用 kubeadm 安装 Kubernetes 可参考文档 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md#cluster-created-by-kubeadm" target="_blank" rel="external">Cluster Created by Kubeadm</a>，不过这个文档的安装配置有问题 <a href="https://github.com/kubernetes/kubeadm/issues/1182" target="_blank" rel="external">kubeadm #1182</a>，如下官方配置不生效:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">kubeProxy:</span></span><br><span class="line"><span class="attr">  config:</span></span><br><span class="line"><span class="attr">    featureGates:</span></span><br><span class="line"><span class="attr">      SupportIPVSProxyMode:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    mode:</span> ipvs</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>需要修改为如下配置:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">mode:</span> ipvs</span><br></pre></td></tr></table></figure></p>
<p>可以通过如下命令确认 kube-proxy 是否修改为 ipvs:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get configmaps kube-proxy -n kube-system -o yaml | awk '/mode/&#123;print $2&#125;'</span></span><br><span class="line">ipvs</span><br></pre></td></tr></table></figure></p>
<h3 id="Service-ClusterIP-原理"><a href="#Service-ClusterIP-原理" class="headerlink" title="Service ClusterIP 原理"></a>Service ClusterIP 原理</h3><p>创建一个 ClusterIP 类似的 Service 如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc | grep kubernetes-bootcamp-v1</span></span><br><span class="line">kubernetes-bootcamp-v1   ClusterIP   10.96.54.11   &lt;none&gt;        8080/TCP   2m11s</span><br></pre></td></tr></table></figure></p>
<p>ClusterIP 10.96.54.11 为我们查看 ipvs 配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ipvsadm -S -n | grep 10.96.54.11</span></span><br><span class="line">-A -t 10.96.54.11:8080 <span class="_">-s</span> rr</span><br><span class="line"><span class="_">-a</span> -t 10.96.54.11:8080 -r 10.244.1.2:8080 -m -w 1</span><br><span class="line"><span class="_">-a</span> -t 10.96.54.11:8080 -r 10.244.1.3:8080 -m -w 1</span><br><span class="line"><span class="_">-a</span> -t 10.96.54.11:8080 -r 10.244.2.2:8080 -m -w 1</span><br></pre></td></tr></table></figure></p>
<p>可见 ipvs 的 LB IP 为 ClusterIP，算法为 rr，RS 为 Pod 的 IP。</p>
<p>另外我们发现使用的模式为 NAT 模式，这是显然的，因为除了 NAT 模式支持端口映射，其他两种均不支持端口映射，所以必须选择 NAT 模式。</p>
<p>由前面的理论知识，ipvs 的 VIP 必须在本地存在，我们可以验证:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ip addr show kube-ipvs0</span></span><br><span class="line">4: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link/ether 46:6b:9e:af:b0:60 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.54.11/32 brd 10.96.54.11 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="comment"># ethtool -i kube-ipvs0 | grep driver</span></span><br><span class="line">driver: dummy</span><br></pre></td></tr></table></figure></p>
<p>可见 kube-proxy 首先会创建一个 dummy 虚拟网卡 kube-ipvs0，然后把所有的 Service IP 添加到 kube-ipvs0 中。</p>
<p>我们知道基于 iptables 的 Service，ClusterIP 是一个虚拟的 IP，因此这个 IP 是 ping 不通的，但 ipvs 中这个 IP 是在每个节点上真实存在的，因此可以 ping 通:<br><img src="/images/k8s/kp-ping_cluster_ip.png" alt="ping_cluster_ip"></p>
<p>当然由于这个 IP 就是配置在本地虚拟网卡上，所以对诊断问题没有一点用处的。</p>
<p>我们接下来研究下 ClusterIP 如何传递的。</p>
<p>当我们通过如下命令连接服务时:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl 10.96.54.11:8080</span><br></pre></td></tr></table></figure></p>
<p>此时由于 10.96.54.11 就在本地，所以会以这个 IP 作为出口地址，即源 IP 和目标 IP 都是 10.96.54.11，此时相当于:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.96.54.11:xxxx -&gt; 10.96.54.11:8080</span><br></pre></td></tr></table></figure></p>
<p>其中 xxxx 为随机端口。</p>
<p>然后经过 ipvs，ipvs 会从 RS ip 列中选择其中一个 Pod ip 作为目标 IP，假设为10.244.2.2:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">10.96.54.11:xxxx -&gt; 10.96.54.11:8080</span><br><span class="line">                 |</span><br><span class="line">                 | IPVS</span><br><span class="line">                 v</span><br><span class="line">10.96.54.11:xxxx -&gt; 10.244.2.2:8080</span><br></pre></td></tr></table></figure></p>
<p>我们从 iptables LOG 可以验证:<br><img src="/images/k8s/kp-iptables_log_1.png" alt="iptables_log_1"></p>
<p>我们查看 OUTPUT 安全组规则如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-A OUTPUT -m comment --comment <span class="string">"kubernetes service portals"</span> -j KUBE-SERVICES</span><br><span class="line">-A KUBE-SERVICES ! <span class="_">-s</span> 10.244.0.0/16 -m comment --comment <span class="string">"Kubernetes service cluster ip + port for masquerade purpose"</span> -m <span class="built_in">set</span> --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure></p>
<p>其中 ipset 集合 <code>KUBE-CLUSTER-IP</code> 保存着所有的 ClusterIP 以及监听端口。</p>
<p>如上规则的意思就是除了 Pod 以外访问ClusterIP的包都打上 <code>0x4000/0x4000</code>。</p>
<p>到了 POSTROUTING 链:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-A POSTROUTING -m comment --comment <span class="string">"kubernetes postrouting rules"</span> -j KUBE-POSTROUTING</span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment <span class="string">"kubernetes service traffic requiring SNAT"</span> -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure></p>
<p>如上规则的意思就是只要匹配 <code>mark 0x4000/0x4000</code> 的包都做 SNAT，由于 10.244.2.2 是从 flannel.1 出去的，因此源 ip 会改成 flannel.1 的 ip <code>10.244.0.0</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">10.96.54.11:xxxx -&gt; 10.96.54.11:8080</span><br><span class="line">                 |</span><br><span class="line">                 | IPVS</span><br><span class="line">                 v</span><br><span class="line">10.96.54.11:xxxx -&gt; 10.244.2.2:8080</span><br><span class="line">                 |</span><br><span class="line">                 | MASQUERADE</span><br><span class="line">                 v</span><br><span class="line">10.244.0.0:xxxx -&gt; 10.244.2.2:8080</span><br></pre></td></tr></table></figure></p>
<p>最后通过 Vxlan 隧道发到 Pod 的 Node上，转发给 Pod 的 veth，回包通过路由到达源Node 节点，源 Node 节点通过之前的 MASQUERADE 再把目标IP还原为 10.96.54.11。</p>
<h3 id="NodeIP-实现原理"><a href="#NodeIP-实现原理" class="headerlink" title="NodeIP 实现原理"></a>NodeIP 实现原理</h3><p>查看 Service 如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ip-192-168-193-172:~<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME                     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes               ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP          30h</span><br><span class="line">kubernetes-bootcamp-v1   NodePort    10.96.54.11   &lt;none&gt;        8080:32016/TCP   8h</span><br></pre></td></tr></table></figure></p>
<p>Service kubernetes-bootcamp-v1 的 NodePort 为 32016。</p>
<p>现在假设集群外的一个 IP 192.168.193.197 访问 192.168.193.172:32016:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:32016</span><br></pre></td></tr></table></figure></p>
<p>最先到达 PREROUTING 链:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-A PREROUTING -m comment --comment <span class="string">"kubernetes service portals"</span> -j KUBE-SERVICES</span><br><span class="line">-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT</span><br><span class="line">-A KUBE-NODE-PORT -p tcp -m comment --comment <span class="string">"Kubernetes nodeport TCP port for masquerade purpose"</span> -m <span class="built_in">set</span> --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure></p>
<p>如上 4 条规则看起来复杂，其实就做一件事，如果目标地址为 NodeIP，则把包标记 <code>0x4000 / 0x4000</code>。</p>
<p>我们查看 ipvs:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ipvsadm -S -n | grep 32016</span></span><br><span class="line">-A -t 192.168.193.172:32016 <span class="_">-s</span> rr</span><br><span class="line"><span class="_">-a</span> -t 192.168.193.172:32016 -r 10.244.1.2:8080 -m -w 1</span><br><span class="line"><span class="_">-a</span> -t 192.168.193.172:32016 -r 10.244.1.3:8080 -m -w 1</span><br><span class="line"><span class="_">-a</span> -t 192.168.193.172:32016 -r 10.244.3.2:8080 -m -w 1</span><br></pre></td></tr></table></figure></p>
<p>我们发现和 ClusterIP 实现原理非常相似，ipvs Service 的 VIP 为 Node IP，端口为NodePort。ipvs 会选择其中一个 Pod IP 作为 DNAT 目标，这里假设为 10.244.3.2：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.193.197:xxxx -&gt; 192.168.193.172:32016</span><br><span class="line">                     |</span><br><span class="line">                     | DNAT</span><br><span class="line">                     v</span><br><span class="line">192.168.193.197:xxx  --&gt; 10.244.3.2:8080</span><br></pre></td></tr></table></figure></p>
<p>剩下的到了 POSTROUTING 链就和 Service ClusterIP 完全一样了，只要匹配 <code>0x4000/0x4000</code> 的包就会做SNAT。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>Kubernetes 的 ClusterIP 和 NodePort 都是通过 ipvs service 实现的，Pod 当作 ipvs service 的 server，通过 NAT MQSQ 实现转发。</p>
<p>简单来说 kube-proxy 主要在所有的 Node 节点做如下三件事:</p>
<ul>
<li>如果没有 dummy 类型虚拟网卡，则创建一个，默认名称为 <code>kube-ipvs0</code>;</li>
<li>把 Kubernetes ClusterIP 地址添加到 <code>kube-ipvs0</code>，同时添加到 ipset 中。</li>
<li>创建 ipvs service，ipvs service 地址为 ClusterIP 以及 Cluster Port，ipvs server 为所有的 Endpoint 地址，即 Pod IP 及端口。</li>
</ul>
<p>使用 ipvs 作为 kube-proxy 后端，不仅提高了转发性能，结合 ipset 还使 iptables 规则变得更“干净”清楚，从此再也不怕 iptables。</p>
<p>更多关于 kube-proxy ipvs 参考 <a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/" target="_blank" rel="external">IPVS-Based In-Cluster Load Balancing Deep Dive</a>。</p>
<h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了 kube-proxy 的功能以及 kube-proxy 基于 iptables 的实现原理，然后简单介绍了 ipvs，了解了 ipvs 支持的三种转发模式，最后介绍了 kube-proxy 基于 ipvs 的实现原理。</p>
<p>ipvs 是专门设计用来做内核态四层负载均衡的，由于使用了 hash 表的数据结构，因此相比 iptables 来说性能会更好。基于 ipvs 实现 Service 转发，Kubernetes 几乎能够具备无限的水平扩展能力。随着 Kubernetes 的部署规模越来越大，应用越来越广泛，ipvs必然会取代 iptables 成为 Kubernetes Service 的默认实现后端。</p>
<p>本文作者：int32bit</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用 eBPF 调试生产环境的 Go 程序]]></title>
      <url>http://team.jiunile.com/blog/2020/10/go-debug-with-bpf.html</url>
      <content type="html"><![CDATA[<p>不用重新编译/部署线上程序而是借助 eBPF 即可实现对程序进行调试，接下来我们会用一个系列文章介绍我们是怎么做的，这是开篇。本篇描述了如何使用 <a href="https://github.com/iovisor/gobpf" target="_blank" rel="external">gobpf</a> 和 uprobe 来构建一个跟踪 Go 程序函数入口参数变化的应用。这里介绍的技术可以扩展到其它编译型语言，如 C++, Rust 等等。本系列文章后续将会讨论如何使用 eBPF 来跟踪 HTTP/gRPC 数据和 SSL 等等。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>当调试程序时，我们一般对捕获程序的运行时状态非常感兴趣。因为这可以让我们检查程序在干什么，并能让我们确定 bug 出现在程序的哪一块。观察运行时状态的一个简单方式是使用调试器。比如针对 Go 程序，我们可以使用 Delve 和 gdb。</p>
<p>Delve 和 gdb 在开发环境中做调试表现没得说，但是我们一般不会在线上使用此类工具。它们的长处同时也是它们的短处，因为调试器会导致线上程序中断，甚至如果在调试过程中不小心改错某个变量的值而导致线上程序出现异常。</p>
<p>为了让线上调试过程的侵入和影响更小，我们将会探索使用增强版的 BPF (<a href="https://ebpf.io/" target="_blank" rel="external">eBPF</a>, Linux 4.x+ 内核可用）和更高级的 Go 库 <a href="https://github.com/iovisor/gobpf" target="_blank" rel="external">gobpf</a> 来达成目标。<br><a id="more"></a></p>
<h2 id="什么是-eBPF"><a href="#什么是-eBPF" class="headerlink" title="什么是 eBPF"></a>什么是 eBPF</h2><p>扩展型 BPF(eBPF) 是一项在 Linux 4.x+ 内核可用的技术。你可以把它看作一个轻量级的沙箱 VM, 它运行在 Linux 内核中并且提供了针对内核内存的可信访问。</p>
<p>就像下面要说的，eBPF 允许内核运行 BPF 字节码。虽然可用的前端（这里指的是编译器前端）语言多样，但通常都是 C 语言的真子集。通常 C 代码先通过 Clang 被编译为 BPF 字节码，然后字节被验证以确保可以安全执行。这些严格的验证保证了机器码不会有意或无意地危及 Linux 内核，同时也确保了 BPF 探针在每次被触发时将会执行有限数目的指令。这些保证确保了 eBPF 可以被用于性能敏感的应用中，比如包过滤，网络监控等等。</p>
<p>从功能上说，eBPF 允许你针对某些事件（如定时器事件，网络事件或是函数调用事件）运行受限的 C 代码。当因为一个函数调用事件被触发时，我们把这些 eBPF 代码叫做探针。这些探针既可以针对内核函数调用事件被触发（这时叫 kprobe, k 即 kernelspace), 也可以针对用户空间的函数调用事件被触发（这时叫 uprobe, u 即 userspace). 本篇文章讲解如何通过 uprobe 实现函数参数的动态追踪。</p>
<h2 id="Uprobes"><a href="#Uprobes" class="headerlink" title="Uprobes"></a>Uprobes</h2><p>Uprobes 允许我们通过插入一个 debug trap 指令（在 x86 上就是 <code>int3</code>) 触发一个软中断从而实现对运行在用户空间的程序进行拦截。这也是调试器的工作原理。uprobe 运行过程本质上与其它 BPF 程序一样，可以总结为下面图示：<br><img src="/images/go/bpf-tracing.jpg" alt="用于跟踪的 BPF（来自 Brendan Gregg)"></p>
<p>编译和验证过的 BPF 程序作为 uprobe 的一部分被执行，同时执行结果写入到一个 buffer 中。</p>
<p>下面让我们研究下 uprobes 如何起作用的。为了演示部署 uprobes 并捕获函数参数，我们会用到这个简单的 demo 应用。该 demo 相关部分下面介绍。</p>
<p><code>main()</code> 方法是一个简单的 HTTP server, 它暴露了一个监听 /e 端点的 GET 接口，该接口通过迭代逼近计算自然常数 <code>e</code>（也叫欧拉数）. <code>computeE</code> 方法有一个参数 iters, 它指定了逼近时的迭代次数。迭代次数越多，结果越精确，当然耗费 CPU 也越多。迭代逼近算法不是我们本次关注重点，感兴趣的可以自己研究下。我们仅对追踪调用 <code>computeE</code> 方法时的参数感兴趣。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">computeE</span><span class="params">(iterations <span class="keyword">int64</span>)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line">  res := <span class="number">2.0</span></span><br><span class="line">  fact := <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i := <span class="keyword">int64</span>(<span class="number">2</span>); i &lt; iterations; i++ &#123;</span><br><span class="line">    fact *= <span class="keyword">float64</span>(i)</span><br><span class="line">    res += <span class="number">1</span> / fact</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> res</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  http.HandleFunc(<span class="string">"/e"</span>, <span class="function"><span class="keyword">func</span><span class="params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;</span><br><span class="line">    <span class="comment">// ... 省略代码用于从 get 请求中解析 iters 参数，若为空则使用默认值</span></span><br><span class="line">    w.Write([]<span class="keyword">byte</span>(fmt.Sprintf(<span class="string">"e = %0.4f\n"</span>, computeE(iters))))</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">// 启动 server...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>为了进行后面的实验以及为最后采用 gdb 验证修改生效，我们采用如下指令编译该代码：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ go build  -gcflags <span class="string">"-N -l"</span> app.go</span><br></pre></td></tr></table></figure></p>
<p>为了理解 uprobe 如何工作的，我们看看可执行文件中要追踪的符号。既然 uprobes 通过插入一个 debug trap 指令到可执行文件来实现，我们先要确定要追踪的函数地址是什么。Go 程序在 Linux 上的二进制采用 ELF 格式存储 debug 信息，该信息甚至在优化过的二进制中也是存在的，除非 debug 数据被裁剪掉了。我们可以使用命令 <code>objdump</code> 来检查二进制文件中的符号：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行下面命令之前需要你先将上面 go 程序编译为名为 app 的二进制文件。</span></span><br><span class="line"><span class="comment"># objdump --syms 可以从可执行程序中导出全部符号，然后通过 grep 查找 computeE.</span></span><br><span class="line"><span class="comment"># 具体输出可能与你机器上不同，这没什么问题。</span></span><br><span class="line">$ objdump --syms app | grep computeE</span><br><span class="line">00000000000x6600e0 g     F .text  000000000000004b             main.computeE</span><br></pre></td></tr></table></figure></p>
<p>从上述输出可以看到，<code>computeE</code> 方法的入口地址为 <code>0x0x6600e0</code>. 为了看一下这个地址附近的指令，我们可以通过 <code>objdump</code> 来反汇编该二进制文件（通过命令行选项 <code>-d</code>). 反汇编代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ objdump <span class="_">-d</span> app | grep -A 1 0x6600e0</span><br><span class="line">00000000000x6600e0 &lt;main.computeE&gt;:</span><br><span class="line">  0x6600e0:       48 8b 44 24 08          mov    0x8(%rsp),%rax</span><br></pre></td></tr></table></figure></p>
<p>从上面汇编代码可以看到当 <code>computeE</code> 方法被调用时会执行哪些指令。第一条指令是 <code>mov 0x8(%rsp),%rax</code>, 该指令将寄存器 <code>rsp</code> 保存的地址（栈指针寄存器保存的是 <code>computeE</code> 方法的入口地址）相对偏移量为 <code>0x8</code> 处的内容移动到寄存器 <code>rax</code> 中。这个被移动的值即为 <code>computeE</code> 方法的入参 <code>iterations</code> 的值。Go 程序的参数通过栈来传递。</p>
<p>好了，记住上面提到的信息，我们来看看如何实现针对 <code>computeE</code> 方法的参数追踪。</p>
<h2 id="构建追踪程序"><a href="#构建追踪程序" class="headerlink" title="构建追踪程序"></a>构建追踪程序</h2><p>我们给这个追踪程序起个名叫 Tracer. 为了捕获前面提到的事件，我们需要注册一个 uprobe 函数，并且还得有个用户态函数负责去读 uprobe 的输出，具体如下图所示：<br><img src="/images/go/app-tracer.png" alt="app-tracer"></p>
<p>我们编写一个叫做 <code>tracer</code> 的应用，由它负责注册 BPF 代码，同时读取这些 BPF 代码的输出。如上图所示，uprobe 将会简单地输出到一个 <code>perf-buffer</code> 中，该结构体是用于 perf 事件的 linux 内核数据结构。</p>
<p>万事俱备，我们来看看当我们增加一个 uprobe 时会发生哪些事情。下面的图显示了 Linux 内核如何使用一个 uprobe 来修改一个已有的二进制程序。前文提到的软中断 <code>int3</code> 作为第一条指令被插入到 <code>main.computeE</code> 方法中。这条指令将会在执行时触发一个软中断，从而允许 Linux 内核来执行 BPF 代码。然后我们把 <code>computeE</code> 每次被调用时的参数输出到 perf-buffer 中，这些值会被我们编写的 <code>tracer</code> 应用异步地读取。<br><img src="/images/go/app-trace.png" alt="app-trace"></p>
<p>就我们这个需求来说，相应的 BPF 代码很简单，C 代码如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;uapi/linux/ptrace.h&gt;</span></span></span><br><span class="line">BPF_PERF_OUTPUT(trace);</span><br><span class="line"><span class="comment">// 该函数将会被注册，以便每次 main.computeE 被调用时该函数也会被调用</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">computeECalled</span><span class="params">(<span class="keyword">struct</span> pt_regs *ctx)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// main.computeE 的入参保存在了 ax 寄存器里。</span></span><br><span class="line">  <span class="keyword">long</span> val = ctx-&gt;ax;</span><br><span class="line">  trace.perf_submit(ctx, &amp;val, <span class="keyword">sizeof</span>(val));</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们注册上面代码以便 <code>main.computeE</code> 方法被调用它们也会被执行。这些代码被执行时，我们仅仅读取函数参数然后写到 perf-buffer 中。实现这个功能需要很多样板代码，为了方便示意这里都省掉了，完整的例子见 <a href="https://github.com/pixie-labs/pixie/blob/main/demos/simple-gotracing/trace_example/trace.go" target="_blank" rel="external">这里</a>.</p>
<p>好了，我们现在有个针对 <code>main.computeE</code> 的功能齐全的端到端参数追踪器了！执行结果见下面动图：<br><img src="/images/go/e2e-demo.gif" alt="End-to-End demo"></p>
<p>上述动图执行步骤如下：</p>
<ol>
<li>在 localhost:9090 启动待追踪程序 <code>./app</code>, 此时我们可以用 curl 访问该应用了，具体命令为 <code>curl http://localhost:9090/e?iters=10</code></li>
<li>启动 trace 应用，注意指定参数 <code>sudo ./trace --binary ../app/app</code>, 参数是第一步中待追踪程序对应的二进制文件的路径。</li>
<li>不停的执行 curl 命令，使其 iters 参数取值不同，则会看到 trace 应用输出你指定的 iters 值。</li>
</ol>
<p>还有个有意思的事情，我们真的可以通过 GDB 看到针对二进制文件的修改。下面我们 dump 出 <code>0x0x6600e0</code> 处的指令，在我们运行 <code>trace</code> 之前是这样的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ gdb ./app</span><br><span class="line">(gdb) display /4i 0x6600e0</span><br><span class="line">1: x/4i 0x6600e0</span><br><span class="line">   0x6600e0 &lt;main.computeE&gt;:    sub    <span class="variable">$0x20</span>,%rsp</span><br><span class="line">   0x6600e4 &lt;main.computeE+4&gt;:  mov    %rbp,0x18(%rsp)</span><br><span class="line">   0x6600e9 &lt;main.computeE+9&gt;:  lea    0x18(%rsp),%rbp</span><br><span class="line">   0x6600ee &lt;main.computeE+14&gt;: xorps  %xmm0,%xmm0</span><br></pre></td></tr></table></figure></p>
<p>在我们运行 <code>trace</code> 之后，再次查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ gdb ./app</span><br><span class="line">(gdb) display /4i 0x65fecf</span><br><span class="line">2: x/4i 0x6600e0</span><br><span class="line">   0x6600e0 &lt;main.computeE&gt;:    int3   </span><br><span class="line">   0x6600e1 &lt;main.computeE+1&gt;:  sub    <span class="variable">$0x20</span>,%esp</span><br><span class="line">   0x6600e4 &lt;main.computeE+4&gt;:  mov    %rbp,0x18(%rsp)</span><br><span class="line">   0x6600e9 &lt;main.computeE+9&gt;:  lea    0x18(%rsp),%rbp</span><br></pre></td></tr></table></figure></p>
<p>看到了吗？<code>0x6600e0</code> 插入了 <code>int3</code> 指令。</p>
<p>尽管我们为这个特定的示例硬编码了跟踪程序，但是可以使这个过程通用化。Go 的许多特性，比如嵌套的指针、接口、通道等，使得这个过程具有挑战性，但解决这些问题可实现现有系统中无法使用的另一种检测模式。而且，由于这个过程是在二进制层面工作的，所以它可以用于其他语言编译的二进制文件 (c++、Rust 等）。我们只需要考虑他们各自 ABI 的差异。</p>
<h2 id="接下来？"><a href="#接下来？" class="headerlink" title="接下来？"></a>接下来？</h2><p>使用 uprobes 进行 BPF 跟踪具有其自身的优点和缺点。当我们需要对二进制状态进行观察时，使用 BPF 是有益的，即使在附加调试器将会有问题或有害的环境中运行（例如，生产二进制文件）。最大的缺点是，即使是很小的应用程序状态的跟踪也需要去编写代码。虽然 BPF 代码是相对容易的，但它的编写和维护是复杂的。如果没有实际的高级工具，就不太可能将其用于通用调试。</p>
<h2 id="番外"><a href="#番外" class="headerlink" title="番外"></a>番外</h2><h3 id="安装-BCC"><a href="#安装-BCC" class="headerlink" title="安装 BCC"></a>安装 BCC</h3><p>编译前文提到的 <code>trace</code> 应用之前需要安装 bcc. 以 Ubuntu 16.04 为例（其它系统请参考 <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md" target="_blank" rel="external">这里</a>):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb https://repo.iovisor.org/apt/<span class="variable">$(lsb_release -cs)</span> <span class="variable">$(lsb_release -cs)</span> main"</span> | sudo tee /etc/apt/sources.list.d/iovisor.list</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)</span><br></pre></td></tr></table></figure></p>
<p>如果安装速度慢，而且你设置了 <code>http_proxy/https_proxy</code>, 请编辑 <code>/etc/sudoers</code> 新增一行 <code>Defaults env_keep = &quot;http_proxy https_proxy&quot;</code>, 这样速度至少会有百倍提升。</p>
<h3 id="too-many-arguments-编译错误"><a href="#too-many-arguments-编译错误" class="headerlink" title="too many arguments 编译错误"></a>too many arguments 编译错误</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># github.com/iovisor/gobpf/bcc</span></span><br><span class="line">../../../../go/pkg/mod/github.com/iovisor/gobpf@v0.0.0-20200614202714<span class="_">-e</span>6b321d32103/bcc/module.go:98:40: too many arguments <span class="keyword">in</span> call to _Cfunc_bpf_module_create_c_from_string</span><br><span class="line">        have (*_C<span class="built_in">type</span>_char, number, **_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_int, _C<span class="built_in">type</span>__Bool, nil)</span><br><span class="line">        want (*_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_uint, **_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_int, _C<span class="built_in">type</span>__Bool)</span><br><span class="line">../../../../go/pkg/mod/github.com/iovisor/gobpf@v0.0.0-20200614202714<span class="_">-e</span>6b321d32103/bcc/module.go:230:28: too many arguments <span class="keyword">in</span> call to _C2func_bcc_func_load</span><br><span class="line">        have (unsafe.Pointer, _C<span class="built_in">type</span>_int, *_C<span class="built_in">type</span>_char, *_C<span class="built_in">type</span>_struct_bpf_insn, _C<span class="built_in">type</span>_int, *_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_uint, _C<span class="built_in">type</span>_int, *_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_uint, nil)</span><br><span class="line">        want (unsafe.Pointer, _C<span class="built_in">type</span>_int, *_C<span class="built_in">type</span>_char, *_C<span class="built_in">type</span>_struct_bpf_insn, _C<span class="built_in">type</span>_int, *_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_uint, _C<span class="built_in">type</span>_int, *_C<span class="built_in">type</span>_char, _C<span class="built_in">type</span>_uint)</span><br></pre></td></tr></table></figure>
<p>原因为 <a href="https://github.com/iovisor/gobpf/commit/3ecafd366e4b239946d03c17f5a4beb5aef4935e#diff-f11d8f44bec322f0ba3a2ee148c82966" target="_blank" rel="external">这一行</a> 增加的特性 Update bcc_func_load to libbcc 0.11 with hardware offload support, 以及 <a href="https://github.com/iovisor/gobpf/commit/cda73bdde3bf14fc898d07b8936073e1aa197708" target="_blank" rel="external">这一行</a> 增加的特性 bcc: update bpf_module_create_c_from_string for bcc 0.11.0 (fixes #202).</p>
<p>我没有深究具体是什么导致的（初步怀疑是系统版本）, 如果你急着看结果，可以根据上面报错地址知道到 <code>module.go</code> 文件，把涉及的两个函数的最后一个 <code>nil</code> 参数去掉就可以顺利编译了。</p>
<p>本文翻译自：Debugging Go in prod using eBPF 作者：Zain Asgar</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[（译）深入理解 Kubernetes 网络模型 - 自己实现 kube-proxy 的功能]]></title>
      <url>http://team.jiunile.com/blog/2020/10/k8s-node-proxy.html</url>
      <content type="html"><![CDATA[<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><ol>
<li>背景知识</li>
<li>节点代理模型</li>
<li>测试环境</li>
<li>实现：通过 userspace socket 实现 proxy</li>
<li>实现：通过 iptables 实现 proxy</li>
<li>实现：通过 ipvs/ipset 实现 proxy</li>
<li>实现：通过 bpf 实现 proxy</li>
<li>总结</li>
<li>参考文献</li>
<li>附录</li>
</ol>
<p>Kubernetes 中有几种类型的代理。其中有 <strong>node proxier</strong> 或 <code>kube-proxy</code>，它在每个节点上反映 Kubernetes API 中定义的服务，可以跨一组后端执行简单的 TCP/UDP/SCTP 流转发 [1]。</p>
<p>为了更好地理解节点代理模型，在这篇文章中，我们将用不同的方法设计和实现我们自己版本的 <code>kube-proxy</code>; 尽管这些只是 <code>toy-proxy</code>，但从<strong>透明流量拦截、转发、负载均衡</strong>等方面来说，它们的工作方式与 K8S 集群中运行的普通 <code>kube-proxy</code> 基本相同。</p>
<p>通过我们的 <code>toy-proxy</code> 程序，非 K8S 节点（不在 K8S 集群中）上的应用程序（无论是宿主本地应用程序，还是在 VM/容器中运行的应用程序）也可以通过 <strong>ClusterIP</strong> 访问 K8S 服务 – <strong>注意，在 kubernetes 的设计中，ClusterIP 只能在 K8S 集群节点中访问。（在某种意义上，我们的 <code>toy-proxy</code> 程序将非 K8S 节点变成了 K8S 节点。)</strong><br><a id="more"></a></p>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>了解 Linux 内核中的流量拦截和代理需要具备以下背景知识。</p>
<h3 id="Netfilter"><a href="#Netfilter" class="headerlink" title="Netfilter"></a>Netfilter</h3><p>Netfilter 是 Linux 内核内部的<strong>包过滤和处理框架</strong>。如果您不熟悉 Iptables 和 Netfilter 体系结构，请参阅 <a href="https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture" target="_blank" rel="external">A Deep Dive into Iptables and Netfilter Architecture</a></p>
<p>一些要点：</p>
<ul>
<li>主机上的<strong>所有数据包</strong>都将通过 netfilter 框架</li>
<li>在 netfilter 框架中有<strong> 5 个钩子</strong>点：<code>PRE_ROUTING</code>, <code>INPUT</code>, <code>FORWARD</code>, <code>OUTPUT</code>, <code>POST_ROUTING</code></li>
<li>命令行工具 <code>iptables</code> 可用于<strong>动态地将规则插入到钩子点中</strong></li>
<li>可以通过组合各种 <code>iptables</code> 规则来操作数据包（接受/重定向/删除/修改，等等）</li>
</ul>
<p><img src="/images/k8s/proxy/proxy_hooks.png" alt="The 5 hook points in netfilter framework"><br>此外，这 5 个钩子点还可以与内核的其他网络设施，如内核路由子系统进行协同工作。</p>
<p>此外，在每个钩子点中，规则被组织到具有预定义优先级的不同链中。为了按目的管理链，链被进一步组织到表中。现在有 5 个表：</p>
<ul>
<li><code>filter</code>: 做正常的过滤，如接受，拒绝/删，跳</li>
<li><code>nat</code>: 网络地址转换，包括 SNAT（源 nat) 和 DNAT（目的 nat)</li>
<li><code>mangle</code>: 修改包属性，例如 TTL</li>
<li><code>raw</code>: 最早的处理点，连接跟踪前的特殊处理 (conntrack 或 CT，也包含在上图中，但这不是链）</li>
<li><code>security</code>: 本文未涉及</li>
</ul>
<p>将表/链添加到上图中，我们可以得到更详细的视图：<br><img src="/images/k8s/proxy/proxy_hooks-and-tables.png" alt="iptables table/chains inside hook points"></p>
<h3 id="VIP-与负载均衡-LB"><a href="#VIP-与负载均衡-LB" class="headerlink" title="VIP 与负载均衡 (LB)"></a>VIP 与负载均衡 (LB)</h3><p>虚拟 IP (IP) 将所有后端 IP 隐藏给客户端/用户，因此客户端/用户总是与 VIP 的后端服务通信，而不需要关心 VIP 后面有多少实例。</p>
<p>VIP 总是伴随着负载均衡，因为它需要在不同的后端之间分配流量。<br><img src="/images/k8s/proxy/proxy_vip-and-lb.png" alt="VIP and load balancing"></p>
<h3 id="Cross-host-网络模型"><a href="#Cross-host-网络模型" class="headerlink" title="Cross-host 网络模型"></a>Cross-host 网络模型</h3><p>主机 A 上的实例（容器、VM 等）如何与主机 B 上的另一个实例通信？有很多解决方案：</p>
<ul>
<li>直接路由：BGP 等</li>
<li>隧道：VxLAN, IPIP, GRE 等</li>
<li>NAT: 例如 docker 的桥接网络模式</li>
<li>其它方式</li>
</ul>
<h2 id="节点代理模型"><a href="#节点代理模型" class="headerlink" title="节点代理模型"></a>节点代理模型</h2><p>在 kubernetes 中，您可以将应用程序定义为 <code>Service</code>。<code>Service</code> 是一种抽象，它定义了一组 Pods 的逻辑集和访问它们的策略。</p>
<h3 id="Service-类型"><a href="#Service-类型" class="headerlink" title="Service 类型"></a>Service 类型</h3><p>K8S 中定义了 4 种 <code>Service</code> 类型：</p>
<ul>
<li><code>ClusterIP</code>: 通过 VIP 访问 Service，但该 VIP 只能在此集群内访问</li>
<li><code>NodePort</code>: 通过 NodeIP:NodePort 访问 Service，这意味着该端口将保留在集群内的所有节点上</li>
<li><code>ExternalIP</code>: 与 <code>ClusterIP</code> 相同，但是这个 VIP 可以从这个集群之外访问</li>
<li><code>LoadBalancer</code></li>
</ul>
<p>这篇文章将关注 <code>ClusterIP</code>，但是其他三种类型在流量拦截和转发方面的底层实现非常相似。</p>
<h3 id="节点代理"><a href="#节点代理" class="headerlink" title="节点代理"></a>节点代理</h3><p>一个 Service 有一个 VIP （本文中的 <code>ClusterIP</code>) 和多个端点（后端 pods)。每个 pod 或节点都可以通过 VIP 直接访问应用程序。要做到这一点，节点代理程序需要在每个节点上运行，它应该能够透明地拦截到任何 <code>ClusterIP:Port</code>[注解 1] 的流量，并将它们重定向到一个或多个后端 pods。<br><img src="/images/k8s/proxy/proxy_k8s-proxier-model.png" alt="Kubernetes proxier model"></p>
<blockquote>
<p>注解 1</p>
<p>对 <code>ClusterIP</code> 的一个常见误解是，<code>ClusterIP</code> 是可访问的——它们不是通过定义访问的。如果 ping 一个 <code>ClusterIP</code>，可能会发现它不可访问。</p>
<p>根据定义，<strong><protocol,clusterip,port></protocol,clusterip,port></strong> 元组独特地定义了一个服务（因此也定义了一个拦截规则）。例如，如果一个服务被定义为 <code>&lt;tcp,10.7.0.100,80&gt;</code>，那么代理只处理 <code>tcp:10.7.0.100:80</code> 的流量，其他流量，例如。<code>tcp:10.7.0.100:8080</code>, <code>udp:10.7.0.100:80</code> 将不会被代理。因此，也无法访问 ClusterIP (ICMP 流量）。</p>
<p>但是，如果您使用的是带有 IPVS 模式的 <code>kube-proxy</code>，那么确实可以通过 ping 访问<code>ClusterIP</code>。这是因为 IPVS 模式实现比定义所需要的做得更多。您将在下面几节中看到不同之处。</p>
</blockquote>
<h3 id="节点代理的角色：反向代理"><a href="#节点代理的角色：反向代理" class="headerlink" title="节点代理的角色：反向代理"></a>节点代理的角色：反向代理</h3><p>想想节点代理的作用，在 K8S 网络模型中，它实际上是一个反向代理，也就是说，在每个节点上，它将：</p>
<ul>
<li>将所有后端 Pods 隐藏到客户端</li>
<li>过滤所有出口流量（对后端的请求）</li>
</ul>
<p>对于 ingress traffic，它什么也不做。</p>
<h3 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a>性能问题</h3><p>如果我们在主机上有一个应用程序，并且在 K8S 集群中有 1K 个服务，那么我们永远无法猜测该应用程序在下一时刻将访问哪个服务（这里忽略网络策略）。因此，为了让应用程序能够访问所有服务，我们必须为节点上的所有服务应用所有代理规则。将这个想法推广到整个集群，这意味着：</p>
<p><strong>所有服务的代理规则应该应用于整个集群中的所有节点。</strong></p>
<p>在某种意义上，这是一个完全分布式的代理模型，因为任何节点都拥有集群的所有规则。</p>
<p>当集群变大时，这会导致严重的性能问题，因为每个节点上可能有数十万条规则 [6,7]。</p>
<h2 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h2><h3 id="集群拓扑和测试环境"><a href="#集群拓扑和测试环境" class="headerlink" title="集群拓扑和测试环境"></a>集群拓扑和测试环境</h3><p>我们将使用以下环境进行测试：</p>
<ul>
<li>一个 k8s 集群<ul>
<li>一个 master 节点</li>
<li>一个 node 节点</li>
<li>网络解决方案：直接路由 (PodIP 可直接路由）</li>
</ul>
</li>
<li>一个非 k8s 节点，但是它可以到达工作节点和 Pod（得益于直接路由网络方案）</li>
</ul>
<p><img src="/images/k8s/proxy/proxy_test-env.png" alt="test env"></p>
<p>我们将在工作节点上部署 Pods，并从 test 节点通过 <code>ClusterIP</code> 访问 Pods 中的应用程序。</p>
<h3 id="创建一个-Service"><a href="#创建一个-Service" class="headerlink" title="创建一个 Service"></a>创建一个 Service</h3><p>创建一个简单的 <code>Statefulset</code>，其中包括一个 <code>Service</code>，该 <code>Service</code> 将有一个或多个后端 Pods:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># see appendix for webapp.yaml</span></span><br><span class="line">$ kubectl create <span class="_">-f</span> webapp.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl get svc -o wide webapp</span><br><span class="line">NAME     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR</span><br><span class="line">webapp   ClusterIP   10.7.111.132   &lt;none&gt;        80/TCP    2m11s   app=webapp</span><br><span class="line"></span><br><span class="line">$ kubectl get pod -o wide | grep webapp</span><br><span class="line">webapp-0    2/2     Running   0    2m12s 10.5.41.204    node1    &lt;none&gt;  &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<p>应用程序在带有 tcp 协议的 80 端口上运行。</p>
<h3 id="可达性测试"><a href="#可达性测试" class="headerlink" title="可达性测试"></a>可达性测试</h3><p>首先访问 PodIP+Port:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl 10.5.41.204:80</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>成功的！然后用 <code>ClusterIP</code> 替换 PodIP 再试一次：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl 10.7.111.132:80</span><br><span class="line">^C</span><br></pre></td></tr></table></figure></p>
<p>正如所料，它是不可访问的！</p>
<p>在下一节中，我们将研究如何使用不同的方法使 <code>ClusterIP</code> 可访问。</p>
<h2 id="实现：通过-userspace-socket-实现-proxy"><a href="#实现：通过-userspace-socket-实现-proxy" class="headerlink" title="实现：通过 userspace socket 实现 proxy"></a>实现：通过 userspace socket 实现 proxy</h2><h3 id="中间人模型"><a href="#中间人模型" class="headerlink" title="中间人模型"></a>中间人模型</h3><p>最容易理解的实现是在此主机上的通信路径中插入我们的 <code>toy-proxy</code> 作为中间人：对于从本地客户端到 ClusterIP:Port 的每个连接，<strong>我们拦截该连接并将其分割为两个单独的连接</strong>:</p>
<ul>
<li>本地客户端和 <code>toy-proxy</code> 之间的连接</li>
<li>连接 <code>toy-proxy</code> 和后端 pods</li>
</ul>
<p>实现此目的的最简单方法是在用户空间中实现它：</p>
<ul>
<li><code>监听资源</code>: 启动一个守护进程，监听 K8S apiserver、监视服务 (ClusterIP) 和端点 (Pod) 的变化</li>
<li><code>代理通信</code>: 对于从本地客户端到服务 (ClusterIP) 的每个连接请求，通过充当中间人来拦截请求</li>
<li><code>动态应用代理规则</code>: 对于任何 Service/Endpoint 更新，相应地更改 <code>toy-proxy</code> 连接设置</li>
</ul>
<p>对于我们上面的测试应用 <code>webapp</code>，数据流程如下图：<br><img src="/images/k8s/proxy/proxy_userspace-proxier.png" alt="userspace-proxier"></p>
<h3 id="POC-实现"><a href="#POC-实现" class="headerlink" title="POC 实现"></a>POC 实现</h3><p>让我们来看看上图的概念验证实现。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>以下代码省略了一些错误处理代码，便于阅读：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	clusterIP := <span class="string">"10.7.111.132"</span></span><br><span class="line">	podIP := <span class="string">"10.5.41.204"</span></span><br><span class="line">	port := <span class="number">80</span></span><br><span class="line">	proto := <span class="string">"tcp"</span></span><br><span class="line"></span><br><span class="line">	addRedirectRules(clusterIP, port, proto)</span><br><span class="line">	createProxy(podIP, port, proto)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">addRedirectRules</span><span class="params">(clusterIP <span class="keyword">string</span>, port <span class="keyword">int</span>, proto <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	p := strconv.Itoa(port)</span><br><span class="line">	cmd := exec.Command(<span class="string">"iptables"</span>, <span class="string">"-t"</span>, <span class="string">"nat"</span>, <span class="string">"-A"</span>, <span class="string">"OUTPUT"</span>, <span class="string">"-p"</span>, <span class="string">"tcp"</span>,</span><br><span class="line">		<span class="string">"-d"</span>, clusterIP, <span class="string">"--dport"</span>, p, <span class="string">"-j"</span>, <span class="string">"REDIRECT"</span>, <span class="string">"--to-port"</span>, p)</span><br><span class="line">	<span class="keyword">return</span> cmd.Run()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createProxy</span><span class="params">(podIP <span class="keyword">string</span>, port <span class="keyword">int</span>, proto <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">	host := <span class="string">""</span></span><br><span class="line">	listener, err := net.Listen(proto, net.JoinHostPort(host, strconv.Itoa(port)))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		inConn, err := listener.Accept()</span><br><span class="line">		outConn, err := net.Dial(proto, net.JoinHostPort(podIP, strconv.Itoa(port)))</span><br><span class="line"></span><br><span class="line">		<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(in, out *net.TCPConn)</span></span> &#123;</span><br><span class="line">			<span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">			wg.Add(<span class="number">2</span>)</span><br><span class="line">			fmt.Printf(<span class="string">"Proxying %v &lt;-&gt; %v &lt;-&gt; %v &lt;-&gt; %v\n"</span>,</span><br><span class="line">				in.RemoteAddr(), in.LocalAddr(), out.LocalAddr(), out.RemoteAddr())</span><br><span class="line">			<span class="keyword">go</span> copyBytes(in, out, &amp;wg)</span><br><span class="line">			<span class="keyword">go</span> copyBytes(out, in, &amp;wg)</span><br><span class="line">			wg.Wait()</span><br><span class="line">		&#125;(inConn.(*net.TCPConn), outConn.(*net.TCPConn))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	listener.Close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">copyBytes</span><span class="params">(dst, src *net.TCPConn, wg *sync.WaitGroup)</span></span> &#123;</span><br><span class="line">	<span class="keyword">defer</span> wg.Done()</span><br><span class="line">	<span class="keyword">if</span> _, err := io.Copy(dst, src); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">if</span> !strings.HasSuffix(err.Error(), <span class="string">"use of closed network connection"</span>) &#123;</span><br><span class="line">			fmt.Printf(<span class="string">"io.Copy error: %v"</span>, err)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	dst.Close()</span><br><span class="line">	src.Close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="一些解释"><a href="#一些解释" class="headerlink" title="一些解释"></a>一些解释</h4><h5 id="traffic-拦截"><a href="#traffic-拦截" class="headerlink" title="traffic 拦截"></a>traffic 拦截</h5><p>我们想拦截所有发往 <code>ClusterIP:Port</code> 的流量，但是在这个节点上任何设备都没有配置<code>ClusterIP</code>，因此我们无法执行诸如 listen（ClusterIP，Port）之类的操作，那么我们如何才能拦截呢？答案是：使用<code>iptables/netfilter</code> 提供的 <code>REDIRECT</code> 能力。</p>
<p>以下命令会将所有发往 <code>ClusterIP:Port</code> 的流量定向到 <code>localhost:Port</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -A OUTPUT -p tcp <span class="_">-d</span> <span class="variable">$CLUSTER_IP</span> --dport <span class="variable">$PORT</span> -j REDIRECT --to-port <span class="variable">$PORT</span></span><br></pre></td></tr></table></figure></p>
<p>如果你现在不能理解这一点，不要害怕。稍后我们将讨论这个问题。</p>
<p>通过下面命令的输出来验证这一点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L -n</span><br><span class="line">...</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>      destination</span><br><span class="line">REDIRECT   tcp  --  0.0.0.0/0   10.7.111.132         tcp dpt:80 redir ports 80</span><br></pre></td></tr></table></figure></p>
<p>在代码中，函数 <code>addRedirectRules()</code> 包装了上述过程。</p>
<h5 id="创建-proxy"><a href="#创建-proxy" class="headerlink" title="创建 proxy"></a>创建 proxy</h5><p>函数 <code>createProxy()</code> 创建用户空间代理，并执行双向转发。</p>
<h4 id="可达性测试-1"><a href="#可达性测试-1" class="headerlink" title="可达性测试"></a>可达性测试</h4><p>编译代码并执行二进制文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ go build toy-proxy-userspace.go</span><br><span class="line">$ sudo ./toy-proxy-userspace</span><br></pre></td></tr></table></figure></p>
<p>现在测试访问：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>成功！我们的代理传达的信息是：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ./toy-proxy-userspace</span><br><span class="line">Creating proxy between &lt;host ip&gt;:53912 &lt;-&gt; 127.0.0.1:80 &lt;-&gt; &lt;host ip&gt;:40194 &lt;-&gt; 10.5.41.204:80</span><br></pre></td></tr></table></figure></p>
<p>表示，对于原 <code>&lt;host ip&gt;:53912 &lt;-&gt; 10.7.111.132:80</code> 的连接请求，将其拆分为两个连接：</p>
<ol>
<li><code>&lt;host ip&gt;:53912 &lt;-&gt; 127.0.0.1:80</code></li>
<li><code>&lt;host ip&gt;:40194 &lt;-&gt; 10.5.41.204:80</code></li>
</ol>
<p>删除这条规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L -n --line-numbers</span><br><span class="line">...</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">num  target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">2    REDIRECT   tcp  --  0.0.0.0/0   10.7.111.132         tcp dpt:80 redir ports 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># iptables -t nat -D OUTPUT &lt;num&gt;</span></span><br><span class="line">$ iptables -t nat -D OUTPUT 2</span><br></pre></td></tr></table></figure></p>
<p>或者删除（刷新）所有规则，如果你把 iptabels 弄的一团糟的情况下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -F <span class="comment"># delete all rules</span></span><br><span class="line">$ iptables -t nat -X <span class="comment"># delete all custom chains</span></span><br></pre></td></tr></table></figure></p>
<h4 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h4><p>在这个 <code>toy-proxy</code> 实现中，我们拦截了 <code>ClusterIP:80</code> 到 <code>localhost:80</code>，但是如果该主机上的本机应用程序也想使用 <code>localhost:80</code> 怎么办？此外，如果多个服务都公开 80 端口会怎样？显然，我们需要区分这些应用程序或服务。解决这个问题的正确方法是：为每个代理分配一个未使用的临时端口 TmpPort，拦截 <code>ClusterIP:Port</code> 到 <code>local:TmpPort</code>。例如，app1 使用 10001, app2 使用 10002。</p>
<p>其次，上面的代码只处理一个后端，如果有多个后端 pods 怎么办？因此，我们需要通过负载均衡算法将请求分发到不同的后端 pods。<br><img src="/images/k8s/proxy/proxy_userspace-proxier-2.png" alt="userspace-proxier-2"></p>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>这种方法非常容易理解和实现，但是，它的性能会很差，因为它必须在两端以及内核和用户空间内存之间复制字节。</p>
<p>我们没有在这上面花太多时间，如果您感兴趣，可以在这里查看用户空间 <code>kube-proxy</code> 的简单实现。</p>
<p>接下来，让我们看看实现这个任务的另一种方法。</p>
<h2 id="实现：通过-iptables-实现-proxy"><a href="#实现：通过-iptables-实现-proxy" class="headerlink" title="实现：通过 iptables 实现 proxy"></a>实现：通过 iptables 实现 proxy</h2><p>用户空间代理程序的主要瓶颈来自内核-用户空间切换和数据复制。<strong>如果我们可以完全在内核空间中实现代理</strong>，它将在性能上大大提高，从而击败用户空间的代理。<code>iptables</code> 可用于实现这一目标。</p>
<p>在开始之前，让我们首先弄清楚在执行 <code>curl ClusterIP:Port</code> 时的流量路径，然后研究如何使用 <code>iptables</code> 规则使其可访问。</p>
<h3 id="Host-gt-ClusterIP-（单一后端）"><a href="#Host-gt-ClusterIP-（单一后端）" class="headerlink" title="Host -&gt; ClusterIP （单一后端）"></a>Host -&gt; ClusterIP （单一后端）</h3><p><code>ClusterIP</code> 不存在于任何网络设备上，所以为了让我们的数据包最终到达后端 Pod，我们需要将 <code>ClusterIP</code> 转换为 PodIP（可路由），即：</p>
<ul>
<li>条件：匹配 <code>dst=ClusterIP,proto=tcp,dport=80</code> 的数据包</li>
<li>操作：将数据包的 IP 报头中的 <code>dst=ClusterIP</code> 替换为 <code>dst=PodIP</code></li>
</ul>
<p>用网络术语来说，这是一个网络地址转换 (NAT) 过程。</p>
<h4 id="在哪里做-DNAT"><a href="#在哪里做-DNAT" class="headerlink" title="在哪里做 DNAT"></a>在哪里做 DNAT</h4><p>通过 curl 查看出口数据包路径（下图展示了数据流向过程）：<br><img src="/images/k8s/proxy/proxy_host-to-clusterip-dnat.png" alt="host-to-clusterip-dnat"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;curl process&gt; -&gt; raw -&gt; CT -&gt; mangle -&gt; dnat -&gt; filter -&gt; security -&gt; snat -&gt; &lt;ROUTING&gt; -&gt; mangle -&gt; snat -&gt; NIC</span><br></pre></td></tr></table></figure>
<p>很明显，在 OUTPUT 钩中只有一个 dnat（链），我们可以在其中进行 DNAT。</p>
<p>让我们看看我们将如何进行黑客入侵。</p>
<h4 id="检查当前的-NAT-规则"><a href="#检查当前的-NAT-规则" class="headerlink" title="检查当前的 NAT 规则"></a>检查当前的 NAT 规则</h4><p><code>NAT</code> 规则被组织到 <code>nat</code> 表中。检查 <code>nat</code> 表中的当前规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -t &lt;table&gt;</span></span><br><span class="line"><span class="comment"># -L list rules</span></span><br><span class="line"><span class="comment"># -n numeric output</span></span><br><span class="line">$ iptables -t nat -L -n</span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">DOCKER     all  --  0.0.0.0/0    !127.0.0.0/8   ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain POSTROUTING (policy ACCEPT)</span><br></pre></td></tr></table></figure></p>
<p>输出显示除了与 DOCKER 相关的规则外，没有其他规则。这些 DOCKER 规则是 DOCKER 在安装时插入的，但它们不会影响我们在这篇文章中的实验。所以我们忽略它们。</p>
<h4 id="增加-DNAT-规则"><a href="#增加-DNAT-规则" class="headerlink" title="增加 DNAT 规则"></a>增加 DNAT 规则</h4><p>为了便于查看，我们不会用 go 代码包装 <code>iptables</code> 命令，而是直接显示命令本身。</p>
<blockquote>
<p>注意：在继续之前，请确保删除了在上一节中添加的所有规则。</p>
</blockquote>
<p>确认目前无法访问 ClusterIP:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure></p>
<p>现在添加我们的出口 NAT 规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ cat ENV</span><br><span class="line">CLUSTER_IP=10.7.111.132</span><br><span class="line">POD_IP=10.5.41.204</span><br><span class="line">PORT=80</span><br><span class="line">PROTO=tcp</span><br><span class="line"></span><br><span class="line"><span class="comment"># -p               &lt;protocol&gt;</span></span><br><span class="line"><span class="comment"># -A               add rule</span></span><br><span class="line"><span class="comment"># --dport          &lt;dst port&gt;</span></span><br><span class="line"><span class="comment"># -d               &lt;dst ip&gt;</span></span><br><span class="line"><span class="comment"># -j               jump to</span></span><br><span class="line"><span class="comment"># --to-destination &lt;ip&gt;:&lt;port&gt;</span></span><br><span class="line">$ iptables -t nat -A OUTPUT -p <span class="variable">$PROTO</span> --dport <span class="variable">$PORT</span> <span class="_">-d</span> <span class="variable">$CLUSTER_IP</span> -j DNAT --to-destination <span class="variable">$POD_IP</span>:<span class="variable">$PORT</span></span><br></pre></td></tr></table></figure></p>
<p>再次检查规则表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L -n</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>      destination</span><br><span class="line">DNAT       tcp  --  0.0.0.0/0   10.7.111.132   tcp dpt:80 to:10.5.41.204:80</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到规则已经被添加。</p>
<h4 id="测试可达性"><a href="#测试可达性" class="headerlink" title="测试可达性"></a>测试可达性</h4><p>现在再一次访问：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>就是这样！访问成功。</p>
<p>但是等等！我们期望出口的交通应该是正确的，但我们没有添加任何 NAT 规则的入口路径，怎么可能交通是正常的两个方向？事实证明，当你为一个方向添加一个 NAT 规则时，Linux 内核会自动为另一个方向添加保留规则！这与 conntrack (CT，连接跟踪）模块协同工作。<br><img src="/images/k8s/proxy/proxy_host-to-clusterip-dnat-ct.png" alt="host-to-clusterip-dnat-ct"></p>
<h4 id="清理"><a href="#清理" class="headerlink" title="清理"></a>清理</h4><p>删除这些规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L -n --line-numbers</span><br><span class="line">...</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">num  target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">2    DNAT       tcp  --  0.0.0.0/0   10.7.111.132   tcp dpt:80 to:10.5.41.204:80</span><br><span class="line"></span><br><span class="line"><span class="comment"># iptables -t &lt;table&gt; -D &lt;chain&gt; &lt;num&gt;</span></span><br><span class="line">$ iptables -t nat -D OUTPUT 2</span><br></pre></td></tr></table></figure></p>
<h3 id="Host-gt-ClusterIP-（多个后端）"><a href="#Host-gt-ClusterIP-（多个后端）" class="headerlink" title="Host -&gt; ClusterIP （多个后端）"></a>Host -&gt; ClusterIP （多个后端）</h3><p>在上一节中，我们展示了如何使用一个后端 Pod 执行 NAT。现在让我们看看多后端情况。</p>
<blockquote>
<p>注意：在继续之前，请确保删除了在上一节中添加的所有规则。</p>
</blockquote>
<h4 id="伸缩-webapp"><a href="#伸缩-webapp" class="headerlink" title="伸缩 webapp"></a>伸缩 webapp</h4><p>首先扩大我们的服务到 2 个后端 pods:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale sts webapp --replicas=2</span><br><span class="line">statefulset.apps/webapp scaled</span><br><span class="line"></span><br><span class="line">$ kubectl get pod -o wide | grep webapp</span><br><span class="line">webapp-0   2/2     Running   0   1h24m   10.5.41.204    node1    &lt;none&gt; &lt;none&gt;</span><br><span class="line">webapp-1   2/2     Running   0   11s     10.5.41.5      node1    &lt;none&gt; &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<h4 id="通过负载平衡添加-DNAT-规则"><a href="#通过负载平衡添加-DNAT-规则" class="headerlink" title="通过负载平衡添加 DNAT 规则"></a>通过负载平衡添加 DNAT 规则</h4><p>我们需要 <code>iptables</code> 中的 <code>statistic</code> 模块以概率的方式将请求分发到后端 Pods，这样才能达到负载均衡的效果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -m &lt;module&gt;</span></span><br><span class="line">$ iptables -t nat -A OUTPUT -p <span class="variable">$PROTO</span> --dport <span class="variable">$PORT</span> <span class="_">-d</span> <span class="variable">$CLUSTER_IP</span> \</span><br><span class="line">    -m statistic --mode random --probability 0.5  \</span><br><span class="line">    -j DNAT --to-destination <span class="variable">$POD1_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">$ iptables -t nat -A OUTPUT -p <span class="variable">$PROTO</span> --dport <span class="variable">$PORT</span> <span class="_">-d</span> <span class="variable">$CLUSTER_IP</span> \</span><br><span class="line">    -m statistic --mode random --probability 1.0  \</span><br><span class="line">    -j DNAT --to-destination <span class="variable">$POD2_IP</span>:<span class="variable">$PORT</span></span><br></pre></td></tr></table></figure></p>
<p>上面的命令指定在两个 Pods 之间随机分配请求，每个都有 50% 的概率。</p>
<p>现在检查这些规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L -n</span><br><span class="line">...</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target  prot opt <span class="built_in">source</span>      destination</span><br><span class="line">DNAT    tcp  --  0.0.0.0/0   10.7.111.132  tcp dpt:80 statistic mode random probability 0.50000000000 to:10.5.41.204:80</span><br><span class="line">DNAT    tcp  --  0.0.0.0/0   10.7.111.132  tcp dpt:80 statistic mode random probability 1.00000000000 to:10.5.41.5:80</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/proxy/proxy_host-to-clusterip-lb-ct.png" alt="host-to-clusterip-lb-ct"></p>
<h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>现在，我们来验证下负载均衡是否生效。我们发出 8 个 请求，并捕获到这个主机通信的真实 PodIPs:</p>
<p>在测试节点上打开一个 shell:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..8&#125;; <span class="keyword">do</span> curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span> 2&gt;&amp;1 &gt;/dev/null; sleep 1; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>测试节点上的另一个 shell 窗口：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth0 port <span class="variable">$PORT</span> | grep <span class="string">"GET /"</span></span><br><span class="line">10.21.0.7.48306 &gt; 10.5.41.5.80:   ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48308 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48310 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48312 &gt; 10.5.41.5.80:   ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48314 &gt; 10.5.41.5.80:   ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48316 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48318 &gt; 10.5.41.5.80:   ... HTTP: GET / HTTP/1.1</span><br><span class="line">10.21.0.7.48320 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br></pre></td></tr></table></figure></p>
<p>在 Pod1 中有 4 次，在 Pod2 中有 4 次，每个 pod 有 50%，这正是我们所期望的。</p>
<h4 id="清理-1"><a href="#清理-1" class="headerlink" title="清理"></a>清理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L -n --line-numbers</span><br><span class="line">...</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">num  target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">2    DNAT    tcp  --  0.0.0.0/0   10.7.111.132  tcp dpt:80 statistic mode random probability 0.50000000000 to:10.5.41.204:80</span><br><span class="line">3    DNAT    tcp  --  0.0.0.0/0   10.7.111.132  tcp dpt:80 statistic mode random probability 1.00000000000 to:10.5.41.5:80</span><br><span class="line"></span><br><span class="line">$ iptables -t nat -D OUTPUT 2</span><br><span class="line">$ iptables -t nat -D OUTPUT 3</span><br></pre></td></tr></table></figure>
<h3 id="Pod-app-A-gt-ClusterIP-app-B"><a href="#Pod-app-A-gt-ClusterIP-app-B" class="headerlink" title="Pod (app A) -&gt; ClusterIP (app B)"></a>Pod (app A) -&gt; ClusterIP (app B)</h3><p>如果想通过 hostA 上的 <code>Pod A</code> 通过 <code>ClusterIP</code> 访问 <code>Pod B</code>, B 的 Pod 驻留在 hostB 上，我们应该做什么？</p>
<p>实际上，这与 <code>Host -&gt; ClusterIP</code> 情况非常相似，但是有一点需要注意：在执行 NAT 之后，源节点 (hostA) 需要将包发送到目的地 Pod 所在的正确目的地节点 (hostB)。根据不同的跨主机网络解决方案，这有很大不同：</p>
<ol>
<li>对于直接路由的情况下，主机只是发送数据包。对应的有这些解决方案<ul>
<li>calico + bird</li>
<li>cilium + kube-router(Cilium BGP 的默认解决方案） </li>
<li>cilium + bird（实际上这只是我们的测试环境网络解决方案）</li>
</ul>
</li>
<li>对于隧道的情况，每个主机上必须有一个代理，它在 DNAT 之后执行 encap，在 SNAT 之前执行 decap。这些解决方案包括：<ul>
<li>calico + VxLAN 模式</li>
<li>flannel + IPIP 模式</li>
<li>flannel + VxLAN 模式</li>
<li>cilium + VxLAN 模式</li>
</ul>
</li>
<li>像 aws 的 ENI 模式：类似于直接路由，但不需要 BGP 代理<ul>
<li>cilium + ENI 模式</li>
</ul>
</li>
</ol>
<p>下图展示了隧道的情况：<br><img src="/images/k8s/proxy/proxy_tunneling.png" alt="tunneling"></p>
<p>代理与隧道相关的职责包括：</p>
<ul>
<li><strong>同步所有节点之间的隧道信息</strong>，例如描述哪个实例在哪个节点上的信息</li>
<li><strong>在 DNAT 之后对 pod 流量执行封装</strong>: 对于所有的出口流量，例如来自 hostA 的<code>dst=&lt;PodIP&gt;</code>，其中 PodIP 在 hostB 上，通过添加另一个头来封装数据包，例如 VxLAN 头，其中封装头有 <code>src=hostA_IP,dst=hostB_IP</code></li>
<li><strong>在 SNAT 之前对 Pod 流量执行解封装</strong>: 解封装每个入口封装的数据包：删除外层（例如 VxLAN 标头）</li>
</ul>
<p>同时，主机需要决定：</p>
<ul>
<li>哪些数据包应该交给解码器 (pod 流量），哪些不应该（例如主机流量）</li>
<li>哪些包应该封装 (pod 流量），哪些不应该（例如主机流量）</li>
</ul>
<h3 id="重新构造-iptables-规则"><a href="#重新构造-iptables-规则" class="headerlink" title="重新构造 iptables 规则"></a>重新构造 iptables 规则</h3><blockquote>
<p>注意：在继续之前，请确保删除了在上一节中添加的所有规则。</p>
</blockquote>
<p>当您有大量的 Service 时，每个节点上的 iptables 规则将相当复杂，因此您需要进行一些结构化工作来组织这些规则。</p>
<p>在本节中，我们将在 nat 表中创建几个专用的 iptables 链，具体如下：</p>
<ul>
<li>链 <code>KUBE-SERVICES</code>: 拦截 nat 表的输出链中所有到此链的出口流量，如果它们被指定为 ClusterIP，则执行 DNAT</li>
<li>链 <code>KUBE-SVC-WEBAPP</code>: 如果 <code>dst</code>、<code>proto</code> 和 <code>port</code> 匹配，则拦截该链 <code>KUBE-SERVICES</code> 中的所有流量</li>
<li>链 <code>KUBE-SEP-WEBAPP1</code>: 拦截 50%的流量在 <code>KUBE-SVC-WEBAPP</code> 到这里</li>
<li>链 <code>KUBE-SEP-WEBAPP2</code>: 拦截 50%的流量在 <code>KUBE-SVC-WEBAPP</code> 到这里</li>
</ul>
<p>DNAT 路径现在为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OUTPUT -&gt; KUBE-SERVICES -&gt; KUBE-SVC-WEBAPP --&gt; KUBE-SEP-WEBAPP1</span><br><span class="line">                                         \</span><br><span class="line">                                          \--&gt; KUBE-SEP-WEBAPP2</span><br></pre></td></tr></table></figure></p>
<p>如果你有多个 Service，DNAT 路径如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">OUTPUT -&gt; KUBE-SERVICES -&gt; KUBE-SVC-A --&gt; KUBE-SEP-A1</span><br><span class="line">                      |              \--&gt; KUBE-SEP-A2</span><br><span class="line">                      |</span><br><span class="line">                      |--&gt; KUBE-SVC-B --&gt; KUBE-SEP-B1</span><br><span class="line">                      |              \--&gt; KUBE-SEP-B2</span><br><span class="line">                      |</span><br><span class="line">                      |--&gt; KUBE-SVC-C --&gt; KUBE-SEP-C1</span><br><span class="line">                                     \--&gt; KUBE-SEP-C2</span><br></pre></td></tr></table></figure></p>
<p>iptables 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ cat add-dnat-structured.sh</span><br><span class="line"><span class="built_in">source</span> ../ENV</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -x</span><br><span class="line"></span><br><span class="line">KUBE_SVCS=<span class="string">"KUBE-SERVICES"</span>        <span class="comment"># chain that serves as kubernetes service portal</span></span><br><span class="line">SVC_WEBAPP=<span class="string">"KUBE-SVC-WEBAPP"</span>     <span class="comment"># chain that serves as DNAT entrypoint for webapp</span></span><br><span class="line">WEBAPP_EP1=<span class="string">"KUBE-SEP-WEBAPP1"</span>    <span class="comment"># chain that performs dnat to pod1</span></span><br><span class="line">WEBAPP_EP2=<span class="string">"KUBE-SEP-WEBAPP2"</span>    <span class="comment"># chain that performs dnat to pod2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># OUTPUT -&gt; KUBE-SERVICES</span></span><br><span class="line">sudo iptables -t nat -N <span class="variable">$KUBE_SVCS</span></span><br><span class="line">sudo iptables -t nat -A OUTPUT -p all <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 -j <span class="variable">$KUBE_SVCS</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># KUBE-SERVICES -&gt; KUBE-SVC-WEBAPP</span></span><br><span class="line">sudo iptables -t nat -N <span class="variable">$SVC_WEBAPP</span></span><br><span class="line">sudo iptables -t nat -A <span class="variable">$KUBE_SVCS</span> -p <span class="variable">$PROTO</span> <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> <span class="variable">$CLUSTER_IP</span> --dport <span class="variable">$PORT</span> -j <span class="variable">$SVC_WEBAPP</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># KUBE-SVC-WEBAPP -&gt; KUBE-SEP-WEBAPP*</span></span><br><span class="line">sudo iptables -t nat -N <span class="variable">$WEBAPP_EP1</span></span><br><span class="line">sudo iptables -t nat -N <span class="variable">$WEBAPP_EP2</span></span><br><span class="line">sudo iptables -t nat -A <span class="variable">$WEBAPP_EP1</span> -p <span class="variable">$PROTO</span> <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 --dport <span class="variable">$PORT</span> -j DNAT --to-destination <span class="variable">$POD1_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">sudo iptables -t nat -A <span class="variable">$WEBAPP_EP2</span> -p <span class="variable">$PROTO</span> <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 --dport <span class="variable">$PORT</span> -j DNAT --to-destination <span class="variable">$POD2_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">sudo iptables -t nat -A <span class="variable">$SVC_WEBAPP</span> -p <span class="variable">$PROTO</span> <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 -m statistic --mode random --probability 0.5  -j <span class="variable">$WEBAPP_EP1</span></span><br><span class="line">sudo iptables -t nat -A <span class="variable">$SVC_WEBAPP</span> -p <span class="variable">$PROTO</span> <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 -m statistic --mode random --probability 1.0  -j <span class="variable">$WEBAPP_EP2</span></span><br></pre></td></tr></table></figure></p>
<p>现在测试我们设计：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ ./add-dnat-structured.sh</span><br><span class="line">++ KUBE_SVCS=KUBE-SERVICES</span><br><span class="line">++ SVC_WEBAPP=KUBE-SVC-WEBAPP</span><br><span class="line">++ WEBAPP_EP1=KUBE-SEP-WEBAPP1</span><br><span class="line">++ WEBAPP_EP2=KUBE-SEP-WEBAPP2</span><br><span class="line">++ sudo iptables -t nat -N KUBE-SERVICES</span><br><span class="line">++ sudo iptables -t nat -A OUTPUT -p all <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 -j KUBE-SERVICES</span><br><span class="line">++ sudo iptables -t nat -N KUBE-SVC-WEBAPP</span><br><span class="line">++ sudo iptables -t nat -A KUBE-SERVICES -p tcp <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 10.7.111.132 --dport 80 -j KUBE-SVC-WEBAPP</span><br><span class="line">++ sudo iptables -t nat -N KUBE-SEP-WEBAPP1</span><br><span class="line">++ sudo iptables -t nat -N KUBE-SEP-WEBAPP2</span><br><span class="line">++ sudo iptables -t nat -A KUBE-SEP-WEBAPP1 -p tcp <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 --dport 80 -j DNAT --to-destination 10.5.41.204:80</span><br><span class="line">++ sudo iptables -t nat -A KUBE-SEP-WEBAPP2 -p tcp <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 --dport 80 -j DNAT --to-destination 10.5.41.5:80</span><br><span class="line">++ sudo iptables -t nat -A KUBE-SVC-WEBAPP -p tcp <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 -m statistic --mode random --probability 0.5 -j KUBE-SEP-WEBAPP1</span><br><span class="line">++ sudo iptables -t nat -A KUBE-SVC-WEBAPP -p tcp <span class="_">-s</span> 0.0.0.0/0 <span class="_">-d</span> 0.0.0.0/0 -m statistic --mode random --probability 1.0 -j KUBE-SEP-WEBAPP2</span><br></pre></td></tr></table></figure></p>
<p>检查这些规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -L -n</span><br><span class="line">...</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-WEBAPP1 (1 references)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:80 to:10.5.41.204:80</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-WEBAPP2 (1 references)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:80 to:10.5.41.5:80</span><br><span class="line"></span><br><span class="line">Chain KUBE-SERVICES (1 references)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">KUBE-SVC-WEBAPP  tcp  --  0.0.0.0/0            10.7.111.132         tcp dpt:80</span><br><span class="line"></span><br><span class="line">Chain KUBE-SVC-WEBAPP (1 references)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">KUBE-SEP-WEBAPP1  tcp  --  0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000</span><br><span class="line">KUBE-SEP-WEBAPP2  tcp  --  0.0.0.0/0            0.0.0.0/0            statistic mode random probability 1.00000000000</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>成功！</p>
<p>如果您将上面的输出与普通的 <code>kube-proxy</code> 规则进行比较，这两个规则是非常相似的，下面是从启用 <code>kube-proxy</code> 的节点提取的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target         prot opt <span class="built_in">source</span>               destination</span><br><span class="line">KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br><span class="line"></span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line">target                     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">KUBE-SVC-YK2SNH4V42VSDWIJ  tcp  --  0.0.0.0/0            10.7.22.18           /* default/nginx:web cluster IP */ tcp dpt:80</span><br><span class="line"></span><br><span class="line">Chain KUBE-SVC-YK2SNH4V42VSDWIJ (1 references)</span><br><span class="line">target                     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">KUBE-SEP-GL2BLSI2B4ICU6WH  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx:web */ statistic mode random probability 0.33332999982</span><br><span class="line">KUBE-SEP-AIRRSG3CIF42U3PX  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx:web */</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-GL2BLSI2B4ICU6WH (1 references)</span><br><span class="line">target          prot opt <span class="built_in">source</span>               destination</span><br><span class="line">DNAT            tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx:web */ tcp to:10.244.3.181:80</span><br><span class="line"></span><br><span class="line">Chain KUBE-SEP-AIRRSG3CIF42U3PX (1 references)</span><br><span class="line">target          prot opt <span class="built_in">source</span>               destination</span><br><span class="line">DNAT            tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx:web */ tcp to:10.244.3.182:80</span><br></pre></td></tr></table></figure></p>
<h3 id="进一步重新构造-iptables-规则"><a href="#进一步重新构造-iptables-规则" class="headerlink" title="进一步重新构造 iptables 规则"></a>进一步重新构造 iptables 规则</h3><p>TODO: 为来自集群外部的流量添加规则。</p>
<h2 id="实现：通过-ipvs-实现-proxy"><a href="#实现：通过-ipvs-实现-proxy" class="headerlink" title="实现：通过 ipvs 实现 proxy"></a>实现：通过 ipvs 实现 proxy</h2><p>虽然基于 iptables 的代理在性能上优于基于用户空间的代理，但在集群服务过多的情况下也会导致性能严重下降 [6,7]。</p>
<p>本质上，这是因为 iptables 判决是基于链的，它是一个复杂度为 O(n) 的线性算法。iptables 的一个好的替代方案是 IPVS——内核中的 L4 负载均衡器，它在底层使用 ipset（哈希实现），因此复杂度为 O(1)。</p>
<p>让我们看看如何使用 ipvs 实现相同的目标。</p>
<blockquote>
<p>注意：在继续之前，请确保删除了在上一节中添加的所有规则。</p>
</blockquote>
<h3 id="安装-IPVS"><a href="#安装-IPVS" class="headerlink" title="安装 IPVS"></a>安装 IPVS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y ipvsadm</span><br><span class="line"></span><br><span class="line"><span class="comment"># -l  list load balancing status</span></span><br><span class="line"><span class="comment"># -n  numeric output</span></span><br><span class="line">$ ipvsadm -ln</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br></pre></td></tr></table></figure>
<p>默认无规则</p>
<h4 id="增加虚拟-真正的-services"><a href="#增加虚拟-真正的-services" class="headerlink" title="增加虚拟/真正的 services"></a>增加虚拟/真正的 services</h4><p>使用 ipvs 实现负载均衡：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -A/--add-service           add service</span></span><br><span class="line"><span class="comment"># -t/--tcp-service &lt;address&gt; VIP + Port</span></span><br><span class="line"><span class="comment"># -s &lt;method&gt;                scheduling-method</span></span><br><span class="line"><span class="comment"># -r/--real-server &lt;address&gt; real backend IP + Port</span></span><br><span class="line"><span class="comment"># -m                         masquerading (NAT)</span></span><br><span class="line">$ ipvsadm -A -t <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span> <span class="_">-s</span> rr</span><br><span class="line">$ ipvsadm <span class="_">-a</span> -t <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span> -r <span class="variable">$POD1_IP</span> -m</span><br><span class="line">$ ipvsadm <span class="_">-a</span> -t <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span> -r <span class="variable">$POD2_IP</span> -m</span><br></pre></td></tr></table></figure></p>
<p>或者使用我的脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./ipvs-add-server.sh</span><br><span class="line">Adding virtual server CLUSTER_IP:PORT=10.7.111.132:80 ...</span><br><span class="line">Adding real servers ...</span><br><span class="line">10.7.111.132:80 -&gt; 10.5.41.204</span><br><span class="line">10.7.111.132:80 -&gt; 10.5.41.5</span><br><span class="line">Done</span><br></pre></td></tr></table></figure></p>
<p>再次检查状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ipvsadm -ln</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.7.111.132:80 rr</span><br><span class="line">  -&gt; 10.5.41.5:80                 Masq    1      0          0</span><br><span class="line">  -&gt; 10.5.41.204:80               Masq    1      0          0</span><br></pre></td></tr></table></figure></p>
<p>一些解释：</p>
<ul>
<li>对于所有发往 <code>10.7.111.132:80</code> 的流量，将负载均衡到 <code>10.5.41.5:80</code> 和<code>10.5.41.204:80</code></li>
<li>使用轮询 (rr) 算法实现负载均衡</li>
<li>两个后端，每个后端的权重为 1（各 50％）</li>
<li>使用 MASQ（增强型 SNAT）在 VIP 和 RealIP 之间进行流量转发</li>
</ul>
<h3 id="验证-1"><a href="#验证-1" class="headerlink" title="验证"></a>验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..8&#125;; <span class="keyword">do</span> curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span> 2&gt;&amp;1 &gt;/dev/null; sleep 1; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">$ tcpdump -nn -i eth0 port <span class="variable">$PORT</span> | grep <span class="string">"HTTP: GET"</span></span><br><span class="line">IP 10.21.0.7.49556 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49558 &gt; 10.5.41.5.80  : ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49560 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49562 &gt; 10.5.41.5.80  : ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49566 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49568 &gt; 10.5.41.5.80  : ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49570 &gt; 10.5.41.204.80: ... HTTP: GET / HTTP/1.1</span><br><span class="line">IP 10.21.0.7.49572 &gt; 10.5.41.5.80  : ... HTTP: GET / HTTP/1.1</span><br></pre></td></tr></table></figure>
<p>完美！</p>
<h3 id="清理-2"><a href="#清理-2" class="headerlink" title="清理"></a>清理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./ipvs-del-server.sh</span><br><span class="line">Deleting real servers ...</span><br><span class="line">10.7.111.132:80 -&gt; 10.5.41.204</span><br><span class="line">10.7.111.132:80 -&gt; 10.5.41.5</span><br><span class="line">Deleting virtual server CLUSTER_IP:PORT=10.7.111.132:80 ...</span><br><span class="line">Done</span><br></pre></td></tr></table></figure>
<h2 id="实现：通过-bpf-实现-proxy"><a href="#实现：通过-bpf-实现-proxy" class="headerlink" title="实现：通过 bpf 实现 proxy"></a>实现：通过 bpf 实现 proxy</h2><p>这也是一个 <code>O(1)</code> 代理，但是与 IPVS 相比具有更高的性能。</p>
<p>让我们看看如何在不到 100 行 C 代码中使用 eBPF 实现代理功能。</p>
<h3 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h3><p>如果你有足够的时间和兴趣来阅读 eBPF/BPF，可以考虑阅读 <a href="https://docs.cilium.io/en/v1.6/bpf/" target="_blank" rel="external">Cilium: BPF and XDP Reference Guide</a>，它对开发人员来说是一个完美的 BPF 文档。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>让我们看看出口部分的基本概念：</p>
<ol>
<li>对于所有流量，匹配 <code>dst=CLUSTER_IP &amp;&amp; proto==TCP &amp;&amp; dport==80</code></li>
<li>更改目标 IP: <code>CLUSTER_IP -&gt; POD_IP</code></li>
<li>更新 IP 和 TCP 报头中的校验和文件（否则我们的数据包将被丢弃）</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">__section(<span class="string">"egress"</span>)</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tc_egress</span><span class="params">(<span class="keyword">struct</span> __sk_buff *skb)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> __be32 cluster_ip = <span class="number">0x846F070A</span>; <span class="comment">// 10.7.111.132</span></span><br><span class="line">    <span class="keyword">const</span> __be32 pod_ip = <span class="number">0x0529050A</span>;     <span class="comment">// 10.5.41.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> l3_off = ETH_HLEN;    <span class="comment">// IP header offset</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> l4_off = l3_off + <span class="number">20</span>; <span class="comment">// TCP header offset: l3_off + sizeof(struct iphdr)</span></span><br><span class="line">    __be32 sum;                     <span class="comment">// IP checksum</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> *data = (<span class="keyword">void</span> *)(<span class="keyword">long</span>)skb-&gt;data;</span><br><span class="line">    <span class="keyword">void</span> *data_end = (<span class="keyword">void</span> *)(<span class="keyword">long</span>)skb-&gt;data_end;</span><br><span class="line">    <span class="keyword">if</span> (data_end &lt; data + l4_off) &#123; <span class="comment">// not our packet</span></span><br><span class="line">        <span class="keyword">return</span> TC_ACT_OK;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">struct</span> iphdr *ip4 = (<span class="keyword">struct</span> iphdr *)(data + l3_off);</span><br><span class="line">    <span class="keyword">if</span> (ip4-&gt;daddr != cluster_ip || ip4-&gt;protocol != IPPROTO_TCP <span class="comment">/* || tcp-&gt;dport == 80 */</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> TC_ACT_OK;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DNAT: cluster_ip -&gt; pod_ip, then update L3 and L4 checksum</span></span><br><span class="line">    sum = csum_diff((<span class="keyword">void</span> *)&amp;ip4-&gt;daddr, <span class="number">4</span>, (<span class="keyword">void</span> *)&amp;pod_ip, <span class="number">4</span>, <span class="number">0</span>);</span><br><span class="line">    skb_store_bytes(skb, l3_off + offsetof(<span class="keyword">struct</span> iphdr, daddr), (<span class="keyword">void</span> *)&amp;pod_ip, <span class="number">4</span>, <span class="number">0</span>);</span><br><span class="line">    l3_csum_replace(skb, l3_off + offsetof(<span class="keyword">struct</span> iphdr, check), <span class="number">0</span>, sum, <span class="number">0</span>);</span><br><span class="line">	l4_csum_replace(skb, l4_off + offsetof(<span class="keyword">struct</span> tcphdr, check), <span class="number">0</span>, sum, BPF_F_PSEUDO_HDR);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> TC_ACT_OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于入口部分，非常类似于出口代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">__section(<span class="string">"ingress"</span>)</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tc_ingress</span><span class="params">(<span class="keyword">struct</span> __sk_buff *skb)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> __be32 cluster_ip = <span class="number">0x846F070A</span>; <span class="comment">// 10.7.111.132</span></span><br><span class="line">    <span class="keyword">const</span> __be32 pod_ip = <span class="number">0x0529050A</span>;     <span class="comment">// 10.5.41.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> l3_off = ETH_HLEN;    <span class="comment">// IP header offset</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> l4_off = l3_off + <span class="number">20</span>; <span class="comment">// TCP header offset: l3_off + sizeof(struct iphdr)</span></span><br><span class="line">    __be32 sum;                     <span class="comment">// IP checksum</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> *data = (<span class="keyword">void</span> *)(<span class="keyword">long</span>)skb-&gt;data;</span><br><span class="line">    <span class="keyword">void</span> *data_end = (<span class="keyword">void</span> *)(<span class="keyword">long</span>)skb-&gt;data_end;</span><br><span class="line">    <span class="keyword">if</span> (data_end &lt; data + l4_off) &#123; <span class="comment">// not our packet</span></span><br><span class="line">        <span class="keyword">return</span> TC_ACT_OK;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">struct</span> iphdr *ip4 = (<span class="keyword">struct</span> iphdr *)(data + l3_off);</span><br><span class="line">    <span class="keyword">if</span> (ip4-&gt;saddr != pod_ip || ip4-&gt;protocol != IPPROTO_TCP <span class="comment">/* || tcp-&gt;dport == 80 */</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> TC_ACT_OK;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SNAT: pod_ip -&gt; cluster_ip, then update L3 and L4 header</span></span><br><span class="line">    sum = csum_diff((<span class="keyword">void</span> *)&amp;ip4-&gt;saddr, <span class="number">4</span>, (<span class="keyword">void</span> *)&amp;cluster_ip, <span class="number">4</span>, <span class="number">0</span>);</span><br><span class="line">    skb_store_bytes(skb, l3_off + offsetof(<span class="keyword">struct</span> iphdr, saddr), (<span class="keyword">void</span> *)&amp;cluster_ip, <span class="number">4</span>, <span class="number">0</span>);</span><br><span class="line">    l3_csum_replace(skb, l3_off + offsetof(<span class="keyword">struct</span> iphdr, check), <span class="number">0</span>, sum, <span class="number">0</span>);</span><br><span class="line">	l4_csum_replace(skb, l4_off + offsetof(<span class="keyword">struct</span> tcphdr, check), <span class="number">0</span>, sum, BPF_F_PSEUDO_HDR);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> TC_ACT_OK;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> __license[] __section(<span class="string">"license"</span>) = <span class="string">"GPL"</span>;</span><br></pre></td></tr></table></figure></p>
<h3 id="编译并加载到内核中"><a href="#编译并加载到内核中" class="headerlink" title="编译并加载到内核中"></a>编译并加载到内核中</h3><p>现在使用我的小脚本编译和加载到内核：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ./compile-and-load.sh</span><br><span class="line">...</span><br><span class="line">++ sudo tc filter show dev eth0 egress</span><br><span class="line">filter protocol all pref 49152 bpf chain 0</span><br><span class="line">filter protocol all pref 49152 bpf chain 0 handle 0x1 toy-proxy-bpf.o:[egress] direct-action not_<span class="keyword">in</span>_hw id 18 tag f5f39a21730006aa jited</span><br><span class="line"></span><br><span class="line">++ sudo tc filter show dev eth0 ingress</span><br><span class="line">filter protocol all pref 49152 bpf chain 0</span><br><span class="line">filter protocol all pref 49152 bpf chain 0 handle 0x1 toy-proxy-bpf.o:[ingress] direct-action not_<span class="keyword">in</span>_hw id 19 tag b41159c5873bcbc9 jited</span><br></pre></td></tr></table></figure></p>
<p>脚本是这样的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ cat compile-and-load.sh</span><br><span class="line"><span class="built_in">set</span> -x</span><br><span class="line"></span><br><span class="line">NIC=eth0</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile c code into bpf code</span></span><br><span class="line">clang -O2 -Wall -c toy-proxy-bpf.c -target bpf -o toy-proxy-bpf.o</span><br><span class="line"></span><br><span class="line"><span class="comment"># add tc queuing discipline (egress and ingress buffer)</span></span><br><span class="line">sudo tc qdisc del dev <span class="variable">$NIC</span> clsact 2&gt;&amp;1 &gt;/dev/null</span><br><span class="line">sudo tc qdisc add dev <span class="variable">$NIC</span> clsact</span><br><span class="line"></span><br><span class="line"><span class="comment"># load bpf code into the tc egress and ingress hook respectively</span></span><br><span class="line">sudo tc filter add dev <span class="variable">$NIC</span> egress bpf da obj toy-proxy-bpf.o sec egress</span><br><span class="line">sudo tc filter add dev <span class="variable">$NIC</span> ingress bpf da obj toy-proxy-bpf.o sec ingress</span><br><span class="line"></span><br><span class="line"><span class="comment"># show info</span></span><br><span class="line">sudo tc filter show dev <span class="variable">$NIC</span> egress</span><br><span class="line">sudo tc filter show dev <span class="variable">$NIC</span> ingress</span><br></pre></td></tr></table></figure></p>
<h3 id="验证-2"><a href="#验证-2" class="headerlink" title="验证"></a>验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="variable">$CLUSTER_IP</span>:<span class="variable">$PORT</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>完美！</p>
<h3 id="清理-3"><a href="#清理-3" class="headerlink" title="清理"></a>清理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tc qdisc del dev <span class="variable">$NIC</span> clsact 2&gt;&amp;1 &gt;/dev/null</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们用不同的方法手工实现了 <code>kube-proxy</code> 的核心功能。希望您现在对 kubernetes 节点代理有了更好的理解，以及关于网络的其他一些配置。</p>
<p>在这篇文章中使用的代码和脚本：<a href="https://github.com/icyxp/icyxp.github.io/tree/master/images/k8s/proxy/code" target="_blank" rel="external">这里</a>。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" rel="external">Kubernetes Doc: CLI - kube-proxy</a></li>
<li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0011-ipvs-proxier.md" target="_blank" rel="external">kubernetes/enhancements: enhancements/0011-ipvs-proxier.md</a></li>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types" target="_blank" rel="external">Kubernetes Doc: Service types</a></li>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/proxies/" target="_blank" rel="external">Proxies in Kubernetes - Kubernetes</a></li>
<li><a href="https://medium.com/@benmeier_/a-quick-minimal-ipvs-load-balancer-demo-d5cc42d0deb4" target="_blank" rel="external">A minimal IPVS Load Balancer demo</a></li>
<li><a href="https://docs.google.com/presentation/d/1BaIAywY2qqeHtyGZtlyAp89JIZs59MZLKcFLxKE6LyM/edit#slide=id.p3" target="_blank" rel="external">Scaling Kubernetes to Support 50,000 Services</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37230013" target="_blank" rel="external">华为云在 K8S 大规模场景下的 Service 性能优化实践</a></li>
</ol>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>webapp.yaml:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> webapp</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> webapp</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    name:</span> web</span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> webapp</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> StatefulSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> webapp</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">"webapp"</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> webapp</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> webapp</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line">      <span class="comment"># affinity:</span></span><br><span class="line">      <span class="comment">#   nodeAffinity:</span></span><br><span class="line">      <span class="comment">#     requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="comment">#       nodeSelectorTerms:</span></span><br><span class="line">      <span class="comment">#       - matchExpressions:</span></span><br><span class="line">      <span class="comment">#         - key: kubernetes.io/hostname</span></span><br><span class="line">      <span class="comment">#           operator: In</span></span><br><span class="line">      <span class="comment">#           values:</span></span><br><span class="line">      <span class="comment">#           - node1</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">        key:</span> smoke</span><br><span class="line"><span class="attr">        operator:</span> Equal</span><br><span class="line"><span class="attr">        value:</span> test</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> webapp</span><br><span class="line"><span class="attr">        image:</span> nginx-slim:<span class="number">0.8</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> web</span><br></pre></td></tr></table></figure></p>
<p>翻译自：Cracking kubernetes node proxy 作者：ArthurChiao</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes CPUThrottlingHigh 告警解析]]></title>
      <url>http://team.jiunile.com/blog/2020/10/k8s-cpu-alert.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在使用Kubernetes的过程中，我们看到过这样一个告警信息：</p>
<blockquote>
<p>[K8S]告警主题: CPUThrottlingHigh<br>告警级别: warning<br>告警类型: CPUThrottlingHigh<br>故障实例: xxxx<br>告警详情: 27% throttling of CPU in namespace kube-system for container kube-proxy in pod kube-proxy-9pj9j.<br>触发时间: 2020-05-08 17:34:17</p>
</blockquote>
<p>这个告警信息说明 <code>kube-proxy</code> 容器被 <code>throttling</code> 了，然而查看该容器的资源使用历史信息，发现该容器以及容器所在的节点的 CPU 资源使用率都不高：</p>
<p><img src="/images/k8s/m-1.png" alt="告警期间容器所在节点CPU使用率"><br><img src="/images/k8s/m-2.png" alt="告警期间kube-proxy的资源使用率"></p>
<p>经过我们的分析，发现该告警实际上是和 Kubernetes 对于 CPU 资源的限制和管控机制有关。Kubernetes 依赖于容器的 <code>runtime</code> 进行 CPU 资源的调度，而容器 <code>runtime</code> 以 Docker 为例，是借助于 <code>cgroup</code> 和 <code>CFS</code> 调度机制进行资源管控。本文基于这个告警案例，首先分析了 <code>CFS</code> 的基本原理，然后对于 Kubernetes 借助 <code>CFS</code> 进行 CPU 资源的调度和管控方法进行了介绍，最后使用一个例子来分析 <code>CFS</code> 的一些调度特性来解释这个告警的 <code>root cause</code> 和解决方案。</p>
<a id="more"></a>
<h2 id="CFS-基本原理"><a href="#CFS-基本原理" class="headerlink" title="CFS 基本原理"></a>CFS 基本原理</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>Linux 在 2.6.23 之后开始引入 CFS 逐步替代O1调度器作为新的进程调度器，正如它名字所描述的，<a href="https://www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt" target="_blank" rel="external">CFS(Completely Fair Scheduler) 调度器</a>追求的是对所有进程的全面公平，实际上它的做法就是在一个特定的调度周期内，保证所有待调度的进程都能被执行一遍，主要和当前已经占用的 CPU 时间经权重除权之后的值(vruntime，见下面公式)来决定本轮调度周期内所能占用的 CPU 时间，vruntime 越少，本轮能占用的 CPU时间越多；总体而言，CFS 就是通过保证各个进程 vruntime 的大小尽量一致来达到公平调度的效果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">进程的运行时间计算公式为:</span><br><span class="line">进程运行时间 = 调度周期 * 进程权重 / 所有进程权重之和</span><br><span class="line"></span><br><span class="line">vruntime = 进程运行时间 * NICE_0_LOAD / 进程权重 = (调度周期 * 进程权重 / 所有进程总权重) * NICE_0_LOAD / 进程权重 = 调度周期 * NICE_0_LOAD / 所有进程总权重</span><br></pre></td></tr></table></figure></p>
<p>通过上面两个公式，可以看到 vruntime 不是进程实际占用 CPU 的时间，而是剔除权重影响之后的 CPU 时间，这样所有进程在被调度决策的时候的依据是一致的，而实际占用 CPU 时间是经进程优先级权重放大的。这种方式使得系统的调度粒度更小来，更加适合高负载和多交互的场景。</p>
<h3 id="Kernel-配置"><a href="#Kernel-配置" class="headerlink" title="Kernel 配置"></a>Kernel 配置</h3><p>在kernel文件系统中，可以通过调整如下几个参数来改变CFS的一些行为：</p>
<ul>
<li><code>/proc/sys/kernel/sched_min_granularity_ns</code>，表示进程最少运行时间，防止频繁的切换，对于交互系统</li>
<li><code>/proc/sys/kernel/sched_nr_migrate</code>，在多 CPU 情况下进行负载均衡时，一次最多移动多少个进程到另一个 CPU 上</li>
<li><code>/proc/sys/kernel/sched_wakeup_granularity_ns</code>，表示进程被唤醒后至少应该运行的时间，这个数值越小，那么发生抢占的概率也就越高</li>
<li><code>/proc/sys/kernel/sched_latency_ns</code>，表示一个运行队列所有进程运行一次的时间长度(<strong>正常情况下的队列调度周期，P</strong>)</li>
<li><code>sched_nr_latency</code>，这个参数是内核内部参数，无法直接设置，是通过<code>sched_latency_ns/sched_min_granularity_ns</code> 这个公式计算出来的；在实际运行中，如果队列排队进程数 <code>nr_running &gt; sched_nr_latency</code>，则调度周期就不是<code>sched_latency_ns</code>，而是 <code>P = sched_min_granularity_ns * nr_running</code>，如果 <code>nr_running &lt;= sched_nr_latency</code>，则 <code>P = sched_latency_ns</code></li>
</ul>
<p>在阿里云的Kubernetes节点上，这些参数配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@iZxxxxxxxxxxxxxxxxxxxxZ ~]<span class="comment"># cat /proc/sys/kernel/sched_min_granularity_ns</span></span><br><span class="line">10000000</span><br><span class="line">[root@iZxxxxxxxxxxxxxxxxxxxxZ ~]<span class="comment"># cat /proc/sys/kernel/sched_nr_migrate</span></span><br><span class="line">32</span><br><span class="line">[root@iZxxxxxxxxxxxxxxxxxxxxZ ~]<span class="comment"># cat /proc/sys/kernel/sched_wakeup_granularity_ns</span></span><br><span class="line">15000000</span><br><span class="line">[root@iZxxxxxxxxxxxxxxxxxxxxZ ~]<span class="comment"># cat /proc/sys/kernel/sched_latency_ns</span></span><br><span class="line">24000000</span><br></pre></td></tr></table></figure></p>
<p>可以算出来 <code>sched_nr_latency = sched_latency_ns / sched_min_granularity_ns = 24000000 / 10000000 = 2.4</code></p>
<p>在阿里云普通的虚拟机上的参数如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@prod-tomcat-01 ~]<span class="comment"># cat /proc/sys/kernel/sched_min_granularity_ns</span></span><br><span class="line">3000000</span><br><span class="line">[root@prod-tomcat-01 ~]<span class="comment"># cat /proc/sys/kernel/sched_latency_ns</span></span><br><span class="line">15000000</span><br></pre></td></tr></table></figure></p>
<p>可以算出来 <code>sched_nr_latency = sched_latency_ns / sched_min_granularity_ns = 15000000 / 3000000 = 5</code></p>
<p>而在普通的CentOS Linux release 7.5.1804 (Core) 上的参数如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node-01 ~]<span class="comment"># cat /proc/sys/kernel/sched_min_granularity_ns</span></span><br><span class="line">3000000</span><br><span class="line">[root@k8s-node-01 ~]<span class="comment"># cat /proc/sys/kernel/sched_nr_migrate</span></span><br><span class="line">32</span><br><span class="line">[root@k8s-node-01 ~]<span class="comment"># cat /proc/sys/kernel/sched_wakeup_granularity_ns</span></span><br><span class="line">4000000</span><br><span class="line">[root@k8s-node-01 ~]<span class="comment"># cat /proc/sys/kernel/sched_latency_ns</span></span><br><span class="line">24000000</span><br></pre></td></tr></table></figure></p>
<p>可以算出来 <code>sched_nr_latency = sched_latency_ns / sched_min_granularity_ns = 24000000 / 3000000 = 8</code></p>
<p>可以看到，阿里云的 Kubernetes 节点设置了更长的最小执行时间，在进程队列稍有等待(2.4) 的时候就开始保证每个进程的最小运行时间不少于 10 毫秒。</p>
<h3 id="运行和观察"><a href="#运行和观察" class="headerlink" title="运行和观察"></a>运行和观察</h3><p>部署这样一个 yaml POD：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> busybox</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> busybox</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> busybox</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"64Mi"</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"250m"</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"128Mi"</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"500m"</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"while true; do sleep 10; done"</span> </span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">    name:</span> busybox</span><br><span class="line"><span class="attr">  restartPolicy:</span> Always</span><br></pre></td></tr></table></figure></p>
<p>可以看到该容器内部的进程对应的CPU调度信息变化如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node-04 ~]<span class="comment"># cat /proc/121133/sched</span></span><br><span class="line">sh (121133, <span class="comment">#threads: 1)</span></span><br><span class="line">-------------------------------------------------------------------</span><br><span class="line">se.exec_start                                :   20229360324.308323</span><br><span class="line">se.vruntime                                  :             0.179610</span><br><span class="line">se.sum_<span class="built_in">exec</span>_runtime                          :            31.190620</span><br><span class="line">se.nr_migrations                             :                   12</span><br><span class="line">nr_switches                                  :                   79</span><br><span class="line">nr_voluntary_switches                        :                   78</span><br><span class="line">nr_involuntary_switches                      :                    1</span><br><span class="line">se.load.weight                               :                 1024</span><br><span class="line">policy                                       :                    0</span><br><span class="line">prio                                         :                  120</span><br><span class="line">clock-delta                                  :                   26</span><br><span class="line">mm-&gt;numa_scan_seq                            :                    0</span><br><span class="line">numa_migrations, 0</span><br><span class="line">numa_faults_memory, 0, 0, 0, 0, -1</span><br><span class="line">numa_faults_memory, 1, 0, 0, 0, -1</span><br><span class="line">numa_faults_memory, 0, 1, 1, 0, -1</span><br><span class="line">numa_faults_memory, 1, 1, 0, 0, -1</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[root@k8s-node-04 ~]<span class="comment"># cat /proc/121133/sched</span></span><br><span class="line">sh (121133, <span class="comment">#threads: 1)</span></span><br><span class="line">-------------------------------------------------------------------</span><br><span class="line">se.exec_start                                :   20229480327.896307</span><br><span class="line">se.vruntime                                  :             0.149504</span><br><span class="line">se.sum_<span class="built_in">exec</span>_runtime                          :            33.325310</span><br><span class="line">se.nr_migrations                             :                   17</span><br><span class="line">nr_switches                                  :                   91</span><br><span class="line">nr_voluntary_switches                        :                   90</span><br><span class="line">nr_involuntary_switches                      :                    1</span><br><span class="line">se.load.weight                               :                 1024</span><br><span class="line">policy                                       :                    0</span><br><span class="line">prio                                         :                  120</span><br><span class="line">clock-delta                                  :                   31</span><br><span class="line">mm-&gt;numa_scan_seq                            :                    0</span><br><span class="line">numa_migrations, 0</span><br><span class="line">numa_faults_memory, 0, 0, 1, 0, -1</span><br><span class="line">numa_faults_memory, 1, 0, 0, 0, -1</span><br><span class="line">numa_faults_memory, 0, 1, 0, 0, -1</span><br><span class="line">numa_faults_memory, 1, 1, 0, 0, -1</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[root@k8s-node-04 ~]<span class="comment"># cat /proc/121133/sched</span></span><br><span class="line">sh (121133, <span class="comment">#threads: 1)</span></span><br><span class="line">-------------------------------------------------------------------</span><br><span class="line">se.exec_start                                :   20229520328.862396</span><br><span class="line">se.vruntime                                  :             1.531536</span><br><span class="line">se.sum_<span class="built_in">exec</span>_runtime                          :            34.053116</span><br><span class="line">se.nr_migrations                             :                   18</span><br><span class="line">nr_switches                                  :                   95</span><br><span class="line">nr_voluntary_switches                        :                   94</span><br><span class="line">nr_involuntary_switches                      :                    1</span><br><span class="line">se.load.weight                               :                 1024</span><br><span class="line">policy                                       :                    0</span><br><span class="line">prio                                         :                  120</span><br><span class="line">clock-delta                                  :                   34</span><br><span class="line">mm-&gt;numa_scan_seq                            :                    0</span><br><span class="line">numa_migrations, 0</span><br><span class="line">numa_faults_memory, 0, 0, 0, 0, -1</span><br><span class="line">numa_faults_memory, 1, 0, 0, 0, -1</span><br><span class="line">numa_faults_memory, 0, 1, 1, 0, -1</span><br><span class="line">numa_faults_memory, 1, 1, 0, 0, -1</span><br></pre></td></tr></table></figure></p>
<p>其中 sum_exec_runtime 表示实际运行的物理时间。</p>
<h2 id="Kubernetes-借助-CFS-进行-CPU-管理"><a href="#Kubernetes-借助-CFS-进行-CPU-管理" class="headerlink" title="Kubernetes 借助 CFS 进行 CPU 管理"></a>Kubernetes 借助 CFS 进行 CPU 管理</h2><h3 id="CFS-进行-CPU-资源限流-throtting-的原理"><a href="#CFS-进行-CPU-资源限流-throtting-的原理" class="headerlink" title="CFS 进行 CPU 资源限流(throtting) 的原理"></a>CFS 进行 CPU 资源限流(throtting) 的原理</h3><p>Kubernetes 的资源定义：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="string">"64Mi"</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"250m"</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="string">"128Mi"</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"500m"</span></span><br></pre></td></tr></table></figure></p>
<p>比如里面的 CPU 需求，会被翻译成容器 runtime 的运行时参数，并最终变成 cgroups 和CFS 的参数配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat cpu.shares</span><br><span class="line">256</span><br><span class="line">cat cpu.cfs_quota_us</span><br><span class="line">50000</span><br><span class="line">cat cpu.cfs_period_us</span><br><span class="line">100000</span><br></pre></td></tr></table></figure></p>
<p>这里有一个默认的参数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/sys/kernel/<span class="built_in">sched</span>_latency_ns</span><br><span class="line">24000000</span><br></pre></td></tr></table></figure></p>
<p>所以在这个节点上，正常压力下，系统的 CFS 调度周期是 24ms，CFS 重分配周期是 100ms，而该 POD 在一个重分配周期最多占用 50ms 的时间，在有压力的情况下，POD 可以占据的 CPU share 比例是 256。</p>
<p>下面一个例子可以说明不同资源需求的 POD 容器是如何在 CFS 的调度下占用 CPU 资源的：<br><img src="/images/k8s/m-3.png" alt="CPU资源配置和CFS调度"></p>
<p>在这个例子中，有如下系统配置情况：</p>
<ul>
<li>CFS 调度周期为 10ms，正常负载情况下，进程 ready 队列里面的进程在每 10ms 的间隔内都会保证被执行一次</li>
<li>CFS 重分配周期为 100ms，用于保证一个进程的 limits 设置会被反映在每 100ms 的重分配周期内可以占用的 CPU 时间数，在多核系统中，limit 最大值可以是 CFS重分配周期*CPU核数</li>
<li>该执行进程队列只有进程A和进程B两个进程</li>
<li>进程A和B定义的 CPU share 占用都一样，所以在系统资源紧张的时候可以保证A和B进程都可以占用可用 CPU 资源的一半</li>
<li>定义的 CFS 重分配周期都是 100ms</li>
<li>进程A在 100ms 内最多占用 50ms，进程B在 100ms 内最多占用 20ms</li>
</ul>
<p>所以在一个 CFS 重分配周期(相当于10个 CFS 调度周期)内，进程队列的执行情况如下：</p>
<ul>
<li>在前面的4个 CFS 调度周期内，进程A和B由于 share 值是一样的，所以每个 CFS 调度内(10ms)，进程A和B都会占用 5ms</li>
<li>在第4个 CFS 调度周期结束的时候，在本 CFS 重分配周期内，进程B已经占用了 20ms，在剩下的8个 CFS 调度周期即 80ms 内，进程B都会被限流，一直到下一个 CFS 重分配周期内，进程B才可以继续占用 CPU</li>
<li>在第5-7这3个 CFS 调度周期内，由于进程B被限流，所以进程A可以完全拥有这3个 CFS 调度的 CPU 资源，占用 30ms 的执行时间，这样在本 CFS 重分配周期内，进程A已经占用了50ms 的 CPU 时间，在后面剩下的3个 CFS 调度周期即后面的 30ms 内，进程A也会被限流，一直到下一个 CFS 重分配周期内，进程A才可以继续占用 CPU</li>
</ul>
<p>如果进程被限流了，可以在如下的路径看到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/fs/cgroup/cpu/kubepods/pod5326d6f4-789d-11ea-b093-fa163e23cb69/69336c973f9f414c3f9fdfbd90200b7083b35f4d54ce302a4f5<span class="built_in">fc</span>330f2889846/cpu.stat </span><br><span class="line"></span><br><span class="line">nr_periods 14001693</span><br><span class="line">nr_throttled 2160435</span><br><span class="line">throttled_time 570069950532853</span><br></pre></td></tr></table></figure></p>
<h3 id="本文开头问题的原因分析"><a href="#本文开头问题的原因分析" class="headerlink" title="本文开头问题的原因分析"></a>本文开头问题的原因分析</h3><p>根据 3.1 描述的原理，很容易理解本文开通的告警信息的出现，是由于在某些特定的 CFS 重分配周期内，<code>kube-proxy</code> 的 CPU 占用率超过了给它分配的 limits ，而参看 <code>kube-proxy daemonset</code> 的配置，确实它的 limits 配置只有 200ms，这就意味着在默认的100ms 的 CFS 重调度周期内，它只能占用 20ms，所以在特定繁忙场景会有问题<br><img src="/images/k8s/m-4.png" alt="resource limit"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat cpu.shares </span><br><span class="line">204</span><br><span class="line">cat cpu.cfs_period_us </span><br><span class="line">100000</span><br><span class="line">cat cpu.cfs_quota_us </span><br><span class="line">20000</span><br></pre></td></tr></table></figure>
<p>注：这里 cpu.shares 的计算方法如下：200x1024/1000~=204</p>
<p><strong>而这个问题的解决方案就是将 CPU limits 提高</strong>。</p>
<p>Zalando 公司有一个分享《<a href="https://www.youtube.com/watch?v=eBChCFD9hfs" target="_blank" rel="external">Optimizing Kubernetes Resource Requests/Limits for Cost-Efficiency and Latency / Henning Jacobs</a>》很好的讲述了 CPU 资源管理的问题，可以参考，这个演讲的PPT在这里可以找到。</p>
<p>更具体问题分析和讨论还可以参考如下文章：</p>
<ul>
<li><a href="https://github.com/kubernetes-monitoring/kubernetes-mixin/issues/108" target="_blank" rel="external">CPUThrottlingHigh false positives #108</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/67577" target="_blank" rel="external">CFS quotas can lead to unnecessary throttling #67577</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt" target="_blank" rel="external">CFS Bandwidth Control</a></li>
<li><a href="https://gist.github.com/bobrik/2030ff040fad360327a5fab7a09c4ff1" target="_blank" rel="external">Overly aggressive CFS</a></li>
</ul>
<p>其中《<a href="https://gist.github.com/bobrik/2030ff040fad360327a5fab7a09c4ff1" target="_blank" rel="external">Overly aggressive CFS</a>》里面还有几个小实验可以帮助大家更好的认识到CFS 进行 CPU m资源管控的特点：<br><img src="/images/k8s/m-5.png" alt="cfs demo"></p>
<p>本文作者：cloudvtech</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[记一次线上Go服务内存占用率过高问题排查]]></title>
      <url>http://team.jiunile.com/blog/2020/10/go-debug-memory2.html</url>
      <content type="html"><![CDATA[<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>某线上埋点上报机器偶尔触发内存占用过多的报警。ssh到机器top发现主要内存被埋点服务占用。之前重启过几次，但是过段时间仍然会发生内存占用过多的警报。下面是报警详情。</p>
<blockquote>
<p>[P1][PROBLEM][ali-e-xxx-service03.bj][][ all(#3) mem.memfree.percent 4.19575<5][o3>2019-10-28 10:20:00]</5][o3></p>
</blockquote>
<h2 id="问题推断"><a href="#问题推断" class="headerlink" title="问题推断"></a>问题推断</h2><p>埋点服务主要接收客户端压缩过的上报请求，并对请求数据做解压，投递到kafka，逻辑功能相对简单。初步怀疑是某些资源没有释放导致的内存泄露或Groutine泄露。</p>
<h2 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h2><p>由于代码不是由我们业务方维护的，首先向相关部门索要了代码权限。阅读了一下源码，所有资源释放都是由defer进行操作的，并没有发现很明显的资源未释放的情况。<br><a id="more"></a></p>
<h3 id="1-修改线上环境配置，打开trace端口"><a href="#1-修改线上环境配置，打开trace端口" class="headerlink" title="1. 修改线上环境配置，打开trace端口"></a>1. 修改线上环境配置，打开trace端口</h3><p>在调试分析问题之前，熟悉go的同学都知道 Golang 提供了非常方便的性能诊断工具 <code>go tool trace</code>;  <code>go tool trace</code> 是 Golang 中的性能大杀器， 能揭露运行中的所有的运行时事件和内存占用等。 它是 Go 生态系统中用于诊断性能问题时（如延迟，并行化和竞争异常）最有用的工具之一。 更多 go tool 工具可查看<a href="http://team.jiunile.com/blog/2020/09/go-pprof.html">如何使用 go pprof 定位内存泄漏</a>。</p>
<p>由于都是采用公司的基础库，基础库专门对 <code>go trace</code> 做了封装。只需要在线上机器修改config 文件，将 trace 信息发送到配置文件中的指定端口就可以使用 <code>go tool</code> 进行分析了。</p>
<p>然后我在本地进行使用 <code>go tool</code> 工具发现网络不通，排查了一下发现 trace 端口没有绑定到0.0.0.0上。随即用 proxy 工具将 9900 端口反向代理到 9999 端口，然后使用 <code>go tool</code> 对正常状态的内存占用做了一个内存火焰图。如下图:</p>
<p><img src="/images/go/flame-graph.jpg" alt="go 火焰图"></p>
<h3 id="2-问题复现"><a href="#2-问题复现" class="headerlink" title="2. 问题复现"></a>2. 问题复现</h3><p>过了几天后又收到了服务器警报，由于 qps 上升，这次比前几次来的都早一些。接到警报后立即对内存做了一个 top<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof -alloc_space http://&#123;addr&#125;/debug/pprof/heap</span><br><span class="line"></span><br><span class="line">Showing top 20 nodes out of 55</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">    2.76GB 59.76% 59.76%     2.76GB 59.76%  compress/flate.NewReader</span><br><span class="line">    0.45GB  9.73% 69.49%      0.45GB  9.73%  net/http.newBufioWriterSize</span><br><span class="line">    0.29GB  6.33% 75.82%     3.05GB 66.09%  compress/gzip.(*Reader).Reset</span><br><span class="line">    0.25GB  5.35% 81.17%     0.25GB  5.35%  net/http.newBufioReader</span><br><span class="line">    0.13GB  2.85% 84.01%     0.13GB  2.85%  runtime.rawstringtmp</span><br><span class="line">    0.11GB  2.31% 86.32%     0.11GB  2.31%  bytes.makeSlice</span><br><span class="line">    0.10GB  2.26% 88.58%     0.10GB  2.26%  runtime.hashGrow</span><br></pre></td></tr></table></figure></p>
<p>每一行表示一个函数的信息。前两列表示函数使用内存以及百分比；第三列是当前所有函数累加使用 Memory 的比例；第四列和第五列代表这个函数以及子函数运行所占用的 Memory 和比例（也被称为累加值 cumulative），应该大于等于前两列的值；最后一列就是函数的名字。如果应用程序有性能问题，上面这些信息应该能告诉我们内存都花费在哪些函数的执行上了，另外 pprof 的 CPU 时间分析也类似。</p>
<p>pprof 不仅能打印出最耗内存的的地方(top)，还能列出函数代码以及对应的取样数据(list)、汇编代码以及对应的取样数据(disasm)，而且能以各种样式进行输出，比如 svg、gv、callgrind、png、gif 等等。</p>
<p>可以看到的是大部分内存都被这些 Reader 占用了.</p>
<h3 id="3-带着问题重新阅读代码"><a href="#3-带着问题重新阅读代码" class="headerlink" title="3.带着问题重新阅读代码"></a>3.带着问题重新阅读代码</h3><p>前面我们进行了占用的初步分析，找到了内存占用多的 Fcuntion 是 <code>flate.NewReader</code>, <code>Package flate</code> 实现了 <code>RFC 1951</code> 中描述的 <code>DEFLATE</code> 压缩数据格式, gzip 包实现了对基于 <code>DEFLATE</code> 的文件格式的访问。所以我们带着问题我们再次定位到相关源码实现，下面是一些关键定义:<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> Gzip GzipPool</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">GetReader</span><span class="params">(src io.Reader)</span> <span class="params">(*gzip.Reader, error)</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> Gzip.GetReader(src)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PutReader</span><span class="params">(reader *gzip.Reader)</span></span> &#123;</span><br><span class="line">    Gzip.PutReader(reader)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// GzipPool manages a pool of gzip.Writer.</span></span><br><span class="line"><span class="comment">// The pool uses sync.Pool internally.</span></span><br><span class="line"><span class="keyword">type</span> GzipPool <span class="keyword">struct</span> &#123;</span><br><span class="line">    readers sync.Pool</span><br><span class="line">    writers sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里用到了 <code>sync.Pool</code> 来优化 GC, 为了验证内存都在 pool 上，我们又使用 <code>go tool</code> 提供的 web 工具对查看了一下 pool 的内存占用，果然大部分占用都在 pool 上。</p>
<h3 id="什么是-sync-Pool"><a href="#什么是-sync-Pool" class="headerlink" title="什么是 sync.Pool?"></a>什么是 sync.Pool?</h3><p>sync包提供了基础的golang并发编程工具。根据官方文档的描述:</p>
<blockquote>
<p>Pool’s purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector. That is, it makes it easy to build efficient, thread-safe free lists. However, it is not suitable for all free lists.</p>
</blockquote>
<p>我们通常用 golang 来构建高并发场景下的应用，但是由于 golang 内建的 GC 机制会影响应用的性能，为了减少 GC，golang 提供了对象重用的机制，也就是 <code>sync.Pool</code> 对象池。 <code>sync.Pool</code> 是可伸缩的，并发安全的。其大小仅受限于内存的大小，可以被看作是一个存放可重用对象的值的容器。 设计的目的是存放已经分配的但是暂时不用的对象，在需要用到的时候直接从 pool 中取。看到这里相信许多熟悉 GC 的同学心里已经有答案的猜测了:  或许和 GC 有关。</p>
<h3 id="So，Golang-的-GC-触发时机是什么？"><a href="#So，Golang-的-GC-触发时机是什么？" class="headerlink" title="So，Golang 的 GC 触发时机是什么？"></a>So，Golang 的 GC 触发时机是什么？</h3><p>Golang GC1.13 版本的 GC 实现是三色标记并配合写屏障和辅助 GC。触发条件主要有两个:</p>
<ol>
<li>超过内存大小阈值</li>
<li>达到定时时间</li>
</ol>
<blockquote>
<p>阈值是由一个 <code>gcpercent</code> 的变量控制的,当新分配的内存占已在使用中的内存的比例超过 <code>gcprecent</code> 时就会触发。比如一次回收完毕后，内存的使用量为 5M，那么下次回收的时机则是内存分配达到 10M 的时候。也就是说，并不是内存分配越多，垃圾回收频率越高。如果一直达不到内存大小的阈值呢？这个时候 GC 就会被定时时间触发，比如一直达不到10M，那就定时（默认2min触发一次）触发一次 GC 保证资源的回收。</p>
</blockquote>
<p>所以我们当内存占用慢慢升高的时候，gc 触发次数会减少并且趋近于 2min，没有 gc 就不会对 pool 中对象进行回收，导致内存占用率逐渐升高。</p>
<h3 id="主动触发GC进行验证"><a href="#主动触发GC进行验证" class="headerlink" title="主动触发GC进行验证"></a>主动触发GC进行验证</h3><p>修改代码，限制 rlimit，并使用一个 goroutine 每 30s 主动调用 gc，然后进行测试后上线。观测线上接口耗时并未发生明显变化，系统运行正常，内存占用报警再也没有被触发过。</p>
<h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><p>刚开始对偶尔触发的内存报警并没有过多的在意，有许多侥幸心理，但问题总归是客观存在的，及时发现问题，定时总结才能不断进步成长，尽量避免一有问题就重启，防止成为SRB(Service ReBoot Boy😁).</p>
<p>本文作者：保护我方李元芳</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[带你认识Kubernetes 网络插件Flannel与Calico]]></title>
      <url>http://team.jiunile.com/blog/2020/10/k8s-cni.html</url>
      <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>容器的网络解决方案有很多种，每支持一种网络实现就进行一次适配显然是不现实的，而 CNI 就是为了兼容多种网络方案而发明的。CNI 是 Container Network Interface 的缩写，是一个标准的通用的接口，用于连接容器管理系统和网络插件。</p>
<p>简单来说，容器 runtime 为容器提供 network namespace，网络插件负责将 network interface 插入该 network namespace 中并且在宿主机做一些必要的配置，最后对 namespace 中的 interface 进行 IP 和路由的配置。</p>
<p>所以网络插件的主要工作就在于为容器提供网络环境，包括为 pod 设置 ip 地址、配置路由保证集群内网络的通畅。目前比较流行的网络插件是 Flannel 和 Calico。<br><a id="more"></a></p>
<h2 id="Flannel"><a href="#Flannel" class="headerlink" title="Flannel"></a>Flannel</h2><p>Flannel 主要提供的是集群内的 Overlay 网络，支持三种后端实现，分别是：UDP 模式、VXLAN 模式、host-gw 模式。</p>
<h3 id="UDP模式"><a href="#UDP模式" class="headerlink" title="UDP模式"></a>UDP模式</h3><p>UDP 模式，是 Flannel 项目最早支持的一种方式，却也是性能最差的一种方式。这种模式提供的是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。工作原理如下图所示。<br><img src="/images/k8s/cni-flannel-udp.jpg" alt="cni flannel"></p>
<p>node1 上的 pod1 请求 node2 上的 pod2 时，流量的走向如下：</p>
<ol>
<li>pod1 里的进程发起请求，发出 IP 包</li>
<li>IP 包根据 pod1 里的 veth 设备对，进入到 cni0 网桥</li>
<li>由于 IP 包的目的 ip 不在 node1 上，根据 flannel 在节点上创建出来的路由规则，进入到 flannel0 中</li>
<li>此时 flanneld 进程会收到这个包，flanneld 判断该包应该在哪台 node 上，然后将其封装在一个 UDP 包中</li>
<li>最后通过 node1 上的网关，发送给 node2</li>
</ol>
<p>flannel0 是一个 TUN 设备（Tunnel 设备）。在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能：在操作系统内核和用户应用程序之间传递 IP 包。</p>
<p>可以看到，这种模式性能差的原因在于，整个包的 UDP 封装过程是 flanneld 程序做的，也就是用户态，而这就带来了一次内核态向用户态的转换，以及一次用户态向内核态的转换。在上下文切换和用户态操作的代价其实是比较高的，而 UDP 模式因为封包拆包带来了额外的性能消耗。</p>
<h3 id="VXLAN-模式"><a href="#VXLAN-模式" class="headerlink" title="VXLAN 模式"></a>VXLAN 模式</h3><p>VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。通过利用 Linux 内核的这种特性，也可以实现在内核态的封装和解封装的能力，从而构建出覆盖网络。其工作原理如下图所示：<br><img src="/images/k8s/cni-flannel-vxlan.jpg" alt="cni flannel"></p>
<p>VXLAN 模式的 flannel 会在节点上创建一个叫 flannel.1 的 VTEP (VXLAN Tunnel End Point，虚拟隧道端点) 设备，跟 UDP 模式一样，该设备将二层数据帧封装在 UDP 包里，再转发出去，而与 UDP 模式不一样的是，整个封装的过程是在内核态完成的。</p>
<p>node1 上的 pod1 请求 node2 上的 pod2 时，流量的走向如下：</p>
<ol>
<li>pod1 里的进程发起请求，发出 IP 包</li>
<li>IP 包根据 pod1 里的 veth 设备对，进入到 cni0 网桥</li>
<li>由于 IP 包的目的 ip 不在 node1 上，根据 flannel 在节点上创建出来的路由规则，进入到 flannel.1 中</li>
<li>flannel.1 将原始 IP 包加上一个目的 MAC 地址，封装成一个二层数据帧；然后内核将数据帧封装进一个 UDP 包里</li>
<li>最后通过 node1 上的网关，发送给 node2</li>
</ol>
<h4 id="抓包验证"><a href="#抓包验证" class="headerlink" title="抓包验证"></a>抓包验证</h4><p>在 node1 上部署一个 nginx pod1，node2 上部署一个 nginx pod2。然后在 pod1 的容器中 curl pod2 容器的 80 端口。</p>
<p>集群网络环境如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">node1 网卡 ens33：192.168.50.10</span><br><span class="line">pod1 veth：10.244.0.7</span><br><span class="line">node1 cni0：10.244.0.1/24</span><br><span class="line">node1 flannel.1：10.244.0.0/32</span><br><span class="line"></span><br><span class="line">node2 网卡 ens33：192.168.50.11</span><br><span class="line">pod2 veth：10.244.1.9</span><br><span class="line">node2 cni0：10.244.1.1/24</span><br><span class="line">node2 flannel.1：10.244.1.0/32</span><br></pre></td></tr></table></figure></p>
<p>node1 上的路由信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ip route</span><br><span class="line">default via 192.168.50.1 dev ens33</span><br><span class="line">10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1</span><br><span class="line">10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink</span><br><span class="line">10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br><span class="line">192.168.50.0/24 dev ens33 proto kernel scope link src 192.168.50.10 metric 100</span><br></pre></td></tr></table></figure></p>
<p>node1 的网卡 ens33 的抓包情况：<br><img src="/images/k8s/cni-flannel-tcpdump.png" alt="cni flannel"></p>
<p>只能看到源 ip 为 node1 ip、目的 ip 为 node2 ip 的 UDP 包。由于 flannel.1 进行了一层 UDP 封包，这里我们在 Wireshark 中设置一下将 UDP 包解析为 VxLAN 格式（端口为 8472），设置过程为 Analyze-&gt;Decode As：<br><img src="/images/k8s/cni-flannel-tcpdump2.png" alt="cni flannel"></p>
<p>然后再来看一下 node1 网卡上收到的包：<br><img src="/images/k8s/cni-flannel-tcpdump3.png" alt="cni flannel"></p>
<p>可以看到源 ip 为 pod1 ip、目的 ip 为 pod2 ip，并且该 IP 包被封装在 UDP 包中。</p>
<h3 id="host-gw-模式"><a href="#host-gw-模式" class="headerlink" title="host-gw 模式"></a>host-gw 模式</h3><p>最后一种 host-gw 模式是一种纯三层网络方案。其工作原理为将每个 Flannel 子网的“下一跳”设置成了该子网对应的宿主机的 IP 地址，这台主机会充当这条容器通信路径里的“网关”。这样 IP 包就能通过二层网络达到目的主机，而正是因为这一点，host-gw 模式要求集群宿主机之间的网络是二层连通的，如下图所示。<br><img src="/images/k8s/cni-flannel-host-gw.jpg" alt="cni flannel"></p>
<p>宿主机上的路由信息是 flanneld 设置的，因为 flannel 子网和主机的信息保存在 etcd 中，所以 flanneld 只需要 watch 这些数据的变化，实时更新路由表即可。在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。</p>
<p>node1 上的 pod1 请求 node2 上的 pod2 时，流量的走向如下：</p>
<ol>
<li>pod1 里的进程发起请求，发出 IP 包，从网络层进入链路层封装成帧</li>
<li>根据主机上的路由规则，数据帧从 Node 1 通过宿主机的二层网络到达 Node 2 上</li>
</ol>
<h2 id="Calico"><a href="#Calico" class="headerlink" title="Calico"></a>Calico</h2><p>Calico 没有使用 CNI 的网桥模式，而是将节点当成边界路由器，组成了一个全连通的网络，通过 BGP 协议交换路由。所以，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。</p>
<p>Calico 的组件：</p>
<ol>
<li>CNI 插件：Calico 与 Kubernetes 对接的部分</li>
<li>Felix：负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。</li>
<li>BIRD (BGP Route Reflector)：是 BGP 的客户端，专门负责在集群里分发路由规则信息。</li>
</ol>
<p>三个组件都是通过一个 DaemonSet 安装的。CNI 插件是通过 initContainer 安装的；而 Felix 和 BIRD 是同一个 pod 的两个 container。</p>
<h3 id="BGP-工作原理"><a href="#BGP-工作原理" class="headerlink" title="BGP 工作原理"></a>BGP 工作原理</h3><p>Calico 采用的 BGP，就是在大规模网络中实现节点路由信息共享的一种协议。全称是 Border Gateway Protocol，即：边界网关协议。它是一个 Linux 内核原生就支持的、专门用在大规模数据中心里维护不同的 “自治系统” 之间路由信息的、无中心的路由协议。</p>
<p>由于没有使用 CNI 的网桥，Calico 的 CNI 插件需要为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上，还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。如下图所示：<br><img src="/images/k8s/cni-calico-bgp.jpg" alt="cni calico"></p>
<p>可以使用 calicoctl 查看 node1 的节点连接情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">~ calicoctl get no</span><br><span class="line">NAME</span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br><span class="line">~ calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| 192.168.50.11 | node-to-node mesh | up    | 2020-09-28 | Established |</span><br><span class="line">| 192.168.50.12 | node-to-node mesh | up    | 2020-09-28 | Established |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br></pre></td></tr></table></figure></p>
<p>可以看到整个 calico 集群上有 3 个节点，node1 和另外两个节点处于连接状态，模式为 “Node-to-Node Mesh”。再看下 node1 上的路由信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">~ ip route</span><br><span class="line">default via 192.168.50.1 dev ens33 proto static metric 100</span><br><span class="line">10.244.104.0/26 via 192.168.50.11 dev ens33 proto bird</span><br><span class="line">10.244.135.0/26 via 192.168.50.12 dev ens33 proto bird</span><br><span class="line">10.244.166.128 dev cali717821d73f3 scope link</span><br><span class="line">blackhole 10.244.166.128/26 proto bird</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br><span class="line">192.168.50.0/24 dev ens33 proto kernel scope link src 192.168.50.10 metric 100</span><br></pre></td></tr></table></figure></p>
<p>其中，第 2 条的路由规则表明 10.244.104.0/26 网段的数据包通过 bird 协议由 ens33 设备发往网关 192.168.50.11。这也就定义了目的 ip 为 node2 上 pod 请求的走向。第 3 条路由规则与之类似。</p>
<h4 id="抓包验证-1"><a href="#抓包验证-1" class="headerlink" title="抓包验证"></a>抓包验证</h4><p>与上面一样，从 node1 上的 pod1 发送一个 http 请求到 node2 上的 pod2。</p>
<p>集群网络环境如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node1 网卡 ens33：192.168.50.10</span><br><span class="line">pod1 ip：10.244.166.128</span><br><span class="line"></span><br><span class="line">node2 网卡 ens33：192.168.50.11</span><br><span class="line">pod2 ip：10.244.104.1</span><br></pre></td></tr></table></figure></p>
<p>node1 的网卡 ens33 的抓包情况：<br><img src="/images/k8s/cni-calico-tcpdump.png" alt="cni calico"></p>
<h3 id="IPIP-模式"><a href="#IPIP-模式" class="headerlink" title="IPIP 模式"></a>IPIP 模式</h3><p>IPIP 模式为了解决两个 node 不在一个子网的问题。只要将名为 calico-node 的 daemonset 的环境变量 CALICO_IPV4POOL_IPIP 设置为 “Always” 即可。如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- name: CALICO_IPV4POOL_IPIP</span><br><span class="line">  value: <span class="string">"Off"</span></span><br></pre></td></tr></table></figure></p>
<p>IPIP 模式的 calico 使用了 tunl0 设备，这是一个 IP 隧道设备。IP 包进入 tunl0 后，内核会将原始 IP 包直接封装在宿主机的 IP 包中；封装后的 IP 包的目的地址为下一跳地址，即 node2 的 IP 地址。由于宿主机之间已经使用路由器配置了三层转发，所以这个 IP 包在离开 node 1 之后，就可以经过路由器，最终发送到 node 2 上。如下图所示。<br><img src="/images/k8s/cni-calico-ipip.jpg" alt="cni calico"></p>
<p>由于 IPIP 模式的 Calico 额外多出了封包和拆包的过程，集群的网络性能受到了影响，所以在集群的二层网络通的情况下，建议不要使用 IPIP 模式。</p>
<p>看下 node1 上的路由信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">~ ip route</span><br><span class="line">default via 192.168.50.1 dev ens33 proto static metric 100</span><br><span class="line">10.244.104.0/26 via 192.168.50.11 dev tunl0 proto bird onlink</span><br><span class="line">10.244.135.0/26 via 192.168.50.12 dev tunl0 proto bird onlink</span><br><span class="line">blackhole 10.244.166.128/26 proto bird</span><br><span class="line">10.244.166.129 dev calif3c799362a5 scope link</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br><span class="line">192.168.50.0/24 dev ens33 proto kernel scope link src 192.168.50.10 metric 100</span><br></pre></td></tr></table></figure></p>
<p>可以看到，与之前不一样的是，目的 IP 为 node2 上的 Pod 的数据包是经由 tunl0 发送到网关 192.168.50.11。</p>
<h4 id="抓包验证-2"><a href="#抓包验证-2" class="headerlink" title="抓包验证"></a>抓包验证</h4><p>从 node1 上的 pod1 发送一个 http 请求到 node2 上的 pod2。</p>
<p>集群网络环境如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">node1 网卡 ens33：192.168.50.10</span><br><span class="line">node1 tunl0：10.244.166.128/32</span><br><span class="line">pod1 ip：10.244.166.129</span><br><span class="line"></span><br><span class="line">node2 网卡 ens33：192.168.50.11</span><br><span class="line">pod2 ip：10.244.104.2</span><br><span class="line">node2 tunl0：10.244.104.0/32</span><br></pre></td></tr></table></figure></p>
<p>tunl0 设备的抓包情况：<br><img src="/images/k8s/cni-calico-ipip-tcpdump.jpg" alt="cni calico"></p>
<p>node1 网卡 ens33 的抓包情况：<br><img src="/images/k8s/cni-calico-ipip-tcpdump2.jpg" alt="cni calico"></p>
<p>可以看到，IP 包在 tunl0 设备中被封装进了另一个 IP 包，其目的 IP 为 node2 的 IP，源 IP 为 node1 的 IP。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Kubernetes 的集群网络插件实现方案有很多种，本文主要分析了社区比较常见的两种 Flannel 和 Calico 的工作原理，针对集群内不同节点的 pod 间通信的场景，抓包分析了网络包的走向。</p>
<p>Flannel 主要提供了 Overlay 的网络方案，UDP 模式由于其封包拆包的过程涉及了多次上下文的切换，导致性能很差，逐渐被社区抛弃；VXLAN 模式的封包拆包过程均在内核态，性能要比 UDP 好很多，也是最经常使用的模式；host-gw 模式不涉及封包拆包，所以性能相对较高，但要求节点间二层互通。</p>
<p>Calico 主要采用了 BGP 协议交换路由，没有采用 cni0 网桥，当二层网络不通的时候，可以采用 IPIP 模式，但由于涉及到封包拆包的过程，性能相对较弱，与 Flannel 的 VXLAN 模式相当。</p>
<p>本文作者：海的澜色 来自CS实验室</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如何使用go pprof定位内存泄露]]></title>
      <url>http://team.jiunile.com/blog/2020/09/go-pprof.html</url>
      <content type="html"><![CDATA[<p>最近解决了我们项目中的一个内存泄露问题，事实再次证明 pprof 是一个好工具，但掌握好工具的正确用法，才能发挥好工具的威力，不然就算你手里有屠龙刀，也成不了天下第一，本文就是带你用 pprof 定位内存泄露问题。</p>
<p>关于Go的内存泄露有这么一句话不知道你听过没有：</p>
<blockquote>
<p>10次内存泄露，有9次是 goroutine 泄露。</p>
</blockquote>
<p>我所解决的问题，也是 goroutine 泄露导致的内存泄露，所以<strong>这篇文章主要介绍Go程序的goroutine 泄露，掌握了如何定位和解决 goroutine 泄露，就掌握了内存泄露的大部分场景</strong>。</p>
<blockquote>
<p>本文草稿最初数据都是生产坏境数据，为了防止敏感内容泄露，全部替换成了demo数据，demo的数据比生产环境数据简单多了，更适合入门理解，有助于掌握 pprof。</p>
</blockquote>
<a id="more"></a>
<h2 id="go-pprof基本知识"><a href="#go-pprof基本知识" class="headerlink" title="go pprof基本知识"></a>go pprof基本知识</h2><p>定位 goroutine 泄露会使用到 pprof，pprof 是Go的性能工具，在开始介绍内存泄露前，先简单介绍下 pprof 的基本使用，更详细的使用给大家推荐了资料。</p>
<h3 id="什么是pprof"><a href="#什么是pprof" class="headerlink" title="什么是pprof"></a>什么是pprof</h3><p>pprof 是Go的性能分析工具，在程序运行过程中，可以记录程序的运行信息，可以是CPU使用情况、内存使用情况、goroutine 运行情况等，当需要性能调优或者定位Bug时候，这些记录的信息是相当重要。</p>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>使用 pprof 有多种方式，Go已经现成封装好了1个：<code>net/http/pprof</code>，使用简单的几行命令，就可以开启 pprof，记录运行信息，并且提供了Web服务，能够通过浏览器和命令行2种方式获取运行数据。</p>
<p>看个最简单的 pprof 的 demo 例子：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 开启pprof，监听请求</span></span><br><span class="line">    ip := <span class="string">"0.0.0.0:6060"</span></span><br><span class="line">    <span class="keyword">if</span> err := http.ListenAndServe(ip, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Printf(<span class="string">"start pprof failed on %s\n"</span>, ip)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="浏览器方式"><a href="#浏览器方式" class="headerlink" title="浏览器方式"></a>浏览器方式</h3><p>将上述的go pprof例子运行起来后通过浏览器访问：<br><img src="/images/go/pprof_1.png" alt="go pprof"><br>输入网址 <code>http://ip:port/debug/pprof/</code> 打开pprof主页，从上到下依次是5类profile信息：</p>
<ol>
<li><strong>block</strong>：goroutine 的阻塞信息，本例就截取自一个 goroutine 阻塞的 demo，但 block 为0，没掌握 block 的用法</li>
<li><strong>goroutine</strong>：所有 goroutine 的信息，下面的 <code>full goroutine stack dump</code> 是输出所有 goroutine 的调用栈，是 goroutine 的 debug=2 ，后面会详细介绍。</li>
<li><strong>heap</strong>：堆内存的信息</li>
<li><strong>mutex</strong>：锁的信息</li>
<li><strong>threadcreate</strong>：线程信息</li>
</ol>
<p>这篇文章我们主要关注 goroutine 和 heap ，这两个都会打印调用栈信息，goroutine 里面还会包含 goroutine 的数量信息，heap 则是内存分配信息，本文用不到的地方就不展示了，最后推荐几篇文章大家去看。</p>
<h3 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h3><p>当连接在服务器终端上的时候，是没有浏览器可以使用的，Go提供了命令行的方式，能够获取以上5类信息，这种方式用起来更方便。</p>
<p>使用命令 <code>go tool pprof url</code> 可以获取指定的 profile 文件，此命令会发起 http 请求，然后下载数据到本地，之后进入交互式模式，就像 gdb 一样，可以使用命令查看运行信息，以下是5类请求的方式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载cpu profile，默认从当前开始收集30s的cpu使用情况，需要等待30s</span></span><br><span class="line">go tool pprof http://localhost:6060/debug/pprof/profile   <span class="comment"># 30-second CPU profile</span></span><br><span class="line">go tool pprof http://localhost:6060/debug/pprof/profile?seconds=120     <span class="comment"># wait 120s</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载heap profile</span></span><br><span class="line">go tool pprof http://localhost:6060/debug/pprof/heap      <span class="comment"># heap profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载goroutine profile</span></span><br><span class="line">go tool pprof http://localhost:6060/debug/pprof/goroutine <span class="comment"># goroutine profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载block profile</span></span><br><span class="line">go tool pprof http://localhost:6060/debug/pprof/block     <span class="comment"># goroutine blocking profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载mutex profile</span></span><br><span class="line">go tool pprof http://localhost:6060/debug/pprof/mutex</span><br></pre></td></tr></table></figure></p>
<p>上面的 pprof/demo.go 太简单了，如果去获取内存 profile，几乎获取不到什么，换一个Demo进行内存 profile 的展示：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 展示内存增长和pprof，并不是泄露</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">    <span class="string">"os"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行一段时间：fatal error: runtime: out of memory</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 开启pprof</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        ip := <span class="string">"0.0.0.0:6060"</span></span><br><span class="line">        <span class="keyword">if</span> err := http.ListenAndServe(ip, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Printf(<span class="string">"start pprof failed on %s\n"</span>, ip)</span><br><span class="line">            os.Exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    tick := time.Tick(time.Second / <span class="number">100</span>)</span><br><span class="line">    <span class="keyword">var</span> buf []<span class="keyword">byte</span></span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">range</span> tick &#123;</span><br><span class="line">        buf = <span class="built_in">append</span>(buf, <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">1024</span>*<span class="number">1024</span>)...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面这个demo会不断的申请内存，把它编译运行起来，然后执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof http://localhost:6060/debug/pprof/heap</span><br><span class="line"></span><br><span class="line">Fetching profile over HTTP from http://localhost:6060/debug/pprof/heap</span><br><span class="line">Saved profile <span class="keyword">in</span> /home/ubuntu/pprof/pprof.demo.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz       //&lt;--- 下载到的内存profile文件</span><br><span class="line">File: demo // 程序名称</span><br><span class="line">Build ID: a9069a125ee9c0df3713b2149ca859e8d4d11d5a</span><br><span class="line">Type: inuse_space</span><br><span class="line">Time: May 16, 2019 at 8:55pm (CST)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof) <span class="built_in">help</span>  // 使用<span class="built_in">help</span>打印所有可用命令</span><br><span class="line">  Commands:</span><br><span class="line">    callgrind        Outputs a graph <span class="keyword">in</span> callgrind format</span><br><span class="line">    comments         Output all profile comments</span><br><span class="line">    disasm           Output assembly listings annotated with samples</span><br><span class="line">    dot              Outputs a graph <span class="keyword">in</span> DOT format</span><br><span class="line">    eog              Visualize graph through eog</span><br><span class="line">    evince           Visualize graph through evince</span><br><span class="line">    gif              Outputs a graph image <span class="keyword">in</span> GIF format</span><br><span class="line">    gv               Visualize graph through gv</span><br><span class="line">    kcachegrind      Visualize report <span class="keyword">in</span> KCachegrind</span><br><span class="line">    list             Output annotated <span class="built_in">source</span> <span class="keyword">for</span> <span class="built_in">functions</span> matching regexp</span><br><span class="line">    pdf              Outputs a graph <span class="keyword">in</span> PDF format</span><br><span class="line">    peek             Output callers/callees of <span class="built_in">functions</span> matching regexp</span><br><span class="line">    png              Outputs a graph image <span class="keyword">in</span> PNG format</span><br><span class="line">    proto            Outputs the profile <span class="keyword">in</span> compressed protobuf format</span><br><span class="line">    ps               Outputs a graph <span class="keyword">in</span> PS format</span><br><span class="line">    raw              Outputs a text representation of the raw profile</span><br><span class="line">    svg              Outputs a graph <span class="keyword">in</span> SVG format</span><br><span class="line">    tags             Outputs all tags <span class="keyword">in</span> the profile</span><br><span class="line">    text             Outputs top entries <span class="keyword">in</span> text form</span><br><span class="line">    top              Outputs top entries <span class="keyword">in</span> text form</span><br><span class="line">    topproto         Outputs top entries <span class="keyword">in</span> compressed protobuf format</span><br><span class="line">    traces           Outputs all profile samples <span class="keyword">in</span> text form</span><br><span class="line">    tree             Outputs a text rendering of call graph</span><br><span class="line">    web              Visualize graph through web browser</span><br><span class="line">    weblist          Display annotated <span class="built_in">source</span> <span class="keyword">in</span> a web browser</span><br><span class="line">    o/options        List options and their current values</span><br><span class="line">    quit/<span class="built_in">exit</span>/^D     Exit pprof</span><br><span class="line">    </span><br><span class="line">    ....</span><br></pre></td></tr></table></figure></p>
<p>以上信息我们只关注2个地方：</p>
<ol>
<li>下载得到的文件：<code>/home/ubuntu/pprof/pprof.demo.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz</code>，这其中包含了程序名 <code>demo</code>，profile 类型 <code>alloc</code> 已分配的内存，<code>inuse</code> 代表使用中的内存。</li>
<li><code>help</code> 可以获取帮助，最先会列出支持的命令，想掌握 pprof，要多看看，多尝试。</li>
</ol>
<p>关于命令，本文只会用到3个，我认为也是最常用的：<code>top</code>、<code>list</code>、<code>traces</code>，分别介绍一下。</p>
<h4 id="top"><a href="#top" class="headerlink" title="top"></a>top</h4><p>按指标大小列出前10个函数，比如内存是按内存占用多少，CPU是按执行时间多少。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 814.62MB, 100% of 814.62MB total</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">  814.62MB   100%   100%   814.62MB   100%  main.main</span><br><span class="line">         0     0%   100%   814.62MB   100%  runtime.main</span><br></pre></td></tr></table></figure></p>
<p>top会列出5个统计数据：</p>
<ul>
<li><strong>flat</strong>: 本函数占用的内存量。</li>
<li><strong>flat%</strong>: 本函数内存占使用中内存总量的百分比。</li>
<li><strong>sum%</strong>: 前面每一行 flat 百分比的和，比如第2行虽然的100% 是 100% + 0%。</li>
<li><strong>cum</strong>: 是累计量，加入main函数调用了函数f，函数f占用的内存量，也会记进来。</li>
<li><strong>cum%</strong>: 是累计量占总量的百分比。</li>
</ul>
<h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><p>查看某个函数的代码，以及该函数每行代码的指标信息，如果函数名不明确，会进行模糊匹配，比如 <code>list main</code> 会列出 <code>main.main</code> 和 <code>runtime.main</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">(pprof) list main.main  // 精确列出函数</span><br><span class="line">Total: 814.62MB</span><br><span class="line">ROUTINE ======================== main.main <span class="keyword">in</span> /home/ubuntu/heap/demo2.go</span><br><span class="line">  814.62MB   814.62MB (flat, cum)   100% of Total</span><br><span class="line">         .          .     20:    &#125;()</span><br><span class="line">         .          .     21:</span><br><span class="line">         .          .     22:    tick := time.Tick(time.Second / 100)</span><br><span class="line">         .          .     23:    var buf []byte</span><br><span class="line">         .          .     24:    <span class="keyword">for</span> range tick &#123;</span><br><span class="line">  814.62MB   814.62MB     25:        buf = append(buf, make([]byte, 1024*1024)...)</span><br><span class="line">         .          .     26:    &#125;</span><br><span class="line">         .          .     27:&#125;</span><br><span class="line">         .          .     28:</span><br><span class="line">(pprof) list main  // 匹配所有函数名带main的函数</span><br><span class="line">Total: 814.62MB</span><br><span class="line">ROUTINE ======================== main.main <span class="keyword">in</span> /home/ubuntu/heap/demo2.go</span><br><span class="line">  814.62MB   814.62MB (flat, cum)   100% of Total</span><br><span class="line">         .          .     20:    &#125;()</span><br><span class="line">         .          .     21:</span><br><span class="line">..... // 省略几行</span><br><span class="line">         .          .     28:</span><br><span class="line">ROUTINE ======================== runtime.main <span class="keyword">in</span> /usr/lib/go-1.10/src/runtime/proc.go</span><br><span class="line">         0   814.62MB (flat, cum)   100% of Total</span><br><span class="line">         .          .    193:        // A program compiled with -buildmode=c-archive or c-shared</span><br><span class="line">..... // 省略几行</span><br></pre></td></tr></table></figure></p>
<p>可以看到在 <code>main.main</code> 中的第25行占用了814.62MB内存，左右2个数据分别是 flat 和cum，含义和 top 中解释的一样。</p>
<h4 id="traces"><a href="#traces" class="headerlink" title="traces"></a>traces</h4><p>打印所有调用栈，以及调用栈的指标信息。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(pprof) traces</span><br><span class="line">File: demo2</span><br><span class="line">Type: inuse_space</span><br><span class="line">Time: May 16, 2019 at 7:08pm (CST)</span><br><span class="line">-----------+-------------------------------------------------------</span><br><span class="line">     bytes:  813.46MB</span><br><span class="line">  813.46MB   main.main</span><br><span class="line">             runtime.main</span><br><span class="line">-----------+-------------------------------------------------------</span><br><span class="line">     bytes:  650.77MB</span><br><span class="line">         0   main.main</span><br><span class="line">             runtime.main</span><br><span class="line">....... // 省略几十行</span><br></pre></td></tr></table></figure></p>
<p>每个 <code>- - - - -</code> 隔开的是一个调用栈，能看到 <code>runtime.main</code> 调用了 <code>main.main</code>，并且 <code>main.main</code> 中占用了813.46MB内存。</p>
<p>其他的 profile 操作和内存是类似的，这里就不展示了。</p>
<p>这里只是简单介绍本文用到的 pprof 的功能，pprof 功能很强大，也经常和 benchmark 结合起来，但这不是本文的重点，所以就不多介绍了，为大家推荐几篇文章，一定要好好研读、实践：</p>
<ol>
<li>Go官方博客关于pprof的介绍，很详细，也包含样例，可以实操：<a href="https://blog.golang.org/profiling-go-programs" target="_blank" rel="external">Profiling Go Programs</a>。</li>
<li>煎鱼的这篇文章也很适合入门： <a href="https://github.com/EDDYCJY/blog/blob/master/golang/2018-09-15-Golang%20%E5%A4%A7%E6%9D%80%E5%99%A8%E4%B9%8B%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90%20PProf.md" target="_blank" rel="external">Golang 大杀器之性能剖析 PProf</a>。</li>
</ol>
<h2 id="什么是内存泄露"><a href="#什么是内存泄露" class="headerlink" title="什么是内存泄露"></a>什么是内存泄露</h2><p>内存泄露指的是程序运行过程中已不再使用的内存，没有被释放掉，导致这些内存无法被使用，直到程序结束这些内存才被释放的问题。</p>
<p>Go虽然有 GC 来回收不再使用的堆内存，减轻了开发人员对内存的管理负担，但这并不意味着Go程序不再有内存泄露问题。在Go程序中，如果没有Go语言的编程思维，也不遵守良好的编程实践，就可能埋下隐患，造成内存泄露问题。</p>
<h2 id="怎么发现内存泄露"><a href="#怎么发现内存泄露" class="headerlink" title="怎么发现内存泄露"></a>怎么发现内存泄露</h2><p>在Go中发现内存泄露有2种方法，一个是通用的监控工具，另一个是 go pprof：</p>
<ol>
<li><strong>监控工具</strong>：固定周期对进程的内存占用情况进行采样，数据可视化后，根据内存占用走势（持续上升），很容易发现是否发生内存泄露。</li>
<li><strong>go pprof</strong>：适合没有监控工具的情况，使用Go提供的 pprof 工具判断是否发生内存泄露。</li>
</ol>
<p>这2种方式分别介绍一下。</p>
<h3 id="监控工具查看进程内在占用情况"><a href="#监控工具查看进程内在占用情况" class="headerlink" title="监控工具查看进程内在占用情况"></a>监控工具查看进程内在占用情况</h3><p><strong>如果使用云平台部署Go程序</strong>，云平台都提供了内存查看的工具，可以查看OS的内存占用情况和某个进程的内存占用情况，比如阿里云，我们在1个云主机上只部署了1个Go服务，所以OS的内存占用情况，基本是也反映了进程内存占用情况，OS内存占用情况如下，可以看到<strong>随着时间的推进，内存的占用率在不断的提高，这是内存泄露的最明显现象</strong>：<br><img src="/images/go/pprof_2.png" alt="go pprof"><br><strong>如果没有云平台这种内存监控工具，可以制作一个简单的内存记录工具</strong>。</p>
<p>1、建立一个脚本 <code>prog_mem.sh</code>，获取进程占用的物理内存情况，脚本内容如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">prog_name=<span class="string">"your_programe_name"</span></span><br><span class="line">prog_mem=$(pidstat  -r -u -h -C <span class="variable">$prog_name</span> |awk <span class="string">'NR==4&#123;print $12&#125;'</span>)</span><br><span class="line">time=$(date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$time</span><span class="string">"\tmemory(Byte)\t"</span><span class="variable">$prog_mem</span> &gt;&gt;/root/prog_mem.log</span><br></pre></td></tr></table></figure></p>
<p>2、然后使用 <code>crontab</code> 建立定时任务，每分钟记录1次。使用 <code>crontab -e</code> 编辑crontab 配置，在最后增加1行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/1 * * * * /root/prog_mem.sh</span><br></pre></td></tr></table></figure></p>
<p>脚本输出的内容保存在 <code>prog_mem.log</code>，只要大体浏览一下就可以发现内存的增长情况，判断是否存在内存泄露。如果需要可视化，可以直接黏贴 <code>prog_mem.log</code> 内容到 Excel 等表格工具，绘制内存占用图。<br><img src="/images/go/pprof_3.png" alt="go pprof"></p>
<h3 id="go-pprof发现存在内存问题"><a href="#go-pprof发现存在内存问题" class="headerlink" title="go pprof发现存在内存问题"></a>go pprof发现存在内存问题</h3><blockquote>
<p>有情提醒：如果对 pprof 不了解，可以先看 go pprof 基本知识</p>
</blockquote>
<p>如果你 Google 或者百度，Go程序内存泄露的文章，它总会告诉你使用 <strong>pprof heap</strong>，能够生成漂亮的调用路径图，火焰图等等，然后你根据调用路径就能定位内存泄露问题，我最初也是对此深信不疑，尝试了若干天后，只是发现内存泄露跟某种场景有关，根本找不到内存泄露的根源，<strong>如果哪位朋友用heap就能定位内存泄露的线上问题，麻烦介绍下</strong>。</p>
<p>后来读了 Dave 的《High Performance Go Workshop》，刷新了对 heap 的认识，内存pprof 的简要内容如下：<br><img src="/images/go/pprof_4.png" alt="go pprof"><br>Dave讲了以下几点：</p>
<ol>
<li><strong>内存 profiling 记录的是堆内存分配的情况，以及调用栈信息</strong>，并不是进程完整的内存情况，猜测这也是在 go pprof 中称为 heap 而不是 memory 的原因。</li>
<li><strong>栈内存的分配是在调用栈结束后会被释放的内存，所以并不在内存 profile 中</strong>。</li>
<li>内存 profiling 是基于抽样的，默认是每1000次堆内存分配，执行1次 profile 记录。</li>
<li>因为内存 profiling 是基于抽样和它跟踪的是已分配的内存，而不是使用中的内存，（比如有些内存已经分配，看似使用，但实际以及不使用的内存，比如内存泄露的那部分），所以<strong>不能使用内存 profiling 衡量程序总体的内存使用情况</strong>。</li>
<li><strong>Dave 个人观点：使用内存 profiling 不能够发现内存泄露</strong>。</li>
</ol>
<p>基于目前对 heap 的认知，我有2个观点：</p>
<ol>
<li><strong>heap 能帮助我们发现内存问题，但不一定能发现内存泄露问题</strong>，这个看法与 Dave 是类似的。heap 记录了内存分配的情况，我们能通过 heap 观察内存的变化，增长与减少，内存主要被哪些代码占用了，程序存在内存问题，这只能说明内存有使用不合理的地方，但并不能说明这是内存泄露。</li>
<li><strong>heap 在帮助定位内存泄露原因上贡献的力量微乎其微</strong>。如第一条所言，能通过 heap 找到占用内存多的位置，但这个位置通常不一定是内存泄露，就算是内存泄露，也只是内存泄露的结果，并不是真正导致内存泄露的根源。</li>
</ol>
<p>接下来，我介绍怎么用 heap 发现问题，然后再解释为什么 heap 几乎不能定位内存泄露的根因。</p>
<h4 id="怎么用heap发现内存问题"><a href="#怎么用heap发现内存问题" class="headerlink" title="怎么用heap发现内存问题"></a>怎么用heap发现内存问题</h4><p>使用 pprof 的 heap 能够获取程序运行时的内存信息，在程序平稳运行的情况下，每个一段时间使用 heap 获取内存的 profile ，<strong>然后使用 <code>base</code> 能够对比两个 profile 文件的差别，就像 <code>diff</code> 命令一样显示出增加和减少的变化</strong>，使用一个简单的demo来说明 heap 和 base 的使用，依然使用demo2进行展示。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 展示内存增长和pprof，并不是泄露</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">    <span class="string">"os"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行一段时间：fatal error: runtime: out of memory</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 开启pprof</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        ip := <span class="string">"0.0.0.0:6060"</span></span><br><span class="line">        <span class="keyword">if</span> err := http.ListenAndServe(ip, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Printf(<span class="string">"start pprof failed on %s\n"</span>, ip)</span><br><span class="line">            os.Exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    tick := time.Tick(time.Second / <span class="number">100</span>)</span><br><span class="line">    <span class="keyword">var</span> buf []<span class="keyword">byte</span></span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">range</span> tick &#123;</span><br><span class="line">        buf = <span class="built_in">append</span>(buf, <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">1024</span>*<span class="number">1024</span>)...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>将上面代码运行起来，执行以下命令获取 profile 文件，Ctrl-D 退出，1分钟后再获取1次。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof http://localhost:6060/debug/pprof/heap</span><br></pre></td></tr></table></figure></p>
<p>我已经获取到了两个 profile 文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls</span><br><span class="line">pprof.demo2.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz</span><br><span class="line">pprof.demo2.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz</span><br></pre></td></tr></table></figure></p>
<p>使用 <code>base</code> 把001文件作为基准，然后用002和001对比，先执行 <code>top</code> 看 <code>top</code> 的对比，然后执行 <code>list main</code> 列出 <code>main</code> 函数的内存对比，结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof -base pprof.demo2.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz pprof.demo2.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz</span><br><span class="line"></span><br><span class="line">File: demo2</span><br><span class="line">Type: inuse_space</span><br><span class="line">Time: May 14, 2019 at 2:33pm (CST)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 970.34MB, 32.30% of 3003.99MB total</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">  970.34MB 32.30% 32.30%   970.34MB 32.30%  main.main   // 看这</span><br><span class="line">         0     0% 32.30%   970.34MB 32.30%  runtime.main</span><br><span class="line">(pprof)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof) list main.main</span><br><span class="line">Total: 2.93GB</span><br><span class="line">ROUTINE ======================== main.main <span class="keyword">in</span> /home/ubuntu/heap/demo2.go</span><br><span class="line">  970.34MB   970.34MB (flat, cum) 32.30% of Total</span><br><span class="line">         .          .     20:    &#125;()</span><br><span class="line">         .          .     21:</span><br><span class="line">         .          .     22:    tick := time.Tick(time.Second / 100)</span><br><span class="line">         .          .     23:    var buf []byte</span><br><span class="line">         .          .     24:    <span class="keyword">for</span> range tick &#123;</span><br><span class="line">  970.34MB   970.34MB     25:        buf = append(buf, make([]byte, 1024*1024)...) // 看这</span><br><span class="line">         .          .     26:    &#125;</span><br><span class="line">         .          .     27:&#125;</span><br><span class="line">         .          .     28:</span><br></pre></td></tr></table></figure></p>
<p><code>top</code> 列出了 <code>main.main</code> 和 <code>runtime.main</code>，<code>main.main</code> 就是我们编写的 main函数，<code>runtime.main</code> 是 runtime 包中的 main 函数，也就是所有 main 函数的入口，这里不多介绍了，有兴趣可以看之前的调度器文章<a href="http://lessisbetter.site/2019/03/26/golang-scheduler-2-macro-view/" target="_blank" rel="external">《Go调度器系列（2）宏观看调度器》</a>。</p>
<p><code>top</code> 显示 <code>main.main</code> 第2次内存占用，比第1次内存占用多了970.34MB。</p>
<p><code>list main.main</code> 告诉了我们增长的内存都在这一行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">buf = append(buf, make([]byte, 1024*1024)...)</span><br></pre></td></tr></table></figure></p>
<p>001和002 profile 的文件不进去看了，你本地测试下计算差值，绝对是刚才对比出的970.34MB。</p>
<h4 id="heap“不能”定位内存泄露"><a href="#heap“不能”定位内存泄露" class="headerlink" title="heap“不能”定位内存泄露"></a>heap“不能”定位内存泄露</h4><p>heap 能显示内存的分配情况，以及哪行代码占用了多少内存，我们能轻易的找到占用内存最多的地方，如果这个地方的数值还在不断怎大，基本可以认定这里就是内存泄露的位置。</p>
<p>曾想按图索骥，从内存泄露的位置，根据调用栈向上查找，总能找到内存泄露的原因，这种方案看起来是不错的，但实施起来却找不到内存泄露的原因，结果是事半功倍。</p>
<p>原因在于一个Go程序，其中有大量的 goroutine，这其中的调用关系也许有点复杂，也许内存泄露是在某个三方包里。举个栗子，比如下面这幅图，每个椭圆代表1个 goroutine，其中的数字为编号，箭头代表调用关系。heap profile 显示 g111（最下方标红节点）这个协程的代码出现了泄露，任何一个从 g101 到 g111 的调用路径都可能造成了 g111 的内存泄露，有2类可能：</p>
<ol>
<li>该 goroutine 只调用了少数几次，但消耗了大量的内存，说明每个 goroutine 调用都消耗了不少内存，<strong>内存泄露的原因基本就在该协程内部</strong>。</li>
<li>该 goroutine 的调用次数非常多，虽然每个协程调用过程中消耗的内存不多，但该调用路径上，协程数量巨大，造成消耗大量的内存，并且这些 goroutine 由于某种原因无法退出，占用的内存不会释放，<strong>内存泄露的原因在到 g111 调用路径上某段代码实现有问题，造成创建了大量的 g111</strong>。</li>
</ol>
<p><strong>第2种情况，就是 goroutine 泄露，这是通过 heap 无法发现的，所以 heap 在定位内存泄露这件事上，发挥的作用不大</strong>。<br><img src="/images/go/pprof_5.png" alt="go pprof"></p>
<h2 id="goroutine-泄露怎么导致内存泄露"><a href="#goroutine-泄露怎么导致内存泄露" class="headerlink" title="goroutine 泄露怎么导致内存泄露"></a>goroutine 泄露怎么导致内存泄露</h2><h3 id="什么是-goroutine-泄露"><a href="#什么是-goroutine-泄露" class="headerlink" title="什么是 goroutine 泄露"></a>什么是 goroutine 泄露</h3><p>如果你启动了1个 goroutine，但并没有符合预期的退出，直到程序结束，此 goroutine 才退出，这种情况就是 goroutine 泄露。</p>
<blockquote>
<p>提前思考：什么会导致 goroutine 无法退出/阻塞？</p>
</blockquote>
<h3 id="goroutine-泄露怎么导致内存泄露-1"><a href="#goroutine-泄露怎么导致内存泄露-1" class="headerlink" title="goroutine 泄露怎么导致内存泄露"></a>goroutine 泄露怎么导致内存泄露</h3><p>每个 goroutine 占用 2KB 内存，泄露1百万 goroutine 至少泄露 <code>2KB * 1000000 = 2GB</code> 内存，为什么说至少呢？</p>
<p>goroutine 执行过程中还存在一些变量，如果这些变量指向堆内存中的内存，GC会认为这些内存仍在使用，不会对其进行回收，这些内存谁都无法使用，造成了内存泄露。</p>
<p>所以 goroutine 泄露有2种方式造成内存泄露：</p>
<ol>
<li>goroutine 本身的栈所占用的空间造成内存泄露。</li>
<li>goroutine 中的变量所占用的堆内存导致堆内存泄露，这一部分是能通过 heap profile 体现出来的。</li>
</ol>
<p>Dave 在文章中也提到了，如果不知道何时停止一个 goroutine ，这个 goroutine 就是潜在的内存泄露：</p>
<blockquote>
<p><a href="https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#know_when_to_stop_a_goroutine" target="_blank" rel="external">7.1.1 Know when to stop a goroutine</a><br>If you don’t know the answer, that’s a potential memory leak as the goroutine will pin its stack’s memory on the heap, as well as any heap allocated variables reachable from the stack.</p>
</blockquote>
<h3 id="怎么确定是goroutine泄露引发的内存泄露"><a href="#怎么确定是goroutine泄露引发的内存泄露" class="headerlink" title="怎么确定是goroutine泄露引发的内存泄露"></a>怎么确定是goroutine泄露引发的内存泄露</h3><p>掌握了前面的 pprof 命令行的基本用法，很快就可以确认是否是 goroutine 泄露导致内存泄露。</p>
<p><strong>判断依据：在节点正常运行的情况下，隔一段时间获取 goroutine 的数量，如果后面获取的那次，某些 goroutine 比前一次多，如果多获取几次，是持续增长的，就极有可能是goroutine 泄露。</strong></p>
<p>goroutine 导致内存泄露的 demo：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// goroutine泄露导致内存泄露</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">    <span class="string">"os"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 开启pprof</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        ip := <span class="string">"0.0.0.0:6060"</span></span><br><span class="line">        <span class="keyword">if</span> err := http.ListenAndServe(ip, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Printf(<span class="string">"start pprof failed on %s\n"</span>, ip)</span><br><span class="line">            os.Exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    outCh := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">    <span class="comment">// 死代码，永不读取</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">if</span> <span class="literal">false</span> &#123;</span><br><span class="line">            &lt;-outCh</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">select</span> &#123;&#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每s起100个goroutine，goroutine会阻塞，不释放内存</span></span><br><span class="line">    tick := time.Tick(time.Second / <span class="number">100</span>)</span><br><span class="line">    i := <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">range</span> tick &#123;</span><br><span class="line">        i++</span><br><span class="line">        fmt.Println(i)</span><br><span class="line">        alloc1(outCh)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">alloc1</span><span class="params">(outCh <span class="keyword">chan</span>&lt;- <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">    <span class="keyword">go</span> alloc2(outCh)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">alloc2</span><span class="params">(outCh <span class="keyword">chan</span>&lt;- <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">    <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">defer</span> fmt.Println(<span class="string">"alloc-fm exit"</span>)</span><br><span class="line">        <span class="comment">// 分配内存，借用一下</span></span><br><span class="line">        buf := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">10</span>)</span><br><span class="line">        _ = <span class="built_in">len</span>(buf)</span><br><span class="line">        fmt.Println(<span class="string">"alloc done"</span>)</span><br><span class="line"></span><br><span class="line">        outCh &lt;- <span class="number">0</span> <span class="comment">// 53行</span></span><br><span class="line">    &#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译并运行以上代码，然后使用 <code>go tool pprof</code> 获取 gorourine 的 profile 文件。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof http://localhost:6060/debug/pprof/goroutine</span><br></pre></td></tr></table></figure></p>
<p>已经通过 pprof 命令获取了2个 goroutine 的 profile 文件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls</span><br><span class="line">/home/ubuntu/pprof/pprof.leak_demo.goroutine.001.pb.gz</span><br><span class="line">/home/ubuntu/pprof/pprof.leak_demo.goroutine.002.pb.gz</span><br></pre></td></tr></table></figure></p>
<p>同 heap 一样，我们可以使用 <code>base</code> 对比2个 goroutine profile 文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$go</span> tool pprof -base pprof.leak_demo.goroutine.001.pb.gz pprof.leak_demo.goroutine.002.pb.gz</span><br><span class="line"></span><br><span class="line">File: leak_demo</span><br><span class="line">Type: goroutine</span><br><span class="line">Time: May 16, 2019 at 2:44pm (CST)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 20312, 100% of 20312 total</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">     20312   100%   100%      20312   100%  runtime.gopark</span><br><span class="line">         0     0%   100%      20312   100%  main.alloc2</span><br><span class="line">         0     0%   100%      20312   100%  main.alloc2.func1</span><br><span class="line">         0     0%   100%      20312   100%  runtime.chansend</span><br><span class="line">         0     0%   100%      20312   100%  runtime.chansend1</span><br><span class="line">         0     0%   100%      20312   100%  runtime.goparkunlock</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p>可以看到运行到 <code>runtime.gopark</code> 的 goroutine 数量增加了20312个。再通过002文件，看一眼执行到 <code>gopark</code> 的 goroutine 数量，即挂起的 goroutine 数量：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof pprof.leak_demo.goroutine.002.pb.gz</span><br><span class="line">File: leak_demo</span><br><span class="line">Type: goroutine</span><br><span class="line">Time: May 16, 2019 at 2:47pm (CST)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 24330, 100% of 24331 total</span><br><span class="line">Dropped 32 nodes (cum &lt;= 121)</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">     24330   100%   100%      24330   100%  runtime.gopark</span><br><span class="line">         0     0%   100%      24326   100%  main.alloc2</span><br><span class="line">         0     0%   100%      24326   100%  main.alloc2.func1</span><br><span class="line">         0     0%   100%      24326   100%  runtime.chansend</span><br><span class="line">         0     0%   100%      24326   100%  runtime.chansend1</span><br><span class="line">         0     0%   100%      24327   100%  runtime.goparkunlock</span><br></pre></td></tr></table></figure></p>
<p>显示有24330个 goroutine 被挂起，这不是 goroutine 泄露这是啥？已经能确定八九成goroutine 泄露了。</p>
<p>是什么导致如此多的 goroutine 被挂起而无法退出？接下来就看怎么定位 goroutine 泄露。</p>
<h2 id="定位goroutine泄露的2种方法"><a href="#定位goroutine泄露的2种方法" class="headerlink" title="定位goroutine泄露的2种方法"></a>定位goroutine泄露的2种方法</h2><p>使用 pprof 有2种方式，一种是web网页，一种是 <code>go tool pprof</code> 命令行交互，这两种方法查看 goroutine 都支持，但有轻微不同，也有各自的优缺点。</p>
<p>我们先看Web的方式，再看命令行交互的方式，这两种都很好使用，结合起来用也不错。</p>
<h3 id="Web可视化查看"><a href="#Web可视化查看" class="headerlink" title="Web可视化查看"></a>Web可视化查看</h3><p>Web方式适合web服务器的端口能访问的情况，使用起来方便，有2种方式：</p>
<ol>
<li><strong>查看某条调用路径上，当前阻塞在此 goroutine 的数量</strong></li>
<li>查看所有 goroutine 的运行栈（调用路径），可以<strong>显示阻塞在此的时间</strong></li>
</ol>
<h4 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h4><p>url请求中设置debug=1：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://ip:port/debug/pprof/goroutine?debug=1</span><br></pre></td></tr></table></figure></p>
<p>效果如下：<br><img src="/images/go/pprof_6.png" alt="go pprof"></p>
<p>看起来密密麻麻的，其实简单又十分有用，看上图标出来的部分，手机上图看起来可能不方便，那就放大图片，或直接看下面各字段的含义：</p>
<ol>
<li><code>goroutine profile: total 32023</code>：32023 是 <strong>goroutine 的总数量</strong></li>
<li><code>32015 @ 0x42e15a 0x42e20e 0x40534b 0x4050e5 ...</code>：32015 代表当前有32015 个 goroutine 运行这个调用栈，并且停在相同位置，@后面的十六进制，现在用不到这个数据，所以暂不深究了</li>
<li>下面是当前 goroutine 的<strong>调用栈</strong>，列出了<strong>函数和所在文件的行数，这个行数对定位很有帮助</strong>，如下：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">32015 @ 0x42e15a 0x42e20e 0x40534b 0x4050e5 0x6d8559 0x6d831b 0x45abe1</span><br><span class="line"><span class="comment">#    0x6d8558    main.alloc2.func1+0xf8    /home/ubuntu/heap/leak_demo.go:53</span></span><br><span class="line"><span class="comment">#    0x6d831a    main.alloc2+0x2a    /home/ubuntu/heap/leak_demo.go:54</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>根据上面的提示，就能判断32015个 goroutine 运行到 <code>leak_demo.go</code> 的53行：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">alloc2</span><span class="params">(outCh <span class="keyword">chan</span>&lt;- <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">    <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">defer</span> fmt.Println(<span class="string">"alloc-fm exit"</span>)</span><br><span class="line">        <span class="comment">// 分配内存，假用一下</span></span><br><span class="line">        buf := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">10</span>)</span><br><span class="line">        _ = <span class="built_in">len</span>(buf)</span><br><span class="line">        fmt.Println(<span class="string">"alloc done"</span>)</span><br><span class="line"></span><br><span class="line">        outCh &lt;- <span class="number">0</span> <span class="comment">// 53行</span></span><br><span class="line">    &#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>阻塞的原因是 outCh 这个写操作无法完成，outCh 是无缓冲的通道，并且由于以下代码是死代码，所以 goroutine 始终没有从 outCh 读数据，造成 outCh 阻塞，进而造成无数个alloc2 的 goroutine 阻塞，形成内存泄露：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">false</span> &#123;</span><br><span class="line">    &lt;-outCh</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="方式二"><a href="#方式二" class="headerlink" title="方式二"></a>方式二</h4><p>url 请求中设置 debug=2 ：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://ip:port/debug/pprof/goroutine?debug=2</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/go/pprof_7.png" alt="go pprof"></p>
<p>第2种方式和第1种方式是互补的，它可以看到每个 goroutine 的信息：</p>
<ol>
<li><code>goroutine 20 [chan send, 2 minutes]</code>：20 是 goroutine id，[]中是当前goroutine 的状态，阻塞在写 channel ，并且阻塞了2分钟，长时间运行的系统，你能看到阻塞时间更长的情况。</li>
<li>同时，也可以看到调用栈，看当前执行停到哪了：<code>leak_demo.go</code> 的53行<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">goroutine 20 [chan send, 2 minutes]:</span><br><span class="line">main.alloc2.func1(0xc42015e060)</span><br><span class="line">    /home/ubuntu/heap/leak_demo.go:53 +0xf9  // 这</span><br><span class="line">main.alloc2(0xc42015e060)</span><br><span class="line">    /home/ubuntu/heap/leak_demo.go:54 +0x2b</span><br><span class="line">created by main.alloc1</span><br><span class="line">    /home/ubuntu/heap/leak_demo.go:42 +0x3f</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="命令行交互式方法"><a href="#命令行交互式方法" class="headerlink" title="命令行交互式方法"></a>命令行交互式方法</h3><p>Web的方法是简单粗暴，无需登录服务器，浏览器打开看看就行了。但就像前面提的，没有浏览器可访问时，命令行交互式才是最佳的方式，并且也是手到擒来，感觉比Web一样方便。</p>
<p>命令行交互式只有1种获取 goroutine profile 的方法，不像Web网页分 <code>debug=1</code> 和<code>debug=2</code> 2种方式，并将 profile 文件保存到本地：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 注意命令没有`debug=1`，debug=1，加debug有些版本的go不支持</span><br><span class="line">$ go tool pprof http://0.0.0.0:6060/debug/pprof/goroutine</span><br><span class="line">Fetching profile over HTTP from http://localhost:6061/debug/pprof/goroutine</span><br><span class="line">Saved profile <span class="keyword">in</span> /home/ubuntu/pprof/pprof.leak_demo.goroutine.001.pb.gz  // profile文件保存位置</span><br><span class="line">File: leak_demo</span><br><span class="line">Type: goroutine</span><br><span class="line">Time: May 16, 2019 at 2:44pm (CST)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p>命令行只需要掌握3个命令就好了，上面介绍过了，详细的倒回去看 <code>top</code>, <code>list</code>, <code>traces</code>：</p>
<ol>
<li><strong>top</strong>：显示正运行到某个函数 goroutine 的数量</li>
<li><strong>traces</strong>：显示所有 goroutine 的调用栈</li>
<li><strong>list</strong>：列出代码详细的信息。</li>
</ol>
<p>我们依然使用 <code>leak_demo.go</code> 这个demo<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$  go tool pprof -base pprof.leak_demo.goroutine.001.pb.gz pprof.leak_demo.goroutine.002.pb.gz</span><br><span class="line">File: leak_demo</span><br><span class="line">Type: goroutine</span><br><span class="line">Time: May 16, 2019 at 2:44pm (CST)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof)</span><br><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 20312, 100% of 20312 total</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">     20312   100%   100%      20312   100%  runtime.gopark</span><br><span class="line">         0     0%   100%      20312   100%  main.alloc2</span><br><span class="line">         0     0%   100%      20312   100%  main.alloc2.func1</span><br><span class="line">         0     0%   100%      20312   100%  runtime.chansend</span><br><span class="line">         0     0%   100%      20312   100%  runtime.chansend1</span><br><span class="line">         0     0%   100%      20312   100%  runtime.goparkunlock</span><br><span class="line">(pprof)</span><br><span class="line">(pprof) traces</span><br><span class="line">File: leak_demo</span><br><span class="line">Type: goroutine</span><br><span class="line">Time: May 16, 2019 at 2:44pm (CST)</span><br><span class="line">-----------+-------------------------------------------------------</span><br><span class="line">     20312   runtime.gopark</span><br><span class="line">             runtime.goparkunlock</span><br><span class="line">             runtime.chansend</span><br><span class="line">             runtime.chansend1 // channel发送</span><br><span class="line">             main.alloc2.func1 // alloc2中的匿名函数</span><br><span class="line">             main.alloc2</span><br><span class="line">-----------+-------------------------------------------------------</span><br></pre></td></tr></table></figure></p>
<p>top 命令在怎么确定是 goroutine 泄露引发的内存泄露介绍过了，直接看 traces 命令，traces 能列出002中比001中多的那些 goroutine 的调用栈，这里只有1个调用栈，有20312个 goroutine 都执行这个调用路径，可以看到 alloc2 中的匿名函数 <code>alloc2.func1</code> 调用了写 channel 的操作，然后阻塞挂起了 goroutine，使用 list 列出 <code>alloc2.func1</code> 的代码，显示有20312个 goroutine 阻塞在53行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(pprof) list main.alloc2.func1</span><br><span class="line">Total: 20312</span><br><span class="line">ROUTINE ======================== main.alloc2.func1 <span class="keyword">in</span> /home/ubuntu/heap/leak_demo.go</span><br><span class="line">         0      20312 (flat, cum)   100% of Total</span><br><span class="line">         .          .     48:        // 分配内存，假用一下</span><br><span class="line">         .          .     49:        buf := make([]byte, 1024*1024*10)</span><br><span class="line">         .          .     50:        _ = len(buf)</span><br><span class="line">         .          .     51:        fmt.Println(<span class="string">"alloc done"</span>)</span><br><span class="line">         .          .     52:</span><br><span class="line">         .      20312     53:        outCh &lt;- 0  // 看这</span><br><span class="line">         .          .     54:    &#125;()</span><br><span class="line">         .          .     55:&#125;</span><br><span class="line">         .          .     56:</span><br></pre></td></tr></table></figure></p>
<p><strong>友情提醒：使用list命令的前提是程序的源码在当前机器，不然可没法列出源码</strong>。服务器上，通常没有源码，那我们咋办呢？刚才介绍了Web查看的方式，那里会列出代码行数，我们可以使用 <code>wget</code> 下载网页：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://localhost:6060/debug/pprof/goroutine?debug=1</span><br></pre></td></tr></table></figure></p>
<p>下载网页后，使用编辑器打开文件，使用关键字 <code>main.alloc2.func1</code> 进行搜索，找到与当前相同的调用栈，就可以看到该 goroutine 阻塞在哪一行了，不要忘记使用 <code>debug=2</code> 还可以看到阻塞了多久和原因，Web方式中已经介绍了，此处省略代码几十行。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="goroutine-泄露的本质"><a href="#goroutine-泄露的本质" class="headerlink" title="goroutine 泄露的本质"></a>goroutine 泄露的本质</h3><p>goroutine 泄露的本质是 channel 阻塞，无法继续向下执行，导致此 goroutine 关联的内存都无法释放，进一步造成内存泄露。</p>
<h3 id="goroutine-泄露的发现和定位"><a href="#goroutine-泄露的发现和定位" class="headerlink" title="goroutine 泄露的发现和定位"></a>goroutine 泄露的发现和定位</h3><p>利用好 go pprof 获取 goroutine profile 文件，然后利用3个命令 <code>top</code>、<code>traces</code>、<code>list</code> 定位内存泄露的原因。</p>
<h3 id="goroutine-泄露的场景"><a href="#goroutine-泄露的场景" class="headerlink" title="goroutine 泄露的场景"></a>goroutine 泄露的场景</h3><p>泄露的场景不仅限于以下两类，但因channel相关的泄露是最多的。</p>
<ol>
<li>channel 的读或者写：<ol>
<li>无缓冲 channel 的阻塞通常是写操作因为没有读而阻塞</li>
<li>有缓冲的 channel 因为缓冲区满了，写操作阻塞</li>
<li>期待从 channel 读数据，结果没有 goroutine 写</li>
</ol>
</li>
<li>select 操作，select 里也是 channel 操作，如果所有 case 上的操作阻塞，goroutine 也无法继续执行。</li>
</ol>
<h3 id="编码-goroutine-泄露的建议"><a href="#编码-goroutine-泄露的建议" class="headerlink" title="编码 goroutine 泄露的建议"></a>编码 goroutine 泄露的建议</h3><p>为避免 goroutine 泄露造成内存泄露，启动 goroutine 前要思考清楚：</p>
<ol>
<li>goroutine 如何退出？</li>
<li>是否会有阻塞造成无法退出？如果有，那么这个路径是否会创建大量的 goroutine？</li>
</ol>
<p>本文作者：大彬</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes hostpath和local volume区别]]></title>
      <url>http://team.jiunile.com/blog/2020/09/k8s-local-volume.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>很多人对hostPath volume和local persistent volume的使用场景还存在很多困惑。下面对着两种volume的使用场景、基本的工作机制进行了分析，介绍了使用时的注意事项，并简单介绍<a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume" target="_blank" rel="external">local volume manager</a>如何帮助administrator进行local persistent volume的生命周期管理的</p>
</blockquote>
<h2 id="hostPath-volume存在的问题"><a href="#hostPath-volume存在的问题" class="headerlink" title="hostPath volume存在的问题"></a>hostPath volume存在的问题</h2><p>过去我们经常会通过hostPath volume让Pod能够使用本地存储，将Node文件系统中的文件或者目录挂载到容器内，但是hostPath volume的使用是很难受的，并不适合在生产环境中使用。</p>
<p>我们先看看hostPath Type有哪些类型：</p>
<table>
<thead>
<tr>
<th><strong>取值</strong></th>
<th><strong>行为</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。</td>
</tr>
<tr>
<td><strong>DirectoryOrCreate</strong></td>
<td>在给定路径上必须存在的目录。</td>
</tr>
<tr>
<td><strong>FileOrCreate</strong></td>
<td>如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 Kubelet 相同的组和所有权。</td>
</tr>
<tr>
<td><strong>File</strong></td>
<td>在给定路径上必须存在的文件。</td>
</tr>
<tr>
<td><strong>Socket</strong></td>
<td>在给定路径上必须存在的 UNIX 套接字。</td>
</tr>
<tr>
<td><strong>CharDevice</strong></td>
<td>在给定路径上必须存在的字符设备。</td>
</tr>
<tr>
<td><strong>BlockDevice</strong></td>
<td>在给定路径上必须存在的块设备。</td>
</tr>
</tbody>
</table>
<p>看起来支持这么多type还是挺好的，但为什么说不适合在生产环境中使用呢？<br><a id="more"></a></p>
<ul>
<li>由于集群内每个节点的差异化，要使用 <code>hostPath Volume</code>，我们需要通过<code>NodeSelector</code> 等方式进行精确调度，这种事情多了，你就会不耐烦了。</li>
<li>注意 <code>DirectoryOrCreate</code> 和 <code>FileOrCreate</code> 两种类型的 <code>hostPath</code>，当Node上没有对应的 <code>File/Directory</code> 时，你需要保证kubelet有在 <code>Node上Create File/Directory</code> 的权限。</li>
<li>另外，如果 Node 上的文件或目录是由 root 创建的，挂载到容器内之后，你通常还要保证容器内进程有权限对该文件或者目录进行写入，比如你需要以 root 用户启动进程并运行于 privileged 容器，或者你需要事先修改好 Node 上的文件权限配置。</li>
<li><code>Scheduler</code> 并不会考虑 <code>hostPath volume</code> 的大小，<code>hostPath</code> 也不能申明需要的 <code>storage size</code>，这样调度时存储的考虑，就需要人为检查并保证。</li>
</ul>
<h2 id="local-persistent-volume工作机制"><a href="#local-persistent-volume工作机制" class="headerlink" title="local persistent volume工作机制"></a>local persistent volume工作机制</h2><p><code>Local persistent volume</code> 就是用来解决 <code>hostPath volume</code> 面临的 <strong>portability, disk accounting, and scheduling</strong> 的缺陷。<code>PV Controller</code> 和 <code>Scheduler</code> 会对 <code>local PV</code> 做特殊的逻辑处理，以实现 Pod 使用本地存储时发生Pod <code>re-schedule</code> 的情况下能再次调度到 <code>local volume</code> 所在的 Node。</p>
<p><code>local pv</code> 在生产中使用，也是需要谨慎的，毕竟它本质上还是使用的是节点上的本地存储，如果没有相应的存储副本机制，那意味着一旦节点或者磁盘异常，使用该volume的Pod也会异常，甚至出现数据丢失，除非你明确知道这个风险不会对你的应用造成很大影响或者允许数据丢失。</p>
<p>那么通常什么情况会使用Local PV呢？</p>
<ul>
<li>比如节点上的目录数据是从远程的网络存储上挂载或者预先读取到本地的，为了能加速Pod读取这些数据的速度，相当于起Cache作用，这种情况下因为只读，不存在惧怕数据丢失。这种AI训练中存在需要重复利用并且训练数据巨大的时候可能会采取的方式。</li>
<li>如果本地节点上目录/磁盘实际是具有副本/分片机制的分布式存储(比如gluster, ceph等)挂载过来的，这种情况也可以使用<code>local pv</code>。</li>
</ul>
<p><code>Local volume</code> 允许挂载本地的 disk, partition, directory 到容器内某个挂载点。在 Kuberentes 1.11 仍然仅支持 <code>local pv</code> 的 <code>static provision</code>，不支持<code>dynamic provision</code>。</p>
<ul>
<li>Kubernetes 使用 <code>PersistentVolume</code> 的 <code>.spec.nodeAffinityfield</code> 来描述<code>local volume</code> 与 Node 的绑定关系。</li>
<li>使用 <code>volumeBindingMode: WaitForFirstConsumer</code> 的 <code>local-storage StorageClass</code> 来实现 PVC 的延迟绑定，使得 <code>PV Controller</code> 并不会立刻为 PVC 做 Bound，而是等待某个需要使用该 <code>local pv</code> 的 Pod 完成调度后，才去做 Bound。</li>
</ul>
<p>下面是定义local pv的Sample：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> PersistentVolume</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> example-pv</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">100</span>Gi</span><br><span class="line">  <span class="comment"># volumeMode field requires BlockVolume Alpha feature gate to be enabled.</span></span><br><span class="line"><span class="attr">  volumeMode:</span> Filesystem</span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> ReadWriteOnce</span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> Retain</span><br><span class="line"><span class="attr">  storageClassName:</span> local-storage</span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line"><span class="attr">    path:</span> /mnt/disks/ssd1</span><br><span class="line"><span class="attr">  nodeAffinity:</span></span><br><span class="line"><span class="attr">    required:</span></span><br><span class="line"><span class="attr">      nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">      - matchExpressions:</span></span><br><span class="line"><span class="attr">        - key:</span> kubernetes.io/hostname</span><br><span class="line"><span class="attr">          operator:</span> In</span><br><span class="line"><span class="attr">          values:</span></span><br><span class="line"><span class="bullet">          -</span> example-node</span><br></pre></td></tr></table></figure></p>
<p>对应的<code>local-storage storageClass</code>定义如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> local-storage</span><br><span class="line"><span class="attr">provisioner:</span> kubernetes.io/<span class="literal">no</span>-provisioner</span><br><span class="line"><span class="attr">volumeBindingMode:</span> WaitForFirstConsumer</span><br></pre></td></tr></table></figure></p>
<h2 id="使用local-persistent-volume注意事项"><a href="#使用local-persistent-volume注意事项" class="headerlink" title="使用local persistent volume注意事项"></a>使用local persistent volume注意事项</h2><ul>
<li>使用 <code>local pv</code> 时必须定义 <code>nodeAffinity</code>，Kubernetes Scheduler 需要使用PV 的 <code>nodeAffinity</code> 描述信息来保证 Pod 能够调度到有对应 <code>local volume</code> 的Node 上。</li>
<li><code>volumeMode</code> 可以是 <code>FileSystem（Default）</code>和 Block，并且需要 <code>enable BlockVolume Alpha feature gate</code>。</li>
<li><p>创建 <code>local PV</code> 之前，你需要先保证有对应的 <code>storageClass</code> 已经创建。并且该<code>storageClass</code> 的 <code>volumeBindingMode</code> 必须是 <code>WaitForFirstConsumer</code> 以标识延迟 <code>Volume Binding</code> 。<code>WaitForFirstConsumer</code> 可以保证正常的 Pod 调度要求（resource requirements, node selectors, Pod affinity, and Pod anti-affinity等），又能保证 Pod 需要的 <code>Local PV</code> 的 <code>nodeAffinity</code> 得到满足，实际上，一共有以下两种 <code>volumeBindingMode</code>：</p>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// VolumeBindingImmediate indicates that PersistentVolumeClaims should be</span></span><br><span class="line"><span class="comment">// immediately provisioned and bound.</span></span><br><span class="line">VolumeBindingImmediate VolumeBindingMode = <span class="string">"Immediate"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// VolumeBindingWaitForFirstConsumer indicates that PersistentVolumeClaims</span></span><br><span class="line"><span class="comment">// should not be provisioned and bound until the first Pod is created that</span></span><br><span class="line"><span class="comment">// references the PeristentVolumeClaim.  The volume provisioning and</span></span><br><span class="line"><span class="comment">// binding will occur during Pod scheduing.</span></span><br><span class="line">VolumeBindingWaitForFirstConsumer VolumeBindingMode = <span class="string">"WaitForFirstConsumer"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>节点上 <code>local volume</code> 的初始化需要我们人为去完成（比如 <code>local disk</code> 需要 pre-partitioned, formatted, and mounted. 共享存储对应的 Directories 也需要 pre-created），并且人工创建这个 <code>local PV</code>，当 Pod 结束，我们还需要手动的清理 <code>local volume</code>，然后手动删除该 <code>local PV</code> 对象。因此，<code>persistentVolumeReclaimPolicy</code> 只能是 <code>Retain</code>。</p>
</li>
</ul>
<h2 id="local-volume-manager"><a href="#local-volume-manager" class="headerlink" title="local volume manager"></a>local volume manager</h2><p>上面这么多事情需要人为的去做预处理的工作，我们必须要有解决方案帮我们自动完成 <code>local volume</code> 的 create 和 cleanup 的工作。官方给出了一个简单的 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume" target="_blank" rel="external">local volume manager</a> ，注意它仍然只是一个 <code>static provisioner</code>，目前主要帮我们做两件事：</p>
<ul>
<li><code>local volume manager</code> 监控配置好的 discovery directory 的新的挂载点，并为每个挂载点根据对应的 storageClassName, path, nodeAffinity, and capacity 创建<code>PersistentVolume object</code>。</li>
<li>当Pod结束并删除了使用 <code>local volume</code> 的 PVC，<code>local volume manager</code> 将自动清理该 local mount 上的所有文件, 然后删除对应的 <code>PersistentVolume object</code>.</li>
</ul>
<p>因此，除了需要人为的完成 <code>local volume</code> 的 mount 操作，<code>local PV</code> 的生命周期管理就全部交给 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume" target="_blank" rel="external">local volume manager</a> 了。下面我们专门介绍下这个 <code>Static Local Volume Provisioner</code>。</p>
<p>后面我会单独写一个博文对 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume" target="_blank" rel="external">local volume manager</a> 进行深度剖析。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文对 <code>hostPath volume</code> 不能在生产环境中很好使用的原因进行了阐述，然后对 <code>local persistent volume</code> 的使用场景、基本的工作机制进行了分析，介绍了使用时的注意事项，最后简单介绍了 <code>local volume manager</code> 如何帮助 administrator 进行 <code>local persistent volume</code> 的生命周期管理的。</p>
<p>来源：oschina</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go Singleflight导致死锁问题分析]]></title>
      <url>http://team.jiunile.com/blog/2020/09/go-singleflight-deadlock.html</url>
      <content type="html"><![CDATA[<h2 id="思路排查"><a href="#思路排查" class="headerlink" title="思路排查"></a>思路排查</h2><h3 id="Dump-堆栈很重要"><a href="#Dump-堆栈很重要" class="headerlink" title="Dump 堆栈很重要"></a>Dump 堆栈很重要</h3><p>线上某个环境发现 S3 上传请求卡住，请求不返回，卡了30分钟，长时间没有发现有效日志。一般来讲，死锁问题还是好排查的，因为现场一般都在。类似于 c 程序，遇到死锁问题都会用 pstack 看一把。golang 死锁排查思路也类似（golang 不适合使用 pstack，因为 golang 调度的是协程，pstack 只能看到线程栈），我们其实是需要知道 S3 程序里 goroutine 的栈状态。golang 遇到这个问题我们有两个办法：</p>
<ol>
<li>方法一：条件允许的话，gcore 出一个堆栈，这个是最有效的方法，因为是把整个 golang 程序的内存镜像 dump 出来，然后用 dlv 分析</li>
<li>方法二：如果你提前开启 net/pprof 库的引用，开启了 debug 接口，那么就可以调用 curl 接口，通过 http 接口获取进程的状态信息</li>
</ol>
<p>需要注意到，golang 程序和 c 程序还是有点区别，goroutine 非常多，成百上千个 goroutine 是常态，甚至上万个也不稀奇。所以我们一般无法在终端上直接看完所有的栈，一般都是把所有的 goroutine 栈 dump 到文件，然用 vi 打开慢慢分析。</p>
<ul>
<li>调试这个 core 文件，意图从堆栈里找到些东西，由于堆栈太多了，所以就使用 <code>gorouties -t -u</code> 这个命令，并且把输出 dump 到文件；</li>
<li><code>curl xxx/debug/pprof/goroutine</code></li>
</ul>
<a id="more"></a>
<h3 id="关键思路"><a href="#关键思路" class="headerlink" title="关键思路"></a>关键思路</h3><p>成千上万个 goroutine ，直接显示到终端是不合适的，我们 dump 到文件 test.txt，然后分析 test.txt 这个文件。<strong>去查找发现了一些可疑堆栈，那么什么是可疑堆栈？重点关注加锁等待的堆栈，关键字是 <code>runtime_notifyListWait</code> 、<code>semaphore</code> 、<code>sync.(*Cond).Wait</code> 、<code>Acquire</code>  这些阻塞场景才会用到的，如果业务堆栈上出现这个加锁调用，就非常可疑</strong>。</p>
<p><strong>划重点：</strong></p>
<ol>
<li>留意阻塞关键字 <code>runtime_notifyListWait</code> 、<code>semaphore</code> 、<code>sync.(*Cond).Wait</code> 、<code>Acquire</code></li>
<li>业务堆栈（非 runtime 的一些内部堆栈）</li>
</ol>
<p><img src="/images/go/singleflight01.png" alt="singleflight-heap"></p>
<p>统计分析发现，有 11 个这个堆栈都在这同一个地方，都是在等同一把锁 <code>blockingKeyCountLimit.lock</code>，所以基本确认了阻塞的位置，就是这个地方阻塞到了所有的请求，但是这把锁我们使用 defer 释放的，使用姿势如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// do someting</span></span><br><span class="line">lock.Acquire(key)</span><br><span class="line"><span class="keyword">defer</span> lock.Release(key)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下为锁内操作；</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>blockingKeyCountLimit 是我们封装针对 key 操作流控的组件。举个例子，如果 limit == 1，key为 “test” 在 g1 上 Acquire 成功，g2 acquire(“test”) 就会等待，这个可以算是我们优化的一个逻辑。如果 limit == 2，那么就允许两个人加锁到，后面的人都等待。</p>
</blockquote>
<p>从代码来看，函数退出一定会释放的，但是偏偏现在锁就卡在这个地方，所以就非常奇怪。我们先找哪个 goroutine 占着这把锁不释放，看看能不能搞清楚怎样导致这里抢不到锁的原因。</p>
<p>通过审查业务代码分析，发现可能的源头函数（这个函数是向后端请求的函数）：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">api.(*Client).getBytesNolc</span><br></pre></td></tr></table></figure></p>
<p>确认是 <code>getBytesNolc</code> 这个函数执行的操作，那么大概率就是卡在这个地方了。用这个 <code>getBytesNolc</code> 字符串搜索堆栈，找下是哪个堆栈 ？搜索到这个堆栈 <code>goroutine 19458</code><br><img src="/images/go/singleflight02.png" alt="singleflight-heap"></p>
<p>大概率就是第 1 个堆栈了，也就是其他的 11 个 goroutine 都在等这 <code>goroutine 19458</code>  来放锁，仔细看这个堆栈。那么为啥这个堆栈不放锁呢？这里有个细节要注意下，这里是卡到 <code>gihub.com/golang/groupcache/singleflight/singleflight.go:48</code> 这一行：<br><img src="/images/go/singleflight03.png" alt="singleflight"></p>
<p>这是一个开源库，singleflight 实现了缓存防击穿的功能。</p>
<blockquote>
<p>简单介绍下 <code>singleflight</code> 的功能，这是一个非常有效的工具。在缓存大量失效的场景，如果针对同一个 key ，其实只需要有一个人穿透到后端请求数据，其他人等待他完成，然后取缓存结果即可。这个就是 <code>singleflight</code> 实现的功能。具体实现就是：来了请求之后，把 key 插入到 map 里，后面的请求如果发现同名 key 在 map 里面，那么就等待它完成就好；</p>
</blockquote>
<p>截屏显示卡到 <code>c.wg.Wait()</code> 这一行，那么说明 map 里面肯定有已经存在的 key，说明 <code>goroutine 19458</code> 不是第一个人？但是外面还有一个 <code>blockingKeyCountLimit</code> 的互斥呢，按道理其他的人也进不来（因为 limit == 1），这里这么讲来肯定要是源头才对？</p>
<h3 id="思路整理"><a href="#思路整理" class="headerlink" title="思路整理"></a>思路整理</h3><p>伪代码显示如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">xxx</span> <span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 大部分协程都卡在这里（11个）</span></span><br><span class="line">    <span class="comment">// 这个锁的效果主要是流控，limit 值初始化赋值，可以是 1，也可以是其他；</span></span><br><span class="line">    <span class="comment">// locker 为 blockingKeyCountLimit 类型</span></span><br><span class="line">    limitLocker.Acquire( key )</span><br><span class="line">    <span class="keyword">defer</span> limitLocker.Release( key )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取数据</span></span><br><span class="line">    getBytesNolc( key , ...)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getBytesNolc</span> <span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 下面就是 singleflight.Group 的用法，防穿透</span></span><br><span class="line">    <span class="comment">// 同一时间只允许一个人去后端更新</span></span><br><span class="line">    ret, err = x.Group.Do(id, <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="params">(<span class="keyword">interface</span>&#123;&#125;, error)</span></span> &#123;</span><br><span class="line">        <span class="comment">// 去服务后台获取，更新数据；</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>图示显示当前的现状：<br><img src="/images/go/singleflight04.png" alt="singleflight"></p>
<p>现状小结：</p>
<ol>
<li>大量的协程都在等 <code>blockingKeyCountLimit</code> 这把锁释放；</li>
<li>协程 <code>goroutine 19458</code> 持有 <code>blockingKeyCountLimit</code> 这把锁；</li>
<li>协程 <code>goroutine 19458</code>  却在等一个相同 key 名字的任务的完成（ <code>singleflight</code> 一个防击穿的库，同一时间相同 key 只允许放到一个后端去执行），却永远没等到，协程因此呈现死锁；</li>
</ol>
<p>当前的疑问就是第一个 key 的任务为啥永远完不成，堆栈也找不到了，去哪里了？</p>
<h3 id="发现蛛丝马迹"><a href="#发现蛛丝马迹" class="headerlink" title="发现蛛丝马迹"></a>发现蛛丝马迹</h3><p>我们再仔细审一下 <code>singleflight</code> 的代码：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *Group)</span> <span class="title">Do</span><span class="params">(key <span class="keyword">string</span>, fn <span class="keyword">func</span>()</span> <span class="params">(<span class="keyword">interface</span>&#123;&#125;, error)</span>) <span class="params">(<span class="keyword">interface</span>&#123;&#125;, error)</span></span> &#123;</span><br><span class="line">    g.mu.Lock()</span><br><span class="line">    <span class="keyword">if</span> g.m == <span class="literal">nil</span> &#123;</span><br><span class="line">        g.m = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]*call)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果找到同名 key 已经存在；</span></span><br><span class="line">    <span class="keyword">if</span> c, ok := g.m[key]; ok &#123;</span><br><span class="line">        g.mu.Unlock()</span><br><span class="line">        <span class="comment">// 等待者走到这个分支：等待第一个人执行完成，最后直接返回它的结果就行了；</span></span><br><span class="line">        c.wg.Wait()</span><br><span class="line">        <span class="keyword">return</span> c.val, c.err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果同名 key 不存在（第一个人走到这个分支）</span></span><br><span class="line">    c := <span class="built_in">new</span>(call)</span><br><span class="line">    c.wg.Add(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// map 里放置 key</span></span><br><span class="line">    g.m[key] = c</span><br><span class="line">    g.mu.Unlock()</span><br><span class="line">    <span class="comment">// 执行任务</span></span><br><span class="line">    c.val, c.err = fn()</span><br><span class="line">    <span class="comment">// 唤醒所有的等待者</span></span><br><span class="line">    c.wg.Done()</span><br><span class="line"></span><br><span class="line">    g.mu.Lock()</span><br><span class="line">    <span class="comment">// 删除 map 里的 key</span></span><br><span class="line">    <span class="built_in">delete</span>(g.m, key)</span><br><span class="line">    g.mu.Unlock()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c.val, c.err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>发现有个线索，我们的 S3 服务程序一个 http 请求对应一个协程处理，为了提高服务端进程的可用性，在框架里会捕捉 <code>panic</code>，这样确保单个协程处理不会影响到其他的请求。基于这个前提，我们假设：如果 <code>fn()</code> 执行异常，<code>panic</code> 掉了，那么就不会走 <code>delete(g.m, key)</code> 的代码，那么 key 就永远都残留在 map 里面，而进程却又还活着。恍然大悟。</p>
<h3 id="完整的推理流程"><a href="#完整的推理流程" class="headerlink" title="完整的推理流程"></a>完整的推理流程</h3><ol>
<li>第一个协程 g1 来了，加了 <code>blockingKeyCountLimit</code> 锁，然后准备穿透到后端，调用函数 <code>getBytesNolc</code> 获取数据，并走进了 <code>singlelight</code> ，添加了一个 key：x， 准备干活；<ol>
<li>干活发生了一些不可预期的异常（后面发现是配置的异常），nil 指针引用之类的， <code>panic</code> 堆栈了，<code>panic</code> 导致后面 <code>delete key</code> 操作没有执行</li>
<li>虽然 g1 现在 <code>panic</code> 了，但是由于在函数 <code>func xxx</code> 里面 <code>blockingKeyCountLimit</code> 是 defer 执行的，所以这把锁还是，但是 <code>singlelight</code> 的 key 还存在，于是残留在 map 里面</li>
<li>但是由于我们服务程序为了高可用是 <code>recover</code> 了 <code>panic</code> 的，单个请求的失败不会导致整个进程挂掉，所以进程还是好好的</li>
</ol>
</li>
<li>第二个 <code>goroutine 19458</code> 协程来了，<code>blockingKeyCountLimit</code> 加锁，然后走到 <code>singlelight</code> 的时候，发现有 <code>key: x</code> 了，于是就等待<ol>
<li>并且等待的是一个永远得不到的锁，因为 g1 早就没了；</li>
</ol>
</li>
<li>后续的 11 个 协程来了，于是被 <code>blockingKeyCountLimit</code> 阻塞住，并且永远不能释放</li>
</ol>
<p>实锤：后续基于这个猜想，再去搜索一遍日志，发现确实是有一条 panic 相关的日志。这个时间点后面的请求全部被卡住。</p>
<h2 id="思考总结"><a href="#思考总结" class="headerlink" title="思考总结"></a>思考总结</h2><p>一般来讲 c 语言写程序容易出现死锁问题，因为各种异常逻辑可能会导致忘记放锁，从而导致抢一个永远都不可能得到的锁。<strong>golang 为了解决这个问题，一般是用 defer 机制来实现，使用姿势如下</strong>：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span> <span class="params">()</span></span> &#123;</span><br><span class="line">    mtx.Lock()</span><br><span class="line">    <span class="keyword">defer</span> mtx.Unlock()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 临界区 */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>golang 的 defer 机制是一个经过经验沉淀下来的有效功能</strong>。我们必须要合理使用。defer 实现原理是和所在函数绑定，保证函数 return 的时候一定能调用到（ panic 退出也能），所以 golang 加锁放锁的有效实践是写在相邻的两行。</p>
<p>其实思考下，<strong><code>singleflight</code> 作为一个通用开源库，其实可以把 <code>delete map key</code> 放到 defer 里，这样就能保证 map 里面的 key 一定是可以被清理的</strong>。</p>
<p>还有一点，<strong>其实 golang 是不提倡异常-捕捉这样的方式编程</strong>，<code>panic</code> 一般不让随便用，如果真是严重的问题，挂掉就挂掉，这个估计还好一些。当然这是要看场景的，还是有一些特殊场景的，毕竟 golang 都已经提供了 <code>panic-recover</code> 这样的一个手段，就说明还是有需求。这个就跟 unsafe 库一样，你只有明确知道自己的行为影响，才去使用这个工具，否则别用。</p>
<p>来源：奇伢云存储</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Istio 实战系列(1) - 应用容器对 Envoy Sidecar 的启动依赖问题]]></title>
      <url>http://team.jiunile.com/blog/2020/09/istio-depend-sidecar.html</url>
      <content type="html"><![CDATA[<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>典型案例：某运维同学反馈：昨天晚上 <code>Istio</code> 环境中应用的心跳检测报 <code>connect reset</code>，然后服务重启了。怀疑是 <code>Istio</code> 环境中网络不稳定导致了服务重启。</p>
<p>该问题的表现是安装了 <code>sidecar proxy</code> 的应用，在启动后的一小段时间内无法通过网络访问 pod 外部的其他服务，例如外部的 HTTP，MySQL，Redis等服务。如果应用没有对依赖服务的异常进行容错处理，该问题还常常会导致应用启动失败。</p>
<p>下面我们以该问题导致的一个典型故障的分析过程为例，对该问题的原因进行说明。<br><a id="more"></a></p>
<h2 id="故障分析"><a href="#故障分析" class="headerlink" title="故障分析"></a>故障分析</h2><p>根据运维同学的反馈，该 pod 曾多次重启。因此我们先用 <code>kubectl logs --previous</code> 命令查询 awesome-app 容器最后一次重启前的日志，以从日志中查找其重启的原因。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs --previous awesome-app-cd1234567-gzgwg -c awesome-app</span><br></pre></td></tr></table></figure></p>
<p>从日志中查询到了其重启前最后的错误信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Logging system failed to initialize using configuration from <span class="string">'http://log-config-server:12345/******/logback-spring.xml'</span></span><br><span class="line">java.net.ConnectException: Connection refused (Connection refused)</span><br><span class="line">        at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)</span><br></pre></td></tr></table></figure></p>
<p>从错误信息可以得知，应用进程在启动时试图通过 HTTP 协议从配置中心拉取 logback 的配置信息，但该操作由于网络异常失败了，导致应用进程启动失败，最终导致容器重启。</p>
<p><strong>是什么导致了网络异常呢？</strong>我们再用 <code>Kubectl get pod</code> 命令查询 Pod 的运行状态，尝试找到更多的线索：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod awesome-app-cd1234567-gzgwg  -o yaml</span><br></pre></td></tr></table></figure></p>
<p>命令输出的 pod 详细内容如下，该 yaml 片段省略了其他无关的细节，只显示了 lastState 和 state 部分的容器状态信息。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">containerStatuses:</span></span><br><span class="line"><span class="attr">  - containerID:</span> </span><br><span class="line"><span class="attr">    lastState:</span></span><br><span class="line"><span class="attr">      terminated:</span></span><br><span class="line"><span class="attr">        containerID:</span> </span><br><span class="line"><span class="attr">        exitCode:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">        finishedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">23</span>Z</span><br><span class="line"><span class="attr">        reason:</span> Error</span><br><span class="line"><span class="attr">        startedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">22</span>Z</span><br><span class="line"><span class="attr">    name:</span> awesome-app</span><br><span class="line"><span class="attr">    ready:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    restartCount:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    state:</span></span><br><span class="line"><span class="attr">      running:</span></span><br><span class="line"><span class="attr">        startedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">36</span>Z</span><br><span class="line"><span class="attr">  - containerID:</span> </span><br><span class="line"><span class="attr">    lastState:</span> &#123;&#125;</span><br><span class="line"><span class="attr">    name:</span> istio-proxy</span><br><span class="line"><span class="attr">    ready:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    restartCount:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">    state:</span></span><br><span class="line"><span class="attr">      running:</span></span><br><span class="line"><span class="attr">        startedAt:</span> <span class="number">2020</span><span class="bullet">-09</span><span class="bullet">-01</span>T13:<span class="number">16</span>:<span class="number">20</span>Z</span><br><span class="line"><span class="attr">  hostIP:</span> <span class="number">10.0</span><span class="number">.6</span><span class="number">.161</span></span><br></pre></td></tr></table></figure></p>
<p>从该输出可以看到 pod 中的应用容器 awesome-app 重启了两次。整理该 pod 中 awesome-app 应用容器和 istio-proxy sidecar 容器的启动和终止的时间顺序，可以得到下面的时间线：</p>
<ol>
<li>2020-09-01T13:16:20Z istio-proxy 启动</li>
<li>2020-09-01T13:16:22Z awesome-app 上一次启动时间</li>
<li>2020-09-01T13:16:23Z awesome-app 上一次异常退出时间</li>
<li>2020-09-01T13:16:36Z awesome-app 最后一次启动，以后就一直正常运行</li>
</ol>
<p>可以看到在 <code>istio-proxy</code> 启动2秒后，awesome-app 启动，并于1秒后异常退出。结合前面的日志信息，我们知道这次启动失败的直接原因是应用访问配置中心失败导致。在 <code>istio-proxy</code> 启动16秒后，awesome-app 再次启动，这次启动成功，之后一直正常运行。</p>
<p><code>istio-proxy</code> 启动和 awesome-app 上一次异常退出的时间间隔很短，只有2秒钟，因此我们基本可以判断此时 <code>istio-proxy</code> 尚未启动初始化完成，导致 awesome-app 不能通过<code>istio-proxy</code> 连接到外部服务，导致其启动失败。待 awesome-app 于 2020-09-01T13:16:36Z 再次启动时，由于 <code>istio-proxy</code> 已经启动了较长时间，完成了从 pilot 获取动态配置的过程，因此 awesome-app 向 pod 外部的网络访问就正常了。</p>
<p>如下图所示，Envoy 启动后会通过 xDS 协议向 pilot 请求服务和路由配置信息，Pilot 收到请求后会根据 Envoy 所在的节点（pod或者VM）组装配置信息，包括 <code>Listener</code>、<code>Route</code>、<code>Cluster</code> 等，然后再通过 xDS 协议下发给 Envoy。根据 Mesh 的规模和网络情况，该配置下发过程需要数秒到数十秒的时间。由于初始化容器已经在 pod 中创建了 Iptables rule 规则，因此这段时间内应用向外发送的网络流量会被重定向到 Envoy ，而此时 Envoy 中尚没有对这些网络请求进行处理的监听器和路由规则，无法对此进行处理，导致网络请求失败。（关于 <code>Envoy sidecar</code> 初始化过程和 <code>Istio</code> 流量管理原理的更多内容，可以参考 <a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/" target="_blank" rel="external">Istio流量管理实现机制深度解析-基于1.4.0更新</a>）</p>
<p><img src="/images/istio/istio-1.png" alt="Envory sidecar初始化"></p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="在应用启动命令中判断-Envoy-初始化状态"><a href="#在应用启动命令中判断-Envoy-初始化状态" class="headerlink" title="在应用启动命令中判断 Envoy 初始化状态"></a>在应用启动命令中判断 Envoy 初始化状态</h3><p>从前面的分析可以得知，该问题的根本原因是由于应用进程对 <code>Envoy sidecar</code> 配置初始化的依赖导致的。因此最直接的解决思路就是：在应用进程启动时判断 <code>Envoy sidecar</code> 的初始化状态，待其初始化完成后再启动应用进程。</p>
<p>Envoy 的健康检查接口 <code>localhost:15020/healthz/ready</code> 会在 xDS 配置初始化完成后才返回 200，否则将返回 503，因此可以根据该接口判断 Envoy 的配置初始化状态，待其完成后再启动应用容器。我们可以在应用容器的启动命令中加入调用 Envoy 健康检查的脚本，如下面的配置片段所示。在其他应用中使用时，将 <code>start-awesome-app-cmd</code> 改为容器中的应用启动命令即可。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> awesome-app-deployment</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> awesome-app</span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> awesome-app</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> awesome-app</span><br><span class="line"><span class="attr">        image:</span> awesome-app</span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        command:</span> [<span class="string">"/bin/bash"</span>, <span class="string">"-c"</span>]</span><br><span class="line"><span class="attr">        args:</span> [<span class="string">"while [[ \"$(curl -s -o /dev/null -w ''<span class="template-variable">%&#123;http_code&#125;</span>'' localhost:15020/healthz/ready)\" != '200' ]]; do echo Waiting for Sidecar;sleep 1; done; echo Sidecar available; start-awesome-app-cmd"</span>]</span><br></pre></td></tr></table></figure></p>
<p>该流程的执行顺序如下：</p>
<ol>
<li><code>Kubernetes</code> 启动 应用容器。</li>
<li>应用容器启动脚本中通过 <code>curl get localhost:15020/healthz/ready</code> 查询 <code>Envoy sidcar</code> 状态，由于此时 <code>Envoy sidecar</code> 尚未就绪，因此该脚本会不断重试。</li>
<li><code>Kubernetes</code> 启动 <code>Envoy sidecar</code>。</li>
<li><code>Envoy sidecar</code> 通过 xDS 连接 Pilot，进行配置初始化。</li>
<li>应用容器启动脚本通过 <code>Envoy sidecar</code> 的健康检查接口判断其初始化已经完成，启动应用进程。</li>
</ol>
<p>该方案虽然可以规避依赖顺序的问题，但需要对应用容器的启动脚本进行修改，对 Envoy 的健康状态进行判断。更理想的方案应该是应用对 <code>Envoy sidecar</code> 不感知。</p>
<h3 id="通过-pod-容器启动顺序进行控制"><a href="#通过-pod-容器启动顺序进行控制" class="headerlink" title="通过 pod 容器启动顺序进行控制"></a>通过 pod 容器启动顺序进行控制</h3><p>通过阅读 <a href="https://github.com/kubernetes/kubernetes/blob/537a602195efdc04cdf2cb0368792afad082d9fd/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L827-L830" target="_blank" rel="external">Kubernetes源码</a> ，我们可以发现当 pod 中有多个容器时，<code>Kubernetes</code> 会在一个线程中依次启动这些容器，如下面的代码片段所示：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Step 7: start containers in podContainerChanges.ContainersToStart.</span></span><br><span class="line"><span class="keyword">for</span> _, idx := <span class="keyword">range</span> podContainerChanges.ContainersToStart &#123;</span><br><span class="line">  start(<span class="string">"container"</span>, containerStartSpec(&amp;pod.Spec.Containers[idx]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>因此我们可以在向 pod 中注入 <code>Envoy sidecar</code> 时将 <code>Envoy sidecar</code> 放到应用容器之前，这样 <code>Kubernetes</code> 会先启动 <code>Envoy sidecar</code>，再启动应用容器。但是还有一个问题，Envoy 启动后我们并不能立即启动应用容器，还需要等待 xDS 配置初始化完成。这时我们就可以采用容器的 <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/" target="_blank" rel="external">postStart lifecycle hook</a> 来达成该目的。<code>Kubernetes</code> 会在启动容器后调用该容器的 <code>postStart hook</code>，<code>postStart hook</code> 会阻塞 pod 中的下一个容器的启动，直到 <code>postStart hook</code> 执行完成。因此如果在 <code>Envoy sidecar</code> 的 <code>postStart hook</code> 中对 Envoy 的配置初始化状态进行判断，待完成初始化后再返回，就可以保证 <code>Kubernetes</code> 在 <code>Envoy sidecar</code> 配置初始化完成后再启动应用容器。该流程的执行顺序如下：</p>
<ol>
<li><code>Kubernetes</code> 启动 <code>Envoy sidecar</code>。</li>
<li><code>Kubernetes</code> 执行 <code>postStart hook</code>。</li>
<li><code>postStart hook</code> 通过 Envoy 健康检查接口判断其配置初始化状态，直到 Envoy 启动完成 。</li>
<li><code>Kubernetes</code> 启动应用容器。</li>
</ol>
<p><code>Istio</code> 已经在 1.7 中合入了该修复方案，参见 <a href="https://github.com/istio/istio/pull/24737" target="_blank" rel="external">Allow users to delay application start until proxy is ready</a>。</p>
<p>插入 <code>sidecar</code> 后的 pod spec 如下面的 yaml 片段所示。<code>postStart hook</code> 配置的 <code>pilot-agent wait</code> 命令会持续调用 <code>Envoy</code> 的健康检查接口 ‘/healthz/ready’ 检查其状态，直到 Envoy 完成配置初始化。这篇文章 <a href="https://medium.com/@marko.luksa/delaying-application-start-until-sidecar-is-ready-2ec2d21a7b74" target="_blank" rel="external">Delaying application start until sidecar is ready</a> 中介绍了更多关于该方案的细节。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sidecar-starts-first</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> istio-proxy</span><br><span class="line"><span class="attr">    image:</span> </span><br><span class="line"><span class="attr">    lifecycle:</span></span><br><span class="line"><span class="attr">      postStart:</span></span><br><span class="line"><span class="attr">        exec:</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">          -</span> pilot-agent</span><br><span class="line"><span class="bullet">          -</span> wait</span><br><span class="line"><span class="attr">  - name:</span> application</span><br><span class="line"><span class="attr">    image:</span> my-application</span><br></pre></td></tr></table></figure></p>
<p>该方案在不对应用进行修改的情况下比较完美地解决了应用容器和 <code>Envoy sidecar</code> 初始化的依赖问题。但是该解决方案对 <code>Kubernetes</code> 有两个隐式依赖条件：<code>Kubernetes</code> 在一个线程中按定义顺序依次启动 pod 中的多个容器，以及前一个容器的 <code>postStart hook</code> 执行完毕后再启动下一个容器。这两个前提条件在目前的 <code>Kuberenetes</code> 代码实现中是满足的，但由于这并不是 <code>Kubernetes</code> 的 API 规范，因此该前提在将来 <code>Kubernetes</code> 升级后很可能被打破，导致该问题再次出现。</p>
<h3 id="Kubernetes-支持定义-pod-中容器之间的依赖关系"><a href="#Kubernetes-支持定义-pod-中容器之间的依赖关系" class="headerlink" title="Kubernetes 支持定义 pod 中容器之间的依赖关系"></a>Kubernetes 支持定义 pod 中容器之间的依赖关系</h3><p>为了彻底解决该问题，避免 <code>Kubernetes</code> 代码变动后该问题再次出现，更合理的方式应该是由 <code>Kubernetes</code> 支持显式定义 pod 中一个容器的启动依赖于另一个容器的健康状态。目前 <code>Kubernetes</code> 中已经有一个 <a href="https://github.com/kubernetes/kubernetes/issues/65502" target="_blank" rel="external">issue Support startup dependencies between containers on the same Pod</a> 对该问题进行跟踪处理。如果 <code>Kubernetes</code> 支持了该特性，则该流程的执行顺序如下：</p>
<ol>
<li><code>Kubernetes</code> 启动 <code>Envoy sidecar</code> 容器。</li>
<li><code>Kubernetes</code> 通过 <code>Envoy sidecar</code> 容器的 readiness probe 检查其状态，直到 readiness probe 反馈 <code>Envoy sidecar</code> 已经 ready，即已经初始化完毕。</li>
<li><code>Kubernetes</code> 启动应用容器。</li>
</ol>
<h3 id="解耦应用服务之间的启动依赖关系"><a href="#解耦应用服务之间的启动依赖关系" class="headerlink" title="解耦应用服务之间的启动依赖关系"></a>解耦应用服务之间的启动依赖关系</h3><p>以上几个解决方案的思路都是控制 pod 中容器的启动顺序，在 <code>Envoy sidecar</code> 初始化完成后再启动应用容器，以确保应用容器启动时能够通过网络正常访问其他服务。但这些方案只是『头痛医头，脚痛医脚』,是治标不治本的方法。因为即使 pod 中对外的网络访问没有问题，应用容器依赖的其他服务也可能由于尚未启动，或者某些问题而不能在此时正常提供服务。要彻底解决该问题，我们需要解耦应用服务之间的启动依赖关系，使应用容器的启动不再强依赖其他服务。</p>
<p>在一个微服务系统中，原单体应用中的各个业务模块被拆分为多个独立进程（服务）。这些服务的启动顺序是随机的，并且服务之间通过不可靠的网络进行通信。微服务多进程部署、跨进程网络通信的特定决定了服务之间的调用出现异常是一个常见的情况。为了应对微服务的该特点，微服务的一个基本的设计原则是 <strong>“design for failure”</strong>，即需要以优雅的方式应对可能出现的各种异常情况。当在微服务进程中不能访问一个依赖的外部服务时，需要通过重试、降级、超时、断路等策略对异常进行容错处理，以尽可能保证系统的正常运行。</p>
<p><code>Envoy sidecar</code> 初始化期间网络暂时不能访问的情况只是放大了微服务系统未能正确处理服务依赖的问题，即使解决了 <code>Envoy sidecar</code> 的依赖顺序，该问题依然存在。例如在本案例中，配置中心也是一个独立的微服务，当一个依赖配置中心的微服务启动时，配置中心有可能尚未启动，或者尚未初始化完成。在这种情况下，如果在代码中没有对该异常情况进行处理，也会导致依赖配置中心的微服务启动失败。在一个更为复杂的系统中，多个微服务进程之间可能存在网状依赖关系，如果没有按照 <strong>“design for failure”</strong> 的原则对微服务进行容错处理，那么只是将整个系统启动起来就将是一个巨大的挑战。对于本例而言，可以采用一个类似这样的简单容错策略：先用一个缺省的 logback 配置启动应用进程，并在启动后对配置中心进行重试，待连接上配置中心后，再使用配置中心下发的配置对 logback 进行设置。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>应用容器对 <code>Envoy Sidecar</code> 启动依赖问题的典型表现是应用容器在刚启动的一小段时间内调用外部服务失败。原因是此时 <code>Envoy sidecar</code> 尚未完成 xDS 配置的初始化，因此不能为应用容器转发网络请求。该调用失败可能导致应用容器不能正常启动。此问题的根本原因是微服务应用中对依赖服务的调用失败没有进行合理的容错处理。</p>
<p>对于遗留系统，为了尽量避免对应用的影响，我们可以通过在应用启动命令中判断 <code>Envoy</code> 初始化状态的方案，或者升级到 <code>Istio 1.7</code> 来缓解该问题。但为了彻底解决服务依赖导致的错误，建议参考 <strong>“design for failure”</strong> 的设计原则，解耦微服务之间的强依赖关系，在出现暂时不能访问一个依赖的外部服务的情况时，通过重试、降级、超时、断路等策略进行处理，以尽可能保证系统的正常运行。</p>
<p>来源：mp.weixin.qq.com/s/iXU2LH90_ZA3VeN7xORmBw</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes securityContext]]></title>
      <url>http://team.jiunile.com/blog/2020/09/k8s-securitycontext.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>kubernetes 中的 <code>securityContext</code> 是什么？在什么场景下来使用？第一感觉反正是和安全相关的东西，来自官方定义如下：</p>
<p>安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。 安全上下文包括但不限于：</p>
<ul>
<li>自主访问控制（Discretionary Access Control）：基于 用户 ID（UID）和组 ID（GID）. 来判定对对象（例如文件）的访问权限</li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%85%A8%E5%A2%9E%E5%BC%BA%E5%BC%8FLinux" target="_blank" rel="external">安全性增强的 Linux（SELinux）</a>： 为对象赋予安全性标签。</li>
<li>以特权模式或者非特权模式运行。</li>
<li><a href="https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/" target="_blank" rel="external">Linux 权能</a>: 为进程赋予 root 用户的部分特权而非全部特权。</li>
<li><a href="https://kubernetes.io/zh/docs/tutorials/clusters/apparmor/" target="_blank" rel="external">AppArmor</a>：使用程序文件来限制单个程序的权限。</li>
<li><a href="https://en.wikipedia.org/wiki/Seccomp" target="_blank" rel="external">Seccomp</a>：限制一个进程访问文件描述符的权限。</li>
<li>AllowPrivilegeEscalation：控制进程是否可以获得超出其父进程的特权。 此布尔值直接控制是否为容器进程设置 <a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt" target="_blank" rel="external">no_new_privs</a> 标志。 当容器以特权模式运行或者具有 <code>CAP_SYS_ADMIN</code> 权能时，AllowPrivilegeEscalation 总是为 <strong><code>true</code></strong>。</li>
<li>readOnlyRootFilesystem：以只读方式加载容器的根文件系统。</li>
</ul>
<p>以上条目不是安全上下文设置的完整列表 – 请参阅 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#securitycontext-v1-core" target="_blank" rel="external">SecurityContext</a> 了解其完整列表。</p>
<p>关于在 Linux 系统中的安全机制的更多信息，可参阅 <a href="https://www.linux.com/learn/overview-linux-kernel-security-features" target="_blank" rel="external">Linux 内核安全性能力概述</a>。</p>
<p>上面的定义有些似懂非懂，能不能更加直白的描述下呢？好吧，下面来给大家来一些实际的应用场景来讲解下。</p>
<a id="more"></a>
<h2 id="Security-Context-应用场景"><a href="#Security-Context-应用场景" class="headerlink" title="Security Context 应用场景"></a>Security Context 应用场景</h2><h3 id="场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户"><a href="#场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户" class="headerlink" title="场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户"></a>场景一：我有个镜像，已非root用户运行，同时我需要挂载一块磁盘，需要将对应权限改成当前运行的用户</h3><p>有了上面的场景，那我们如何来进行设定呢？假设运行用户对应的 uid为999 gid为999<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> StatefulSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sc-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      runAsUser:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">      runAsGroup:</span> <span class="number">999</span></span><br><span class="line"><span class="attr">      fsGroup:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">    containers:</span></span><br><span class="line"><span class="attr">    - name:</span> sc-demo</span><br><span class="line"><span class="attr">      image:</span> xxxxxx</span><br><span class="line"><span class="attr">      command:</span> [ <span class="string">"sh"</span>, <span class="string">"-c"</span>, <span class="string">"sleep 1h"</span> ]</span><br><span class="line"><span class="attr">      volumeMounts:</span></span><br><span class="line"><span class="attr">      - name:</span> sc-vol</span><br><span class="line"><span class="attr">        mountPath:</span> /data/demo</span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        runAsUser:</span> <span class="number">999</span></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">    - metadata:</span></span><br><span class="line"><span class="attr">        name:</span> sc-vol</span><br><span class="line"><span class="attr">      spec:</span></span><br><span class="line"><span class="attr">        accessModes:</span></span><br><span class="line"><span class="bullet">          -</span> ReadWriteOnce</span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            storage:</span> <span class="number">30</span>Gi</span><br><span class="line"><span class="attr">        storageClassName:</span> gp2</span><br></pre></td></tr></table></figure></p>
<p>这里有两个 <code>securityContext</code> 设定，一个是针对 pod 级别的，另外一个则是针对 container 级别的，那两者的优先级如何定义呢？规则就是：<strong>container中的会覆写pod中的定义</strong>。了解了优先级后，来讲解下 runAsUser、runAsGroup、fsGroup 这三个参数的意义。</p>
<ul>
<li><code>runAsUser</code> 字段指定 Pod 中的所有容器内的进程都使用 用户ID 1000 来运行。<code>但这里 sc-demo 容器进行了覆写，如果 sc-demo 容器内的进行使用用户 ID 为999来运行</code>。</li>
<li><code>runAsGroup</code> 字段指定所有容器中的进程都以主 组ID 999 来运行。 如果忽略此字段，则容器的 主组ID 将是 root（0）。 当 <code>runAsGroup</code> 被设置时，所有创建的文件也会划归为 用户1000 和 组999。</li>
<li><code>fsGroup</code> 由于 <code>fsGroup</code> 被设置，容器中所有进程也会是附 组ID 0 (root) 的一部分。 卷 <code>/data/demo</code> 及在该卷中创建的任何文件的属主都会是 组ID 0 (root)。</li>
</ul>
<p>进入容器中查看<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ id</span><br><span class="line">uid=999(xx) gid=999(xx) groups=999(xx),0(root)</span><br><span class="line"></span><br><span class="line">$ ls <span class="_">-l</span> /data/demo</span><br><span class="line">drwxrwsr-x 3 xx root  4096 Sep  8 17:51 <span class="built_in">test</span></span><br></pre></td></tr></table></figure></p>
<p><strong>为 Pod 配置卷访问权限和属主变更策略</strong></p>
<blockquote>
<p>FEATURE STATE: Kubernetes v1.18 [alpha]</p>
</blockquote>
<p>默认情况下，Kubernetes 在挂载一个卷时，会递归地更改每个卷中的内容的属主和访问权限，使之与 Pod 的 <code>securityContext</code> 中指定的 <code>fsGroup</code> 匹配。 对于较大的数据卷，检查和变更属主与访问权限可能会花费很长时间，降低 Pod 启动速度。 你可以在 <code>securityContext</code> 中使用 <code>fsGroupChangePolicy</code> 字段来控制 Kubernetes 检查和管理卷属主和访问权限的方式。</p>
<p><strong>fsGroupChangePolicy</strong> - <code>fsGroupChangePolicy</code> 定义在卷被暴露给 Pod 内部之前对其 内容的属主和访问许可进行变更的行为。此字段仅适用于那些支持使用 <code>fsGroup</code> 来 控制属主与访问权限的卷类型。此字段的取值可以是：</p>
<ul>
<li><code>OnRootMismatch</code>：只有根目录的属主与访问权限与卷所期望的权限不一致时，才改变其中内容的属主和访问权限。这一设置有助于缩短更改卷的属主与访问权限所需要的时间。</li>
<li><code>Always</code>：在挂载卷时总是更改卷中内容的属主和访问权限。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">  runAsUser:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">  runAsGroup:</span> <span class="number">3000</span></span><br><span class="line"><span class="attr">  fsGroup:</span> <span class="number">2000</span></span><br><span class="line"><span class="attr">  fsGroupChangePolicy:</span> <span class="string">"OnRootMismatch"</span></span><br></pre></td></tr></table></figure>
<p>这是一个 Alpha 阶段的功能特性。要使用此特性，需要在 kube-apiserver、kube-controller-manager 和 kubelet 上启用 ConfigurableFSGroupPolicy <a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/" target="_blank" rel="external">特性门控</a>。</p>
<h3 id="场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。"><a href="#场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。" class="headerlink" title="场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。"></a>场景二：我需要对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> StatefulSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sc-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    containers:</span></span><br><span class="line"><span class="attr">    - name:</span> sc-demo</span><br><span class="line"><span class="attr">      image:</span> xxxxxx</span><br><span class="line"><span class="attr">      command:</span> [ <span class="string">"sh"</span>, <span class="string">"-c"</span>, <span class="string">"sleep 1h"</span> ]</span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        privileged:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>给予 <code>privileged: true</code> 是一个比较粗的权限，一般不建议如此，可以为权限定义更细粒度的权限，类似需要在容器中使用 <code>perf</code> 命令，则可以进行如下定义：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">    capabilities:</span></span><br><span class="line"><span class="attr">        add:</span> [<span class="string">"SYS_ADMIN"</span>]</span><br></pre></td></tr></table></figure></p>
<p>具体 <code>capabilities</code> 的使用规则可参考：<a href="http://team.jiunile.com/blog/2019/12/capabilities.html">在 Kubernetes 中配置 Container Capabilities</a></p>
<h3 id="场景三：我需要对启动的容器赋予SELinux标签"><a href="#场景三：我需要对启动的容器赋予SELinux标签" class="headerlink" title="场景三：我需要对启动的容器赋予SELinux标签"></a>场景三：我需要对启动的容器赋予SELinux标签</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">  seLinuxOptions:</span></span><br><span class="line"><span class="attr">    level:</span> <span class="string">"s0:c123,c456"</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>要指定 SELinux，需要在宿主操作系统中装载 SELinux 安全性模块。<code>seLinuxOptions</code> 字段的取值是一个 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#selinuxoptions-v1-core" target="_blank" rel="external">SELinuxOptions</a> 对象</p>
</blockquote>
<p>参考</p>
<ul>
<li><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/security-context/" target="_blank" rel="external">https://kubernetes.io/zh/docs/tasks/configure-pod-container/security-context/</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes in Kuberntes]]></title>
      <url>http://team.jiunile.com/blog/2020/06/k8s-k8s-in-k8s.html</url>
      <content type="html"><![CDATA[<p><code>KinD</code> 是一个非常轻量级的 Kubernetes 安装工具，他将 Docker 容器当成 Kubernetes 的节点，使用非常方便。既然在 Docker 容器中可以运行 Kubernetes 集群，那么我们自然就会想到是否可以在 Pod 中来运行呢？在 Pod 中运行会遇到哪些问题呢？</p>
<h2 id="在-Pod-中安装-Docker-Daemon"><a href="#在-Pod-中安装-Docker-Daemon" class="headerlink" title="在 Pod 中安装 Docker Daemon"></a>在 Pod 中安装 Docker Daemon</h2><p><code>KIND</code> 当前依赖于 Docker（尽管他们计划很快支持其他容器运行时，例如<code>podman</code>）。因此，第一步是创建一个容器映像，该映像允许您在 Pod 内运行 Docker守护程序，以便使诸如<code>docker run</code>之类的命令在Pod内运行（又名 Docker-in-Docker 或 DIND ）。</p>
<p>Docker-in-Docker 是一个众所周知的问题，并且已经解决了相当一段时间。尽管如此，当尝试在生产 Kubernetes 集群中正确设置 Docker-in-Docker 时，我们仍然遇到很多问题。<br><a id="more"></a></p>
<h3 id="MTU问题"><a href="#MTU问题" class="headerlink" title="MTU问题"></a>MTU问题</h3><p>MTU 问题的性质实际上取决于生产 Kubernetes 集群的网络提供商。我们用于 CI 的Kubernetes 发行版是 <a href="https://d2iq.com/solutions/ksphere/konvoy" target="_blank" rel="external">Konvoy</a>。Konvoy 使用<code>Calico</code>作为其默认网络提供商，并且默认情况下使用<code>IPIP</code>封装。<code>IPIP</code>封装产生20字节的开销。换句话说，如果群集中主机网络的主网络接口的<code>MTU</code>为1500，则Pod中网络接口的<code>MTU</code>将为1480。如果您的生产群集在某些云提供商（例如GCE）上运行，则<code>MTU Pod</code>的最大值甚至更低（1460-20 = 1440）。</p>
<p>重要的是，我们在 Pod 内配置默认 Docker 网络的<code>MTU</code>（<code>dockerd</code> 的 <code>--mtu</code>标志），使其等于或小于 Pod 的网络接口的<code>MTU</code>。否则，您将无法与外界建立连接（例如，从互联网上获取容器图像时）。</p>
<h3 id="PID-1-的问题"><a href="#PID-1-的问题" class="headerlink" title="PID 1 的问题"></a>PID 1 的问题</h3><p>比如我们需要在一个容器中去运行 <code>Docker Daemon</code> 以及一些 Kubernetes 的集群测试，而这些测试依赖于 <code>KinD</code> 和 <code>Docker Damon</code>，在一个容器中运行多个服务我们可能会去使用 <code>systemd</code>，但是使用 <code>systemd</code> 也会有一些问题。</p>
<p>比如我们需要保留测试的退出状态，Kubernetes 中使用的容器运行时可以 watch 到容器中的第一个进程（PID 1）的退出状态。如果我们使用 <code>systemd</code> 的话，那么我们测试的进程退出状态不会被转发到 Kubernetes。</p>
<p>此外获取测试的日志也是非常重要的，在 Kubernetes 中会自动获取写入到 stdout 和 stderr 的容器日志，但是如果使用 <code>systemd</code> 的话，要想获取应用的日志就比较麻烦的。</p>
<p>为了解决上面的问题，我们可以在容器镜像中使用如下所示的启动脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dockerd &amp;</span><br><span class="line"><span class="comment"># Wait until dockerd is ready.</span></span><br><span class="line">until docker ps &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Waiting for dockerd..."</span></span><br><span class="line">  sleep 1</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$@</span>"</span></span><br></pre></td></tr></table></figure></p>
<p>但是需要注意的是我们不能将上面的脚本作为容器的 entrypoint，在镜像中定义的 entrypoint 会在容器中以 PID 1 的形式运行在一个单独的 <code>pid namespace</code> 中。PID 1 是一个内核中的一个特殊进程，它的行为和其他进程不同。</p>
<p>本质上，接收信号的进程是 PID 1：它会被内核做特殊处理；如果它没有为信号注册一个处理器，内核就不会回到默认行为（即杀死进程）。由于当收到 SIGTERM 信号时，内核会默认杀死这个进程，所以一些进程也许不会为 SIGTERM 信号注册信号处理程序。如果出现了这种情况，当 Kubernetes 尝试终止 Pod 时，SIGTERM 将被吞噬，你会注意到 Pod 会被卡在 Terminating 的状态下。</p>
<p>这其实不是一个什么新鲜的问题，但是了解这个问题的人却并不多，而且还一直在构建有这样问题的容器。我们可以使用 tini 这个应用来解决这个问题，将其作为镜像的入口点，如在 <code>Dockerfile</code> 中所示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENTRYPOINT [<span class="string">"/usr/bin/tini"</span>, <span class="string">"--"</span>, <span class="string">"/entrypoint.sh"</span>]</span><br></pre></td></tr></table></figure></p>
<p>这个程序会正确注册信号处理程序和转发信号。它还会执行一些其他 PID 1 的事情，比如回收容器中的僵尸进程。</p>
<h3 id="挂载-cgroups"><a href="#挂载-cgroups" class="headerlink" title="挂载 cgroups"></a>挂载 cgroups</h3><p>由于 <code>Docker Daemon</code> 需要控制 cgroups，所以需要将 cgroup 文件系统挂载到容器中去。但是由于 cgroups 和宿主机是共享的，所以我们需要确保 <code>Docker Daemon</code> 控制的 cgroups 不会影响到其他容器或者宿主机进程使用的其他 cgroups，还需要确保 <code>Docker Daemon</code> 在容器中创建的 cgroups 在容器退出后不会被泄露。</p>
<p><code>Docker Daemon</code> 中有一个 <code>--cgroup—parent</code> 参数来告诉 Daemon 将所有容器的 cgroups 嵌套在指定的 cgroup 下面。当容器运行在 Kubernetes 集群下面时，我们在容器中设置 <code>Docker Daemon</code> 的 <code>--cgroup—parent</code> 参数，这样它的所有 cgroups 就会被嵌套在 Kubernetes 为容器创建的 cgroup 下面了。</p>
<p>在以前为了让 cgroup 文件系统在容器中可用，一些用户会将宿主机中的 <code>/sys/fs/cgroup</code> 挂载到容器中的这个位置，如果这样使用的话，我们就需要在容器启动脚本中把 <code>--cgroup—parent</code> 设置为下面的内容，这样 <code>Docker Daemon</code> 创建的 cgroups 就可以正确被嵌套了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CGROUP_PARENT=<span class="string">"<span class="variable">$(grep systemd /proc/self/cgroup | cut -d: -f3)</span>/docker"</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意：<code>/proc/self/cgroup</code> 显示的是调用进程的 cgroup 路径。</p>
</blockquote>
<p>但是我们要知道，挂载宿主机的 <code>/sys/fs/cgroup</code> 文件是非常危险的事情，因为他把整个宿主机的 cgroup 层次结构都暴露给了容器。以前为了解决这个问题，Docker 用了一个小技巧把不相关的 cgroups 隐藏起来，不让容器看到。Docker 从容器的 cgroups 对每个 cgroup 系统的 cgroup 层次结构的根部进行绑定挂载。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm debian findmnt -lo <span class="built_in">source</span>,target -t cgroup</span><br><span class="line">SOURCE                                                                               TARGET</span><br><span class="line">cpuset[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/cpuset</span><br><span class="line">cpu[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]        /sys/fs/cgroup/cpu</span><br><span class="line">cpuacct[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]    /sys/fs/cgroup/cpuacct</span><br><span class="line">blkio[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/blkio</span><br><span class="line">memory[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/memory</span><br><span class="line"> </span><br><span class="line">cgroup[/docker/451b803b3<span class="built_in">cd</span>7<span class="built_in">cd</span>2b69dde64<span class="built_in">cd</span>833fdd799ae16f9d2d942386ec382f6d55bffac]     /sys/fs/cgroup/systemd</span><br></pre></td></tr></table></figure></p>
<p>从上面我们可以看出 cgroups 通过将宿主机 cgroup 文件系统上的 <code>/sys/fs/cgroup/memory/memory.limit_in_bytes</code> 文件映射到 <code>/sys/fs/cgroup/memory/docker/&lt;CONTAINER_ID&gt;/memory.limit_in_bytes</code> 来控制容器内 cgroup 层次结构根部的文件，这种方式可以防止容器进程意外地修改宿主机的 cgroup。</p>
<p>但是这种方式有时候会让 cadvisor 和 kubelet 这样的应用感动困惑，因为绑定挂载并不会改变 <code>/proc/&lt;PID&gt;/cgroup</code> 里面的内容。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm debian cat /proc/1/cgroup</span><br><span class="line">14:name=systemd:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line"> </span><br><span class="line">5:memory:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">4:blkio:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">3:cpuacct:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">2:cpu:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">1:cpuset:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141</span><br><span class="line">0::/</span><br></pre></td></tr></table></figure></p>
<p>cadvisor 会通过查看 <code>/proc/&lt;PID&gt;/cgroup</code> 来获取给定进程的 cgroup，并尝试从对应的 cgroup 中获取 CPU 或内存统计数据。但是由于 <code>Docker Daemon</code> 进程做了绑定挂载，cadvisor 就无法找到容器进程对应的 cgroup。为了解决这个问题，我们在容器内部又做了一次挂载，从 <code>/sys/fs/cgroup/memory</code> 挂载到 <code>/sys/fs/cgroup/memory/docker/&lt;CONTAINER_ID&gt;/</code>（针对所有的 cgroup 子系统），这个方法可以很好的解决这个问题。</p>
<p>现在新的解决方法是使用 <code>cgroup namespace</code>，如果你运行在一个内核版本 4.6+ 的 Linux 系统下面，runc 和 docker 都加入了 cgroup 命名空间的支持。但是目前 Kubernetes 暂时还不支持 cgroup 命名空间，但是很快会作为 <code>cgroups v2</code> 支持的一部分。</p>
<h3 id="IPtables"><a href="#IPtables" class="headerlink" title="IPtables"></a>IPtables</h3><p>在使用的时候我们发现在线上的 Kubernetes 集群运行时，有时候容器内的 <code>Docker Daemon</code> 启动的嵌套容器无法访问外网，但是在本地开发电脑上却可以很正常的工作，大部分开发者应该都会经常遇到这种情况。</p>
<p>最后发现当出现这个问题的时候，来自嵌套的 Docker 容器的数据包并没有打到 iptables 的 POSTROUTING 链，所以没有做 masqueraded。</p>
<p>这个问题是因为包含 <code>Docker Daemon</code> 的镜像是基于 Debian buster 的，而默认情况下，Debian buster 使用的是 nftables 作为 iptables 的默认后端，然而 Docker 本身还不支持 nftables。要解决这个问题只需要在容器镜像中切换到 iptables 命令即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN update-alternatives --set iptables  /usr/sbin/iptables-legacy || <span class="literal">true</span> &amp;&amp; \</span><br><span class="line">    update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy || <span class="literal">true</span> &amp;&amp; \</span><br><span class="line">    update-alternatives --set arptables /usr/sbin/arptables-legacy || <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>完整的 Dockerfile 文件和启动脚本可以在 <a href="https://github.com/jieyu/docker-images/tree/master/dind" target="_blank" rel="external">GitHub</a> 上面获取，也可以直接使用 <code>jieyu/dind-buster:v0.1.8</code> 这个镜像来测试。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm --privileged jieyu/dind-buster:v0.1.8 docker run alpine wget baidu.com</span><br></pre></td></tr></table></figure></p>
<p>在 Kubernetes 集群下使用如下所示的 Pod 资源清单部署即可：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> dind</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> jieyu/dind-buster:v0<span class="number">.1</span><span class="number">.8</span></span><br><span class="line"><span class="attr">    name:</span> dind</span><br><span class="line"><span class="attr">    stdin:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    tty:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> /bin/bash</span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /var/lib/docker</span><br><span class="line"><span class="attr">      name:</span> varlibdocker</span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> varlibdocker</span><br><span class="line"><span class="attr">    emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="在-Pod-中运行-KinD"><a href="#在-Pod-中运行-KinD" class="headerlink" title="在 Pod 中运行 KinD"></a>在 Pod 中运行 KinD</h2><p>上面我们成功配置了 Docker-in-Docker(DinD)，接下来我们就来在该容器中使用 KinD 启动 Kubernetes 集群。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -ti --rm --privileged jieyu/dind-buster:v0.1.8 /bin/bash</span><br><span class="line">Waiting <span class="keyword">for</span> dockerd...</span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># curl -Lso ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64</span></span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># chmod +x ./kind</span></span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># mv ./kind /usr/bin/</span></span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># kind create cluster</span></span><br><span class="line">Creating cluster <span class="string">"kind"</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.18.2) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line">Set kubectl context to <span class="string">"kind-kind"</span></span><br><span class="line">You can now use your cluster with:</span><br><span class="line">kubectl cluster-info --context kind-kind</span><br><span class="line">Have a nice day! 👋</span><br><span class="line">[root@257b543a91a5 /]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                 STATUS   ROLES    AGE   VERSION</span><br><span class="line">kind-control-plane   Ready    master   11m   v1.18.2</span><br></pre></td></tr></table></figure></p>
<p>由于某些原因可能你用上面的命令下载不了 kind，我们可以想办法提前下载到宿主机上面，然后直接挂载到容器中去也可以，我这里将 kind 和 kubectl 命令都挂载到容器中去，使用下面的命令启动容器即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it --rm --privileged -v /usr/<span class="built_in">local</span>/bin/kind:/usr/bin/kind -v /usr/<span class="built_in">local</span>/bin/kubectl:/usr/bin/kubectl jieyu/dind-buster:v0.1.8 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/k8s/k8s_in_k8s_1.png" alt="k8s-in-k8s"><br>可以看到在容器中可以很好的使用 KinD 来创建 Kubernetes 集群。接下来我们直接在 Kubernetes 中来测试一次：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply <span class="_">-f</span> dind.yaml</span><br><span class="line">$ kubectl <span class="built_in">exec</span> -ti dind /bin/bash</span><br><span class="line">root@dind:/<span class="comment"># curl -Lso ./kind https://kind.sigs.k8s.io/dl/v0.7.0/kind-$(uname)-amd64</span></span><br><span class="line">root@dind:/<span class="comment"># chmod +x ./kind</span></span><br><span class="line">root@dind:/<span class="comment"># mv ./kind /usr/bin/</span></span><br><span class="line">root@dind:/<span class="comment"># kind create cluster</span></span><br><span class="line">Creating cluster <span class="string">"kind"</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.17.0) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✗ Starting control-plane 🕹️</span><br><span class="line">ERROR: failed to create cluster: failed to init node with kubeadm: <span class="built_in">command</span> <span class="string">"docker exec --privileged kind-control-plane kubeadm init --ignore-preflight-errors=all --config=/kind/kubeadm.conf --skip-token-print --v=6"</span> failed with error: <span class="built_in">exit</span> status 137</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到在 Pod 中使用 KinD 来创建集群失败了，这是因为在 KinD 节点嵌套容器内运行的 kubelet 会随机杀死顶层容器内的进程，这其实还是和上面讨论的 cgroups 的挂载有关。</p>
<p>但其实我自己在使用 v0.8.1 版本的 KinD 的时候，在上面的 Pod 中是可以正常创建集群的，不知道是否是 KinD 搭建的集群有什么特殊处理，这里需要再深入研究：<br><img src="/images/k8s/k8s_in_k8s_2.png" alt="k8s-in-k8s"></p>
<p>如果你在使用的过程中也遇到了上述的问题，则可以继续往下看解决方案。</p>
<p>当顶层容器（DIND）在 Kubernetes  Pod 中运行的时候，对于每个 cgroup 子系统（比如内存），从宿主机的角度来看，它的 cgroup 路径是 <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;</code>。</p>
<p>当 KinD 在 DIND 容器内的嵌套节点容器内启动 kubelet 的时候，kubelet 将在 <code>/kubepods/burstable/</code> 下相对于嵌套 KIND 节点容器的根 cgroup 为其 Pods 来操作 cgroup。从宿主机的角度来看，cgroup 路径就是 <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;/docker/&lt;KIND_CID&gt;/kubepods/burstable/</code>。</p>
<p>这些都是正确的，但是在嵌套的 KinD 节点容器中，有另一个 cgroup 存在于 <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;/docker/&lt;DIND_CID&gt;</code> 下面，相对于嵌套的 KinD 节点容器的根 cgroup，在 kubelet 启动之前就存在了，这是上面我们讨论过的 cgroups 挂载造成的，通过 KinD entrypoint 脚本设置。而如果你在 KinD 节点容器里面做一个 <code>cat /kubepods/burstable/&lt;POD_ID&gt;/docker/&lt;DIND_CID&gt;/tasks</code>，你会看到 DinD 容器的进程。<br><img src="/images/k8s/k8s_in_k8s_3.png" alt="k8s-in-k8s"></p>
<p>这就是最根本的原因，KinD 节点容器里面的 kubelet 看到了这个 cgroup，以为应该由它来管理，但是却找不到和这个 cgroup 相关联的 Pod，所以就会尝试来杀死属于这个 cgroup 的进程来删除这个 cgroup。这个操作的结果就是随机进程被杀死。解决这个问题的方法可以通过设置 kubelet 的 <code>--cgroup-root</code> 参数，通过该标志来指示 KinD 节点容器内的 kubelet 为其 Pods 使用不同的 cgroup 根路径（比如 /kubelet）。这样就可以在 Kubernetes 集群中来启动 KinD 集群了，我们可以通过下面的 YAML 资源清单文件来修复这个问题。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kind-cluster</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> jieyu/kind-cluster-buster:v0<span class="number">.1</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    name:</span> kind-cluster</span><br><span class="line"><span class="attr">    stdin:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    tty:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> /bin/bash</span><br><span class="line"><span class="attr">    env:</span></span><br><span class="line"><span class="attr">    - name:</span> API_SERVER_ADDRESS</span><br><span class="line"><span class="attr">      valueFrom:</span></span><br><span class="line"><span class="attr">        fieldRef:</span></span><br><span class="line"><span class="attr">          fieldPath:</span> status.podIP</span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /var/lib/docker</span><br><span class="line"><span class="attr">      name:</span> varlibdocker</span><br><span class="line"><span class="attr">    - mountPath:</span> /lib/modules</span><br><span class="line"><span class="attr">      name:</span> libmodules</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">    - containerPort:</span> <span class="number">30001</span></span><br><span class="line"><span class="attr">      name:</span> api-server-port</span><br><span class="line"><span class="attr">      protocol:</span> TCP</span><br><span class="line"><span class="attr">    readinessProbe:</span></span><br><span class="line"><span class="attr">      failureThreshold:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">      httpGet:</span></span><br><span class="line"><span class="attr">        path:</span> /healthz</span><br><span class="line"><span class="attr">        port:</span> api-server-port</span><br><span class="line"><span class="attr">        scheme:</span> HTTPS</span><br><span class="line"><span class="attr">      initialDelaySeconds:</span> <span class="number">120</span></span><br><span class="line"><span class="attr">      periodSeconds:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">      successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> varlibdocker</span><br><span class="line"><span class="attr">    emptyDir:</span> &#123;&#125;</span><br><span class="line"><span class="attr">  - name:</span> libmodules</span><br><span class="line"><span class="attr">    hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /lib/modules</span><br></pre></td></tr></table></figure></p>
<p>使用上面的资源清单文件创建完成后，稍等一会儿我们就可以进入 Pod 中来验证。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> -ti kind-cluster /bin/bash</span><br><span class="line">root@kind-cluster:/<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                 STATUS   ROLES    AGE   VERSION</span><br><span class="line">kind-control-plane   Ready    master   72s   v1.17.0</span><br></pre></td></tr></table></figure></p>
<p>同样也可以直接使用 Docker CLI 来进行测试：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -ti --rm --privileged jieyu/kind-cluster-buster:v0.1.0 /bin/bash</span><br><span class="line">Waiting <span class="keyword">for</span> dockerd...</span><br><span class="line">Setting up KIND cluster</span><br><span class="line">Creating cluster <span class="string">"kind"</span> ...</span><br><span class="line"> ✓ Ensuring node image (jieyu/kind-node:v1.17.0) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line"> ✓ Waiting ≤ 15m0s <span class="keyword">for</span> control-plane = Ready ⏳</span><br><span class="line"> • Ready after 31s 💚</span><br><span class="line">Set kubectl context to <span class="string">"kind-kind"</span></span><br><span class="line">You can now use your cluster with:</span><br><span class="line">kubectl cluster-info --context kind-kind</span><br><span class="line">Have a nice day! 👋</span><br><span class="line">root@d95fa1302557:/<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                 STATUS   ROLES    AGE   VERSION</span><br><span class="line">kind-control-plane   Ready    master   71s   v1.17.0</span><br><span class="line">root@d95fa1302557:/<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>上面镜像对应的 Dockerfile 和启动脚本地址：<a href="https://github.com/jieyu/docker-images/tree/master/kind-cluster" target="_blank" rel="external">https://github.com/jieyu/docker-images/tree/master/kind-cluster</a></p>
</blockquote>
<p>下图是我在 KinD 搭建的 Kubernetes 集群中，创建的一个 Pod，然后在 Pod 中创建的一个独立的 Kubernetes 集群最终效果：<br><img src="/images/k8s/k8s_in_k8s_4.png" alt="k8s-in-k8s"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在实现上面功能的时候，过程中还是遇到了不少的障碍，其中大部分都是因为 Docker 容器没有提供和宿主机完全隔离的功能造成的，某些内核资源比如 cgroups 是在内核中共享的，如果很多容器同时操作它们，也可能会造成潜在的冲突。但是一旦解决了这些问题，我们就可以非常方便的在 Kubernetes 集群 Pod 中轻松地运行一个独立的 Kubernetes 集群了，这应该算真正的 Kubernetes IN Kubernetes 了吧~</p>
<p>参考：<a href="https://d2iq.com/blog/running-kind-inside-a-kubernetes-cluster-for-continuous-integration" target="_blank" rel="external">https://d2iq.com/blog/running-kind-inside-a-kubernetes-cluster-for-continuous-integration</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[多集群kubernetes dashboard 通过ldap统一登录与授权]]></title>
      <url>http://team.jiunile.com/blog/2020/06/k8s-mutiboard-ldap.html</url>
      <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/icyxp/kubernetes-dashboard-ldap/master/assets/images/logo.png" alt="logo.png"></p>
<h1 id="工具由来"><a href="#工具由来" class="headerlink" title="工具由来"></a>工具由来</h1><p>为什么要写这样的一个工具呢？这是因为我司有多个 <code>kubernetes</code> 集群(8+)，且都是云托管服务无法接触到Apiserver配置，这就给我们带来一个痛点，<strong>开发、sre需要登录k8s dashbaord且不同部门和角色间需要不同的授权</strong>，原先都是通过 <code>sa token</code> 进行登录dashboard，但随着k8s集群的增长，每增加一个集群，就需要告知使用方对应dashboard访问地址以及对应的token，这不管是提供方还是使用方都让人感觉非常的痛苦。那是否有一款工具能<strong>提供统一地址统一登录多集群dashboard的方案</strong>呢？经过一番搜索后，发现并没有，市面上大多数是单集群集成 <code>LDAP</code> 的方案，主要是以 <code>DEX</code> 为主，但光单集群的统一登录授权方案就让人感觉非常的困难。难道就没有简单方便的工具供我们使用吗？好吧，那我就来打造这样一款工具吧。</p>
<p>Dashboard LDAP集成方案：</p>
<ul>
<li><a href="https://k2r2bai.com/2019/09/29/ironman2020/day14/" target="_blank" rel="external">https://k2r2bai.com/2019/09/29/ironman2020/day14/</a></li>
<li><a href="https://blog.inkubate.io/access-your-kubernetes-cluster-with-your-active-directory-credentials" target="_blank" rel="external">https://blog.inkubate.io/access-your-kubernetes-cluster-with-your-active-directory-credentials</a></li>
</ul>
<p>以上两篇文档是成LDAP的方案，个人感觉还不错，供有需要的人参考！<br><a id="more"></a></p>
<h1 id="如何打造"><a href="#如何打造" class="headerlink" title="如何打造"></a>如何打造</h1><p>好吧既然没有，那就自动动手打造一个！</p>
<blockquote>
<p>目标： <strong>简单使用</strong>！！！通过访问同一地址，使用LDAP登录且可切换不同集群的dashboard，同时对应不同的集群权限可单独配置！</p>
</blockquote>
<p>有了上面的目标，那如何来实现呢？</p>
<p>实现方式其实很简单，首先写一个登录界面与公司的AD进行打通获取用户与组，然后将用户或者组与k8s集群中的 <code>service account</code> 进行关联就实现了对应的rbac与登录token，最后在登录后实现一个反向代理服务即可完成。</p>
<p>是不是非常的简单！！！</p>
<p>实现技术栈：golang(gin、client-go、viper、ldap) + Kubernetes Dashboard</p>
<h1 id="如何部署"><a href="#如何部署" class="headerlink" title="如何部署"></a>如何部署</h1><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>在使用此工具前，需要有以下一些条件约束：</p>
<ol>
<li>已在各k8s集群部署 <code>dashboard</code> 且能被此工具访问到</li>
<li>已有 <code>ldap</code> 且有管理权限能进行访问操作</li>
<li>各集群中有对应的 <code>service account</code> 可进行映射，如需对不同用户和组需要有不同的操作权限，则对sa进行rbac授权即可，下面会详细说明。</li>
<li>此工具需要操作各集群的api，故需要获取每个集群的 <code>apiserver地址</code>、<code>ca.crt</code> 以及 <code>token</code> 进行配置，至于每个集群的 <code>ca.crt</code> 和 <code>token</code> 如果获取，后面会进行说明</li>
</ol>
<h2 id="如何获取-ca-crt-及-token"><a href="#如何获取-ca-crt-及-token" class="headerlink" title="如何获取 ca.crt 及 token"></a>如何获取 ca.crt 及 token</h2><p>此工具需要操作每个集群的api来获取对应的 sa 以及 token，故需要有对各集群操作的权限。那如何在各集群生成对应的 ca证书 及 token 呢？答案就是创建一个 sa 并给予一定的权限。</p>
<p>在每个k8s集群中执行如下yaml文件:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap-crb</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap-view</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> mutiboard-ldap-view</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> serviceaccounts</span><br><span class="line"><span class="bullet">  -</span> secrets</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> get</span><br><span class="line"><span class="bullet">  -</span> list</span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> namespaces</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> get</span><br><span class="line"><span class="bullet">  -</span> list</span><br><span class="line"><span class="bullet">  -</span> watch</span><br></pre></td></tr></table></figure></p>
<p>此yaml文件的含义：创建一个名为<code>mutiboard-ldap</code>的 sa，并且给予<code>serviceaccounts</code>和<code>secrets</code>的get和list的权限。</p>
<p>获取 <code>mutiboard-ldap</code> 的 ca.crt：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(⎈|aws-local:default)❯ <span class="built_in">echo</span> $(kubectl get secret $(kubectl get secret | grep mutiboard-ldap | awk <span class="string">'&#123;print $1&#125;'</span>) -o go-template=<span class="string">'&#123;&#123;index .data "ca.crt"&#125;&#125;'</span>) | base64 <span class="_">-d</span></span><br><span class="line"></span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl</span><br><span class="line">cm5ldGVzMB4XDTE5MDEyNTEwMTgzNFoXDTI5MDEyMjEwMTgzNFowFTETMBEGA1UE</span><br><span class="line">AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMmc</span><br><span class="line">TW0stLLP+M6Pc9wpRgZufg6eQ7puBfbYgik20QlO4LFtocgNUDa0y+aSXjxheA2C</span><br><span class="line">A+o9wW0IC3GHQHKgeFY8KXIJu6wM0TO+JNQy5XZAWfbsLeXU/sLhKuWET/KJzVWT</span><br><span class="line">0uBE+GCADAAQIec1oQXMbQ551hU5gBFcr67NXHpa2qwEGA1mGtZ7ztmW4+IFUD74</span><br><span class="line">G166z4AOgmR4YWxBs/+8NhfWudFD32xevBfSKuHRxRGG5dtffY8QnRbnrmy70HE5</span><br><span class="line">yzLtBvAGfCwtHLTP2ngCAnn2Fb6IeMdIYGpI1544ZjRbzT1YIWsG1v3dlu6tvK1q</span><br><span class="line">X5Pj+UTDmJuf2SW52A0CAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB</span><br><span class="line">/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKE2hV0DIG8fSf4/eOi5R2sPRfBW</span><br><span class="line">qTwgZDDT9dxZNhbxEInALdruwRUbKRpwaUBOGVpIlaK3/rZkAfjUwoDJ+J4fmmCX</span><br><span class="line">w3ySrYFjx6tqVFqCPjDkBHh4xpMwUlvsvryRuCEQUQgjqBvj6sWm9GERF2n3VYBF</span><br><span class="line">S8bjsQQAZJoE4W+OKchlEoSFlKhxAoeZx9CD3Rxnhj2og6<span class="keyword">do</span>VoGCUqAMh4WZWX+w</span><br><span class="line">pENnui6M96SysH3SkrA02RXWTGeKzK4E6Av3IG+2a2hauHorbqVfaM6HeL3hkU/B</span><br><span class="line">JCWpOgN3T4Fw7E359CBQxnSHPasmZ5VBoyIk/HUU6ZlMK6Xo6JlbS7ZvVl4=</span><br><span class="line">-----END CERTIFICATE-----</span><br></pre></td></tr></table></figure></p>
<p>获取 <code>mutiboard-ldap</code> 的 token：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(⎈|aws-local:default)❯ <span class="built_in">echo</span> $(kubectl get secret $(kubectl get secret | grep mutiboard-ldap | awk <span class="string">'&#123;print $1&#125;'</span>) -o go-template=<span class="string">'&#123;&#123;.data.token&#125;&#125;'</span>) | base64 <span class="_">-d</span></span><br><span class="line"></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im11dGlib2FyZC1sZGFwLXRva2VuLWJ3NWdmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im11dGlib2FyZC1sZGFwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMjVmZjI4MGQtYWJhMi0xMWVhLWFlOGEtMDIzOTBjMzcyNzhlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6bXV0aWJvYXJkLWxkYXA<span class="keyword">if</span>Q.q14hqEu2p70_YczDviR6c8McDM5vfnKPzjO9usCsC-uQUxciBbuJU_PK9j3uawppUNlrs3rAPrZIGUS7Jv14rifEXpGxIIfGR6n8-le0b-9YvMZCgs9-jhf-1r01EAnZFh6gcXfxESFguFQI0vYOsX4P2LQvZ9XTMzsqXbW3KGYao5elAjCE4e8Rg4--9e_zU8NGTEycsvUMxP-9p0SaAzn9Iak3saZtAnzJq5hkSf1t7l2_CgEsYN-3b7uGpHupK_zdgAeOflj9ze4Cz2YScv5eixwVXJ-RcI4lgSFCgt5yzSbnIuHgxRZyN3NcYLrSBYKftezZysWm3jELgLPogQ</span><br></pre></td></tr></table></figure></p>
<p>至此，各集群的 <code>ca.crt</code> 和 <code>token</code> 都已获取，下面会告知如何进行配置使用这些 <code>ca.crt</code> 和 <code>token</code></p>
<h2 id="SA-RBAC列子"><a href="#SA-RBAC列子" class="headerlink" title="SA RBAC列子"></a>SA RBAC列子</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-role</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">  resources:</span> [<span class="string">"namespaces"</span>]</span><br><span class="line"><span class="attr">  verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-listnamespace</span><br><span class="line"><span class="attr">roleRef:</span> <span class="comment">#引用的角色</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> ops-role</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">subjects:</span> <span class="comment">#主体</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> RoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-ci-admin</span><br><span class="line"><span class="attr">  namespace:</span> ops-ci</span><br><span class="line"><span class="attr">roleRef:</span> <span class="comment">#引用的角色</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> admin</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">subjects:</span> <span class="comment">#主体</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> RoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> ops-qa-admin</span><br><span class="line"><span class="attr">  namespace:</span> ops-qa</span><br><span class="line"><span class="attr">roleRef:</span> <span class="comment">#引用的角色</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> admin</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">subjects:</span> <span class="comment">#主体</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> ops-admin</span><br><span class="line"><span class="attr">  namespace:</span> default</span><br></pre></td></tr></table></figure>
<p>这段YAML的意思为：创建了一个ops-admin的sa，并为这个sa赋予了两个命名空间(ops-ci、ops-qa) admin 的权限。</p>
<p>具体想了解更多rbac相关的说明，可参考：<a href="https://www.cnblogs.com/wlbl/p/10694364.html" target="_blank" rel="external">https://www.cnblogs.com/wlbl/p/10694364.html</a></p>
<h2 id="ldap说明"><a href="#ldap说明" class="headerlink" title="ldap说明"></a>ldap说明</h2><p>我司<code>ldap</code>目录规则如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|--域</span><br><span class="line">|--|---公司</span><br><span class="line">|--|----|----分公司</span><br><span class="line">|--|----|-----|----部门</span><br><span class="line">|--|----|-----|-----|-----用户</span><br></pre></td></tr></table></figure></p>
<p>对应的<code>Distinguished Name</code>显示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CN=Peng Xu,OU=部门,OU=分公司,OU=公司,DC=corp,DC=xxx,DC=com</span><br></pre></td></tr></table></figure></p>
<p>这里我会获取第一个<code>OU</code>作为<code>group</code>，如果你的需求和我不一样，可以给我提 issue 进行适配</p>
<p>ldap 详细说明请参考：<a href="https://blog.poychang.net/ldap-introduction" target="_blank" rel="external">https://blog.poychang.net/ldap-introduction</a></p>
<h2 id="configmap-yaml-配置说明"><a href="#configmap-yaml-配置说明" class="headerlink" title="configmap.yaml 配置说明"></a>configmap.yaml 配置说明</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ldap:</span></span><br><span class="line"><span class="attr">  addr:</span> ldap://<span class="number">192.168</span><span class="number">.3</span><span class="number">.81</span>:<span class="number">389</span></span><br><span class="line"><span class="attr">  adminUser:</span> xxxxx</span><br><span class="line"><span class="attr">  adminPwd:</span> xxxxxx</span><br><span class="line"><span class="attr">  baseDN:</span> dc=corp,dc=patsnap,dc=com</span><br><span class="line"><span class="attr">  filter:</span> (&amp;(objectClass=person)(sAMAccountName=%s))</span><br><span class="line"><span class="attr">  attributes:</span> user_dn</span><br><span class="line"><span class="attr">  orgUnitName:</span> OU=</span><br><span class="line"><span class="comment">#全局用户/用户组与SA的映射</span></span><br><span class="line"><span class="attr">rbac:</span></span><br><span class="line">  DevOps team:</span><br><span class="line"><span class="attr">    sa:</span> ops-admin</span><br><span class="line"><span class="attr">    ns:</span> kube-system</span><br><span class="line"><span class="attr">  xupeng:</span></span><br><span class="line"><span class="attr">    sa:</span> inno-admin</span><br><span class="line"><span class="attr">    ns:</span> default</span><br><span class="line"><span class="attr">clusters:</span></span><br><span class="line">  <span class="comment">#集群别名，在登录下拉框中显示的key，这个别名需要和secret.sh中的ca.crt和token的键名一一对应</span></span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line">    <span class="comment">#apiserver地址，能够被当前工具访问到</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-dev.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">6443</span></span><br><span class="line">    <span class="comment">#kubernetes dashboard地址，能够被当前工具访问到</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-dev.jiunile.com</span><br><span class="line">    <span class="comment">#集群说明，在登录下拉框中显示的名称</span></span><br><span class="line"><span class="attr">    desc:</span> Dev Cluster</span><br><span class="line">    <span class="comment">#针对单独集群细分</span></span><br><span class="line">    <span class="comment">#rbac:</span></span><br><span class="line">    <span class="comment">#  DevOps team:</span></span><br><span class="line">    <span class="comment">#    sa: admin</span></span><br><span class="line">    <span class="comment">#    ns: kube-system</span></span><br><span class="line">    <span class="comment">#  xupeng:</span></span><br><span class="line">    <span class="comment">#    sa: ops-admin</span></span><br><span class="line">    <span class="comment">#    ns: default</span></span><br><span class="line"><span class="attr">  cnrelease:</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-cn-release.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-cn-release.jiunile.com</span><br><span class="line"><span class="attr">    desc:</span> CN Release Cluster</span><br><span class="line"><span class="attr">  usrelease:</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-us-release.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-us-release.jiunile.com</span><br><span class="line"><span class="attr">    desc:</span> US Release Cluster</span><br><span class="line"><span class="attr">  euprod:</span></span><br><span class="line"><span class="attr">    apiServer:</span> apiserver-eu-prod.jiunile.com</span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    dashboard:</span> dashboard-eu-prod.jiunile.com</span><br><span class="line"><span class="attr">    desc:</span> EU Prod Cluster</span><br></pre></td></tr></table></figure>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ol>
<li>修改并部署<code>deploy/configmap.yaml</code></li>
<li>将各集群获取的 <code>ca.crt</code> 和 <code>token</code> 写入到对应的deploy/token下</li>
<li><p>执行 deploy 下的 secret.sh 脚本 <code>sh deploy/secret.sh</code></p>
<blockquote>
<p>注意: secret.sh 中的<code>xx_token/xxx_ca.crt</code>中的 <code>xx</code> 对应于<code>configmap.yaml</code> 中的<strong>集群别名，必须要一一对应</strong></p>
</blockquote>
</li>
<li><p>部署<code>deploy/deployment.yaml</code></p>
</li>
</ol>
<h2 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h2><p><a href="http://{nodeip}:31000" target="_blank" rel="external">http://{nodeip}:31000</a></p>
<p>视频地址：<a href="http://www.youtube.com/watch?v=ILiviSLbSq8" target="_blank" rel="external">http://www.youtube.com/watch?v=ILiviSLbSq8</a></p>
<p>git地址：<a href="https://github.com/icyxp/kubernetes-dashboard-ldap" target="_blank" rel="external">https://github.com/icyxp/kubernetes-dashboard-ldap</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[自定义 Kubernetes 调度器]]></title>
      <url>http://team.jiunile.com/blog/2020/06/k8s-custom-scheduler.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><code>kube-scheduler</code> 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。</p>
<h2 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h2><p>默认情况下，<code>kube-scheduler</code> 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。</p>
<p><code>kube-scheduler</code> 的主要作用就是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上去，是一个独立的二进制程序，启动之后会一直监听 API Server，获取到 <code>PodSpec.NodeName</code> 为空的 Pod，对每个 Pod 都会创建一个 binding。<br><img src="/images/k8s/kube-scheduler-overview.png" alt="kube-scheduler-overview"><br><a id="more"></a><br>这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：</p>
<ul>
<li>如何保证全部的节点调度的公平性？要知道并不是所有节点资源配置一定都是一样的</li>
<li>如何保证每个节点都能被分配资源？</li>
<li>集群资源如何能够被高效利用？</li>
<li>集群资源如何才能被最大化使用？</li>
<li>如何保证 Pod 调度的性能和效率？</li>
<li>用户是否可以根据自己的实际需求定制自己的调度策略？</li>
</ul>
<p>考虑到实际环境中的各种复杂情况，kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</p>
<p>kubernetes 调度器的源码位于 <code>kubernetes/pkg/scheduler</code> 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/scheduler</span><br><span class="line">-- scheduler.go         //调度相关的具体实现</span><br><span class="line">|-- algorithm</span><br><span class="line">|   |-- predicates      //节点筛选策略</span><br><span class="line">|   |-- priorities      //节点打分策略</span><br><span class="line">|-- algorithmprovider</span><br><span class="line">|   |-- defaults         //定义默认的调度器</span><br></pre></td></tr></table></figure></p>
<p>其中 Scheduler 创建和运行的核心程序，对应的代码在 <code>pkg/scheduler/scheduler.go</code>，如果要查看 <code>kube-scheduler</code> 的入口程序，对应的代码在 <code>cmd/kube-scheduler/scheduler.go</code>。</p>
<h2 id="自定义调度器"><a href="#自定义调度器" class="headerlink" title="自定义调度器"></a>自定义调度器</h2><p>一般来说，我们有4种扩展 Kubernetes 调度器的方法。</p>
<ul>
<li>一种方法就是直接 clone 官方的 kube-scheduler 源代码，在合适的位置直接修改代码，然后重新编译运行修改后的程序，当然这种方法是最不建议使用的，也不实用，因为需要花费大量额外的精力来和上游的调度程序更改保持一致。</li>
<li>第二种方法就是和默认的调度程序一起运行独立的调度程序，默认的调度器和我们自定义的调度器可以通过 Pod 的 <code>spec.schedulerName</code> 来覆盖各自的 Pod，默认是使用 default 默认的调度器，但是多个调度程序共存的情况下也比较麻烦，比如当多个调度器将 Pod 调度到同一个节点的时候，可能会遇到一些问题，因为很有可能两个调度器都同时将两个 Pod 调度到同一个节点上去，但是很有可能其中一个 Pod 运行后其实资源就消耗完了，并且维护一个高质量的自定义调度程序也不是很容易的，因为我们需要全面了解默认的调度程序，整体 Kubernetes 的架构知识以及各种 Kubernetes API 对象的各种关系或限制。</li>
<li>第三种方法是<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md" target="_blank" rel="external">调度器扩展程序</a>，这个方案目前是一个可行的方案，可以和上游调度程序兼容，所谓的调度器扩展程序其实就是一个可配置的 Webhook 而已，里面包含 <code>过滤器</code> 和 <code>优先级</code> 两个端点，分别对应调度周期中的两个主要阶段（过滤和打分）。</li>
<li>第四种方法是通过调度框架（Scheduling Framework），Kubernetes v1.15 版本中引入了可插拔架构的调度框架，使得定制调度器这个任务变得更加的容易。调库框架向现有的调度器中添加了一组插件化的 API，该 API 在保持调度程序“核心”简单且易于维护的同时，使得大部分的调度功能以插件的形式存在，而且在我们现在的 v1.16 版本中上面的 <code>调度器扩展程序</code> 也已经被废弃了，所以以后调度框架才是自定义调度器的核心方式。</li>
</ul>
<p>这里我们可以简单介绍下后面两种方式的实现。</p>
<h3 id="调度器扩展程序"><a href="#调度器扩展程序" class="headerlink" title="调度器扩展程序"></a>调度器扩展程序</h3><p>在进入调度器扩展程序之前，我们再来了解下 Kubernetes 调度程序是如何工作的:</p>
<ol>
<li>默认调度器根据指定的参数启动（我们使用 kubeadm 搭建的集群，启动配置文件位于 <code>/etc/kubernetes/manifests/kube-schdueler.yaml</code>）</li>
<li>watch apiserver，将 <code>spec.nodeName</code> 为空的 Pod 放入调度器内部的调度队列中</li>
<li>从调度队列中 Pop 出一个 Pod，开始一个标准的调度周期</li>
<li>从 Pod 属性中检索“硬性要求”（比如 CPU/内存请求值，nodeSelector/nodeAffinity），然后过滤阶段发生，在该阶段计算出满足要求的节点候选列表</li>
<li>从 Pod 属性中检索“软需求”，并应用一些默认的“软策略”（比如 Pod 倾向于在节点上更加聚拢或分散），最后，它为每个候选节点给出一个分数，并挑选出得分最高的最终获胜者</li>
<li>和 apiserver 通信（发送绑定调用），然后设置 Pod 的 <code>spec.nodeName</code> 属性以表示将该 Pod 调度到的节点。</li>
</ol>
<p>我们可以通过查看<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" rel="external">官方文档</a>，可以通过 <code>--config</code> 参数指定调度器将使用哪些参数，该配置文件应该包含一个 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration" target="_blank" rel="external">KubeSchedulerConfiguration</a> 对象，如下所示格式：（/etc/kubernetes/scheduler-extender.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 通过"--config" 传递文件内容</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeSchedulerConfiguration</span><br><span class="line"><span class="attr">clientConnection:</span></span><br><span class="line"><span class="attr">  kubeconfig:</span> <span class="string">"/etc/kubernetes/scheduler.conf"</span></span><br><span class="line"><span class="attr">algorithmSource:</span></span><br><span class="line"><span class="attr">  policy:</span></span><br><span class="line"><span class="attr">    file:</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">"/etc/kubernetes/scheduler-extender-policy.yaml"</span>  <span class="comment"># 指定自定义调度策略文件</span></span><br></pre></td></tr></table></figure></p>
<p>我们在这里应该输入的关键参数是 <code>algorithmSource.policy</code>，这个策略文件可以是本地文件也可以是 ConfigMap 资源对象，这取决于调度程序的部署方式，比如我们这里默认的调度器是静态 Pod 方式启动的，所以我们可以用本地文件的形式来配置。</p>
<p>该策略文件 <code>/etc/kubernetes/scheduler-extender-policy.yaml</code> 应该遵循 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#Policy" target="_blank" rel="external">kubernetes/pkg/scheduler/apis/config/legacy_types.go#L28</a> 的要求，在我们这里的 v1.16.2 版本中已经支持 JSON 和 YAML 两种格式的策略文件，下面是我们定义的一个简单的示例，可以查看 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#Extender" target="_blank" rel="external">Extender</a> 描述了解策略文件的定义规范：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Policy</span><br><span class="line"><span class="attr">extenders:</span></span><br><span class="line"><span class="attr">- urlPrefix:</span> <span class="string">"http://127.0.0.1:8888/"</span></span><br><span class="line"><span class="attr">  filterVerb:</span> <span class="string">"filter"</span></span><br><span class="line"><span class="attr">  prioritizeVerb:</span> <span class="string">"prioritize"</span></span><br><span class="line"><span class="attr">  weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  enableHttps:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>我们这里的 Policy 策略文件是通过定义 <code>extenders</code> 来扩展调度器的，有时候我们不需要去编写代码，可以直接在该配置文件中通过指定 <code>predicates</code> 和 <code>priorities</code> 来进行自定义，如果没有指定则会使用默认的 <code>DefaultProvier</code>：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="attr">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">    <span class="attr">"predicates"</span>: [&#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"MatchNodeSelector"</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"PodFitsResources"</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"PodFitsHostPorts"</span></span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"HostName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"priorities"</span>: [&#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"EqualPriority"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">2</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"ImageLocalityPriority"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">4</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"LeastRequestedPriority"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">2</span></span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"BalancedResourceAllocation"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"extenders"</span>: [&#123;</span><br><span class="line">        <span class="attr">"urlPrefix"</span>: <span class="string">"/prefix"</span>,</span><br><span class="line">        <span class="attr">"filterVerb"</span>: <span class="string">"filter"</span>,</span><br><span class="line">        <span class="attr">"prioritizeVerb"</span>: <span class="string">"prioritize"</span>,</span><br><span class="line">        <span class="attr">"weight"</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">"bindVerb"</span>: <span class="string">"bind"</span>,</span><br><span class="line">        <span class="attr">"enableHttps"</span>: <span class="literal">false</span></span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>改策略文件定义了一个 HTTP 的扩展程序服务，该服务运行在 <code>127.0.0.1:8888</code> 下面，并且已经将该策略注册到了默认的调度器中，这样在过滤和打分阶段结束后，可以将结果分别传递给该扩展程序的端点 <code>&lt;urlPrefix&gt;/&lt;filterVerb&gt;</code> 和 <code>&lt;urlPrefix&gt;/&lt;prioritizeVerb&gt;</code>，在扩展程序中，我们可以进一步过滤并确定优先级，以适应我们的特定业务需求。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>我们直接用 golang 来实现一个简单的调度器扩展程序，当然你可以使用其他任何编程语言，如下所示：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    router := httprouter.New()</span><br><span class="line">    router.GET(<span class="string">"/"</span>, Index)</span><br><span class="line">    router.POST(<span class="string">"/filter"</span>, Filter)</span><br><span class="line">    router.POST(<span class="string">"/prioritize"</span>, Prioritize)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(<span class="string">":8888"</span>, router))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后接下来我们需要实现 <code>/filter</code> 和 <code>/prioritize</code> 两个端点的处理程序。</p>
<p>其中 <code>Filter</code> 这个扩展函数接收一个输入类型为 <code>schedulerapi.ExtenderArgs</code> 的参数，然后返回一个类型为 <code>*schedulerapi.ExtenderFilterResult</code> 的值。在函数中，我们可以进一步过滤输入的节点：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// filter 根据扩展程序定义的预选规则来过滤节点</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">filter</span><span class="params">(args schedulerapi.ExtenderArgs)</span> *<span class="title">schedulerapi</span>.<span class="title">ExtenderFilterResult</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> filteredNodes []v1.Node</span><br><span class="line">	failedNodes := <span class="built_in">make</span>(schedulerapi.FailedNodesMap)</span><br><span class="line">	pod := args.Pod</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _, node := <span class="keyword">range</span> args.Nodes.Items &#123;</span><br><span class="line">		fits, failReasons, _ := podFitsOnNode(pod, node)</span><br><span class="line">		<span class="keyword">if</span> fits &#123;</span><br><span class="line">			filteredNodes = <span class="built_in">append</span>(filteredNodes, node)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			failedNodes[node.Name] = strings.Join(failReasons, <span class="string">","</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	result := schedulerapi.ExtenderFilterResult&#123;</span><br><span class="line">		Nodes: &amp;v1.NodeList&#123;</span><br><span class="line">			Items: filteredNodes,</span><br><span class="line">		&#125;,</span><br><span class="line">		FailedNodes: failedNodes,</span><br><span class="line">		Error:       <span class="string">""</span>,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在过滤函数中，我们循环每个节点然后用我们自己实现的业务逻辑来判断是否应该批准该节点，这里我们实现比较简单，在 <code>podFitsOnNode()</code> 函数中我们只是简单的检查随机数是否为偶数来判断即可，如果是的话我们就认为这是一个幸运的节点，否则拒绝批准该节点。<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> predicatesSorted = []<span class="keyword">string</span>&#123;LuckyPred&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> predicatesFuncs = <span class="keyword">map</span>[<span class="keyword">string</span>]FitPredicate&#123;</span><br><span class="line">    LuckyPred: LuckyPredicate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> FitPredicate <span class="function"><span class="keyword">func</span><span class="params">(pod *v1.Pod, node v1.Node)</span> <span class="params">(<span class="keyword">bool</span>, []<span class="keyword">string</span>, error)</span></span><br><span class="line"></span><br><span class="line"><span class="title">func</span> <span class="title">podFitsOnNode</span><span class="params">(pod *v1.Pod, node v1.Node)</span> <span class="params">(<span class="keyword">bool</span>, []<span class="keyword">string</span>, error)</span></span> &#123;</span><br><span class="line">    fits := <span class="literal">true</span></span><br><span class="line">    <span class="keyword">var</span> failReasons []<span class="keyword">string</span></span><br><span class="line">    <span class="keyword">for</span> _, predicateKey := <span class="keyword">range</span> predicatesSorted &#123;</span><br><span class="line">        fit, failures, err := predicatesFuncs[predicateKey](pod, node)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>, <span class="literal">nil</span>, err</span><br><span class="line">        &#125;</span><br><span class="line">        fits = fits &amp;&amp; fit</span><br><span class="line">        failReasons = <span class="built_in">append</span>(failReasons, failures...)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fits, failReasons, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LuckyPredicate</span><span class="params">(pod *v1.Pod, node v1.Node)</span> <span class="params">(<span class="keyword">bool</span>, []<span class="keyword">string</span>, error)</span></span> &#123;</span><br><span class="line">    lucky := rand.Intn(<span class="number">2</span>) == <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> lucky &#123;</span><br><span class="line">        log.Printf(<span class="string">"pod %v/%v is lucky to fit on node %v\n"</span>, pod.Name, pod.Namespace, node.Name)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>, <span class="literal">nil</span>, <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">    log.Printf(<span class="string">"pod %v/%v is unlucky to fit on node %v\n"</span>, pod.Name, pod.Namespace, node.Name)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>, []<span class="keyword">string</span>&#123;LuckyPredFailMsg&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>同样的打分功能用同样的方式来实现，我们在每个节点上随机给出一个分数：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// it's webhooked to pkg/scheduler/core/generic_scheduler.go#PrioritizeNodes()</span></span><br><span class="line"><span class="comment">// 这个函数输出的分数会被添加会默认的调度器</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">prioritize</span><span class="params">(args schedulerapi.ExtenderArgs)</span> *<span class="title">schedulerapi</span>.<span class="title">HostPriorityList</span></span> &#123;</span><br><span class="line">	pod := args.Pod</span><br><span class="line">	nodes := args.Nodes.Items</span><br><span class="line"></span><br><span class="line">	hostPriorityList := <span class="built_in">make</span>(schedulerapi.HostPriorityList, <span class="built_in">len</span>(nodes))</span><br><span class="line">	<span class="keyword">for</span> i, node := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">		score := rand.Intn(schedulerapi.MaxPriority + <span class="number">1</span>)  <span class="comment">// 在最大优先级内随机取一个值</span></span><br><span class="line">		log.Printf(luckyPrioMsg, pod.Name, pod.Namespace, score)</span><br><span class="line">		hostPriorityList[i] = schedulerapi.HostPriority&#123;</span><br><span class="line">			Host:  node.Name,</span><br><span class="line">			Score: score,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;hostPriorityList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以使用下面的命令来编译打包我们的应用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GOOS=linux GOARCH=amd64 go build -o app</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>本节调度器扩展程序完整的代码获取地址：<a href="https://github.com/icyxp/sample-scheduler-extender" target="_blank" rel="external">https://github.com/icyxp/sample-scheduler-extender</a>。</p>
</blockquote>
<p>构建完成后，将应用 <code>app</code> 拷贝到 <code>kube-scheduler</code> 所在的节点直接运行即可。现在我们就可以将上面的策略文件配置到 <code>kube-scheduler</code> 组件中去了，我们这里集群是 kubeadm 搭建的，所以直接修改文件 <code>/etc/kubernetes/manifests/kube-schduler.yaml</code> 文件即可，内容如下所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    component:</span> kube-scheduler</span><br><span class="line"><span class="attr">    tier:</span> control-plane</span><br><span class="line"><span class="attr">  name:</span> kube-scheduler</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> kube-scheduler</span><br><span class="line"><span class="bullet">    -</span> --authentication-kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="bullet">    -</span> --authorization-kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="bullet">    -</span> --bind-address=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="bullet">    -</span> --kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="bullet">    -</span> --leader-elect=<span class="literal">true</span></span><br><span class="line"><span class="bullet">    -</span> --config=/etc/kubernetes/scheduler-extender.yaml</span><br><span class="line"><span class="bullet">    -</span> --v=<span class="number">9</span></span><br><span class="line"><span class="attr">    image:</span> gcr.azk8s.cn/google_containers/kube-scheduler:v1<span class="number">.16</span><span class="number">.2</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">    livenessProbe:</span></span><br><span class="line"><span class="attr">      failureThreshold:</span> <span class="number">8</span></span><br><span class="line"><span class="attr">      httpGet:</span></span><br><span class="line"><span class="attr">        host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">        path:</span> /healthz</span><br><span class="line"><span class="attr">        port:</span> <span class="number">10251</span></span><br><span class="line"><span class="attr">        scheme:</span> HTTP</span><br><span class="line"><span class="attr">      initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">      timeoutSeconds:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">    name:</span> kube-scheduler</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="attr">      name:</span> kubeconfig</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /etc/kubernetes/scheduler-extender.yaml</span><br><span class="line"><span class="attr">      name:</span> extender</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    - mountPath:</span> /etc/kubernetes/scheduler-extender-policy.yaml</span><br><span class="line"><span class="attr">      name:</span> extender-policy</span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  priorityClassName:</span> system-cluster-critical</span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /etc/kubernetes/scheduler.conf</span><br><span class="line"><span class="attr">      type:</span> FileOrCreate</span><br><span class="line"><span class="attr">    name:</span> kubeconfig</span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /etc/kubernetes/scheduler-extender.yaml</span><br><span class="line"><span class="attr">      type:</span> FileOrCreate</span><br><span class="line"><span class="attr">    name:</span> extender</span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> /etc/kubernetes/scheduler-extender-policy.yaml</span><br><span class="line"><span class="attr">      type:</span> FileOrCreate</span><br><span class="line"><span class="attr">    name:</span> extender-policy</span><br><span class="line"><span class="attr">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>当然我们这个地方是直接在系统默认的 <code>kube-scheduler</code> 上面配置的，我们也可以复制一个调度器的 YAML 文件然后更改下 schedulerName 来部署，这样就不会影响默认的调度器了，然后在需要使用这个测试的调度器的 Pod 上面指定 <code>spec.schedulerName</code> 即可。对于多调度器的使用可以查看官方文档 <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-multiple-schedulers/" target="_blank" rel="external">配置多个调度器</a>。</p>
</blockquote>
<p><code>kube-scheduler</code> 重新配置后可以查看日志来验证是否重启成功，需要注意的是一定需要将 <code>/etc/kubernetes/scheduler-extender.yaml</code> 和 <code>/etc/kubernetes/scheduler-extender-policy.yaml</code> 两个文件挂载到 Pod 中去：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs <span class="_">-f</span> kube-scheduler-ydzs-master -n kube-system</span><br><span class="line">I0102 15:17:38.824657       1 serving.go:319] Generated self-signed cert <span class="keyword">in</span>-memory</span><br><span class="line">I0102 15:17:39.472276       1 server.go:143] Version: v1.16.2</span><br><span class="line">I0102 15:17:39.472674       1 defaults.go:91] TaintNodesByCondition is enabled, PodToleratesNodeTaints predicate is mandatory</span><br><span class="line">W0102 15:17:39.479704       1 authorization.go:47] Authorization is disabled</span><br><span class="line">W0102 15:17:39.479733       1 authentication.go:79] Authentication is disabled</span><br><span class="line">I0102 15:17:39.479777       1 deprecated_insecure_serving.go:51] Serving healthz insecurely on [::]:10251</span><br><span class="line">I0102 15:17:39.480559       1 secure_serving.go:123] Serving securely on 127.0.0.1:10259</span><br><span class="line">I0102 15:17:39.682180       1 leaderelection.go:241] attempting to acquire leader lease  kube-system/kube-scheduler...</span><br><span class="line">I0102 15:17:56.500505       1 leaderelection.go:251] successfully acquired lease kube-system/kube-scheduler</span><br></pre></td></tr></table></figure></p>
<p>到这里我们就创建并配置了一个非常简单的调度扩展程序，现在我们来运行一个 Deployment 查看其工作原理，我们准备一个包含20个副本的部署 Yaml：(test-scheduler.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> pause</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> pause</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> pause</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> pause</span><br><span class="line"><span class="attr">        image:</span> gcr.azk8s.cn/google_containers/pause:<span class="number">3.1</span></span><br></pre></td></tr></table></figure></p>
<p>直接创建上面的资源对象：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kuectl apply <span class="_">-f</span> <span class="built_in">test</span>-scheduler.yaml</span><br><span class="line">deployment.apps/pause created</span><br></pre></td></tr></table></figure></p>
<p>这个时候我们去查看下我们编写的调度器扩展程序日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./app</span><br><span class="line">......</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is unlucky to fit on node ydzs-node1</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 7</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 9</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node3</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node4</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node1</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node2</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 4</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 8</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 Pod 调度的过程，另外默认调度程序会定期重试失败的 Pod，因此它们将一次又一次地重新传递到我们的调度扩展程序上，我们的逻辑是检查随机数是否为偶数，所以最终所有 Pod 都将处于运行状态。</p>
<p>调度器扩展程序可能是在一些情况下可以满足我们的需求，但是他仍然有一些限制和缺点：</p>
<ul>
<li>通信成本：数据在默认调度程序和调度器扩展程序之间以 <code>http（s）</code> 传输，在执行序列化和反序列化的时候有一定成本</li>
<li>有限的扩展点：扩展程序只能在某些阶段的末尾参与，例如 <code>“Filter”</code> 和 <code>“Prioritize”</code> ，它们不能在任何阶段的开始或中间被调用</li>
<li>减法优于加法：与默认调度程序传递的节点候选列表相比，我们可能有一些需求需要添加新的候选节点列表，但这是比较冒险的操作，因为不能保证新节点可以通过其他要求，所以，调度器扩展程序最好执行 <code>“减法”</code>（进一步过滤），而不是 <code>“加法”</code>（添加节点）</li>
<li>缓存共享：上面只是一个简单的测试示例，但在真实的项目中，我们是需要通过查看整个集群的状态来做出调度决策的，默认调度程序可以很好地调度决策，但是无法共享其缓存，这意味着我们必须构建和维护自己的缓存</li>
</ul>
<p>由于这些局限性，Kubernetes 调度小组就提出了上面第四种方法来进行更好的扩展，也就是<code>调度框架（Scheduler Framework）</code>，它基本上可以解决我们遇到的所有难题，现在也已经成官方推荐的扩展方式，所以这将是以后扩展调度器的最主流的方式。</p>
<h2 id="调度框架"><a href="#调度框架" class="headerlink" title="调度框架"></a>调度框架</h2><p>调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑（我们称之为扩展），并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知。</p>
<p>我们知道每当调度一个 Pod 时，都会按照两个过程来执行：调度过程和绑定过程。</p>
<p>调度过程为 Pod 选择一个合适的节点，绑定过程则将调度过程的决策应用到集群中（也就是在被选定的节点上运行 Pod），将调度过程和绑定过程合在一起，称之为调度上下文（<strong>scheduling context</strong>）。需要注意的是调度过程是<code>同步</code>运行的（同一时间点只为一个 Pod 进行调度），绑定过程可异步运行（同一时间点可并发为多个 Pod 执行绑定）。</p>
<p>调度过程和绑定过程遇到如下情况时会中途退出：</p>
<ul>
<li>调度程序认为当前没有该 Pod 的可选节点</li>
<li>内部错误</li>
</ul>
<p>这个时候，该 Pod 将被放回到 待调度队列，并等待下次重试。</p>
<h3 id="扩展点（Extension-Points）"><a href="#扩展点（Extension-Points）" class="headerlink" title="扩展点（Extension Points）"></a>扩展点（Extension Points）</h3><p>下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。<br><img src="/images/k8s/scheduling-framework-extensions.png" alt="scheduling-framework-extensions"></p>
<ol>
<li><code>QueueSort</code> 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，<code>QueueSort</code> 扩展本质上只需要实现一个方法 <code>Less(Pod1, Pod2)</code> 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 <code>QueueSort</code> 插件生效。</li>
<li><code>Pre-filter</code> 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 <code>pre-filter</code> 返回了 error，则调度过程终止。</li>
<li><code>Filter</code> 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 <code>filter</code> 扩展；如果任何一个 <code>filter</code> 将节点标记为不可选，则余下的 <code>filter</code> 扩展将不会被执行。调度器可以同时对多个节点执行 <code>filter</code> 扩展。</li>
<li><code>Post-filter</code> 是一个通知类型的扩展点，调用该扩展的参数是 <code>filter</code> 阶段结束后被筛选为可选节点的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。</li>
<li><code>Scoring</code> 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 <code>Soring</code> 扩展，评分结果是一个范围内的整数。在 <code>normalize scoring</code> 阶段，调度器将会把每个 <code>scoring</code> 扩展对具体某个节点的评分结果和该扩展的权重合并起来，作为最终评分结果。</li>
<li><code>Normalize scoring</code> 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 <code>scoring</code> 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 <code>normalize scoring</code> 扩展一次。</li>
<li><code>Reserve</code> 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况。（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 <code>reserved</code> 状态以后，要么在绑定失败时触发 <code>Unreserve</code> 扩展，要么在绑定成功时，由 <code>Post-bind</code> 扩展结束绑定过程。</li>
<li><code>Permit</code> 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项：<ul>
<li><code>approve</code>（批准）：当所有的 <code>permit</code> 扩展都 <code>approve</code> 了 Pod 与节点的绑定，调度器将继续执行绑定过程</li>
<li><code>deny</code>（拒绝）：如果任何一个 <code>permit</code> 扩展 <code>deny</code> 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展</li>
<li><code>wait</code>（等待）：如果一个 <code>permit</code> 扩展返回了 <code>wait</code>，则 Pod 将保持在 <code>permit</code> 阶段，直到被其他扩展 <code>approve</code>，如果超时事件发生，<code>wait</code> 状态变成 <code>deny</code>，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展</li>
</ul>
</li>
<li><code>Pre-bind</code> 扩展用于在 Pod 绑定之前执行某些逻辑。例如，<code>pre-bind</code> 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 <code>pre-bind</code> 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展。</li>
<li><code>Bind</code> 扩展用于将 Pod 绑定到节点上：<ul>
<li>只有所有的 <code>pre-bind</code> 扩展都成功执行了，<code>bind</code> 扩展才会执行</li>
<li>调度框架按照 <code>bind</code> 扩展注册的顺序逐个调用 <code>bind</code> 扩展</li>
<li>具体某个 <code>bind</code> 扩展可以选择处理或者不处理该 Pod</li>
<li>如果某个 <code>` 扩展处理了该 Pod 与节点的绑定，余下的</code>bind` 扩展将被忽略</li>
</ul>
</li>
<li><code>Post-bind</code> 是一个通知性质的扩展：<ul>
<li><code>Post-bind</code> 扩展在 Pod 成功绑定到节点上之后被动调用</li>
<li><code>Post-bind</code> 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作</li>
</ul>
</li>
<li><code>Unreserve</code> 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 <code>unreserve</code> 扩展将被调用。<code>Unreserve</code> 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，<code>reserve</code> 扩展和 <code>unreserve</code> 扩展应该成对出现。</li>
</ol>
<p>如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现扩展点接口，对应的扩展点接口我们可以在源码 <code>pkg/scheduler/framework/v1alpha1/interface.go</code> 文件中找到，如下所示：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Plugin is the parent type for all the scheduling framework plugins.</span></span><br><span class="line"><span class="keyword">type</span> Plugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Name() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> QueueSortPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Less(*PodInfo, *PodInfo) <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreFilterPlugin is an interface that must be implemented by "prefilter" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called at the beginning of the scheduling cycle.</span></span><br><span class="line"><span class="keyword">type</span> PreFilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PreFilter(pc *PluginContext, p *v1.Pod) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// FilterPlugin is an interface for Filter plugins. These plugins are called at the</span></span><br><span class="line"><span class="comment">// filter extension point for filtering out hosts that cannot run a pod.</span></span><br><span class="line"><span class="comment">// This concept used to be called 'predicate' in the original scheduler.</span></span><br><span class="line"><span class="comment">// These plugins should return "Success", "Unschedulable" or "Error" in Status.code.</span></span><br><span class="line"><span class="comment">// However, the scheduler accepts other valid codes as well.</span></span><br><span class="line"><span class="comment">// Anything other than "Success" will lead to exclusion of the given host from</span></span><br><span class="line"><span class="comment">// running the pod.</span></span><br><span class="line"><span class="keyword">type</span> FilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Filter(pc *PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PostFilterPlugin is an interface for Post-filter plugin. Post-filter is an</span></span><br><span class="line"><span class="comment">// informational extension point. Plugins will be called with a list of nodes</span></span><br><span class="line"><span class="comment">// that passed the filtering phase. A plugin may use this data to update internal</span></span><br><span class="line"><span class="comment">// state or to generate logs/metrics.</span></span><br><span class="line"><span class="keyword">type</span> PostFilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PostFilter(pc *PluginContext, pod *v1.Pod, nodes []*v1.Node, filteredNodesStatuses NodeToStatusMap) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlugin is an interface that must be implemented by "score" plugins to rank</span></span><br><span class="line"><span class="comment">// nodes that passed the filtering phase.</span></span><br><span class="line"><span class="keyword">type</span> ScorePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Score(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) (<span class="keyword">int</span>, *Status)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScoreWithNormalizePlugin is an interface that must be implemented by "score"</span></span><br><span class="line"><span class="comment">// plugins that also need to normalize the node scoring results produced by the same</span></span><br><span class="line"><span class="comment">// plugin's "Score" method.</span></span><br><span class="line"><span class="keyword">type</span> ScoreWithNormalizePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	ScorePlugin</span><br><span class="line">	NormalizeScore(pc *PluginContext, p *v1.Pod, scores NodeScoreList) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ReservePlugin is an interface for Reserve plugins. These plugins are called</span></span><br><span class="line"><span class="comment">// at the reservation point. These are meant to update the state of the plugin.</span></span><br><span class="line"><span class="comment">// This concept used to be called 'assume' in the original scheduler.</span></span><br><span class="line"><span class="comment">// These plugins should return only Success or Error in Status.code. However,</span></span><br><span class="line"><span class="comment">// the scheduler accepts other valid codes as well. Anything other than Success</span></span><br><span class="line"><span class="comment">// will lead to rejection of the pod.</span></span><br><span class="line"><span class="keyword">type</span> ReservePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Reserve(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreBindPlugin is an interface that must be implemented by "prebind" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called before a pod being scheduled.</span></span><br><span class="line"><span class="keyword">type</span> PreBindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PreBind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PostBindPlugin is an interface that must be implemented by "postbind" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called after a pod is successfully bound to a node.</span></span><br><span class="line"><span class="keyword">type</span> PostBindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	PostBind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// UnreservePlugin is an interface for Unreserve plugins. This is an informational</span></span><br><span class="line"><span class="comment">// extension point. If a pod was reserved and then rejected in a later phase, then</span></span><br><span class="line"><span class="comment">// un-reserve plugins will be notified. Un-reserve plugins should clean up state</span></span><br><span class="line"><span class="comment">// associated with the reserved Pod.</span></span><br><span class="line"><span class="keyword">type</span> UnreservePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Unreserve(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PermitPlugin is an interface that must be implemented by "permit" plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called before a pod is bound to a node.</span></span><br><span class="line"><span class="keyword">type</span> PermitPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Permit(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) (*Status, time.Duration)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// BindPlugin is an interface that must be implemented by "bind" plugins. Bind</span></span><br><span class="line"><span class="comment">// plugins are used to bind a pod to a Node.</span></span><br><span class="line"><span class="keyword">type</span> BindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">	Plugin</span><br><span class="line">	Bind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对于调度框架插件的启用或者禁用，我们同样可以使用上面的 <a href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration" target="_blank" rel="external">KubeSchedulerConfiguration</a> 资源对象来进行配置。下面的例子中的配置启用了一个实现了 <code>reserve</code> 和 <code>preBind</code> 扩展点的插件，并且禁用了另外一个插件，同时为插件 foo 提供了一些配置信息：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeSchedulerConfiguration</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line"><span class="attr">  reserve:</span></span><br><span class="line"><span class="attr">    enabled:</span></span><br><span class="line"><span class="attr">    - name:</span> foo</span><br><span class="line"><span class="attr">    - name:</span> bar</span><br><span class="line"><span class="attr">    disabled:</span></span><br><span class="line"><span class="attr">    - name:</span> baz</span><br><span class="line"><span class="attr">  preBind:</span></span><br><span class="line"><span class="attr">    enabled:</span></span><br><span class="line"><span class="attr">    - name:</span> foo</span><br><span class="line"><span class="attr">    disabled:</span></span><br><span class="line"><span class="attr">    - name:</span> baz</span><br><span class="line"></span><br><span class="line"><span class="attr">pluginConfig:</span></span><br><span class="line"><span class="attr">- name:</span> foo</span><br><span class="line"><span class="attr">  args:</span> <span class="string">&gt;</span><br><span class="line">    foo插件可以解析的任意内容</span></span><br></pre></td></tr></table></figure></p>
<p>扩展的调用顺序如下：</p>
<ul>
<li>如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展</li>
<li>如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展</li>
<li>默认插件的扩展始终被最先调用，然后按照 <code>KubeSchedulerConfiguration</code> 中扩展的激活 <code>enabled</code> 顺序逐个调用扩展点的扩展</li>
<li>可以先禁用默认插件的扩展，然后在 <code>enabled</code> 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序</li>
</ul>
<p>假设默认插件 foo 实现了 <code>reserve</code> 扩展点，此时我们要添加一个插件 bar，想要在 foo 之前被调用，则应该先禁用 foo 再按照 bar foo 的顺序激活。示例配置如下所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeSchedulerConfiguration</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line"><span class="attr">  reserve:</span></span><br><span class="line"><span class="attr">    enabled:</span></span><br><span class="line"><span class="attr">    - name:</span> bar</span><br><span class="line"><span class="attr">    - name:</span> foo</span><br><span class="line"><span class="attr">    disabled:</span></span><br><span class="line"><span class="attr">    - name:</span> foo</span><br></pre></td></tr></table></figure></p>
<p>在源码目录 <code>pkg/scheduler/framework/plugins/examples</code> 中有几个示范插件，我们可以参照其实现方式。</p>
<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><p>其实要实现一个调度框架的插件，并不难，我们只要实现对应的扩展点，然后将插件注册到调度器中即可，下面是默认调度器在初始化的时候注册的插件：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewRegistry</span><span class="params">()</span> <span class="title">Registry</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> Registry&#123;</span><br><span class="line">		<span class="comment">// FactoryMap:</span></span><br><span class="line">		<span class="comment">// New plugins are registered here.</span></span><br><span class="line">		<span class="comment">// example:</span></span><br><span class="line">		<span class="comment">// &#123;</span></span><br><span class="line">		<span class="comment">//  stateful_plugin.Name: stateful.NewStatefulMultipointExample,</span></span><br><span class="line">		<span class="comment">//  fooplugin.Name: fooplugin.New,</span></span><br><span class="line">		<span class="comment">// &#125;</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是可以看到默认并没有注册一些插件，所以要想让调度器能够识别我们的插件代码，就需要自己来实现一个调度器了，当然这个调度器我们完全没必要完全自己实现，直接调用默认的调度器，然后在上面的 <code>NewRegistry()</code> 函数中将我们的插件注册进去即可。在 <code>kube-scheduler</code> 的源码文件 <code>kubernetes/cmd/kube-scheduler/app/server.go</code> 中有一个 <code>NewSchedulerCommand</code> 入口函数，其中的参数是一个类型为 <code>Option</code> 的列表，而这个 <code>Option</code> 恰好就是一个插件配置的定义：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Option configures a framework.Registry.</span></span><br><span class="line"><span class="keyword">type</span> Option <span class="function"><span class="keyword">func</span><span class="params">(framework.Registry)</span> <span class="title">error</span></span><br><span class="line"></span><br><span class="line">// <span class="title">NewSchedulerCommand</span> <span class="title">creates</span> <span class="title">a</span> *<span class="title">cobra</span>.<span class="title">Command</span> <span class="title">object</span> <span class="title">with</span> <span class="title">default</span> <span class="title">parameters</span> <span class="title">and</span> <span class="title">registryOptions</span></span><br><span class="line"><span class="title">func</span> <span class="title">NewSchedulerCommand</span><span class="params">(registryOptions ...Option)</span> *<span class="title">cobra</span>.<span class="title">Command</span></span> &#123;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>所以我们完全就可以直接调用这个函数来作为我们的函数入口，并且传入我们自己实现的插件作为参数即可，而且该文件下面还有一个名为 <code>WithPlugin</code> 的函数可以来创建一个 <code>Option</code> 实例：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WithPlugin creates an Option based on plugin name and factory.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithPlugin</span><span class="params">(name <span class="keyword">string</span>, factory framework.PluginFactory)</span> <span class="title">Option</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="function"><span class="keyword">func</span><span class="params">(registry framework.Registry)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">		<span class="keyword">return</span> registry.Register(name, factory)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>所以最终我们的入口函数如下所示：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	rand.Seed(time.Now().UTC().UnixNano())</span><br><span class="line"></span><br><span class="line">	command := app.NewSchedulerCommand(</span><br><span class="line">		app.WithPlugin(sample.Name, sample.New), </span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	logs.InitLogs()</span><br><span class="line">	<span class="keyword">defer</span> logs.FlushLogs()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err := command.Execute(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		_, _ = fmt.Fprintf(os.Stderr, <span class="string">"%v\n"</span>, err)</span><br><span class="line">		os.Exit(<span class="number">1</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中 <code>app.WithPlugin(sample.Name, sample.New)</code> 就是我们接下来要实现的插件，从 <code>WithPlugin</code> 函数的参数也可以看出我们这里的 <code>sample.New</code> 必须是一个 <code>framework.PluginFactory</code> 类型的值，而 <code>PluginFactory</code> 的定义就是一个函数：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> PluginFactory = <span class="function"><span class="keyword">func</span><span class="params">(configuration *runtime.Unknown, f FrameworkHandle)</span> <span class="params">(Plugin, error)</span></span></span><br></pre></td></tr></table></figure></p>
<p>所以 <code>sample.New</code> 实际上就是上面的这个函数，在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可，插件实现如下所示，我们这里只是简单获取下数据打印日志，如果你有实际需求的可以根据获取的数据就行处理即可，我们这里只是实现了 <code>PreFilter</code>、<code>Filter</code>、<code>PreBind</code> 三个扩展点，其他的可以用同样的方式来扩展即可：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插件名称</span></span><br><span class="line"><span class="keyword">const</span> Name = <span class="string">"sample-plugin"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Args <span class="keyword">struct</span> &#123;</span><br><span class="line">	FavoriteColor  <span class="keyword">string</span> <span class="string">`json:"favorite_color,omitempty"`</span></span><br><span class="line">	FavoriteNumber <span class="keyword">int</span>    <span class="string">`json:"favorite_number,omitempty"`</span></span><br><span class="line">	ThanksTo       <span class="keyword">string</span> <span class="string">`json:"thanks_to,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Sample <span class="keyword">struct</span> &#123;</span><br><span class="line">	args   *Args</span><br><span class="line">	handle framework.FrameworkHandle</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">Name</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> Name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">PreFilter</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">	klog.V(<span class="number">3</span>).Infof(<span class="string">"prefilter pod: %v"</span>, pod.Name)</span><br><span class="line">	<span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">""</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">Filter</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">	klog.V(<span class="number">3</span>).Infof(<span class="string">"filter pod: %v, node: %v"</span>, pod.Name, nodeName)</span><br><span class="line">	<span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">""</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">PreBind</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> nodeInfo, ok := s.handle.NodeInfoSnapshot().NodeInfoMap[nodeName]; !ok &#123;</span><br><span class="line">		<span class="keyword">return</span> framework.NewStatus(framework.Error, fmt.Sprintf(<span class="string">"prebind get node info error: %+v"</span>, nodeName))</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		klog.V(<span class="number">3</span>).Infof(<span class="string">"prebind node info: %+v"</span>, nodeInfo.Node())</span><br><span class="line">		<span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">""</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(configuration *runtime.Unknown, f framework.FrameworkHandle)</span> <span class="params">(framework.Plugin, error)</span></span> &#123;</span><br><span class="line">	args := &amp;Args&#123;&#125;</span><br><span class="line">	<span class="keyword">if</span> err := framework.DecodeInto(configuration, args); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line">	klog.V(<span class="number">3</span>).Infof(<span class="string">"get plugin config args: %+v"</span>, args)</span><br><span class="line">	<span class="keyword">return</span> &amp;Sample&#123;</span><br><span class="line">		args: args,</span><br><span class="line">		handle: f,</span><br><span class="line">	&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>完整代码可以前往仓库 <a href="https://github.com/icyxp/sample-scheduler-framework" target="_blank" rel="external">https://github.com/icyxp/sample-scheduler-framework</a> 获取。</p>
</blockquote>
<p>实现完成后，编译打包成镜像即可，然后我们就可以当成普通的应用用一个 <code>Deployment</code> 控制器来部署即可，由于我们需要去获取集群中的一些资源对象，所以当然需要申请 RBAC 权限，然后同样通过 <code>--config</code> 参数来配置我们的调度器，同样还是使用一个 <code>KubeSchedulerConfiguration</code> 资源对象配置，可以通过 <code>plugins</code> 来启用或者禁用我们实现的插件，也可以通过 <code>pluginConfig</code> 来传递一些参数值给插件：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-clusterrole</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> endpoints</span><br><span class="line"><span class="bullet">      -</span> events</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> nodes</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> pods</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> delete</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> bindings</span><br><span class="line"><span class="bullet">      -</span> pods/binding</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> pods/status</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> patch</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> replicationcontrollers</span><br><span class="line"><span class="bullet">      -</span> services</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> apps</span><br><span class="line"><span class="bullet">      -</span> extensions</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> replicasets</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> apps</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> statefulsets</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> policy</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> poddisruptionbudgets</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> persistentvolumeclaims</span><br><span class="line"><span class="bullet">      -</span> persistentvolumes</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> configmaps</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"storage.k8s.io"</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> storageclasses</span><br><span class="line"><span class="bullet">      -</span> csinodes</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"coordination.k8s.io"</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> leases</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"events.k8s.io"</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> events</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> create</span><br><span class="line"><span class="bullet">      -</span> patch</span><br><span class="line"><span class="bullet">      -</span> update</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-sa</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-clusterrolebinding</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-clusterrole</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">  name:</span> sample-scheduler-sa</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> scheduler-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  scheduler-config.yaml: <span class="string">|</span><br><span class="line"></span><span class="attr">    apiVersion:</span> kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">    kind:</span> KubeSchedulerConfiguration</span><br><span class="line"><span class="attr">    schedulerName:</span> sample-scheduler</span><br><span class="line"><span class="attr">    leaderElection:</span></span><br><span class="line"><span class="attr">      leaderElect:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      lockObjectName:</span> sample-scheduler</span><br><span class="line"><span class="attr">      lockObjectNamespace:</span> kube-system</span><br><span class="line"><span class="attr">    plugins:</span></span><br><span class="line"><span class="attr">      preFilter:</span></span><br><span class="line"><span class="attr">        enabled:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">      filter:</span></span><br><span class="line"><span class="attr">        enabled:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">      preBind:</span></span><br><span class="line"><span class="attr">        enabled:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">    pluginConfig:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">"sample-plugin"</span></span><br><span class="line"><span class="attr">      args:</span></span><br><span class="line"><span class="attr">        favorite_color:</span> <span class="string">"#326CE5"</span></span><br><span class="line"><span class="attr">        favorite_number:</span> <span class="number">7</span></span><br><span class="line"><span class="attr">        thanks_to:</span> <span class="string">"thockin"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> sample-scheduler</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    component:</span> sample-scheduler</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      component:</span> sample-scheduler</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        component:</span> sample-scheduler</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccount:</span> sample-scheduler-sa</span><br><span class="line"><span class="attr">      priorityClassName:</span> system-cluster-critical</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> scheduler-config</span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> scheduler-config</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> scheduler-ctrl</span><br><span class="line"><span class="attr">          image:</span> cnych/sample-scheduler:v0<span class="number">.1</span><span class="number">.6</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> sample-scheduler-framework</span><br><span class="line"><span class="bullet">            -</span> --config=/etc/kubernetes/scheduler-config.yaml</span><br><span class="line"><span class="bullet">            -</span> --v=<span class="number">3</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="string">"50m"</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> scheduler-config</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/kubernetes</span><br></pre></td></tr></table></figure></p>
<p>直接部署上面的资源对象即可，这样我们就部署了一个名为 <code>sample-scheduler</code> 的调度器了，接下来我们可以部署一个应用来使用这个调度器进行调度：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> test-scheduler</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> test-scheduler</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> test-scheduler</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      schedulerName:</span> sample-scheduler</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> nginx</span><br><span class="line"><span class="attr">        imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">        name:</span> nginx</span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure></p>
<p>这里需要注意的是我们现在手动指定了一个 <code>schedulerName</code> 的字段，将其设置成上面我们自定义的调度器名称 <code>sample-scheduler</code>。</p>
<p>我们直接创建这个资源对象，创建完成后查看我们自定义调度器的日志信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system <span class="_">-l</span> component=sample-scheduler</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">sample-scheduler-7c469787f-rwhhd   1/1     Running   0          13m</span><br><span class="line">$ kubectl logs <span class="_">-f</span> sample-scheduler-7c469787f-rwhhd -n kube-system</span><br><span class="line">I0104 08:24:22.087881       1 scheduler.go:530] Attempting to schedule pod: default/<span class="built_in">test</span>-scheduler-6d779d9465-rq2bb</span><br><span class="line">I0104 08:24:22.087992       1 plugins.go:23] prefilter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb</span><br><span class="line">I0104 08:24:22.088657       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node1</span><br><span class="line">I0104 08:24:22.088797       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node2</span><br><span class="line">I0104 08:24:22.088871       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node3</span><br><span class="line">I0104 08:24:22.088946       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-node4</span><br><span class="line">I0104 08:24:22.088992       1 plugins.go:28] filter pod: <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb, node: ydzs-master</span><br><span class="line">I0104 08:24:22.090653       1 plugins.go:36] prebind node info: &amp;Node&#123;ObjectMeta:&#123;ydzs-node3   /api/v1/nodes/ydzs-node3 1ff6e228-4d98-4737-b6d3-30a5d55ccdc2 15466372 0 2019-11-10 09:05:09 +0000 UTC &lt;nil&gt; &lt;nil&gt; ......&#125;</span><br><span class="line">I0104 08:24:22.091761       1 factory.go:610] Attempting to <span class="built_in">bind</span> <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb to ydzs-node3</span><br><span class="line">I0104 08:24:22.104994       1 scheduler.go:667] pod default/<span class="built_in">test</span>-scheduler-6d779d9465-rq2bb is bound successfully on node <span class="string">"ydzs-node3"</span>, 5 nodes evaluated, 4 nodes were found feasible. Bound node resource: <span class="string">"Capacity: CPU&lt;4&gt;|Memory&lt;8008820Ki&gt;|Pods&lt;110&gt;|StorageEphemeral&lt;17921Mi&gt;; Allocatable: CPU&lt;4&gt;|Memory&lt;7906420Ki&gt;|Pods&lt;110&gt;|StorageEphemeral&lt;16912377419&gt;."</span>.</span><br></pre></td></tr></table></figure></p>
<p>可以看到当我们创建完 Pod 后，在我们自定义的调度器中就出现了对应的日志，并且在我们定义的扩展点上面都出现了对应的日志，证明我们的示例成功了，也可以通过查看 Pod 的 <code>schedulerName</code> 来验证：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>-scheduler-6d779d9465-rq2bb           1/1     Running   0          22m</span><br><span class="line">$ kubectl get pod <span class="built_in">test</span>-scheduler-6d779d9465-rq2bb -o yaml</span><br><span class="line">......</span><br><span class="line">restartPolicy: Always</span><br><span class="line">schedulerName: sample-scheduler</span><br><span class="line">securityContext: &#123;&#125;</span><br><span class="line">serviceAccount: default</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>在最新的 Kubernetes v1.17 版本中，<code>Scheduler Framework</code> 内置的预选和优选函数已经全部插件化，所以要扩展调度器我们应该掌握并理解调度框架这种方式。</p>
<p>参考：明阳的博客</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[k8s v1.17 新增拓扑感知服务路由]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-service-topology.html</url>
      <content type="html"><![CDATA[<h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><ul>
<li><code>拓扑域</code>: 表示在集群中的某一类 “地方”，比如某节点、某机架、某可用区或某地域等，这些都可以作为某种拓扑域。</li>
<li><code>endpoint</code>: k8s 某个服务的某个 ip+port，通常是 pod 的 ip+port。</li>
<li><code>service</code>: k8s 的 service 资源(服务)，关联一组 endpoint ，访问 service 会被转发到关联的某个 endpoint 上。</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>拓扑感知服务路由，此特性最初由杜军大佬提出并设计。为什么要设计此特性呢？想象一下，k8s 集群节点分布在不同的地方，service 对应的 endpoints 分布在不同节点，传统转发策略会对所有 endpoint 做负载均衡，通常会等概率转发，当访问 service 时，流量就可能被分散打到这些不同的地方。虽然 service 转发做了负载均衡，但如果 endpoint 距离比较远，流量转发过去网络时延就相对比较高，会影响网络性能，在某些情况下甚至还可能会付出额外的流量费用。要是如能实现 service 就近转发 endpoint，是不是就可以实现降低网络时延，提升网络性能了呢？是的！这也正是该特性所提出的目的和意义。<br><a id="more"></a></p>
<h2 id="k8s-亲和性"><a href="#k8s-亲和性" class="headerlink" title="k8s 亲和性"></a>k8s 亲和性</h2><p>service 的就近转发实际就是一种网络的亲和性，倾向于转发到离自己比较近的 endpoint。在此特性之前，已经在调度和存储方面有一些亲和性的设计与实现:</p>
<ul>
<li><code>节点亲和性 (Node Affinity)</code>: 让 Pod 被调度到符合一些期望条件的 Node 上，比如限制调度到某一可用区，或者要求节点支持 GPU，这算是调度亲和，调度结果取决于节点属性。</li>
<li><code>Pod 亲和性与反亲和性 (Pod Affinity/AntiAffinity)</code>: 让一组 Pod 调度到同一拓扑域的节点上，或者打散到不同拓扑域的节点， 这也算是调度亲和，调度结果取决于其它 Pod。</li>
<li><code>数据卷拓扑感知调度 (Volume Topology-aware Scheduling)</code>: 让 Pod 只被调度到符合其绑定的存储所在拓扑域的节点上，这算是调度与存储的亲和，调度结果取决于存储的拓扑域。</li>
<li><code>本地数据卷 (Local Persistent Volume)</code>: 让 Pod 使用本地数据卷，比如高性能 SSD，在某些需要高 IOPS 低时延的场景很有用，它还会保证 Pod 始终被调度到同一节点，数据就不会不丢失，这也算是调度与存储的亲和，调度结果取决于存储所在节点。</li>
<li><code>数据卷拓扑感知动态创建 (Topology-Aware Volume Dynamic Provisioning)</code>: 先调度 Pod，再根据 Pod 所在节点的拓扑域来创建存储，这算是存储与调度的亲和，存储的创建取决于调度的结果。</li>
</ul>
<p>而 k8s 目前在网络方面还没有亲和性能力，拓扑感知服务路由这个新特性恰好可以补齐这个的空缺，此特性使得 service 可以实现就近转发而不是所有 endpoint 等概率转发。</p>
<h2 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h2><p>我们知道，service 转发主要是 node 上的 kube-proxy 进程通过 watch apiserver 获取 service 对应的 endpoint，再写入 iptables 或 ipvs 规则来实现的; 对于 headless service，主要是通过 kube-dns 或 coredns 动态解析到不同 endpoint ip 来实现的。实现 service 就近转发的关键点就在于如何将流量转发到跟当前节点在同一拓扑域的 endpoint 上，也就是会进行一次 endpoint 筛选，选出一部分符合当前节点拓扑域的 endpoint 进行转发。</p>
<p>那么如何判断 endpoint 跟当前节点是否在同一拓扑域里呢？只要能获取到 endpoint 的拓扑信息，用它跟当前节点拓扑对比下就可以知道了。那又如何获取 endpoint 的拓扑信息呢？答案是通过 endpoint 所在节点的 label，我们可以使用 node label 来描述拓扑域。</p>
<p>通常在节点初始化的时候，controller-manager 就会为节点打上许多 label，比如 <code>kubernetes.io/hostname</code> 表示节点的 hostname 来区分节点；另外，在云厂商提供的 k8s 服务，或者使用 cloud-controller-manager 的自建集群，通常还会给节点打上 <code>failure-domain.beta.kubernetes.io/zone</code> 和 <code>failure-domain.beta.kubernetes.io/region</code> 以区分节点所在可用区和所在地域，但自 v1.17 开始将会改名成 <code>topology.kubernetes.io/zone</code> 和 <code>topology.kubernetes.io/region</code>，参见 <a href="https://github.com/kubernetes/kubernetes/pull/81431" target="_blank" rel="external">PR #81431</a>。</p>
<p>如何根据 endpoint 查到它所在节点的这些 label 呢？答案是通过 <code>Endpoint Slice</code>，该特性在 v1.16 发布了 alpha，在 v1.17 将会进入 beta，它相当于 Endpoint API 增强版，通过将 endpoint 做数据分片来解决大规模 endpoint 的性能问题，并且可以携带更多的信息，包括 endpoint 所在节点的拓扑信息，拓扑感知服务路由特性会通过 <code>Endpoint Slice</code> 获取这些拓扑信息实现 endpoint 筛选 (过滤出在同一拓扑域的 endpoint)，然后再转换为 iptables 或 ipvs 规则写入节点以实现拓扑感知的路由转发。</p>
<p>细心的你可能已经发现，之前每个节点上转发 service 的 iptables/ipvs 规则基本是一样的，但启用了拓扑感知服务路由特性之后，每个节点上的转发规则就可能不一样了，因为不同节点的拓扑信息不一样，导致过滤出的 endpoint 就不一样，也正是因为这样，service 转发变得不再等概率，灵活的就近转发才得以实现。</p>
<p>当前还不支持 headless service 的拓扑路由，计划在 beta 阶段支持。由于 <a href="https://zhuanlan.zhihu.com/p/54153164" target="_blank" rel="external">headless service</a> 不是通过 kube-proxy 生成转发规则，而是通过 dns 动态解析实现的，所以需要改 kube-dns/coredns 来支持这个特性。</p>
<h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>启用当前 alpha 实现的拓扑感知服务路由特性需要满足以下前提条件:</p>
<ul>
<li>集群版本在 v1.17 及其以上。</li>
<li>Kube-proxy 以 iptables 或 IPVS 模式运行 (alpha 阶段暂时只实现了这两种模式)。</li>
<li>启用了 <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/" target="_blank" rel="external">Endpoint Slices</a> (此特性虽然在 v1.17 进入 beta，但没有默认开启)。</li>
</ul>
<h2 id="如何启用此特性"><a href="#如何启用此特性" class="headerlink" title="如何启用此特性"></a>如何启用此特性</h2><p>给所有 k8s 组件打开 <code>ServiceTopology</code> 和 <code>EndpointSlice</code> 这两个 feature:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--feature-gates=<span class="string">"ServiceTopology=true,EndpointSlice=true"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>在 Service spec 里加上 <code>topologyKeys</code> 字段，表示该 Service 优先顺序选用的拓扑域列表，对应节点标签的 key；当访问此 Service 时，会找是否有 endpoint 有对应 topology key 的拓扑信息并且 value 跟当前节点也一样，如果是，那就选定此 topology key 作为当前转发的拓扑域，并且筛选出其余所有在这个拓扑域的 endpoint 来进行转发；如果没有找到任何 endpoint 在当前 topology key 对应拓扑域，就会尝试第二个 topology key，依此类推；如果遍历完所有 topology key 也没有匹配到 endpoint 就会拒绝转发，就像此 service 没有后端 endpoint 一样。</p>
<p>有一个特殊的 topology key “<code>*</code>”，它可以匹配所有 endpoint，如果 <code>topologyKeys</code> 包含了 <code>*</code>，它必须在列表末尾，通常是在没有匹配到合适的拓扑域来实现就近转发时，就打消就近转发的念头，可以转发到任意 endpoint 上。</p>
<p>当前 topology key 支持以下可能的值（未来会增加更多）:</p>
<ul>
<li><code>kubernetes.io/hostname</code>: 节点的 hostname，通常将它放列表中第一个，表示如果本机有 endpoint 就直接转发到本机的 endpoint。</li>
<li><code>topology.kubernetes.io/zone</code>: 节点所在的可用区，通常将它放在 <code>kubernetes.io/hostname</code> 后面，表示如果本机没有对应 endpoint，就转发到当前可用区其它节点上的 endpoint（部分云厂商跨可用区通信会收取额外的流量费用）。</li>
<li><code>topology.kubernetes.io/region</code>: 表示节点所在的地域，表示转发到当前地域的 endpoint，这个用的应该会比较少，因为通常集群所有节点都只会在同一个地域，如果节点跨地域了，节点之间通信延时将会很高。</li>
<li><code>*</code>: 忽略拓扑域，匹配所有 endpoint，相当于一个保底策略，避免丢包，只能放在列表末尾。</li>
</ul>
<p>除此之外，还有以下约束:</p>
<ul>
<li><code>topologyKeys</code> 与 <code>externalTrafficPolicy=Local</code> 不兼容，是互斥的，如果 <code>externalTrafficPolicy</code> 为 <code>Local</code>，就不能定义 <code>topologyKeys</code>，反之亦然。</li>
<li>topology key 必须是合法的 label 格式，并且最多定义 16 个 key。</li>
</ul>
<p>这里给出一个简单的 Service 示例:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> ClusterIP</span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> http</span><br><span class="line"><span class="attr">    port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    protocol:</span> TCP</span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">  topologyKeys:</span> [<span class="string">"kubernetes.io/hostname"</span>, <span class="string">"topology.kubernetes.io/zone"</span>, <span class="string">"*"</span>]</span><br></pre></td></tr></table></figure></p>
<p>解释: 当访问 nginx 服务时，首先看本机是否有这个服务的 endpoint，如果有就直接本机路由过去；如果没有，就看是否有 endpoint 位于当前节点所在可用区，如果有，就转发过去，如果还是没有，就转发给任意 endpoint。<br><img src="/images/k8s/service-topology.png" alt="service-topology"></p>
<p>上图就是其中一次转发的例子：Pod 访问 nginx 这个 service 时，发现本机没有 endpoint，就找当前可用区的，找到了就转发过去，也就不会考虑转发给另一可用区的 endpoint。</p>
<p>参考：imroc</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用NodeLocal DNSCache来提升CoreDNS的性能及压力]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-nodelocal-dnscache.html</url>
      <content type="html"><![CDATA[<h2 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h2><p>之前在解决 CoreDNS 的5秒超时问题的时候，除了通过 <code>dnsConfig</code> 去强制使用 tcp 方式解析之外，我们提到过使用 <code>NodeLocal DNSCache</code> 来解决这个问题。<code>NodeLocal DNSCache</code> 通过在集群节点上运行一个 DaemonSet 来提高 clusterDNS 性能和可靠性。处于 <code>ClusterFirst</code> 的 DNS 模式下的 Pod 可以连接到 <code>kube-dns</code> 的 serviceIP 进行 DNS 查询。通过 <code>kube-proxy</code> 组件添加的 <code>iptables</code> 规则将其转换为 <code>CoreDNS</code> 端点。通过在每个集群节点上运行 DNS 缓存，<code>NodeLocal DNSCache</code> 可以缩短 DNS 查找的延迟时间、使 DNS 查找时间更加一致，以及减少发送到 <code>kube-dns</code> 的 DNS 查询次数。</p>
<p>在集群中运行 NodeLocal DNSCache 有如下几个好处：</p>
<ul>
<li>如果本地没有 CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须到另一个节点进行解析，使用 NodeLocal DNSCache 后，拥有本地缓存将有助于改善延迟</li>
<li>跳过 iptables DNAT 和连接跟踪将有助于减少 <code>conntrack</code> 竞争并避免 UDP DNS 条目填满 <code>conntrack</code> 表（常见的5s超时问题就是这个原因造成的）</li>
<li>从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP，TCP conntrack 条目将在连接关闭时被删除，而 UDP 条目必须超时(<a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt" target="_blank" rel="external">默认 nf_conntrack_udp_timeout 是 30 秒</a>)</li>
<li>将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）</li>
</ul>
<a id="more"></a>
<p><img src="/images/k8s/dnscache.png" alt="NodeLocal DNSCache"></p>
<h2 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h2><p>要安装 NodeLocal DNSCache 也非常简单，直接获取官方的资源清单即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml</span><br></pre></td></tr></table></figure></p>
<p>该资源清单文件中包含几个变量，其中：</p>
<ul>
<li><code>__PILLAR__DNS__SERVER__</code> ：表示 <code>kube-dns</code> 这个 Service 的 ClusterIP，可以通过命令 <code>kubectl get svc -n kube-system | grep kube-dns | awk &#39;{ print $3 }&#39;</code> 获取。</li>
<li><code>__PILLAR__LOCAL__DNS__</code>：表示 DNSCache 本地的 IP，默认为 169.254.20.10</li>
<li><code>__PILLAR__DNS__DOMAIN__</code>：表示集群域，默认就是 cluster.local</li>
</ul>
<p>另外还有两个参数 <code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code>，这两个参数会通过镜像 1.15.6 版本以上的去进行配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的 Upstream Server 配置。直接执行如下所示的命令即可安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sed <span class="string">'s/k8s.gcr.io/cnych/g</span><br><span class="line">s/__PILLAR__DNS__SERVER__/10.96.0.10/g</span><br><span class="line">s/__PILLAR__LOCAL__DNS__/169.254.20.10/g</span><br><span class="line">s/__PILLAR__DNS__DOMAIN__/cluster.local/g'</span> nodelocaldns.yaml |</span><br><span class="line">kubectl apply <span class="_">-f</span> -</span><br></pre></td></tr></table></figure></p>
<p>可以通过如下命令来查看对应的 Pod 是否已经启动成功：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system | grep node-local-dns</span><br><span class="line">node-local-dns-8zm2f                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-dd4xg                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-hs8qq                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-pxfxn                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-stjm9                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-wjxvz                    1/1     Running     0          9m54s</span><br><span class="line">node-local-dns-wn5wc                    1/1     Running     0          7m49s</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>需要注意的是这里使用 DaemonSet 部署 node-local-dns 使用了 <code>hostNetwork=true</code>，会占用宿主机的 8080 端口，所以需要保证该端口未被占用。</p>
</blockquote>
<p>但是到这里还没有完，如果 kube-proxy 组件使用的是 ipvs 模式的话我们还需要修改 kubelet 的 <code>--cluster-dns</code> 参数，将其指向 <code>169.254.20.10</code>，Daemonset 会在每个节点创建一个网卡来绑这个 IP，Pod 向本节点这个 IP 发 DNS 请求，缓存没有命中的时候才会再代理到上游集群 DNS 进行查询。 <code>iptables</code> 模式下 Pod 还是向原来的集群 DNS 请求，节点上有这个 IP 监听，会被本机拦截，再请求集群上游 DNS，所以不需要更改 <code>--cluster-dns</code> 参数。</p>
<h3 id="ipvs使用localDNS修改方式一"><a href="#ipvs使用localDNS修改方式一" class="headerlink" title="ipvs使用localDNS修改方式一"></a>ipvs使用localDNS修改方式一</h3><p>由于我这里使用的是 kubeadm 安装的 1.16 版本的集群，所以我们只需要替换节点上 <code>/var/lib/kubelet/config.yaml</code> 文件中的 <code>clusterDNS</code> 这个参数值，然后重启即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">'s/10.96.0.10/169.254.20.10/g'</span> /var/lib/kubelet/config.yaml</span><br><span class="line">$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure></p>
<h3 id="ipvs使用localDNS修改方式二-（不推荐）"><a href="#ipvs使用localDNS修改方式二-（不推荐）" class="headerlink" title="ipvs使用localDNS修改方式二 （不推荐）"></a>ipvs使用localDNS修改方式二 （不推荐）</h3><p>我们也可以完全在官方的 DaemonSet 资源对象中添加一个 initContainer 来完成这个工作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">initContainers:  <span class="comment"># ipvs模式下需要修改dns配置，重启kubelet</span></span><br><span class="line">  - name: setup</span><br><span class="line">    image: alpine</span><br><span class="line">    tty: <span class="literal">true</span></span><br><span class="line">    stdin: <span class="literal">true</span></span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: <span class="literal">true</span></span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">    - nsenter</span><br><span class="line">    - --target</span><br><span class="line">    - <span class="string">"1"</span></span><br><span class="line">    - --mount</span><br><span class="line">    - --uts</span><br><span class="line">    - --ipc</span><br><span class="line">    - --net</span><br><span class="line">    - --pid</span><br><span class="line">    - --</span><br><span class="line">    - bash</span><br><span class="line">    - -c</span><br><span class="line">    - |</span><br><span class="line">      <span class="comment"># 确保 kubelet --cluster-dns 被设置为 169.254.20.10</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"Configuring kubelet --cluster-dns=169.254.20.10"</span></span><br><span class="line">      sed -i <span class="string">'s/10.96.0.10/169.254.20.10/g'</span> /var/lib/kubelet/config.yaml</span><br><span class="line">      systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure></p>
<p>但是需要注意的是对于线上环境还是不推荐用上面的方式，因为它会优先将 kubelet 的 <code>cluster-dns</code> 参数进行修改，然后再去安装 NodeLocal，这中间毕竟有一段真空期。</p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>待 <code>node-local-dns</code> 安装配置完成后，我们可以部署一个新的 Pod 来验证下：(test-node-local-dns.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> test-node-local-dns</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> local-dns</span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>, <span class="string">"sleep 60m"</span>]</span><br></pre></td></tr></table></figure></p>
<p>直接部署：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply <span class="_">-f</span> <span class="built_in">test</span>-node-local-dns.yaml</span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it <span class="built_in">test</span>-node-local-dns /bin/sh</span><br><span class="line">/ <span class="comment"># cat /etc/resolv.conf</span></span><br><span class="line">nameserver 169.254.20.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 <code>nameserver</code> 已经变成 <code>169.254.20.10</code> 了，当然对于之前的历史 Pod 要想使用 <code>node-local-dns</code> 则需要重建，当然如果要想去跟踪 DNS 的解析过程的话可以去通过抓包来观察。</p>
<h2 id="番外篇"><a href="#番外篇" class="headerlink" title="番外篇"></a>番外篇</h2><p>在使用了<code>NodeLocal DNSCache</code>后，如果在配置自定义域名？</p>
<p>首先我们需要在 CoreDNS 的 ConfigMap 中添加 hosts 插件：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hosts &#123;</span><br><span class="line">  <span class="number">192.168</span><span class="number">.3</span><span class="number">.211</span> git.k8s.local</span><br><span class="line">  fallthrough</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其次我们需要修改 <code>NodeLocal DNSCache</code> 的 ConfigMap，当前配置如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">cluster.local:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache &#123;</span><br><span class="line">            success <span class="number">9984</span> <span class="number">30</span></span><br><span class="line">            denial <span class="number">9984</span> <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . <span class="number">10.96</span><span class="number">.207</span><span class="number">.156</span> &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    health <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span>:<span class="number">8080</span></span><br><span class="line">    &#125;</span><br><span class="line">in-addr.arpa:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . <span class="number">10.96</span><span class="number">.207</span><span class="number">.156</span> &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    &#125;</span><br><span class="line">ip6.arpa:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . <span class="number">10.96</span><span class="number">.207</span><span class="number">.156</span> &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    &#125;</span><br><span class="line">.:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    forward . /etc/resolv.conf &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9253</span></span><br><span class="line">    &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>分析上面的 LocalDNS 的配置信息，其中 <code>10.96.0.10</code> 为 CoreDNS 的 Service ClusterIP，<code>169.254.20.10</code> 为 LocalDNS 的 IP 地址，<code>10.96.207.156</code> 是 LocalDNS 新建的一个 Service ClusterIP，该 Service 和 CoreDNS 一样都是关联以前的 CoreDNS 的 Endpoints 列表。</p>
<p>仔细观察可以发现 <code>cluster.local</code>、<code>in-addr.arpa</code> 以及 <code>ip6.arpa</code> 都会通过 forward 转发到 <code>10.96.207.156</code>，也就是去 CoreDNS 解析，其他的则是 <strong><code>forward . /etc/resolv.conf</code></strong> 通过 <code>resolv.conf</code> 文件去解析，该文件的内容如下所示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 169.254.20.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure></p>
<p>所以当我们解析域名 <code>git.k8s.local</code> 的时候需要走一遍搜索域，而 <code>k8s.local</code> 不在 <code>cluster.local</code>、<code>in-addr.arpa</code> 以及 <code>ip6.arpa</code> 这些域中，所以就会走到 <code>/etc/resolv.conf</code> 去解析。这样就会导致 <code>git.k8s.local</code> 无法进行解析。这个时候我们需要把 <code>forward . /etc/resolv.conf</code> 更改成 <code>forward . 10.96.207.156</code>，这样就会去 CoreDNS 解析了，在 NodeLocalDNS 的 ConfigMap 中做如下的修改即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit cm node-local-dns -n kube-system</span><br><span class="line">......</span><br><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    <span class="built_in">bind</span> 169.254.20.10 10.96.0.10</span><br><span class="line">    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">&#125;</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>同样修改完成后，需要重建 NodeLocalDNS 的 Pod 才会生效。</p>
<blockquote>
<p><code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code> 这两个参数在镜像 1.15.6 版本以上中会自动进行配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的 Upstream Server 地址。</p>
</blockquote>
<p>参考：明阳的博客</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 性能优化实战]]></title>
      <url>http://team.jiunile.com/blog/2020/05/go-performance.html</url>
      <content type="html"><![CDATA[<h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><p>网关服务作为统一接入服务，是大部分服务的统一入口。为了避免成功瓶颈，需要对其进行尽可能地优化。因此，特别总结一下 golang 后台服务性能优化的方式，并对网关服务进行优化。</p>
<p>技术背景：</p>
<ul>
<li>基于 tarsgo 框架的 http 接入服务，下游服务使用 tarsgo 协议进行交互</li>
</ul>
<h2 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h2><p>网关服务本身没有业务逻辑处理，仅作为统一入口进行请求转发，因此我们主要关注下列指标</p>
<ul>
<li>吞吐量：每秒钟可以处理的请求数</li>
<li>响应时间：从客户端发出请求，到收到回包的总耗时</li>
</ul>
<h2 id="定位瓶颈"><a href="#定位瓶颈" class="headerlink" title="定位瓶颈"></a>定位瓶颈</h2><p>一般后台服务的瓶颈主要为 CPU，内存，IO 操作中的一个或多个。若这三者的负载都不高，但系统吞吐量低，基本就是代码逻辑出问题了。</p>
<p>在代码正常运行的情况下，我们要针对某个方面的高负载进行优化，才能提高系统的性能。golang 可通过 benchmark 加 pprof 来定位具体的性能瓶颈。</p>
<a id="more"></a>
<h3 id="benchmark-简介"><a href="#benchmark-简介" class="headerlink" title="benchmark 简介"></a>benchmark 简介</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -v gate_test.go -run=none -bench=. -benchtime=3s -cpuprofile cpu.prof -memprofile mem.prof</span><br></pre></td></tr></table></figure>
<ul>
<li>-run 知道单次测试，一般用于代码逻辑验证</li>
<li>-bench=. 执行所有 Benchmark，也可以通过用例函数名来指定部分测试用例</li>
<li>-benchtime 指定测试执行时长</li>
<li>-cpuprofile 输出 cpu 的 pprof 信息文件</li>
<li>-memprofile 输出 heap 的 pprof 信息文件。</li>
<li>-blockprofile 阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置</li>
<li>-mutexprofile 互斥锁分析，报告互斥锁的竞争情况</li>
</ul>
<p><strong>benchmark 测试用例常用函数</strong></p>
<ul>
<li>b.ReportAllocs() 输出单次循环使用的内存数量和对象 allocs 信息</li>
<li>b.RunParallel() 使用协程并发测试</li>
<li>b.SetBytes(n int64) 设置单次循环使用的内存数量</li>
</ul>
<h3 id="pprof-简介"><a href="#pprof-简介" class="headerlink" title="pprof 简介"></a>pprof 简介</h3><p><strong>生成方式</strong></p>
<ul>
<li><code>runtime/pprof</code>: 手动调用如<code>runtime.StartCPUProfile</code>或者<code>runtime.StopCPUProfile</code>等 API 来生成和写入采样文件，灵活性高。主要用于本地测试。</li>
<li><code>net/http/pprof</code>: 通过 http 服务获取 Profile 采样文件，简单易用，适用于对应用程序的整体监控。通过 runtime/pprof 实现。主要用于服务器端测试。</li>
<li><code>go test</code>: 通过 <code>go test -bench . -cpuprofile cpuprofile.out</code> 生成采样文件，主要用于本地基准测试。可用于重点测试某些函数。</li>
</ul>
<p><strong>查看方式</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof [options][binary] ...</span><br></pre></td></tr></table></figure></p>
<ul>
<li>–text 纯文本</li>
<li>–web 生成 svg 并用浏览器打开（如果 svg 的默认打开方式是浏览器)</li>
<li>–svg 只生成 svg</li>
<li>–list funcname 筛选出正则匹配 funcname 的函数的信息</li>
<li>-http=”:port” 直接本地浏览器打开 profile 查看（包括 top，graph，火焰图等）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof -base profile1 profile2</span><br></pre></td></tr></table></figure>
<p>对比查看 2 个 profile，一般用于代码修改前后对比，定位差异点。</p>
<p>通过命令行方式查看 profile 时，可以在命令行对话中，使用下列命令，查看相关信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flat flat%   sum%        cum   cum%</span><br><span class="line">5.95s 27.56% 27.56%      5.95s 27.56%  runtime.usleep</span><br><span class="line">4.97s 23.02% 50.58%      5.08s 23.53%  sync.(*RWMutex).RLock</span><br><span class="line">4.46s 20.66% 71.24%      4.46s 20.66%  sync.(*RWMutex).RUnlock</span><br><span class="line">2.69s 12.46% 83.70%      2.69s 12.46%  runtime.pthread_cond_<span class="built_in">wait</span></span><br><span class="line">1.50s  6.95% 90.64%      1.50s  6.95%  runtime.pthread_cond_signal</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>lat</code>: 采样时，该函数正在运行的次数*采样频率(10ms)，即得到估算的函数运行”采样时间”。这里不包括函数等待子函数返回。</li>
<li><code>flat%</code>: flat / 总采样时间值</li>
<li><code>sum%</code>: 前面所有行的 flat% 的累加值，如第三行 sum% = 71.24% = 27.56% + 50.58%</li>
<li><code>cum</code>: 采样时，该函数出现在调用堆栈的采样时间，包括函数等待子函数返回。因此 flat &lt;= cum</li>
<li><code>cum%</code>: cum / 总采样时间值</li>
</ul>
<p><code>topN [-cum]</code> 查看前 N 个数据：</p>
<p><code>list ncname</code> 查看某个函数的详细信息，可以明确具体的资源（cpu，内存等）是由哪一行触发的。</p>
<h3 id="pprof-接入"><a href="#pprof-接入" class="headerlink" title="pprof 接入"></a>pprof 接入</h3><p>服务中 main 方法插入代码<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cfg := tars.GetServerConfig()</span><br><span class="line">profMux := &amp;tars.TarsHttpMux&#123;&#125;</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/"</span>, pprof.Index)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/cmdline"</span>, pprof.Cmdline)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/profile"</span>, pprof.Profile)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/symbol"</span>, pprof.Symbol)</span><br><span class="line">profMux.HandleFunc(<span class="string">"/debug/pprof/trace"</span>, pprof.Trace)</span><br><span class="line">tars.AddHttpServant(profMux, cfg.App+<span class="string">"."</span>+cfg.Server+<span class="string">".ProfObj"</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="查看服务的-pprof"><a href="#查看服务的-pprof" class="headerlink" title="查看服务的 pprof"></a>查看服务的 pprof</h3><ul>
<li>保证开发机能直接访问到节点部署的 ip 和 port。</li>
<li>查看 profile(http 地址中的 ip,port 为 ProfObj 的 ip 和 port)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载cpu profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/profile?seconds=120 <span class="comment"># 等待120s，不带此参数时等待30s</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载heap profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/heap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载goroutine profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/goroutine</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载block profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/block</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载mutex profile</span></span><br><span class="line">go tool pprof http://ip:port/debug/pprof/mutex</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载20秒的trace记录（遇到棘手问题时，查看trace会比较容易定位)</span></span><br><span class="line"> curl http://100.97.1.35:10078/debug/pprof/trace?seconds=20 &gt; trace.out</span><br><span class="line"> go tool trace trace.out 查看</span><br></pre></td></tr></table></figure>
<ul>
<li>直接在终端中通过 pprof 命令查看</li>
<li>sz 上面命令执行时出现的<code>Saved profile in /root/pprof/pprof.binary.alloc_objects.xxxxxxx.xxxx.pb.gz</code>到本地 </li>
<li>在本地环境，执行<code>go tool pprof -http=&quot;:8081&quot; pprof.binary.alloc_objects.xxxxxxx.xxxx.pb.gz</code> 即可直接通过<code>http://localhost:8081</code>页面查看。包括topN，火焰图信息等,会更方便一点。</li>
</ul>
<h3 id="GC-Trace"><a href="#GC-Trace" class="headerlink" title="GC Trace"></a>GC Trace</h3><p>golang 具备 GC 功能，而 GC 是最容易被忽视的性能影响因素。尤其是在本地使用 benchmark 测试时，由于时间较短，占用内存较少。往往不会触发 GC。而一旦线上出现 GC 问题，又不太好定位。目前常用的定位方式有两种：</p>
<h4 id="本地-gctrace"><a href="#本地-gctrace" class="headerlink" title="本地 gctrace"></a>本地 gctrace</h4><p>在执行程序前加 <code>GODEBUG=gctrace=1</code>，每次 gc 时会输出一行如下内容<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gc 1 @0.001s 11%: 0.007+1.5+0.004 ms clock, 0.089+1.5/2.8/0.27+0.054 ms cpu, 4-&gt;4-&gt;3 MB, 5 MB goal, 12 P</span><br><span class="line">scvg: inuse: 4, idle: 57, sys: 62, released: 57, consumed: 4 (MB)</span><br></pre></td></tr></table></figure></p>
<p>也通过日志转为图形化：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GODEBUG=gctrace=1 godoc -index -http=:6060 2&gt; stderr.log</span><br><span class="line">cat stderr.log | gcvis</span><br></pre></td></tr></table></figure></p>
<ul>
<li>inuse：使用了多少 M 内存</li>
<li>idle：剩下要清除的内存</li>
<li>sys：系统映射的内存</li>
<li>released：释放的系统内存</li>
<li>consumed：申请的系统内存</li>
<li>gc 1 表示第 1 次 gc</li>
<li>@0.001s 表示程序执行的总时间</li>
<li>11% 表示垃圾回收时间占用总的运行时间百分比</li>
<li>0.007+1.5+0.004 ms clock 表示工作线程完成 GC 的 stop-the-world,sweeping,marking 和 waiting 的时间</li>
<li>0.089+1.5/2.8/0.27+0.054 ms cpu 垃圾回收占用 cpu 时间</li>
<li>4-&gt;4-&gt;3 MB 表示堆的大小，gc 后堆的大小，存活堆的大小</li>
<li>5 MB goal 整体堆的大小</li>
<li>12 P 使用的处理器数量</li>
<li>scvg: inuse: 4, idle: 57, sys: 62, released: 57, consumed: 4 (MB) 表示系统内存回收信息</li>
<li>采用图形化的方式查看：<a href="https://github.com/davecheney/gcvis" target="_blank" rel="external">https://github.com/davecheney/gcvis</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GODEBUG=gctrace=1 go <span class="built_in">test</span> -v *.go -bench=. -run=none -benchtime 3m |&amp; gcvis</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="线上-trace"><a href="#线上-trace" class="headerlink" title="线上 trace"></a>线上 trace</h4><p>在线上业务中添加<strong>net/http/pprof</strong>后，可通过下列命令采集 20 秒的 trace 信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://ip:port/debug/pprof/trace?seconds=20 &gt; trace.out</span><br></pre></td></tr></table></figure></p>
<p>再通过<code>go tool trace trace.out</code> 即可在本地浏览器中查看 trace 信息。<br><img src="/images/go/performance_1.png" alt="go performance analysis"></p>
<ul>
<li>View trace：查看跟踪</li>
<li>Goroutine analysis：Goroutine 分析</li>
<li>Network blocking profile：网络阻塞概况</li>
<li>Synchronization blocking profile：同步阻塞概况</li>
<li>Syscall blocking profile：系统调用阻塞概况</li>
<li>Scheduler latency profile：调度延迟概况</li>
<li>User defined tasks：用户自定义任务</li>
<li>User defined regions：用户自定义区域</li>
<li>Minimum mutator utilization：最低 Mutator 利用率</li>
</ul>
<p>GC 相关的信息可以在 View trace 中看到<br><img src="/images/go/performance_2.png" alt="go performance analysis"></p>
<p>可通过点击 heap 的色块区域，查看 heap 信息。<br><img src="/images/go/performance_3.png" alt="go performance analysis"></p>
<p>点击 GC 对应行的蓝色色块，查看 GC 耗时及相关回收信息。<br><img src="/images/go/performance_4.png" alt="go performance analysis"></p>
<p>通过这两个信息就可以确认是否存在 GC 问题，以及造成高 GC 的可能原因。</p>
<h4 id="使用问题"><a href="#使用问题" class="headerlink" title="使用问题"></a>使用问题</h4><p>trace 的展示仅支持 chrome 浏览器。但是目前常用的 chrome 浏览器屏蔽了 go tool trace 使用的 HTML import 功能。即打开“view trace”时，会出现一片空白。并可以在 console 中看到警告信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HTML Imports is deprecated and has now been removed as of M80. See https://www.chromestatus.com/features/5144752345317376 and https://developers.google.com/web/updates/2019/07/web-components-time-to-upgrade <span class="keyword">for</span> more details.</span><br></pre></td></tr></table></figure></p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p><strong>申请 token</strong></p>
<ul>
<li><a href="https://developers.chrome.com/origintrials/#/register_trial/2431943798780067841" target="_blank" rel="external">https://developers.chrome.com/origintrials/#/register_trial/2431943798780067841</a> 然后登录</li>
<li>web origin 处填写 <a href="http://localhost:8001" target="_blank" rel="external">http://localhost:8001</a> 端口只能是 8000 - 8003，支持 http 和 https。（也可以填写 127.0.0.1:8001,依赖于你浏览器中显示的地址，否则对不上的话，还要手动改一下)<br><img src="/images/go/performance_5.png" alt="go performance analysis"></li>
<li>点击注册后即可看到 token</li>
</ul>
<p><strong>修改 trace.go</strong><br>编辑<code>${GOROOT}/src/cmd/trace/trace.go</code> 文件，在文件中找到 templTrace 然后在  标签的下一行添加<code>&lt;meta http-equiv=&quot;origin-trial&quot; content=&quot;你复制的token&quot;&gt;</code></p>
<p><strong>重新编译 go</strong></p>
<ul>
<li>${GOROOT}/src 目录，执行 ./all.bash</li>
<li>若提示：<code>ERROR: Cannot find go1.4\bin\go Set GOROOT_BOOTSTRAP to a working Go tree &gt;= Go 1.4</code> 则需要先安装一个 go1.4 的版本，再通过它来编译 go。（下载链接<a href="https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz）" target="_blank" rel="external">https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz）</a> 在 <code>go1.4/src</code> 下执行 <code>./make.bash</code> . 指定 GOROOT_BOOTSTRAP 为 go1.4 的根目录。然后就可以重新编译 go</li>
</ul>
<p><strong>查看 trace</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool trace -http=localhost:8001 trace.out</span><br></pre></td></tr></table></figure></p>
<p>若打开 view trace 还是空白,则检查一下浏览器地址栏中的地址，是否与注册时的一样。即注册用的 localhost 或 127.0.0.1 则地址栏中也要一样。</p>
<h2 id="常见性能瓶颈"><a href="#常见性能瓶颈" class="headerlink" title="常见性能瓶颈"></a>常见性能瓶颈</h2><h3 id="业务逻辑"><a href="#业务逻辑" class="headerlink" title="业务逻辑"></a>业务逻辑</h3><p>出现无效甚至降低性能的逻辑。常见的有：</p>
<ul>
<li>逻辑重复：相同的操作在不同的位置做了多次或循环跳出的条件设置不当。</li>
<li>资源未复用：内存频繁申请和释放，数据库链接频繁建立和销毁等。</li>
<li>无效代码。</li>
</ul>
<h3 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h3><p>未选择恰当的存储方式，常见的有：</p>
<ul>
<li>临时数据存放到数据库中，导致频繁读写数据库。</li>
<li>将复杂的树状结构的数据用 SQL 数据库存储，出现大量冗余列，并且在读写时要进行拆解和拼接。</li>
<li>数据库表设计不当，无法有效利用索引查询，导致查询操作耗时高甚至出现大量慢查询。</li>
<li>热点数据未使用缓存，导致数据库负载过高，响应速度下降。</li>
</ul>
<h3 id="并发处理"><a href="#并发处理" class="headerlink" title="并发处理"></a>并发处理</h3><p>并发操作的问题主要出现在资源竞争上，常见的有：</p>
<ul>
<li>死锁/活锁导致大量阻塞，性能严重下降。</li>
<li>资源竞争激烈：大量的线程或协程抢夺一个锁。</li>
<li>临界区过大：将不必要的操作也放入临界区，导致锁的释放速度过慢，引起其他线程或协程阻塞。</li>
</ul>
<h2 id="golang-部分细节简介"><a href="#golang-部分细节简介" class="headerlink" title="golang 部分细节简介"></a>golang 部分细节简介</h2><p>在优化之前，我们需要对 golang 的实现细节有一个简单的了解，才能明白哪些地方有问题，哪些地方可以优化，以及怎么优化。以下内容的详细讲解建议查阅网上优秀的 blog。对语言的底层实现机制最好有个基本的了解，否则有时候掉到坑里都不知道为啥。</p>
<h3 id="协程调度"><a href="#协程调度" class="headerlink" title="协程调度"></a>协程调度</h3><p>Golang 调度是非抢占式多任务处理，由协程主动交出控制权。遇到如下条件时，才有可能交出控制权</p>
<ul>
<li>I/O,select</li>
<li>channel</li>
<li>等待锁</li>
<li>函数调用（是一个切换的机会，是否会切换由调度器决定）</li>
<li>runtime.Gosched()</li>
</ul>
<p>因此，若存在较长时间的 for 循环处理，并且循环内没有上述逻辑时，会阻塞住其他的协程调度。在实际编码中一定要注意。</p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>Go 为每个逻辑处理器（P）提供了一个称为mcache的本地内存线程缓存。每个 mcache 中持有 67 个级别的 mspan。每个 msapn 又包含两种：scan（包含指针的对象）和 noscan（不包含指针的对象）。<strong>在进行垃圾收集时，GC 无需遍历 noscan 对象</strong>。<br><img src="/images/go/performance_6.png" alt="go performance analysis"></p>
<h3 id="GC-处理"><a href="#GC-处理" class="headerlink" title="GC 处理"></a>GC 处理</h3><p>GC 的工作就是确定哪些内存可以释放，它是通过扫描内存查找内存分配的指针来完成这个工作的。GC 触发时机：</p>
<ul>
<li>到达堆阈值：默认情况下，它将在堆大小加倍时运行，可通过 GOGC 来设定更高阈值（不建议变更此配置）</li>
<li>到达时间阈值：每两分钟会强制启动一次 GC 循环</li>
</ul>
<p>为啥要注意 GC，是因为 GC 时出现 2 次 Stop the world，即停止所有协程，进行扫描操作。若是 GC 耗时高，则会严重影响服务器性能。<br><img src="/images/go/performance_7.png" alt="go performance analysis"></p>
<h3 id="变量逃逸"><a href="#变量逃逸" class="headerlink" title="变量逃逸"></a>变量逃逸</h3><blockquote>
<p>注意，golang 中的栈是跟函数绑定的，函数结束时栈被回收。</p>
</blockquote>
<p><strong>变量内存回收：</strong></p>
<ul>
<li>如果分配在栈中，则函数执行结束可自动将内存回收；</li>
<li>如果分配在堆中，则函数执行结束可交给 GC（垃圾回收）处理；</li>
</ul>
<p>而变量逃逸就意味着增加了堆中的对象个数，影响 GC 耗时。一般要尽量避免逃逸。</p>
<p><strong>逃逸分析不变性：</strong></p>
<ul>
<li>指向栈对象的指针不能存在于堆中；</li>
<li>指向栈对象的指针不能在栈对象回收后存活；</li>
</ul>
<p>在逃逸分析过程中，凡是发现出现违反上述约定的变量，就将其移到堆中。</p>
<p><strong>逃逸常见的情况：</strong></p>
<ul>
<li>指针逃逸：返回局部变量的地址（不变性 2）</li>
<li>栈空间不足</li>
<li>动态类型逃逸：如 fmt.Sprintf,json.Marshel 等接受变量为…interface{}函数的调用，会导致传入的变量逃逸。</li>
<li>闭包引用</li>
</ul>
<h3 id="包含指针类型的底层结构"><a href="#包含指针类型的底层结构" class="headerlink" title="包含指针类型的底层结构"></a>包含指针类型的底层结构</h3><p><strong>string</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> StringHeader <span class="keyword">struct</span> &#123;</span><br><span class="line"> Data <span class="keyword">uintptr</span></span><br><span class="line"> Len  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>slice</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> SliceHeader <span class="keyword">struct</span> &#123;</span><br><span class="line"> Data <span class="keyword">uintptr</span></span><br><span class="line"> Len  <span class="keyword">int</span></span><br><span class="line"> Cap  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>map</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> hmap <span class="keyword">struct</span> &#123;</span><br><span class="line"> count     <span class="keyword">int</span></span><br><span class="line"> flags     <span class="keyword">uint8</span></span><br><span class="line"> B         <span class="keyword">uint8</span></span><br><span class="line"> noverflow <span class="keyword">uint16</span></span><br><span class="line"> hash0     <span class="keyword">uint32</span></span><br><span class="line"> buckets    unsafe.Pointer</span><br><span class="line"> oldbuckets unsafe.Pointer</span><br><span class="line"> nevacuate  <span class="keyword">uintptr</span></span><br><span class="line"> extra *mapextra</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这些是常见会包含指针的对象。尤其是 string，在后台应用中大量出现。并经常会作为 map 的 key 或 value。若数据量较大时，就会引发 GC 耗时上升。同时，我们可以注意到 string 和 slice 非常相似，从某种意义上说它们之间是可以直接互相转换的。这就可以避免 string 和[]byte 之间类型转换时，进行内存拷贝</p>
<p><strong>类型转换优化</strong><br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">String</span><span class="params">(b []<span class="keyword">byte</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"> <span class="keyword">return</span> *(*<span class="keyword">string</span>)(unsafe.Pointer(&amp;b))</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Str2Bytes</span><span class="params">(s <span class="keyword">string</span>)</span> []<span class="title">byte</span></span> &#123;</span><br><span class="line"> x := (*[<span class="number">2</span>]<span class="keyword">uintptr</span>)(unsafe.Pointer(&amp;s))</span><br><span class="line"> h := [<span class="number">3</span>]<span class="keyword">uintptr</span>&#123;x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">1</span>]&#125;</span><br><span class="line"> <span class="keyword">return</span> *(*[]<span class="keyword">byte</span>)(unsafe.Pointer(&amp;h))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="性能测试方式"><a href="#性能测试方式" class="headerlink" title="性能测试方式"></a>性能测试方式</h2><h3 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h3><p>将服务处理的核心逻辑，使用 go test 的 benchmark 加 pprof 来测试。建议上线前，就对整个业务逻辑的性能进行测试，提前优化瓶颈。</p>
<h3 id="线上测试"><a href="#线上测试" class="headerlink" title="线上测试"></a>线上测试</h3><p>一般 http 服务可以通过常见的测试工具进行压测，如 wrk，locust 等。taf 服务则需要我们自己编写一些测试脚本。同时，要注意的是，压测的目的是定位出服务的最佳性能，而不是盲目的高并发请求测试。因此，一般需要逐步提升并发请求数量，来定位出服务的最佳性能点。</p>
<blockquote>
<p>注意：由于 taf 平台具备扩容功能，因此为了更准确的测试，我们应该在测试前关闭要测试节点的自动扩容。</p>
</blockquote>
<h2 id="实际项目优化"><a href="#实际项目优化" class="headerlink" title="实际项目优化"></a>实际项目优化</h2><p>为了避免影响后端服务，也为了避免后端服务影响网关自身。因此，我们需要在压测前，将对后端服务的调用屏蔽。</p>
<ul>
<li>测试准备：屏蔽远程调用：下游服务调用，健康度上报，统计上报，远程日志。以便关注网关自身性能。<h3 id="QPS-现状"><a href="#QPS-现状" class="headerlink" title="QPS 现状"></a>QPS 现状</h3>首先看下当前业务的性能指标，使用 wrk 压测网关服务<br><img src="/images/go/performance_8.png" alt="go performance analysis"><br>可以看出，在总链接数为 70 的时候，QPS 最高，为 13245。<h3 id="火焰图"><a href="#火焰图" class="headerlink" title="火焰图"></a>火焰图</h3><img src="/images/go/performance_9.png" alt="go performance analysis"><br>根据火焰图我们定位出 cpu 占比较高的几个方法为：</li>
<li>json.Marshal</li>
<li>json.Unmarshal</li>
<li>rogger.Infof</li>
</ul>
<p>为了方便测试，将代码改为本地运行，并通过 benchmark 的方式来对比修改前后的差异。</p>
<blockquote>
<p>由于正式环境使用的 golang 版本为 1.12，因此本地测试时，也要使用同样的版本。</p>
</blockquote>
<h3 id="benchmark"><a href="#benchmark" class="headerlink" title="benchmark"></a>benchmark</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	50000000	      3669 ns/op	    4601 B/op	      73 allocs/op</span><br></pre></td></tr></table></figure>
<p>查看 cpu 和 memory 的 profile，发现健康度上报的数据结构填充占比较高。这部分逻辑基于 tars 框架实现。暂时忽略，为避免影响其他测试，先注释掉。再看看 benchmark。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	  500000	      3146 ns/op	    2069 B/op	      55 allocs/op</span><br></pre></td></tr></table></figure></p>
<h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><h3 id="JSON-优化"><a href="#JSON-优化" class="headerlink" title="JSON 优化"></a>JSON 优化</h3><p>先查看 json 解析的部分，看看是否有优化空间</p>
<h4 id="请求处理"><a href="#请求处理" class="headerlink" title="请求处理"></a>请求处理</h4><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RootHandle view.ReadReq2Json readJsonReq 中进行json解析</span></span><br><span class="line"><span class="keyword">type</span> GatewayReqBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayReqBodyHeader   <span class="string">`json:"header"`</span></span><br><span class="line"> Payload <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> dataMap := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line"> err := jsoniter.Unmarshal(data, &amp;dataMap)</span><br><span class="line"> ...</span><br><span class="line">  headerMap, ok := header.(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line">  businessName, ok := headerMap[<span class="string">"businessName"</span>]</span><br><span class="line">  qua, ok := headerMap[<span class="string">"qua"</span>]</span><br><span class="line">  sessionId, ok := headerMap[<span class="string">"sessionId"</span>]</span><br><span class="line">  ...</span><br><span class="line">  payload, ok := dataMap[<span class="string">"payload"</span>]</span><br><span class="line">  req.Payload, ok = payload.(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数本质上将 data 解析为 model.GatewayReqBody 类型的结构体。但是这里却存在 2 个问题</p>
<ol>
<li>使用了复杂的解析方式，先将 data 解析为 map，再通过每个字段的名字来取值，并进行类型转换。</li>
<li>Req.Playload 解析为一个 map。但又未使用。我们看看后面这个 payload 是用来做啥。确认是否为无效代码。</li>
</ol>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">invokeTafServant</span><span class="params">(resp http.ResponseWriter, gatewayHttpReq *model.GatewayHttpReq)</span></span> &#123;</span><br><span class="line"> ...</span><br><span class="line">  payloadBytes, err := json.Marshal(gatewayHttpReq.ReqBody.Payload)</span><br><span class="line"> <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">  commonReq.Payload = <span class="keyword">string</span>(payloadBytes)</span><br><span class="line"> &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  responseData(gatewayHttpReq, StatusInternalServerError, <span class="string">"封装json异常"</span>, <span class="string">""</span>, resp)</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line"> &#125;</span><br><span class="line">  ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>后续的使用中，我们可以看到，又将这个 payload 转为 string。因此，我们可以确定，上面的 json 解析是没有意义，同时也会浪费资源（payload 数据量一般不小）。</p>
<h4 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h4><ul>
<li>golang 自带的 json 解析性能较低，这里我们可以替换为github.com/json-iterator来提升性能</li>
<li>在 golang 中，遇到不需要解析的 json 数据，可以将其类型声明为json.RawMessage. 即，可以将上述 2 个方法优化为</li>
</ul>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> GatewayReqBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayReqBodyHeader <span class="string">`json:"header"`</span></span><br><span class="line"> Payload json.RawMessage      <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> err := jsoniter.Unmarshal(data, req)</span><br><span class="line"> <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> jsonParseErr</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">for</span> k, v := <span class="keyword">range</span> req.Header.Qua &#123;</span><br><span class="line">  req.Header.Qua[k] = v</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(req.Header.QuaStr) == <span class="number">0</span> &#123;</span><br><span class="line">   req.Header.QuaStr = k + <span class="string">"="</span> + v</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   req.Header.QuaStr += <span class="string">"&amp;"</span> + k + <span class="string">"="</span> + v</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">invokeTafServant</span><span class="params">(resp http.ResponseWriter, gatewayHttpReq *model.GatewayHttpReq)</span></span> &#123;</span><br><span class="line"> commonReq.Payload = <span class="keyword">string</span>(gatewayHttpReq.ReqBody.Payload)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>这里注意！出现了 string 和[]byte 之间的类型转换.为了避免内存拷贝，这里将 string()改为上面的类型转换优化中所定义的转换函数，即 <code>commonReq.Payload = encode.String(gatewayHttpReq.ReqBody.Payload)</code></li>
</ul>
<h4 id="回包处理"><a href="#回包处理" class="headerlink" title="回包处理"></a>回包处理</h4><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> GatewayRespBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayRespBodyHeader  <span class="string">`json:"header"`</span></span><br><span class="line"> Payload <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">responseData</span><span class="params">(gatewayReq *model.GatewayHttpReq, code <span class="keyword">int32</span>, message <span class="keyword">string</span>, payload <span class="keyword">string</span>, resp http.ResponseWriter)</span></span> &#123;</span><br><span class="line"> jsonPayload := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> <span class="built_in">len</span>(payload) != <span class="number">0</span> &#123;</span><br><span class="line">  err := json.Unmarshal([]<span class="keyword">byte</span>(payload), &amp;jsonPayload)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">   ...</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> body := model.GatewayRespBody&#123;</span><br><span class="line">  Header: model.GatewayRespBodyHeader&#123;</span><br><span class="line">   Code:    code,</span><br><span class="line">   Message: message,</span><br><span class="line">  &#125;,</span><br><span class="line">  Payload: jsonPayload,</span><br><span class="line"> &#125;</span><br><span class="line">  data, err := view.RenderResp(<span class="string">"json"</span>, &amp;body)</span><br><span class="line">  ...</span><br><span class="line">  resp.WriteHeader(http.StatusOK)</span><br><span class="line"> resp.Write(data)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样的，这里的 jsonPayload，也是出现了不必要的 json 解析。我们可以改为<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> GatewayRespBody <span class="keyword">struct</span> &#123;</span><br><span class="line"> Header  GatewayRespBodyHeader  <span class="string">`json:"header"`</span></span><br><span class="line"> Payload json.RawMessage <span class="string">`json:"payload"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">body := model.GatewayRespBody&#123;</span><br><span class="line">  Header: model.GatewayRespBodyHeader&#123;</span><br><span class="line">   Code:    code,</span><br><span class="line">   Message: message,</span><br><span class="line">  &#125;,</span><br><span class="line">  Payload: encode.Str2Bytes(payload),</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<p>然后在 view.RenderResp 方法中<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RenderResp</span><span class="params">(format <span class="keyword">string</span>, resp <span class="keyword">interface</span>&#123;&#125;)</span> <span class="params">([]<span class="keyword">byte</span>, error)</span></span> &#123;</span><br><span class="line"> <span class="keyword">if</span> <span class="string">"json"</span> == format &#123;</span><br><span class="line">  <span class="keyword">return</span> jsoniter.Marshal(resp)</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">"format error"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="benchmark-1"><a href="#benchmark-1" class="headerlink" title="benchmark"></a>benchmark</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	  500000	      3326 ns/op	    2842 B/op	      50 allocs/op</span><br></pre></td></tr></table></figure>
<p>虽然对象 alloc 减少了，但单次操作内存使用增加了，且性能下降了。这就有点奇怪了。我们来对比一下 2 个情况下的 pprof。</p>
<h3 id="逃逸分析及处理"><a href="#逃逸分析及处理" class="headerlink" title="逃逸分析及处理"></a>逃逸分析及处理</h3><h4 id="go-tool-pprof-base"><a href="#go-tool-pprof-base" class="headerlink" title="go tool pprof -base"></a>go tool pprof -base</h4><ul>
<li>cpu 差异</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flat  flat%   sum%        cum   cum%</span><br><span class="line"> 0.09s  1.17%  1.17%      0.40s  5.20%  runtime.mallocgc</span><br><span class="line"> 0.01s  0.13%  1.30%      0.35s  4.55%  /vendor/github.com/json-iterator/go.(*Iterator).readObjectStart</span><br><span class="line"> 0      0%     1.30%      0.35s  4.55%  /vendor/github.com/json-iterator/go.(*twoFieldsStructDecoder).Decode</span><br></pre></td></tr></table></figure>
<ul>
<li>mem 差异</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    flat  flat%   sum%        cum   cum%</span><br><span class="line">478.96MB 20.33% 20.33%   279.94MB 11.88%  gateway.RootHandle</span><br><span class="line">       0     0% 20.33%   279.94MB 11.88%  <span class="built_in">command</span>-line-arguments.BenchmarkTestHttp.func1</span><br><span class="line">       0     0% 20.33%   279.94MB 11.88%  testing.(*B).RunParallel.func1</span><br></pre></td></tr></table></figure>
<p>可以看出 RootHandle 多了 478.96M 的内存使用。通过 list RootHandle 对比 2 个情况下的内存使用。发现修改后的 RootHandle 中多出了这一行：<code>475.46MB 475.46MB 158: gatewayHttpReq := model.GatewayHttpReq{}</code> 这一般意味着变量 gatewayHttpReq 出现了逃逸。</p>
<ul>
<li>go build -gcflags “-m -m” gateway/*.go</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gateway/logic.go:270:26: &amp;gatewayHttpReq escapes to heap</span><br></pre></td></tr></table></figure>
<p>可以看到确实出现了逃逸。这个对应的代码为<code>err = view.ReadReq2Json(&amp;gatewayHttpReq)</code>,而造成逃逸的本质是因为上面改动了函数 readJsonReq（动态类型逃逸，即函数参数为 interface 类型，无法在编译时确定具体类型的）<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> err := jsoniter.Unmarshal(data, req)</span><br><span class="line"> ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>因此，这里需要特殊处理一下，改为<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readJsonReq</span><span class="params">(data []<span class="keyword">byte</span>, req *model.GatewayReqBody)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> tmp model.GatewayReqBody</span><br><span class="line">	err := jsoniter.Unmarshal(data, &amp;tmp)</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="benchmark-2"><a href="#benchmark-2" class="headerlink" title="benchmark"></a>benchmark</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	  500000	      2994 ns/op	    1892 B/op	      50 allocs/op</span><br></pre></td></tr></table></figure>
<p>可以看到堆内存使用明显下降。性能也提升了。再看一下 pprof，寻找下个瓶颈。</p>
<h4 id="cpu-profile"><a href="#cpu-profile" class="headerlink" title="cpu profile"></a>cpu profile</h4><p><img src="/images/go/performance_10.png" alt="go performance analysis"><br>抛开 responeseData(他内部主要是日志打印占比高），占比较高的为 util.GenerateSessionId，先来看看这个怎么优化。</p>
<h3 id="随机字符串生成"><a href="#随机字符串生成" class="headerlink" title="随机字符串生成"></a>随机字符串生成</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> letterRunes = []<span class="keyword">rune</span>(<span class="string">"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RandStringRunes</span><span class="params">(n <span class="keyword">int</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"> b := <span class="built_in">make</span>([]<span class="keyword">rune</span>, n)</span><br><span class="line"> <span class="keyword">for</span> i := <span class="keyword">range</span> b &#123;</span><br><span class="line">  b[i] = letterRunes[rand.Intn(<span class="built_in">len</span>(letterRunes))]</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="keyword">string</span>(b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目前的生成方式使用的类型是 rune，但其实用 byte 就够了。另外，letterRunes 是 62 个字符，即最大需要 6 位的 index 就可以遍历完成了。而随机数获取的是 63 位。即每个随机数，其实可以产生 10 个随机字符。而不用每个字符都获取一次随机数。所以我们改为<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line"> letterBytes   = <span class="string">"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></span><br><span class="line"> letterIdxBits = <span class="number">6</span></span><br><span class="line"> letterIdxMask = <span class="number">1</span>&lt;&lt;letterIdxBits - <span class="number">1</span></span><br><span class="line"> letterIdxMax  = <span class="number">63</span> / letterIdxBits</span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RandStringRunes</span><span class="params">(n <span class="keyword">int</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line"> b := <span class="built_in">make</span>([]<span class="keyword">byte</span>, n)</span><br><span class="line"> <span class="keyword">for</span> i, cache, remain := n<span class="number">-1</span>, rand.Int63(), letterIdxMax; i &gt;= <span class="number">0</span>; &#123;</span><br><span class="line">  <span class="keyword">if</span> remain == <span class="number">0</span> &#123;</span><br><span class="line">   cache, remain = rand.Int63(), letterIdxMax</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> idx := <span class="keyword">int</span>(cache &amp; letterIdxMask); idx &lt; <span class="built_in">len</span>(letterBytes) &#123;</span><br><span class="line">   b[i] = letterBytes[idx]</span><br><span class="line">   i--</span><br><span class="line">  &#125;</span><br><span class="line">  cache &gt;&gt;= letterIdxBits</span><br><span class="line">  remain--</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="keyword">string</span>(b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="benchmark-3"><a href="#benchmark-3" class="headerlink" title="benchmark"></a>benchmark</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Benchmark   	 1000000	      1487 ns/op	    1843 B/op	      50 allocs/op</span><br></pre></td></tr></table></figure>
<h3 id="类型转换及字符串拼接"><a href="#类型转换及字符串拼接" class="headerlink" title="类型转换及字符串拼接"></a>类型转换及字符串拼接</h3><p>一般情况下，都会说将 string 和[]byte 的转换改为 unsafe；以及在字符串拼接时，用 byte.Buffer 代替 fmt.Sprintf。但是网关这里的情况比较特殊，字符串的操作基本集中在打印日志的操作。而 tars 的日志打印本身就是通过 byte.Buffer 拼接的。所以这可以避免。另外，由于日志打印量大，使用 unsafe 转换[]byte 为 string 带来的收益，往往会因为逃逸从而影响 GC，反正会影响性能。因此，不同的场景下，不能简单的套用一些优化方法。需要通过压测及结果分析来判断具体的优化策略。</p>
<h2 id="优化结果"><a href="#优化结果" class="headerlink" title="优化结果"></a>优化结果</h2><p><img src="/images/go/performance_11.png" alt="go performance analysis"><br>可以看到优化后，最大链接数为 110，最高 QPS 为<strong>21153.35</strong>。对比之前的<strong>13245</strong>，大约提升 60%。</p>
<h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>从 pprof 中可以看到日志打印，远程日志，健康上报等信息占用较多 cpu 资源，且导致多个数据逃逸（尤其是日志打印）。过多的日志基本等于没有日志。后续可考虑裁剪日志，仅保留出错时的上下文信息。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>性能查看工具 pprof,trace 及压测工具 wrk 或其他压测工具的使用要比较了解。</li>
<li>代码逻辑层面的走读非常重要，要尽量避免无效逻辑。</li>
<li>对于 golang 自身库存在缺陷的，可以寻找第三方库或自己改造。</li>
<li>golang 版本尽量更新，这次的测试是在 golang1.12 下进行的。而 go1.13 甚至 go1.14 在很多地方进行了改进。比如 fmt.Sprintf，sync.Pool 等。替换成新版本应该能进一步提升性能。</li>
<li>本地 benchmark 结果不等于线上运行结果。尤其是在使用缓存来提高处理速度时，要考虑 GC 的影响。</li>
<li>传参数或返回值时，尽量按 golang 的设计哲学，少用指针，多用值对象，避免引起过多的变量逃逸，导致 GC 耗时暴涨。struct 的大小一般在 2K 以下的拷贝传值，比使用指针要快（可针对不同的机器压测，判断各自的阈值)。</li>
<li>值类型在满足需要的情况下，越小越好。能用 int8，就不要用 int64。</li>
<li>资源尽量复用,在 golang1.13 以上，可以考虑使用 sync.Pool 缓存会重复申请的内存或对象。或者自己使用并管理大块内存，用来存储小对象，避免 GC 影响（如本地缓存的场景)。</li>
</ul>
<p>推荐阅读: <a href="https://mp.weixin.qq.com/s?__biz=MzAxMTA4Njc0OQ==&amp;mid=2651439020&amp;idx=1&amp;sn=c2094f4dccb53385dc207958e7f42f9e&amp;chksm=80bb615eb7cce8481eb7a8f09d4a13e2974b3785c241dd31245647cd7540dde414d64f2b3719&amp;scene=21#wechat_redirect" target="_blank" rel="external">滴滴实战分享：通过 profiling 定位 golang 性能问题 - 内存篇</a></p>
<p>来源：trumanyan</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[调试golang的bug以及性能问题的实践方法]]></title>
      <url>http://team.jiunile.com/blog/2020/05/go-debug-practice.html</url>
      <content type="html"><![CDATA[<h2 id="场景1：-如何分析程序的运行时间与CPU利用率情况？"><a href="#场景1：-如何分析程序的运行时间与CPU利用率情况？" class="headerlink" title="场景1： 如何分析程序的运行时间与CPU利用率情况？"></a>场景1： 如何分析程序的运行时间与CPU利用率情况？</h2><h3 id="shell内置time指令"><a href="#shell内置time指令" class="headerlink" title="shell内置time指令"></a>shell内置time指令</h3><p>这个方法不算新颖，但是确很实用。 time是Unix/Linux内置多命令，使用时一般不用传过多参数，直接跟上需要调试多程序即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ time go run <span class="built_in">test</span>2.go </span><br><span class="line">&amp;&#123;&#123;0 0&#125; 张三 0&#125;</span><br><span class="line"></span><br><span class="line">real    0m0.843s</span><br><span class="line">user    0m0.216s</span><br><span class="line">sys 0m0.389s</span><br></pre></td></tr></table></figure></p>
<p>上面是使用time对 <code>go run test2.go</code> 对执行程序坐了性能分析，得到3个指标。</p>
<ul>
<li><code>real</code>：从程序开始到结束，实际度过的时间；</li>
<li><code>user</code>：程序在用户态度过的时间；</li>
<li><code>sys</code>：程序在内核态度过的时间</li>
</ul>
<p>一般情况下 <code>real</code> &gt;= <code>user</code> + <code>sys</code>，因为系统还有其它进程(切换其他进程中间对于本进程回有空白期)。</p>
<a id="more"></a>
<h3 id="usr-bin-time指令"><a href="#usr-bin-time指令" class="headerlink" title="/usr/bin/time指令"></a>/usr/bin/time指令</h3><p>这个指令比内置的time更加详细一些，使用的时候需要用绝对路径，而且要加上参数 <code>-v</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/bin/time -v go run <span class="built_in">test</span>2.go  </span><br><span class="line"></span><br><span class="line">    Command being timed: <span class="string">"go run test2.go"</span></span><br><span class="line">    User time (seconds): 0.12</span><br><span class="line">    System time (seconds): 0.06</span><br><span class="line">    Percent of CPU this job got: 115%</span><br><span class="line">    Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.16</span><br><span class="line">    Average shared text size (kbytes): 0</span><br><span class="line">    Average unshared data size (kbytes): 0</span><br><span class="line">    Average stack size (kbytes): 0</span><br><span class="line">    Average total size (kbytes): 0</span><br><span class="line">    Maximum resident <span class="built_in">set</span> size (kbytes): 41172</span><br><span class="line">    Average resident <span class="built_in">set</span> size (kbytes): 0</span><br><span class="line">    Major (requiring I/O) page faults: 1</span><br><span class="line">    Minor (reclaiming a frame) page faults: 15880</span><br><span class="line">    Voluntary context switches: 897</span><br><span class="line">    Involuntary context switches: 183</span><br><span class="line">    Swaps: 0</span><br><span class="line">    File system inputs: 256</span><br><span class="line">    File system outputs: 2664</span><br><span class="line">    Socket messages sent: 0</span><br><span class="line">    Socket messages received: 0</span><br><span class="line">    Signals delivered: 0</span><br><span class="line">    Page size (bytes): 4096</span><br><span class="line">    Exit status: 0</span><br></pre></td></tr></table></figure></p>
<p>可以看到这里的功能要强大多了，除了之前的信息外，还包括了：</p>
<ul>
<li>CPU占用率；</li>
<li>内存使用情况；</li>
<li>Page Fault 情况；</li>
<li>进程切换情况；</li>
<li>文件系统IO；</li>
<li>Socket 使用情况；</li>
<li>……</li>
</ul>
<h2 id="场景2：-如何分析golang程序的内存使用情况？"><a href="#场景2：-如何分析golang程序的内存使用情况？" class="headerlink" title="场景2： 如何分析golang程序的内存使用情况？"></a>场景2： 如何分析golang程序的内存使用情况？</h2><h3 id="内存占用情况查看"><a href="#内存占用情况查看" class="headerlink" title="内存占用情况查看"></a>内存占用情况查看</h3><p>我们先写一段demo例子代码<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"runtime"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//slice 会动态扩容，用slice来做堆内存申请</span></span><br><span class="line">    container := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">32</span>*<span class="number">1000</span>*<span class="number">1000</span>; i++ &#123;</span><br><span class="line">        container = <span class="built_in">append</span>(container, i)</span><br><span class="line">    &#125;</span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(<span class="string">"Start."</span>)</span><br><span class="line"></span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">"force gc."</span>)</span><br><span class="line">    runtime.GC() <span class="comment">//强制调用gc回收</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">"Done."</span>)</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3600</span> * time.Second) <span class="comment">//睡眠，保持程序不退出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$go</span> build -o snippet_mem &amp;&amp; ./snippet_mem</span><br></pre></td></tr></table></figure></p>
<p>然后在./snippet_mem进程没有执行完，我们再开一个窗口，通过 <code>top</code> 命令查看进程的内存占用情况<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$top</span> -p $(pidof snippet_mem)</span><br></pre></td></tr></table></figure></p>
<p>得到结果如下：<br><img src="/images/go/godebug_1.png" alt="go debug top"></p>
<p>我们看出来，没有退出的snippet_mem进程有约830m的内存被占用。</p>
<p>直观上来说，这个程序在 <code>test()</code> 函数执行完后，切片 <code>contaner</code> 的内存应该被释放，不应该占用830M那么大。</p>
<p>下面让我们使用GODEBUG来分析程序的内存使用情况。</p>
<h3 id="GODEBUG与gctrace"><a href="#GODEBUG与gctrace" class="headerlink" title="GODEBUG与gctrace"></a>GODEBUG与gctrace</h3><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><p>执行<code>snippet_mem</code>程序之前添加环境变量<code>GODEBUG=&#39;gctrace=1&#39;</code>来跟踪打印垃圾回收器信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GODEBUG=<span class="string">'gctrace=1'</span> ./snippet_mem</span><br></pre></td></tr></table></figure></p>
<p>设置<code>gctrace=1</code>会使得垃圾回收器在每次回收时汇总所回收内存的大小以及耗时，<br>并将这些内容汇总成单行内容打印到标准错误输出中。</p>
<h4 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gc <span class="comment"># @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #-&gt;#-&gt;# MB, # MB goal, # P</span></span><br></pre></td></tr></table></figure>
<p>含义<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gc <span class="comment">#        GC次数的编号，每次GC时递增</span></span><br><span class="line">@<span class="comment">#s         距离程序开始执行时的时间</span></span><br><span class="line"><span class="comment">#%          GC占用的执行时间百分比</span></span><br><span class="line"><span class="comment">#+...+#     GC使用的时间</span></span><br><span class="line"><span class="comment">#-&gt;#-&gt;# MB  GC开始，结束，以及当前活跃堆内存的大小，单位M</span></span><br><span class="line"><span class="comment"># MB goal   全局堆内存大小</span></span><br><span class="line"><span class="comment"># P         使用processor的数量</span></span><br></pre></td></tr></table></figure></p>
<p>如果每条信息最后，以<code>(forced)</code>结尾，那么该信息是由<code>runtime.GC()</code>调用触发</p>
<p>我们来选择其中一行来解释一下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gc 17 @0.149s 1%: 0.004+36+0.003 ms clock, 0.009+0/0.051/36+0.006 ms cpu, 181-&gt;181-&gt;101 MB, 182 MB goal, 2 P</span><br></pre></td></tr></table></figure></p>
<p>该条信息含义如下：</p>
<ul>
<li><code>gc 17</code>: Gc 调试编号为17</li>
<li><code>@0.149s</code>: 此时程序已经执行了0.149s</li>
<li><code>1%</code>: 0.149s中其中gc模块占用了1%的时间</li>
<li><code>0.004+36+0.003 ms clock</code>: 垃圾回收的时间，分别为STW（stop-the-world）清扫的时间+并发标记和扫描的时间+STW标记的时间</li>
<li><code>0.009+0/0.051/36+0.006 ms cpu</code>: 垃圾回收占用cpu时间</li>
<li><code>181-&gt;181-&gt;101 MB</code>: GC开始前堆内存181M， GC结束后堆内存181M，当前活跃的堆内存101M</li>
<li><code>182 MB goal</code>: 全局堆内存大小</li>
<li><code>2 P</code>: 本次GC使用了2个P(调度器中的Processer)</li>
</ul>
<p>了解了GC的调试信息读法后，接下来我们来分析一下本次GC的结果。</p>
<p>我们还是执行GODEBUG调试<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GODEBUG=<span class="string">'gctrace=1'</span> ./snippet_mem</span><br></pre></td></tr></table></figure></p>
<p>结果如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">2020/03/02 11:22:37 Start.</span><br><span class="line">2020/03/02 11:22:37  ===&gt; loop begin.</span><br><span class="line">gc 1 @0.002s 5%: 0.14+0.45+0.002 ms clock, 0.29+0/0.042/0.33+0.005 ms cpu, 4-&gt;4-&gt;0 MB, 5 MB goal, 2 P</span><br><span class="line">gc 2 @0.003s 4%: 0.13+3.7+0.019 ms clock, 0.27+0/0.037/2.8+0.038 ms cpu, 4-&gt;4-&gt;2 MB, 5 MB goal, 2 P</span><br><span class="line">gc 3 @0.008s 3%: 0.002+1.1+0.001 ms clock, 0.005+0/0.083/1.0+0.003 ms cpu, 6-&gt;6-&gt;2 MB, 7 MB goal, 2 P</span><br><span class="line">gc 4 @0.010s 3%: 0.003+0.99+0.002 ms clock, 0.006+0/0.041/0.82+0.004 ms cpu, 5-&gt;5-&gt;2 MB, 6 MB goal, 2 P</span><br><span class="line">gc 5 @0.011s 4%: 0.079+0.80+0.003 ms clock, 0.15+0/0.046/0.51+0.006 ms cpu, 6-&gt;6-&gt;3 MB, 7 MB goal, 2 P</span><br><span class="line">gc 6 @0.013s 4%: 0.15+3.7+0.002 ms clock, 0.31+0/0.061/3.3+0.005 ms cpu, 8-&gt;8-&gt;8 MB, 9 MB goal, 2 P</span><br><span class="line">gc 7 @0.019s 3%: 0.004+2.5+0.005 ms clock, 0.008+0/0.051/2.1+0.010 ms cpu, 20-&gt;20-&gt;6 MB, 21 MB goal, 2 P</span><br><span class="line">gc 8 @0.023s 5%: 0.014+3.7+0.002 ms clock, 0.029+0.040/1.2/0+0.005 ms cpu, 15-&gt;15-&gt;8 MB, 16 MB goal, 2 P</span><br><span class="line">gc 9 @0.031s 4%: 0.003+1.6+0.001 ms clock, 0.007+0.094/0/0+0.003 ms cpu, 19-&gt;19-&gt;10 MB, 20 MB goal, 2 P</span><br><span class="line">gc 10 @0.034s 3%: 0.006+5.2+0.004 ms clock, 0.013+0/0.045/5.0+0.008 ms cpu, 24-&gt;24-&gt;13 MB, 25 MB goal, 2 P</span><br><span class="line">gc 11 @0.040s 3%: 0.12+2.6+0.002 ms clock, 0.24+0/0.043/2.5+0.004 ms cpu, 30-&gt;30-&gt;16 MB, 31 MB goal, 2 P</span><br><span class="line">gc 12 @0.043s 3%: 0.11+4.4+0.002 ms clock, 0.23+0/0.044/4.1+0.005 ms cpu, 38-&gt;38-&gt;21 MB, 39 MB goal, 2 P</span><br><span class="line">gc 13 @0.049s 3%: 0.008+10+0.040 ms clock, 0.017+0/0.045/10+0.080 ms cpu, 47-&gt;47-&gt;47 MB, 48 MB goal, 2 P</span><br><span class="line">gc 14 @0.070s 2%: 0.004+12+0.002 ms clock, 0.008+0/0.062/12+0.005 ms cpu, 122-&gt;122-&gt;41 MB, 123 MB goal, 2 P</span><br><span class="line">gc 15 @0.084s 2%: 0.11+11+0.038 ms clock, 0.22+0/0.064/3.9+0.076 ms cpu, 93-&gt;93-&gt;93 MB, 94 MB goal, 2 P</span><br><span class="line">gc 16 @0.122s 1%: 0.005+25+0.010 ms clock, 0.011+0/0.12/24+0.021 ms cpu, 238-&gt;238-&gt;80 MB, 239 MB goal, 2 P</span><br><span class="line">gc 17 @0.149s 1%: 0.004+36+0.003 ms clock, 0.009+0/0.051/36+0.006 ms cpu, 181-&gt;181-&gt;101 MB, 182 MB goal, 2 P</span><br><span class="line">gc 18 @0.187s 1%: 0.12+19+0.004 ms clock, 0.25+0/0.049/19+0.008 ms cpu, 227-&gt;227-&gt;126 MB, 228 MB goal, 2 P</span><br><span class="line">gc 19 @0.207s 1%: 0.096+27+0.004 ms clock, 0.19+0/0.077/0.73+0.009 ms cpu, 284-&gt;284-&gt;284 MB, 285 MB goal, 2 P</span><br><span class="line">gc 20 @0.287s 0%: 0.005+944+0.040 ms clock, 0.011+0/0.048/1.3+0.081 ms cpu, 728-&gt;728-&gt;444 MB, 729 MB goal, 2 P</span><br><span class="line">2020/03/02 11:22:38  ===&gt; loop end.</span><br><span class="line">2020/03/02 11:22:38 force gc.</span><br><span class="line">gc 21 @1.236s 0%: 0.004+0.099+0.001 ms clock, 0.008+0/0.018/0.071+0.003 ms cpu, 444-&gt;444-&gt;0 MB, 888 MB goal, 2 P (forced)</span><br><span class="line">2020/03/02 11:22:38 Done.</span><br><span class="line">GC forced</span><br><span class="line">gc 22 @122.455s 0%: 0.010+0.15+0.003 ms clock, 0.021+0/0.025/0.093+0.007 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 23 @242.543s 0%: 0.007+0.075+0.002 ms clock, 0.014+0/0.022/0.085+0.004 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 24 @362.545s 0%: 0.018+0.19+0.006 ms clock, 0.037+0/0.055/0.15+0.013 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 25 @482.548s 0%: 0.012+0.25+0.005 ms clock, 0.025+0/0.025/0.11+0.010 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 26 @602.551s 0%: 0.009+0.10+0.003 ms clock, 0.018+0/0.021/0.075+0.006 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 27 @722.554s 0%: 0.012+0.30+0.005 ms clock, 0.025+0/0.15/0.22+0.011 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">GC forced</span><br><span class="line">gc 28 @842.556s 0%: 0.027+0.18+0.003 ms clock, 0.054+0/0.11/0.14+0.006 ms cpu, 0-&gt;0-&gt;0 MB, 4 MB goal, 2 P</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>先看在<code>test()</code>函数执行完后立即打印的<code>gc 21</code>那行的信息。<code>444-&gt;444-&gt;0 MB, 888 MB goal</code>表示垃圾回收器已经把<code>444M</code>的内存标记为非活跃的内存。</p>
<p>再看下一个记录<code>gc 22。0-&gt;0-&gt;0 MB, 4 MB goal</code>表示垃圾回收器中的全局堆内存大小由<code>888M</code>下降为<code>4M</code>。</p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ol>
<li><strong>在test()函数执行完后，demo程序中的切片容器所申请的堆空间都被垃圾回收器回收了</strong>。</li>
<li>如果此时在<code>top</code>指令查询内存的时候，如果依然先死800+MB，说明<strong>垃圾回收器回收了应用层的内存后，（可能）并不会立即将内存归还给系统</strong>。具体分析原因可见：<a href="https://segmentfault.com/a/1190000022472459" target="_blank" rel="external">踩坑记：go服务内存暴涨</a></li>
</ol>
<h3 id="runtime-ReadMemStats"><a href="#runtime-ReadMemStats" class="headerlink" title="runtime.ReadMemStats"></a>runtime.ReadMemStats</h3><p>接下来我么换另一种方式查看内存的方式 利用 runtime库里的<code>ReadMemStats()</code>方法<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// demo2.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"runtime"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readMemStats</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> ms runtime.MemStats</span><br><span class="line">    runtime.ReadMemStats(&amp;ms)</span><br><span class="line">    log.Printf(<span class="string">" ===&gt; Alloc:%d(bytes) HeapIdle:%d(bytes) HeapReleased:%d(bytes)"</span>, ms.Alloc, ms.HeapIdle, ms.HeapReleased)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//slice 会动态扩容，用slice来做堆内存申请</span></span><br><span class="line">    container := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">32</span>*<span class="number">1000</span>*<span class="number">1000</span>; i++ &#123;</span><br><span class="line">        container = <span class="built_in">append</span>(container, i)</span><br><span class="line">        <span class="keyword">if</span> ( i == <span class="number">16</span>*<span class="number">1000</span>*<span class="number">1000</span>) &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Start]."</span>)</span><br><span class="line"></span><br><span class="line">    readMemStats()</span><br><span class="line">    test()</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [force gc]."</span>)</span><br><span class="line">    runtime.GC() <span class="comment">//强制调用gc回收</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Done]."</span>)</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">            time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3600</span> * time.Second) <span class="comment">//睡眠，保持程序不退出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里我们， 封装了一个函数<code>readMemStats()</code>，这里面主要是调用<code>runtime</code>中的<code>ReadMemStats()</code>方法获得内存信息，然后通过<code>log</code>打印出来。</p>
<p>我们执行一下代码并运行:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ go run demo2.go </span><br><span class="line">2020/03/02 18:21:17  ===&gt; [Start].</span><br><span class="line">2020/03/02 18:21:17  ===&gt; Alloc:71280(bytes) HeapIdle:66633728(bytes) HeapReleased:66600960(bytes)</span><br><span class="line">2020/03/02 18:21:17  ===&gt; loop begin.</span><br><span class="line">2020/03/02 18:21:18  ===&gt; Alloc:132535744(bytes) HeapIdle:336756736(bytes) HeapReleased:155721728(bytes)</span><br><span class="line">2020/03/02 18:21:38  ===&gt; loop end.</span><br><span class="line">2020/03/02 18:21:38  ===&gt; Alloc:598300600(bytes) HeapIdle:609181696(bytes) HeapReleased:434323456(bytes)</span><br><span class="line">2020/03/02 18:21:38  ===&gt; [force gc].</span><br><span class="line">2020/03/02 18:21:38  ===&gt; [Done].</span><br><span class="line">2020/03/02 18:21:38  ===&gt; Alloc:55840(bytes) HeapIdle:1207427072(bytes) HeapReleased:434266112(bytes)</span><br><span class="line">2020/03/02 18:21:38  ===&gt; Alloc:56656(bytes) HeapIdle:1207394304(bytes) HeapReleased:434266112(bytes)</span><br><span class="line">2020/03/02 18:21:48  ===&gt; Alloc:56912(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(bytes)</span><br><span class="line">2020/03/02 18:21:58  ===&gt; Alloc:57488(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(bytes)</span><br><span class="line">2020/03/02 18:22:08  ===&gt; Alloc:57616(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(bytes)</span><br><span class="line">c2020/03/02 18:22:18  ===&gt; Alloc:57744(bytes) HeapIdle:1207394304(bytes) HeapReleased:1206493184(by</span><br></pre></td></tr></table></figure></p>
<p>可以看到，打印<code>[Done].</code>之后那条trace信息，Alloc已经下降，即内存已被垃圾回收器回收。在<code>2020/03/02 18:21:38</code>和<code>2020/03/02 18:21:48</code>的两条trace信息中，HeapReleased开始上升，即垃圾回收器把内存归还给系统。</p>
<p>另外，MemStats还可以获取其它哪些信息以及字段的含义可以参见官方文档：<a href="http://golang.org/pkg/runtime/#MemStats" target="_blank" rel="external">http://golang.org/pkg/runtime/#MemStats</a></p>
<h3 id="pprof工具"><a href="#pprof工具" class="headerlink" title="pprof工具"></a>pprof工具</h3><p>pprof工具支持网页上查看内存的使用情况，需要在代码中添加一个协程即可。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>(</span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>))</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure></p>
<p>具体添加的完整代码如下：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//demo3.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"runtime"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readMemStats</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> ms runtime.MemStats</span><br><span class="line">    runtime.ReadMemStats(&amp;ms)</span><br><span class="line">    log.Printf(<span class="string">" ===&gt; Alloc:%d(bytes) HeapIdle:%d(bytes) HeapReleased:%d(bytes)"</span>, ms.Alloc, ms.HeapIdle, ms.HeapReleased)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//slice 会动态扩容，用slice来做堆内存申请</span></span><br><span class="line">    container := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">32</span>*<span class="number">1000</span>*<span class="number">1000</span>; i++ &#123;</span><br><span class="line">        container = <span class="built_in">append</span>(container, i)</span><br><span class="line">        <span class="keyword">if</span> ( i == <span class="number">16</span>*<span class="number">1000</span>*<span class="number">1000</span>) &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//启动pprof</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        log.Println(http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>))</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Start]."</span>)</span><br><span class="line"></span><br><span class="line">    readMemStats()</span><br><span class="line">    test()</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [force gc]."</span>)</span><br><span class="line">    runtime.GC() <span class="comment">//强制调用gc回收</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; [Done]."</span>)</span><br><span class="line">    readMemStats()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            readMemStats()</span><br><span class="line">            time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3600</span> * time.Second) <span class="comment">//睡眠，保持程序不退出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们正常运行程序，然后同时打开浏览器，</p>
<p>输入地址：<a href="http://127.0.0.1:10000/debug/pprof/heap?debug=1" target="_blank" rel="external">http://127.0.0.1:10000/debug/pprof/heap?debug=1</a></p>
<p>浏览器的内容其中有一部分如下，记录了目前的内存情况<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># runtime.MemStats</span></span><br><span class="line"><span class="comment"># Alloc = 228248</span></span><br><span class="line"><span class="comment"># TotalAlloc = 1293696976</span></span><br><span class="line"><span class="comment"># Sys = 834967896</span></span><br><span class="line"><span class="comment"># Lookups = 0</span></span><br><span class="line"><span class="comment"># Mallocs = 2018</span></span><br><span class="line"><span class="comment"># Frees = 671</span></span><br><span class="line"><span class="comment"># HeapAlloc = 228248</span></span><br><span class="line"><span class="comment"># HeapSys = 804913152</span></span><br><span class="line"><span class="comment"># HeapIdle = 804102144</span></span><br><span class="line"><span class="comment"># HeapInuse = 811008</span></span><br><span class="line"><span class="comment"># HeapReleased = 108552192</span></span><br><span class="line"><span class="comment"># HeapObjects = 1347</span></span><br><span class="line"><span class="comment"># Stack = 360448 / 360448</span></span><br><span class="line"><span class="comment"># MSpan = 28288 / 32768</span></span><br><span class="line"><span class="comment"># MCache = 3472 / 16384</span></span><br><span class="line"><span class="comment"># BuckHashSys = 1449617</span></span><br><span class="line"><span class="comment"># GCSys = 27418976</span></span><br><span class="line"><span class="comment"># OtherSys = 776551</span></span><br><span class="line"><span class="comment"># NextGC = 4194304</span></span><br><span class="line"><span class="comment"># LastGC = 1583203571137891390</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure></p>
<h2 id="场景3-如何分析Golang程序的CPU性能情况？"><a href="#场景3-如何分析Golang程序的CPU性能情况？" class="headerlink" title="场景3: 如何分析Golang程序的CPU性能情况？"></a>场景3: 如何分析Golang程序的CPU性能情况？</h2><h3 id="性能分析注意事项"><a href="#性能分析注意事项" class="headerlink" title="性能分析注意事项"></a>性能分析注意事项</h3><ul>
<li>性能分析必须在一个可重复的、稳定的环境中来进行。</li>
<li>机器必须闲置<ul>
<li>不要在共享硬件上进行性能分析;</li>
<li>不要在性能分析期间，在同一个机器上去浏览网页</li>
</ul>
</li>
<li>注意省电模式和过热保护，如果突然进入这些模式，会导致分析数据严重不准确</li>
<li><strong>不要使用虚拟机、共享的云主机</strong>，太多干扰因素，分析数据会很不一致</li>
<li>不要在 macOS 10.11 及以前的版本运行性能分析，有 bug，之后的版本修复了</li>
</ul>
<p>如果承受得起，购买专用的性能测试分析的硬件设备，上架。</p>
<ul>
<li>关闭电源管理、过热管理</li>
<li>绝不要升级，以保证测试的一致性，以及具有可比性</li>
</ul>
<p>如果没有这样的环境，那就一定要在多个环境中，执行多次，以取得可参考的、具有相对一致性的测试结果。</p>
<h3 id="CPU性能分析"><a href="#CPU性能分析" class="headerlink" title="CPU性能分析"></a>CPU性能分析</h3><p>我们来用下面的代码进行测试<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//demo4.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"bytes"</span></span><br><span class="line">    <span class="string">"math/rand"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">test</span><span class="params">()</span></span> &#123;</span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop begin."</span>)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line">        log.Println(genSomeBytes())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">" ===&gt; loop end."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//生成一个随机字符串</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">genSomeBytes</span><span class="params">()</span> *<span class="title">bytes</span>.<span class="title">Buffer</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> buff bytes.Buffer</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">20000</span>; i++ &#123;</span><br><span class="line">        buff.Write([]<span class="keyword">byte</span>&#123;<span class="string">'0'</span> + <span class="keyword">byte</span>(rand.Intn(<span class="number">10</span>))&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &amp;buff</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            test()</span><br><span class="line">            time.Sleep(time.Second * <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动pprof</span></span><br><span class="line">    http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里面还是启动了pprof的坚挺,有关<code>pprof</code>启动的代码如下:<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    _ <span class="string">"net/http/pprof"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">//启动pprof</span></span><br><span class="line">  http.ListenAndServe(<span class="string">"0.0.0.0:10000"</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>main()</code>里的流程很简单,启动一个goroutine去无限循环调用<code>test()</code>方法,休眠1s.</p>
<p><code>test()</code>的流程是生成1000个20000个字符的随机字符串.并且打印.</p>
<p>我们将上面的代码编译成可执行的二进制文件 <code>demo4</code>(记住这个名字,稍后我们能用到)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ go build demo4.go</span><br></pre></td></tr></table></figure></p>
<p>接下来我们启动程序,程序会无限循环的打印字符串.</p>
<p>接下来我们通过几种方式来查看进程的cpu性能情况.</p>
<h4 id="A-Web界面查看"><a href="#A-Web界面查看" class="headerlink" title="A. Web界面查看"></a>A. Web界面查看</h4><p>浏览器访问: <a href="http://127.0.0.1:10000/debug/pprof/" target="_blank" rel="external">http://127.0.0.1:10000/debug/pprof/</a></p>
<p>我们会看到如下画面<br><img src="/images/go/godebug_2.png" alt="go pprof web"></p>
<p>这里面能够通过pprof查看包括(阻塞信息、cpu信息、内存堆信息、锁信息、goroutine信息等等), 我们这里关心的cpu的性能的<code>profile</code>信息.</p>
<p>有关<code>profile</code>下面的英文解释大致如下:</p>
<blockquote>
<p>“CPU配置文件。您可以在秒GET参数中指定持续时间。获取概要文件后，请使用go tool pprof命令调查概要文件。”</p>
</blockquote>
<p>所以我们要是想得到cpu性能,就是要获取到当前进程的<code>profile</code>文件,这个文件默认是30s生成一个,所以你的程序要至少运行30s以上(这个参数也可以修改,稍后我们介绍)</p>
<p>我们可以直接点击网页的<code>profile</code>,浏览器会给我们下载一个<code>profile</code>文件. 记住这个文件的路径, 可以拷贝到与<code>demo4</code>所在的同一文件夹下.</p>
<h4 id="B-使用pprof工具查看"><a href="#B-使用pprof工具查看" class="headerlink" title="B. 使用pprof工具查看"></a>B. 使用pprof工具查看</h4><p>pprof 的格式如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof [binary] [profile]</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>binary</code>: 必须指向生成这个性能分析数据的那个二进制可执行文件；</li>
<li><code>profile</code>: 必须是该二进制可执行文件所生成的性能分析数据文件。</li>
</ul>
<p><code>binary</code> 和 <code>profile</code> <strong>必须严格匹配</strong>。</p>
<p>我们来查看一下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof ./demo4 profile</span><br><span class="line"></span><br><span class="line">File: demo4</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Mar 3, 2020 at 11:18pm (CST)</span><br><span class="line">Duration: 30.13s, Total samples = 6.27s (20.81%)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p><strong>help</strong>可以查看一些指令,我么可以通过<strong>top</strong>来查看cpu的性能情况.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 5090ms, 81.18% of 6270ms total</span><br><span class="line">Dropped 80 nodes (cum &lt;= 31.35ms)</span><br><span class="line">Showing top 10 nodes out of 60</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">    1060ms 16.91% 16.91%     2170ms 34.61%  math/rand.(*lockedSource).Int63</span><br><span class="line">     850ms 13.56% 30.46%      850ms 13.56%  sync.(*Mutex).Unlock (inline)</span><br><span class="line">     710ms 11.32% 41.79%     2950ms 47.05%  math/rand.(*Rand).Int31n</span><br><span class="line">     570ms  9.09% 50.88%      990ms 15.79%  bytes.(*Buffer).Write</span><br><span class="line">     530ms  8.45% 59.33%      540ms  8.61%  syscall.Syscall</span><br><span class="line">     370ms  5.90% 65.23%      370ms  5.90%  runtime.procyield</span><br><span class="line">     270ms  4.31% 69.54%     4490ms 71.61%  main.genSomeBytes</span><br><span class="line">     250ms  3.99% 73.52%     3200ms 51.04%  math/rand.(*Rand).Intn</span><br><span class="line">     250ms  3.99% 77.51%      250ms  3.99%  runtime.memmove</span><br><span class="line">     230ms  3.67% 81.18%      690ms 11.00%  runtime.suspendG</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p>这里面有几列数据,需要说明一下.</p>
<ul>
<li>flat：当前函数占用CPU的耗时</li>
<li>flat%：:当前函数占用CPU的耗时百分比</li>
<li>sun%：函数占用CPU的耗时累计百分比</li>
<li>cum：当前函数加上调用当前函数的函数占用CPU的总耗时</li>
<li>cum%：当前函数加上调用当前函数的函数占用CPU的总耗时百分比</li>
<li>最后一列：函数名称</li>
</ul>
<p>通过结果我们可以看出, 该程序的大部分cpu性能消耗在 <code>main.getSoneBytes()</code>方法中,其中math/rand取随机数消耗比较大.</p>
<h4 id="C-通过go-tool-pprof得到profile文件"><a href="#C-通过go-tool-pprof得到profile文件" class="headerlink" title="C. 通过go tool pprof得到profile文件"></a>C. 通过go tool pprof得到profile文件</h4><p>我们上面的profile文件是通过web浏览器下载的,这个profile的经过时间是30s的,默认值我们在浏览器上修改不了,如果你想得到时间更长的cpu利用率,可以通过<code>go tool pprof</code>指令与程序交互来获取到</p>
<p>首先,我们先启动程序<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./demo4</span><br></pre></td></tr></table></figure></p>
<p>然后再打开一个终端<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof http://localhost:10000/debug/pprof/profile?seconds=60</span><br></pre></td></tr></table></figure></p>
<p>这里制定了生成profile文件的时间间隔60s</p>
<p>等待60s之后, 终端就会有结果出来,我们继续使用top来查看.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof http://localhost:10000/debug/pprof/profile?seconds=60</span><br><span class="line">Fetching profile over HTTP from http://localhost:10000/debug/pprof/profile?seconds=60</span><br><span class="line">Saved profile <span class="keyword">in</span> /home/itheima/pprof/pprof.demo4.samples.cpu.005.pb.gz</span><br><span class="line">File: demo4</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Mar 3, 2020 at 11:59pm (CST)</span><br><span class="line">Duration: 1mins, Total samples = 12.13s (20.22%)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof) top</span><br><span class="line">Showing nodes accounting <span class="keyword">for</span> 9940ms, 81.95% of 12130ms total</span><br><span class="line">Dropped 110 nodes (cum &lt;= 60.65ms)</span><br><span class="line">Showing top 10 nodes out of 56</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">    2350ms 19.37% 19.37%     4690ms 38.66%  math/rand.(*lockedSource).Int63</span><br><span class="line">    1770ms 14.59% 33.97%     1770ms 14.59%  sync.(*Mutex).Unlock (inline)</span><br><span class="line">    1290ms 10.63% 44.60%     6040ms 49.79%  math/rand.(*Rand).Int31n</span><br><span class="line">    1110ms  9.15% 53.75%     1130ms  9.32%  syscall.Syscall</span><br><span class="line">     810ms  6.68% 60.43%     1860ms 15.33%  bytes.(*Buffer).Write</span><br><span class="line">     620ms  5.11% 65.54%     6660ms 54.91%  math/rand.(*Rand).Intn</span><br><span class="line">     570ms  4.70% 70.24%      570ms  4.70%  runtime.procyield</span><br><span class="line">     500ms  4.12% 74.36%     9170ms 75.60%  main.genSomeBytes</span><br><span class="line">     480ms  3.96% 78.32%      480ms  3.96%  runtime.memmove</span><br><span class="line">     440ms  3.63% 81.95%      440ms  3.63%  math/rand.(*rngSource).Uint64</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure></p>
<p>依然会得到cpu性能的结果, 我们发现这次的结果与上次30s的结果百分比类似.</p>
<h4 id="D-可视化查看"><a href="#D-可视化查看" class="headerlink" title="D.可视化查看"></a>D.可视化查看</h4><p>我们还是通过<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof ./demo4 profile</span><br></pre></td></tr></table></figure></p>
<p>进入profile文件查看,然后我们输入<code>web</code>指令.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ go tool pprof ./demo4 profileFile: demo4</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Mar 3, 2020 at 11:18pm (CST)</span><br><span class="line">Duration: 30.13s, Total samples = 6.27s (20.81%)</span><br><span class="line">Entering interactive mode (<span class="built_in">type</span> <span class="string">"help"</span> <span class="keyword">for</span> commands, <span class="string">"o"</span> <span class="keyword">for</span> options)</span><br><span class="line">(pprof) web</span><br></pre></td></tr></table></figure></p>
<p>这里如果报找不到<code>graphviz</code>工具，需要安装一下。这里自行百度如何安装</p>
<p>然后我们得到一个<code>svg</code>的可视化文件在<code>/tmp</code>路径下<br><img src="/images/go/godebug_3.png" alt="go pprof web"></p>
<p>这样我们就能比较清晰的看到函数之间的调用关系,方块越大的表示cpu的占用越大.</p>
<p>来源：刘丹冰Aceld 简书</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes Etcd 数据备份与恢复]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-etcd-backup-restore.html</url>
      <content type="html"><![CDATA[<h2 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h2><blockquote>
<p>Etcd 版本：3.4.3<br>Kubernetes 版本：1.17.4<br>Kubernetes 安装方式：Kubeadm</p>
</blockquote>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kubernetes 使用 Etcd 数据库实时存储集群中的数据，可以说 Etcd 是 Kubernetes 的核心组件，犹如人类的大脑。如果 Etcd 数据损坏将导致 Kubernetes 不可用，在生产环境中 Etcd 数据是一定要做好高可用与数据备份，这里介绍下如何备份与恢复 Etcd 数据。</p>
<h2 id="备份-Etcd-数据"><a href="#备份-Etcd-数据" class="headerlink" title="备份 Etcd 数据"></a>备份 Etcd 数据</h2><p>采用镜像方式部署的 Etcd，所以操作 Etcd 需要使用 Etcd 镜像提供的 Etcdctl 工具。如果你是非镜像方式部署 Etcd，可以直接使用 Etcdctl 命令备份数据。</p>
<p>运行 Etcd 镜像，并且使用镜像内部的 etcdctl 工具连接 etcd 集群，执行数据快照备份：</p>
<ul>
<li>/bin/sh -c：执行 shell 命令</li>
<li>–env：设置环境变量，指定 etcdctl 工具使用的 API 版本</li>
<li>-v：docker 挂载选项，用于挂载 Etcd 证书相关目录以及备份数据存放的目录</li>
<li>–cacert：etcd CA 证书</li>
<li>–key：etcd 客户端证书 key</li>
<li>–cert：etcd 客户端证书 crt</li>
<li>–endpoints：指定 ETCD 连接地址</li>
<li>etcdctl snapshot save：etcd 数据备份</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm                                    \</span><br><span class="line">-v /data/backup:/backup                              \</span><br><span class="line">-v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \</span><br><span class="line">--env ETCDCTL_API=3                                  \</span><br><span class="line">k8s.gcr.io/etcd:3.4.3-0                              \</span><br><span class="line">/bin/sh -c <span class="string">"etcdctl --endpoints=https://192.168.2.11:2379 \</span><br><span class="line">--cacert=/etc/kubernetes/pki/etcd/ca.crt                  \</span><br><span class="line">--key=/etc/kubernetes/pki/etcd/healthcheck-client.key     \</span><br><span class="line">--cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt    \</span><br><span class="line">snapshot save /backup/etcd-snapshot.db"</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="恢复-ETCD-数据"><a href="#恢复-ETCD-数据" class="headerlink" title="恢复 ETCD 数据"></a>恢复 ETCD 数据</h2><p>在 Etcd 数据损坏时，可以通过 Etcd 备份数据进行数据恢复，先暂停 Kubernetes 相关组件，然后进入 Etcd 镜像使用 etcdctl 工具执行恢复操作。</p>
<h3 id="暂停-Kube-Apiserver-与-Etcd-镜像"><a href="#暂停-Kube-Apiserver-与-Etcd-镜像" class="headerlink" title="暂停 Kube-Apiserver 与 Etcd 镜像"></a>暂停 Kube-Apiserver 与 Etcd 镜像</h3><p>在恢复 Etcd 数据前，需要停止 <code>kube-apiserver</code> 与 <code>etcd</code> 镜像，因为当这俩镜像停止后 Kubernetes 会自动重启这俩镜像，所以我们可以先暂时移除 <code>/etc/kubernetes/manifests</code> 目录，Kubernetes 检测这个目录文件不存在时会停止 Kubernetes 系统相关镜像，使其不能重启，方便我们进行后续的操作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 移除且备份 /etc/kubernetes/manifests 目录</span></span><br><span class="line">$ mv /etc/kubernetes/manifests /etc/kubernetes/manifests.bak</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 kube-apiserver、etcd 镜像是否停止</span></span><br><span class="line">$ docker ps|grep etcd &amp;&amp; docker ps|grep kube-apiserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份现有 Etcd 数据</span></span><br><span class="line">$ mv /var/lib/etcd /var/lib/etcd.bak</span><br></pre></td></tr></table></figure></p>
<h3 id="恢复-Etcd-数据"><a href="#恢复-Etcd-数据" class="headerlink" title="恢复 Etcd 数据"></a>恢复 Etcd 数据</h3><p>运行 Etcd 镜像，然后执行数据恢复，默认会恢复到 <code>/default.etcd/member/</code> 目录下，这里使用 <code>mv</code> 命令在移动到挂载目录 <code>/var/lib/etcd/</code> 下。</p>
<ul>
<li>/bin/sh -c：执行 shell 命令</li>
<li>–env：设置环境变量，指定 etcdctl 工具使用的 API 版本</li>
<li>-v：docker 挂载选项，用于挂载 Etcd 证书相关目录以及备份数据存放的目录</li>
<li>etcdctl snapshot restore：etcd 数据恢复。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm              \</span><br><span class="line">-v /data/backup:/backup        \</span><br><span class="line">-v /var/lib/etcd:/var/lib/etcd \</span><br><span class="line">--env ETCDCTL_API=3            \</span><br><span class="line">k8s.gcr.io/etcd:3.4.3-0        \</span><br><span class="line">/bin/sh -c <span class="string">"etcdctl snapshot restore /backup/etcd-snapshot.db; mv /default.etcd/member/ /var/lib/etcd/"</span></span><br></pre></td></tr></table></figure>
<h3 id="恢复-Kube-Apiserver-与-Etcd-镜像"><a href="#恢复-Kube-Apiserver-与-Etcd-镜像" class="headerlink" title="恢复 Kube-Apiserver 与 Etcd 镜像"></a>恢复 Kube-Apiserver 与 Etcd 镜像</h3><p>将 <code>/etc/kubernetes/manifests</code> 目录恢复，使 Kubernetes 重启 <code>Kube-Apiserver</code> 与 <code>Etcd</code> 镜像：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests</span><br></pre></td></tr></table></figure></p>
<h3 id="执行-Kubectl-命令进行检测"><a href="#执行-Kubectl-命令进行检测" class="headerlink" title="执行 Kubectl 命令进行检测"></a>执行 Kubectl 命令进行检测</h3><p>执行 Kubectl 命令进行检测，查看命令是否能够正常执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure></p>
<p>参考：mydlq.club</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm 添加新 Master 节点到集群出现 ETCD 健康检查失败错误]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-kubeadm-upgrade-master-problem.html</url>
      <content type="html"><![CDATA[<h2 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h2><blockquote>
<p>Docker 版本：18.06.3<br>Kubeadm 版本：1.17.4<br>Kubernetes 版本：1.17.4<br>Kubernetes Master 数量：3<br>Kubernetes 安装方式：Kubeadm</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>Kubernetes 集群中总共有三台 Master，分别是：</p>
<p>k8s-master-2-11、k8s-master-2-12、k8s-master-2-13</p>
<p>对其中 k8s-master-2-11 Master 节点服务器进行了内核和软件升级操作，从而先将其暂时剔出集群，然后进行升级，完成后准备重新加入到 Kubernetes 集群，通过 Kubeadm 执行，输入下面命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm join mydlq.club:16443 \</span><br><span class="line">--token 6w0nwi.zag57qgfcdhi76vd \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:efa49231e4ffd836ff996921741c98ac4c5655dc729d7c32aa48c608232f0f08 \</span><br><span class="line">--control-plane --certificate-key a64e9da7346153bd64dba1e5126a644a97fdb63c878bb73de07911d1add8e26b</span><br></pre></td></tr></table></figure></p>
<p>在执行过程中，输出下面日志，提示 etcd 监控检查失败：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">W0329 00:01:51.364121   19209 manifests.go:214] the default kube-apiserver authorization-mode is <span class="string">"Node,RBAC"</span>; using <span class="string">"Node,RBAC"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">W0329 00:01:51.373807   19209 manifests.go:214] the default kube-apiserver authorization-mode is <span class="string">"Node,RBAC"</span>; using <span class="string">"Node,RBAC"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line"></span><br><span class="line">error execution phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://10.8.18.105:2379 </span><br><span class="line">with maintenance client: context deadline exceeded</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure></p>
<p>根据关键信息 <code>&quot;error execution phase check-etcd&quot;</code> 可知，可能是在执行加入 <code>etcd</code> 时候出现的错误，导致 <code>master</code> 无法加入原先的 <code>kubernetes</code> 集群。</p>
<a id="more"></a>
<h2 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h2><h3 id="查看集群节点列表"><a href="#查看集群节点列表" class="headerlink" title="查看集群节点列表"></a>查看集群节点列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node</span><br><span class="line"></span><br><span class="line">NAME              STATUS    ROLES     VERSION</span><br><span class="line">k8s-master-2-12    Ready    master    v1.17.4</span><br><span class="line">k8s-master-2-13    Ready    master    v1.17.4</span><br><span class="line">k8s-node-2-14      Ready    &lt;none&gt;    v1.17.4</span><br><span class="line">k8s-node-2-15      Ready    &lt;none&gt;    v1.17.4</span><br><span class="line">k8s-node-2-16      Ready    &lt;none&gt;    v1.17.4</span><br></pre></td></tr></table></figure>
<p>可以看到，k8s-master-2-11 节点确实不在节点列表中</p>
<h3 id="查看-Kubeadm-配置信息"><a href="#查看-Kubeadm-配置信息" class="headerlink" title="查看 Kubeadm 配置信息"></a>查看 Kubeadm 配置信息</h3><p>在看看 Kubernetes 集群中的 kubeadm 配置信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe configmaps kubeadm-config -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>获取到的内容如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Name:         kubeadm-config</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">...</span><br><span class="line">ClusterStatus:</span><br><span class="line">----</span><br><span class="line">apiEndpoints:</span><br><span class="line">  k8s-master-2-11:</span><br><span class="line">    advertiseAddress: 192.168.2.11</span><br><span class="line">    <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  k8s-master-2-12:</span><br><span class="line">    advertiseAddress: 192.168.2.12</span><br><span class="line">    <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  k8s-master-2-13:</span><br><span class="line">    advertiseAddress: 192.168.2.13</span><br><span class="line">    <span class="built_in">bind</span>Port: 6443</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterStatus</span><br></pre></td></tr></table></figure></p>
<p>可也看到 <code>k8s-master-2-11</code> 节点信息还存在与 <code>kubeadm</code> 配置中，说明 <code>etcd</code> 中还存储着 <code>k8s-master-2-11</code> 相关信息。</p>
<h3 id="分析问题所在及解决方案"><a href="#分析问题所在及解决方案" class="headerlink" title="分析问题所在及解决方案"></a>分析问题所在及解决方案</h3><p>因为集群是通过 <code>kubeadm</code> 工具搭建的，且使用了 <code>etcd</code> 镜像方式与 <code>master</code> 节点一起，所以每个 <code>Master</code> 节点上都会存在一个 <code>etcd</code> 容器实例。当剔除一个 <code>master</code> 节点时 <code>etcd</code> 集群未删除剔除的节点的 <code>etcd</code> 成员信息，该信息还存在 <code>etcd</code> 集群列表中。</p>
<p>所以，我们需要 <strong>进入 etcd 手动删除 etcd 成员信息</strong>。</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><h3 id="获取-Etcd-镜像列表"><a href="#获取-Etcd-镜像列表" class="headerlink" title="获取 Etcd 镜像列表"></a>获取 Etcd 镜像列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system | grep etcd</span><br><span class="line"></span><br><span class="line">etcd-k8s-master-2-12   1/1   Running   0</span><br><span class="line">etcd-k8s-master-2-13   1/1   Running   0</span><br></pre></td></tr></table></figure>
<h3 id="进入-Etcd-容器并删除节点信息"><a href="#进入-Etcd-容器并删除节点信息" class="headerlink" title="进入 Etcd 容器并删除节点信息"></a>进入 Etcd 容器并删除节点信息</h3><p>选择上面两个 etcd 中任意一个 pod，通过 kubectl 工具进入 pod 内部：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> -it etcd-k8s-master-2-12 sh -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>进入容器后，按下面步执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置环境</span></span><br><span class="line">$ <span class="built_in">export</span> ETCDCTL_API=3</span><br><span class="line">$ <span class="built_in">alias</span> etcdctl=<span class="string">'etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 etcd 集群成员列表</span></span><br><span class="line">$ etcdctl member list</span><br><span class="line"></span><br><span class="line">63bfe05c4646fb08, started, k8s-master-2-11, https://192.168.2.11:2380, https://192.168.2.11:2379, <span class="literal">false</span></span><br><span class="line">8e41efd8164c6e3d, started, k8s-master-2-12, https://192.168.2.12:2380, https://192.168.2.12:2379, <span class="literal">false</span></span><br><span class="line">a61d0bd53c1cbcb6, started, k8s-master-2-13, https://192.168.2.13:2380, https://192.168.2.13:2379, <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 etcd 集群成员 k8s-master-2-11</span></span><br><span class="line">$ etcdctl member remove 63bfe05c4646fb08</span><br><span class="line"></span><br><span class="line">Member 63bfe05c4646fb08 removed from cluster ed984b9o8w35<span class="built_in">cap</span>2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看 etcd 集群成员列表</span></span><br><span class="line">$ etcdctl member list</span><br><span class="line"></span><br><span class="line">8e41efd8164c6e3d, started, k8s-master-2-12, https://192.168.2.12:2380, https://192.168.2.12:2379, <span class="literal">false</span></span><br><span class="line">a61d0bd53c1cbcb6, started, k8s-master-2-13, https://192.168.2.13:2380, https://192.168.2.13:2379, <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出容器</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<h3 id="通过-kubeadm-命令再次尝试加入集群"><a href="#通过-kubeadm-命令再次尝试加入集群" class="headerlink" title="通过 kubeadm 命令再次尝试加入集群"></a>通过 kubeadm 命令再次尝试加入集群</h3><p>通过 <code>kubeadm</code> 命令再次尝试将 <code>k8s-master-2-11</code> 节点加入集群，在执行前首先进入到 <code>k8s-master-2-11</code> 节点服务器，执行 <code>kubeadm</code> 的清除命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm reset</span><br></pre></td></tr></table></figure></p>
<p>然后尝试加入 kubernetes 集群：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm join mydlq.club:16443 \</span><br><span class="line">--token 6w0nwi.zag57qgfcdhi76vd \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:efa49231e4ffd836ff996921741c98ac4c5655dc729d7c32aa48c608232f0f08 \</span><br><span class="line">--control-plane --certificate-key a64e9da7346153bd64dba1e5126a644a97fdb63c878bb73de07911d1add8e26b</span><br></pre></td></tr></table></figure></p>
<p>参考：mydlq.club</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes 1.18+ 使用ipvs后coredns无法解析域名采坑记]]></title>
      <url>http://team.jiunile.com/blog/2020/05/k8s-1-18-ipvs-problem.html</url>
      <content type="html"><![CDATA[<h2 id="环境现象"><a href="#环境现象" class="headerlink" title="环境现象"></a>环境现象</h2><blockquote>
<p>CoreDNS 版本：1.6.7<br>Docker 版本：18.06.3-ce<br>Kubernetes 版本：1.18.1<br>操作系统版本：CentOS 7.x<br>CentOS 内核版本：3.10.0-693.2.2.el7.x86_64</p>
</blockquote>
<h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>由于最近要向组内演示 Istio 1.5，故在云服务器上安装了kubernetes 1.18.1 (使用ipvs) 来配合 Istio 1.5 的演示，kubernetes 与 istio 安装完后一切顺利，bookinfo demo跑的也非常的顺利。但在一次重启整个集群后，一切都变的不那么美好了。istio-proxy起不来了，提示pilot not ready，更可悲的是service name 无法解析了，导致很多服务之间的调用都没法正常工作，有依赖的服务都不能正常启动了，为了更好的定位问题，于是卸载 Istio 1.5，重新回到k8s 1.18.1上，在一步步排查中发现，在pod中使用pod ip相互访问是可以的，由此判断，初步怀疑很可能是 DNS 出现了问题，后来慢慢发现 kube-proxy 中的错误，再定位到 IPVS parseIP Error 错误，再到解决问题。以下是整个问题定位分析</p>
<a id="more"></a>
<h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>为了更好的重现问题，我重新安装了一套新的 kubernetes 1.18.1，安装完后验证一切都是ok，接下来我就重启整个集群，问题就产生了，在pod中无法解析service name了，所有的pod运行都是ok的，一切都变的”很神奇”，好吧，开启定位之旅。</p>
<h3 id="部署DNS调试工具"><a href="#部署DNS调试工具" class="headerlink" title="部署DNS调试工具"></a>部署DNS调试工具</h3><p>为了探针是否为 DNS 问题，这里需要提前部署用于测试 DNS 问题的 dnsutils 镜像，该镜像中包含了用于测试 DNS 问题的工具包，非常利于我们分析与发现问题。</p>
<p>部署 ndsutils.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> dnsutils</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> dnsutils</span><br><span class="line"><span class="attr">    image:</span> mydlqclub/dnsutils:<span class="number">1.3</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"sleep"</span>,<span class="string">"3600"</span>]</span><br></pre></td></tr></table></figure></p>
<h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><h4 id="1-进入-DNS-工具-Pod-的命令行"><a href="#1-进入-DNS-工具-Pod-的命令行" class="headerlink" title="1. 进入 DNS 工具 Pod 的命令行"></a>1. 进入 DNS 工具 Pod 的命令行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it dnsutils sh</span><br></pre></td></tr></table></figure>
<h4 id="2-通过-Ping-和-Nsloopup-命令测试"><a href="#2-通过-Ping-和-Nsloopup-命令测试" class="headerlink" title="2. 通过 Ping 和 Nsloopup 命令测试"></a>2. 通过 Ping 和 Nsloopup 命令测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ping 集群外部，例如这里 ping 一下百度</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line">ping: bad address <span class="string">'www.baidu.com'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 集群内部 kube-apiserver 的 Service 地址</span></span><br><span class="line">$ ping kubernetes.default</span><br><span class="line">ping: bad address <span class="string">'kubernetes.default'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 nslookup 命令查询域名信息</span></span><br><span class="line">$ nslookup kubernetes.default</span><br><span class="line">;; connection timed out; no servers could be reached</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接 Ping 集群内部 kube-apiserver 的 IP 地址</span></span><br><span class="line">$ ping 10.96.0.1</span><br><span class="line">PING 10.96.0.1 (10.96.0.1): 56 data bytes</span><br><span class="line">64 bytes from 10.96.0.1: seq=0 ttl=64 time=0.096 ms</span><br><span class="line">64 bytes from 10.96.0.1: seq=1 ttl=64 time=0.050 ms</span><br><span class="line">64 bytes from 10.96.0.1: seq=2 ttl=64 time=0.068 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 dnsutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>可以观察到两次 ping 域名都不能 ping 通，且使用 nsloopup 分析域名信息时超时。然而，使用 ping kube-apiserver 的 IP 地址 “10.96.0.1” 则可以正常通信，所以，排除网络插件（flannel、calico 等）的问题。初步判断，很可能是 CoreDNS 组件的错误引起的某些问题，所以接下来我们测试 CoreDNS 是否正常。</p>
<h4 id="3-检测-CoreDNS-应用是否正常运行"><a href="#3-检测-CoreDNS-应用是否正常运行" class="headerlink" title="3. 检测 CoreDNS 应用是否正常运行"></a>3. 检测 CoreDNS 应用是否正常运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods <span class="_">-l</span> k8s-app=kube-dns -n kube-system</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-669f77d7cc-8pkpw   1/1     Running   2          6h5m</span><br><span class="line">coredns-669f77d7cc-jk9wk   1/1     Running   2          6h5m</span><br></pre></td></tr></table></figure>
<p>可也看到 CoreDNS 两个 Pod 均正常启动，所以再查看两个 Pod 中的日志信息，看看有无错误日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br></pre></td></tr></table></figure></p>
<p>通过上面信息可以观察到，日志中信息也是正常启动没有问题。再接下来，查看下 CoreDNS 的入口 Service “kube-dns” 是否存在：</p>
<blockquote>
<p>kube-dns 的 IP 为 10.96.0.10，集群内的 Pod 都是通过该 IP 与 DNS 组件进行交互，查询 DNS 信息。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get service -n kube-system | grep kube-dns</span><br><span class="line">NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)               </span><br><span class="line">kube-dns                    ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP</span><br></pre></td></tr></table></figure>
<p>上面显示 Service “kube-dns” 也存在，但是 Service 是通过 endpoints 和 Pod 进行绑定的，所以看看这个 CoreDNS 的 endpoints 是否存在，及信息是否正确：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get endpoints kube-dns -n kube-system</span><br><span class="line">NAME       ENDPOINTS                                      </span><br><span class="line">kube-dns   10.244.0.21:53,d10.244.2.82:53,10.244.0.21:9153</span><br></pre></td></tr></table></figure></p>
<p>可以看到 endpoints 配置也是正常的，正确的与两个 CporeDNS Pod 进行了关联。</p>
<p>经过上面一系列检测 CoreDNS 组件确实是正常运行。接下来，观察 CoreDNS 域名解析日志，进而确定 Pod 中的域名解析请求是否能够正常进入 CoreDNS。</p>
<h4 id="4-观察-CoreDNS-域名解析日志信息"><a href="#4-观察-CoreDNS-域名解析日志信息" class="headerlink" title="4. 观察 CoreDNS 域名解析日志信息"></a>4. 观察 CoreDNS 域名解析日志信息</h4><p>使用 <code>kubectl edit</code> 命令来修改存储于 Kubernetes <code>ConfigMap</code> 中的 CoreDNS 配置参数信息，添加 <code>log</code> 参数，让 CoreDNS 日志中显示域名解析信息：</p>
<blockquote>
<p>CoreDNS 配置参数说明：</p>
<ul>
<li>errors: 输出错误信息到控制台。</li>
<li>health：CoreDNS 进行监控检测，检测地址为 <a href="http://localhost:8080/health" target="_blank" rel="external">http://localhost:8080/health</a> 如果状态为不健康则让 Pod 进行重启。</li>
<li>ready: 全部插件已经加载完成时，将通过 endpoints 在 8081 端口返回 HTTP 状态 200。</li>
<li>kubernetes：CoreDNS 将根据 Kubernetes 服务和 pod 的 IP 回复 DNS 查询。</li>
<li>prometheus：是否开启 CoreDNS Metrics 信息接口，如果配置则开启，接口地址为 <a href="http://localhost:9153/metrics" target="_blank" rel="external">http://localhost:9153/metrics</a></li>
<li>forward：任何不在Kubernetes 集群内的域名查询将被转发到预定义的解析器 (/etc/resolv.conf)。</li>
<li>cache：启用缓存，30 秒 TTL。</li>
<li>loop：检测简单的转发循环，如果找到循环则停止 CoreDNS 进程。</li>
<li>reload：监听 CoreDNS 配置，如果配置发生变化则重新加载配置。</li>
<li>loadbalance：DNS 负载均衡器，默认 round_robin。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑 coredns 配置</span></span><br><span class="line">$ kubectl edit configmap coredns -n kube-system</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        <span class="built_in">log</span>            <span class="comment">#添加log</span></span><br><span class="line">        errors</span><br><span class="line">        health &#123;</span><br><span class="line">           lameduck 5s</span><br><span class="line">        &#125;</span><br><span class="line">        ready</span><br><span class="line">        kubernetes cluster.local <span class="keyword">in</span>-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           fallthrough <span class="keyword">in</span>-addr.arpa ip6.arpa</span><br><span class="line">           ttl 30</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        forward . /etc/resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">        loop</span><br><span class="line">        reload</span><br><span class="line">        loadbalance</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>保存更改后 CoreDNS 会自动重新加载配置信息，不过可能需要等上一两分钟才能将这些更改传播到 CoreDNS Pod。等一段时间后，再次查看 CoreDNS 日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] 127.0.0.1:47278 - 55171 <span class="string">"HINFO IN 4940754309314083739.5160468069505858354. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.040844011s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 127.0.0.1:32896 - 49064 <span class="string">"HINFO IN 1027842207973621585.7098421666386159336. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.044098742s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br></pre></td></tr></table></figure></p>
<p>可以看到 CoreDNS 已经重新加载了配置，我们再次进入 dnsuitls Pod 中执行 ping 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 DNSutils Pod 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils sh </span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 ping 命令</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 dnsutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>然后，再次查看 CoreDNS 的日志信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] 127.0.0.1:47278 - 55171 <span class="string">"HINFO IN 4940754309314083739.5160468069505858354. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.040844011s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 127.0.0.1:32896 - 49064 <span class="string">"HINFO IN 1027842207973621585.7098421666386159336. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.044098742s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br></pre></td></tr></table></figure></p>
<p>发现和之前没有执行 ping 命令时候一样，没有 DNS 域名解析的日志信息，说明 Pod 执行域名解析时，请求并没有进入 CoreDNS 中。接下来在查看 Pod 中 DNS 配置信息，进而分析问题。</p>
<h4 id="5-查看-Pod-中的-DNS-配置信息"><a href="#5-查看-Pod-中的-DNS-配置信息" class="headerlink" title="5. 查看 Pod 中的 DNS 配置信息"></a>5. 查看 Pod 中的 DNS 配置信息</h4><p>一般 Pod 中的 <code>DNS</code> 策略默认为 <code>ClusterFirst</code>，该参数起到的作用是，优先从 <code>Kubernetes DNS</code> 插件地址读取 <code>DNS</code> 配置。所以，我们正常创建的 Pod 中，DNS 配置 DNS 服务器地址应该指定为 Kubernetes 集群的 DNS 插件 Service 提供的虚拟 IP 地址。</p>
<blockquote>
<p>注：其中 DNS 策略（dnsPolicy）支持四种类型：</p>
<ul>
<li>Default： 从 DNS 所在节点继承 DNS 配置，即该 Pod 的 DNS 配置与宿主机完全一致。</li>
<li>ClusterFirst：预先从 Kubenetes 的 DNS 插件中进行 DNS 解析，如果解析不成功，才会使用宿主机的 DNS 进行解析。</li>
<li>ClusterFirstWithHostNet：Pod 是用 HOST 模式启动的（hostnetwork），用 HOST 模式表示 Pod 中的所有容器，都使用宿主机的 /etc/resolv.conf 配置进行 DNS 解析，但如果使用了 HOST 模式，还继续使用 Kubernetes 的 DNS 服务，那就将 dnsPolicy 设置为 ClusterFirstWithHostNet。</li>
<li>None：完全忽略 kubernetes 系统提供的 DNS，以 Pod Spec 中 dnsConfig 配置为主。</li>
</ul>
</blockquote>
<p>为了再分析原因，我们接着进入 dnsutils Pod 中，查看 Pod 中 DNS 配置文件 <code>/etc/resolv.conf</code> 配置参数是否正确：</p>
<blockquote>
<p>resolv.conf 配置参数说明：</p>
<ul>
<li>search： 指明域名查询顺序。</li>
<li>nameserver： 指定 DNS 服务器的 IP 地址，可以配置多个 nameserver。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils sh </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 resolv.conf 配置文件</span></span><br><span class="line">$ cat /etc/resolv.conf</span><br><span class="line"></span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search kube-system.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 DNSutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>可以看到 Pod 内部的 resolv.conf 内容，其中 nameserver 指定 DNS 解析服务器 IP 为 “10.96.0.10” ，这个 IP 地址正是本人 Kubernetes 集群 CoreDNS 的 Service “kube-dns” 的 cluterIP，说明当 Pod 内部进行域名解析时，确实是将查询请求发送到 Service “kube-dns” 提供的虚拟 IP 进行域名解析。</p>
<p>那么，既然 Pod 中 DNS 配置文件没问题，且 CoreDNS 也没问题，会不会是 Pod 本身域名解析不正常呢？或者 Service “kube-dns” 是否能够正常转发域名解析请求到 CoreDNS Pod 中？</p>
<p>当然，猜想是没有用的，进行一下测试来观察问题到底出在哪里。</p>
<h4 id="6-进行观察来定位问题所在"><a href="#6-进行观察来定位问题所在" class="headerlink" title="6. 进行观察来定位问题所在"></a>6. 进行观察来定位问题所在</h4><p>上面怀疑是 Pod 本身解析域名有问题，不能正常解析域名。或者 Pod 没问题，但是请求域名解析时将请求发送到 Service “kube-dns” 后不能正常转发请求到 CoreDNS Pod。 为了验证这两点，我们可以修改 Pod 中的 /etc/resolv.conf 配置来进行测试验证。</p>
<p>修改 <code>resolv.conf</code> 中 <code>DNS</code> 解析请求地址为 <code>阿里云 DNS</code> 服务器地址，然后执行 <code>ping</code> 命令验证是否为 <code>Pod</code> 解析域名是否有问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编辑 /etc/resolv.conf 文件，修改 nameserver 参数为阿里云提供的 DNS 服务器地址</span></span><br><span class="line">$ vi /etc/resolv.conf</span><br><span class="line"></span><br><span class="line">nameserver 223.5.5.5</span><br><span class="line"><span class="comment">#nameserver 10.96.0.10</span></span><br><span class="line">search kube-system.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改完后再进行 ping 命令测试，看看是否能够解析 www.baidu.com 网址</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line"></span><br><span class="line">PING www.a.shifen.com (180.101.49.11) 56(84) bytes of data.</span><br><span class="line">64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=1 ttl=48 time=14.0 ms</span><br><span class="line">64 bytes from 180.101.49.11 (180.101.49.11): icmp_seq=2 ttl=48 time=14.0 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 DNSutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>上面可也观察到 Pod 中更换 DNS 服务器地址后，域名解析正常，说明 Pod 本身域名解析是没有问题的。</p>
<p>接下来再修改 <code>resolv.conf</code> 中 <code>DNS</code> 解析请求地址为 <code>CoreDNS Pod</code> 的 <code>IP</code> 地址，这样让 <code>Pod</code> 直接连接 <code>CoreDNS Pod</code> 的 <code>IP</code>，而不通过 <code>Service</code> 进行转发，再进行 <code>ping</code> 命令测试，进而判断 <code>Service kube-dns</code> 是否能够正常转发请求到 <code>CoreDNS Pod</code> 的问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 CoreDNS Pod 的 IP 地址</span></span><br><span class="line">$ kubectl get pods -n kube-system -o wide | grep coredns</span><br><span class="line"></span><br><span class="line">coredns-669f77d7cc-rss5f     1/1     Running   0     10.244.2.155   k8s-node-2-13</span><br><span class="line">coredns-669f77d7cc-rt8l6     1/1     Running   0     10.244.1.163   k8s-node-2-12</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编辑 /etc/resolv.conf 文件，修改 nameserver 参数为阿里云提供的 DNS 服务器地址</span></span><br><span class="line">$ vi /etc/resolv.conf</span><br><span class="line"></span><br><span class="line">nameserver 10.244.2.155</span><br><span class="line">nameserver 10.244.1.163</span><br><span class="line"><span class="comment">#nameserver 10.96.0.10</span></span><br><span class="line">search kube-system.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改完后再进行 ping 命令测试，看看是否能够解析 www.baidu.com 网址</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line"></span><br><span class="line">PING www.baidu.com (39.156.66.18): 56 data bytes</span><br><span class="line">64 bytes from 39.156.66.18: seq=0 ttl=127 time=6.054 ms</span><br><span class="line">64 bytes from 39.156.66.18: seq=1 ttl=127 time=4.678 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出 DNSutils Pod 命令行</span></span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察 CoreDNS 日志信息，查看有无域名解析相关日志</span></span><br><span class="line">$ <span class="keyword">for</span> p <span class="keyword">in</span> $(kubectl get pods --namespace=kube-system <span class="_">-l</span> k8s-app=kube-dns -o name); \</span><br><span class="line">  <span class="keyword">do</span> kubectl logs --namespace=kube-system <span class="variable">$p</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] 127.0.0.1:47278 - 55171 <span class="string">"HINFO IN 4940754309314083739.5160468069505858354. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.040844011s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 10.244.1.162:40261 - 21083 <span class="string">"AAAA IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000398875s</span><br><span class="line">[INFO] 10.244.1.162:40261 - 20812 <span class="string">"A IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000505793s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53460 <span class="string">"AAAA IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000215384s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53239 <span class="string">"A IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000267642s</span><br><span class="line"></span><br><span class="line">.:53</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = 6434d0912b39645ed0707a3569fd69dc</span><br><span class="line">CoreDNS-1.6.7</span><br><span class="line">linux/amd64, go1.13.6, da7f65b</span><br><span class="line">[INFO] Reloading</span><br><span class="line">[INFO] plugin/health: Going into lameduck mode <span class="keyword">for</span> 5s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 127.0.0.1:32896 - 49064 <span class="string">"HINFO IN 1027842207973621585.7098421666386159336. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 57 0.044098742s</span><br><span class="line">[INFO] plugin/reload: Running configuration MD5 = a4809ab99f6713c362194263016e6fac</span><br><span class="line">[INFO] Reloading complete</span><br><span class="line">[INFO] 10.244.1.162:40261 - 21083 <span class="string">"AAAA IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000217299s</span><br><span class="line">[INFO] 10.244.1.162:40261 - 20812 <span class="string">"A IN www.baidu.com.kube-system.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000264552s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53460 <span class="string">"AAAA IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000144795s</span><br><span class="line">[INFO] 10.244.1.162:55066 - 53239 <span class="string">"A IN www.baidu.com.svc.cluster.local. udp 50 false 512"</span> NXDOMAIN qr,aa,rd 143 0.000221163s</span><br></pre></td></tr></table></figure></p>
<p>经过上面两个测试，已经可以得知，如果 <code>Pod DNS</code> 配置中直接修改 <code>DNS</code> 服务器地址为 <code>CoreDNS Pod</code> 的 <code>IP</code> 地址，<code>DNS</code> 解析确实没有问题，能够正常解析。不过，正常的情况下 Pod 中 DNS 配置的服务器地址一般是 CoreDNS 的 Service 地址，不直接绑定 Pod IP（因为 Pod 每次重启 IP 都会发生变化）。 所以问题找到了，正是在 Pod 向 CoreDNS 的 Service “kube-dns” 进行域名解析请求转发时，出现了问题，一般 <code>Service</code> 的问题都跟 <code>Kube-proxy</code> 组件有关，接下来观察该组件是否存在问题。</p>
<h4 id="7-分析-Kube-Proxy-是否存在问题"><a href="#7-分析-Kube-Proxy-是否存在问题" class="headerlink" title="7. 分析 Kube-Proxy 是否存在问题"></a>7. 分析 Kube-Proxy 是否存在问题</h4><p>观察 Kube-proxy 的日志，查看是否存在问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 kube-proxy Pod 列表</span></span><br><span class="line">$ kubectl get pods -n kube-system | grep kube-proxy</span><br><span class="line"></span><br><span class="line">kube-proxy-6kdj2          1/1     Running   3          9h</span><br><span class="line">kube-proxy-lw2q6          1/1     Running   3          9h</span><br><span class="line">kube-proxy-mftlt          1/1     Running   3          9h</span><br><span class="line"> 选择一个 kube-proxy Pod，查看最后 5 条日志内容</span><br><span class="line">$ kubectl logs kube-proxy-6kdj2 --tail=5  -n kube-system</span><br><span class="line"></span><br><span class="line">E0326 15:20:23.159364  1 proxier.go:1950] Failed to list IPVS destinations, error: parseIP Error ip=[10 96 0 10 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159388  1 proxier.go:1192] Failed to sync endpoint <span class="keyword">for</span> service: 10.8.0.10:53/UPD, err: parseIP Error ip=[10 96 0 16 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159479  1 proxier.go:1950] Failed to list IPVS destinations, error: parseIP Error ip=[10 96 0 10 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159501  1 proxier.go:1192] Failed to sync endpoint <span class="keyword">for</span> service: 10.8.0.10:53/TCP, err: parseIP Error ip=[10 96 0 16 0 0 0 0 0 0 0 0 0 0 0 0]</span><br><span class="line">E0326 15:20:23.159595  1 proxier.go:1950] Failed to list IPVS destinations, error: parseIP Error ip=[10 96 0 10 0 0 0 0 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure></p>
<p>通过 kube-proxy Pod 的日志可以看到，里面有很多 Error 级别的日志信息，根据关键字 <code>IPVS</code>、<code>parseIP Error</code> 可知，可能是由于 IPVS 模块对 IP 进行格式化导致出现问题。</p>
<p>因为这个问题是升级到 kubernetes 1.18 版本才出现的，所以去 Kubernetes Github 查看相关 issues，发现有人在升级 Kubernetes 版本到 1.18 后，也遇见了相同的问题，经过 issue 中 Kubernetes 维护人员讨论，分析出原因可能为新版 Kubernetes 使用的 IPVS 模块是比较新的，需要系统内核版本支持，本人使用的是 CentOS 系统，内核版本为 3.10，里面的 IPVS 模块比较老旧，缺少新版 Kubernetes IPVS 所需的依赖。</p>
<p>根据该 issue 讨论结果，解决该问题的办法是，更新内核为新的版本。</p>
<blockquote>
<p>注：该 issues 地址为 <a href="https://github.com/kubernetes/kubernetes/issues/89520" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/issues/89520</a></p>
</blockquote>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><h3 id="升级系统内核版本"><a href="#升级系统内核版本" class="headerlink" title="升级系统内核版本"></a>升级系统内核版本</h3><p>升级 Kubernetes 集群各个节点的 CentOS 系统内核版本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入公钥</span></span><br><span class="line">$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 ELRepo 最新版本</span></span><br><span class="line">$ yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出可以使用的 kernel 包版本</span></span><br><span class="line">$ yum list available --disablerepo=* --enablerepo=elrepo-kernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装指定的 kernel 版本：</span></span><br><span class="line">$ yum install -y kernel<span class="_">-lt</span>-4.4.222-1.el7.elrepo --enablerepo=elrepo-kernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统可用内核</span></span><br><span class="line">$ cat /boot/grub2/grub.cfg | grep menuentry</span><br><span class="line"></span><br><span class="line">menuentry <span class="string">'CentOS Linux (3.10.0-1062.el7.x86_64) 7 (Core)'</span> --class centos （略）</span><br><span class="line">menuentry <span class="string">'CentOS Linux (4.4.222-1.el7.elrepo.x86_64) 7 (Core)'</span> --class centos ...（略）</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置开机从新内核启动</span></span><br><span class="line">$ grub2-set-default <span class="string">"CentOS Linux (4.4.222-1.el7.elrepo.x86_64) 7 (Core)"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看内核启动项</span></span><br><span class="line">$ grub2-editenv list</span><br><span class="line">saved_entry=CentOS Linux (4.4.222-1.el7.elrepo.x86_64) 7 (Core)</span><br></pre></td></tr></table></figure></p>
<p>重启系统使内核生效，启动完成查看内核版本是否更新：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uname -r</span><br><span class="line">4.4.222-1.el7.elrepo.x86_64</span><br></pre></td></tr></table></figure></p>
<h3 id="测试-Pod-中-DNS-是否能够正常解析"><a href="#测试-Pod-中-DNS-是否能够正常解析" class="headerlink" title="测试 Pod 中 DNS 是否能够正常解析"></a>测试 Pod 中 DNS 是否能够正常解析</h3><p>进入 Pod 内部使用 ping 命令测试 DNS 是否能正常解析：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 dnsutils Pod 内部 sh 命令行</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it dnsutils /bin/sh -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 集群外部，例如这里 ping 一下百度</span></span><br><span class="line">$ ping www.baidu.com</span><br><span class="line">64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=1 ttl=127 time=7.20 ms</span><br><span class="line">64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=2 ttl=127 time=6.60 ms</span><br><span class="line">64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=3 ttl=127 time=6.38 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 集群内部 kube-api 的 Service 地址</span></span><br><span class="line">$ ping kubernetes.default</span><br><span class="line">64 bytes from kubernetes.default.svc.cluster.local (10.96.0.1): icmp_seq=1 ttl=64 time=0.051 ms</span><br><span class="line">64 bytes from kubernetes.default.svc.cluster.local (10.96.0.1): icmp_seq=2 ttl=64 time=0.051 ms</span><br><span class="line">64 bytes from kubernetes.default.svc.cluster.local (10.96.0.1): icmp_seq=3 ttl=64 time=0.064 ms</span><br></pre></td></tr></table></figure></p>
<p>可以看到 Pod 中的域名解析已经恢复正常。</p>
<p>参考: mydlq.club</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes 在 aws 上的应用]]></title>
      <url>http://team.jiunile.com/blog/2020/01/k8s-k8s-in-aws-arch.html</url>
      <content type="html"><![CDATA[<h2 id="kubernetes-in-aws-Overview（EKS）"><a href="#kubernetes-in-aws-Overview（EKS）" class="headerlink" title="kubernetes in aws Overview（EKS）"></a>kubernetes in aws Overview（EKS）</h2><p><img src="/images/k8s/aws_eks_arch.png" alt="eks_in_aws"><br><a id="more"></a><br>上图是 kubernetes(EKS) 在 aws 上的整体情况，下面我会对每块进行展开说明</p>
<h2 id="分解说明"><a href="#分解说明" class="headerlink" title="分解说明"></a>分解说明</h2><h3 id="kubernetes-or-eks"><a href="#kubernetes-or-eks" class="headerlink" title="kubernetes or eks"></a>kubernetes or eks</h3><p>kubernetes 原生支持在 aws 上使用，具体如何启用可参考<a href="http://team.jiunile.com/blog/2019/03/k8s-cloud.html">kuberenetes 云应用实践</a>，这里不在累赘说明。既然选择在云上使用 k8s ，那就直接使用云本身提供的能力（<a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a>），就没必要自己再去适配一遍，毕竟云厂商已经提供了这样的服务，为什么不去使用呢？不过自己也可以去了解EKS为我们做了什么？怎么去管理它？</p>
<h4 id="EKS"><a href="#EKS" class="headerlink" title="EKS"></a>EKS</h4><p><a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 其实就是帮我们托管了master节点（3 master集群），其次更好的与aws资源进行了集成，帮助我们在上层更轻松的使用，<a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 默认会集成以下组件:  </p>
<ul>
<li>kube-proxy</li>
<li>coredns</li>
<li><a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a> (网络插件-替代calico，下面会介绍为何使用它)</li>
</ul>
<hr>
<p>使用 <a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 后，我们就不需要在关注 k8s master 的安装以及高可用，以及怎么在云上（aws）的集成，同时在开源的 kubernetes 上做了优化。当然，你可以可以选择自建，毕竟这些你都可以搭建出来，我们在 aws 中国就是自建的，毕竟 <a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 在国内还未上线，但如果是在国外或者国内上线的前提下需衡量付出的时间成本是否合算？</p>
<h4 id="如何管理"><a href="#如何管理" class="headerlink" title="如何管理"></a>如何管理</h4><p><a href="https://www.terraform.io" target="_blank" rel="external">terrafrom</a> or <a href="https://eksctl.io" target="_blank" rel="external">eksctl</a>，最终我选择了后者，理由也很简单，在我看来一个就好比是”学生”，另一个则是”亲儿子”，<a href="https://eksctl.io" target="_blank" rel="external">eksctl</a> 由 aws 官方提供专门为 <a href="https://eksworkshop.com/" target="_blank" rel="external">EKS</a> 提供的命令行工具，它提供了更多的参数同时管理起来也非常的方便，如果你是深度管理，则使用它，<a href="https://www.terraform.io" target="_blank" rel="external">terrafrom</a> 则是大众普照中的一个，提供的参数也是有限，如果简单管理可以使用它。</p>
<h3 id="负载均衡器ELB"><a href="#负载均衡器ELB" class="headerlink" title="负载均衡器ELB"></a>负载均衡器ELB</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>aws 官方提供了三种负载均衡器，我简单成为 <code>clb</code>、<code>alb</code>、<code>nlb</code>，<code>clb</code> 即将淘汰也是最早的一代，这里主要阐述 <code>alb</code> 7层负载  与 <code>nlb</code> 4层负载 ，该如何选择，则取决于你的业务，他们之间的特征也很明显，如果你是需要关注 <code>header</code> 头信息的，如 <code>x-forward-for</code> 或 <code>x-forward-proto</code> 等信息的则使用 <code>alb</code>，如果不需要则使用 <code>nlb</code> 性能高。具体想了解，可参考aws官方文档：<a href="https://aws.amazon.com/cn/elasticloadbalancing/" target="_blank" rel="external">https://aws.amazon.com/cn/elasticloadbalancing/</a></p>
<h4 id="管理"><a href="#管理" class="headerlink" title="管理"></a>管理</h4><p>同样我们也面临着管理的问题，是使用 <a href="https://www.terraform.io" target="_blank" rel="external">terrafrom</a> 还是别的呢？毕竟我们使用了 k8s ，它有很好的 <code>ingress</code> 帮我们管理着对外的访问不是么？最终，我选择了 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">alb-ingress-controller</a>，同样也是 aws 出品，它很好的与 ELB 进行了集成，我们只需要编写 <code>ingress</code> 文件，加上对应的注解，就可以很方便的使用 ELB，目前支持 <code>alb</code> 与 <code>nlb</code> 这两个负载均衡器的使用。</p>
<p>对 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">alb-ingress-controller</a> 的使用也有一定的注意，推荐是一个对外暴露服务一个 <code>ingress</code>, <strong>同时注意不要人为去 aws consol 界面去修改已有通过 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">alb-ingress-controller</a>  创建出来的负载均衡器，如果你修改了，你会发现过一会就会还原了</strong>，因为在 k8s 中的 alb 控制器会帮你复原，所以一切的变更请使用 <code>ingress</code>, 同时还有一个不好的弊端就是在 alb/nlb 目标群组中，后端协议只能二选一，无法 http/https 同时存在，唯一的方法是使用两个 <code>ingress</code> ，不过我们大多数后端访问都是 http 协议。</p>
<p>详细了解可参考：</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/" target="_blank" rel="external">https://github.com/kubernetes-sigs/aws-alb-ingress-controller/</a></li>
<li><a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/" target="_blank" rel="external">https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/</a></li>
<li><a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/alb-ingress.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/alb-ingress.html</a></li>
<li><a href="https://aws.amazon.com/cn/blogs/china/kubernetes-ingress-aws-alb-ingress-controller/" target="_blank" rel="external">https://aws.amazon.com/cn/blogs/china/kubernetes-ingress-aws-alb-ingress-controller/</a></li>
</ul>
<h3 id="网络插件"><a href="#网络插件" class="headerlink" title="网络插件"></a>网络插件</h3><p>同样，在网络插件上，我也面临着选择的问题，不过现在回想起来，居然选择了云，那就基本大概率上去拥抱云原生提供的插件，不是么，毕竟它和云结合的很好，这里我介绍下 <a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a>。</p>
<p><a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a> 我就说一件事，<strong>pod 与你现有的 vpc 打通</strong>，但劣势也很明显，<code>就是 pod 的 ip 会占有你划分的 vpc ip池</code>，如果你的 <code>subnet</code> 网段规划的不好的话，你的 ip 将会很快用光，所有我建议你的 <code>vpc</code> 网段是 <code>16</code> 位，<code>subnet</code> 网段是 <code>20</code> 位的，具体不同的网段可以放多少个 pod 可以参考这边文章了解下：<a href="https://www.yiibai.com/ipv4/ipv4_subnetting.html，" target="_blank" rel="external">https://www.yiibai.com/ipv4/ipv4_subnetting.html，</a> 这里也不在累赘。 </p>
<p>同时 <a href="https://github.com/aws/amazon-vpc-cni-k8s" target="_blank" rel="external">amazon-k8s-cni</a> 对主机类型上能起多少 pod 也是有限制的，不同的机型上启动的 pod 数量是不一样的，这里我推荐尽可能的使用中等规模的机型来取缔多个小机型。具体可参考：<a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/f7a7d53baf769846c20467cfa27f0172eca570c1/pkg/awsutils/vpc_ip_resource_limit.go" target="_blank" rel="external">vpc_ip_resource_limit</a> </p>
<p>当然如果你是本地机房或者云未给提供网络插件的话，那我还是推荐你使用 <a href="https://www.projectcalico.org/" target="_blank" rel="external">calico</a>，毕竟它还是非常的强大！有兴趣的伙伴还是可以好好了解下。</p>
<h3 id="访问安全"><a href="#访问安全" class="headerlink" title="访问安全"></a>访问安全</h3><p>容器访问安全也是令我非常困扰的一个问题，aws 在资源访问方面有着非常好的权限控制 <code>iam role</code> ，但放到 k8s 容器中，一切都变的非常不美好了， 直到最近 aws eks 推出了 <a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">service account for iam role</a>， 在 eks 没正式推出 <a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">service account for iam role</a> 之前，有两种方式可以进行访问控制：</p>
<ul>
<li>access key/secret key</li>
<li><a href="https://github.com/jtblin/kube2iam" target="_blank" rel="external">kube2iam</a></li>
</ul>
<hr>
<p>针对以上两个访问，都有一定的缺陷，用key来控制的问题在于本身管理key就是一件头痛的事情，key的泄漏和定期更换非常痛苦，使用 <a href="https://github.com/jtblin/kube2iam" target="_blank" rel="external">kube2iam</a> 问题在于你需要保证这个插件的稳定性和性能，一旦出现问题，你会发现那就是灾难性的，而且也不易排查，毕竟多层组件就多提升了出问题的概率，毕竟还是如此重要的组件。</p>
<p>最后，我隆重推荐大家使用 <a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">service account for iam role</a> ，具体教程可点击链接查看。 但对于自建的小伙们只能退而求其次 使用 key 或者 <a href="https://github.com/jtblin/kube2iam" target="_blank" rel="external">kube2iam</a> 来进行管理控制了</p>
<h3 id="Why-kong"><a href="#Why-kong" class="headerlink" title="Why kong?"></a>Why kong?</h3><p>为什么选型 <a href="http://getkong.org" target="_blank" rel="external">kong</a> ，我想它的官方文档能更好的为你阐述，我从0.9开始就使用它了，直到现在维持在0.14.1，往后引入了 <code>service mesh</code> 的概念，选型 <a href="http://getkong.org" target="_blank" rel="external">kong</a> 的理由也很简单，开源、插件丰富、易于二次开发。后面可以专门分享一下我们是如何使用  <a href="http://getkong.org" target="_blank" rel="external">kong</a> 作为我们的业务网关，同时我们是如何改造 <code>JWT</code> 插件来实现鉴授权，以及通过网关审计日志定位问题，对外核心服务进行了限流等控制。如果你的需求也是如此，让业务保持足够简单专注于业务的实现，跨语言，微服务化，但同时需要一些服务治理相关的需求，还没有上 k8s 或者还没到 <a href="https://istio.io" target="_blank" rel="external">isito</a> 这种重 <code>service mesh</code> 的地步，那  <a href="http://getkong.org" target="_blank" rel="external">kong</a> 就是你最好的选择。</p>
<h3 id="资源池划分"><a href="#资源池划分" class="headerlink" title="资源池划分"></a>资源池划分</h3><p>在资源池划分上，我将网关和业务的池子单独分开，同时开辟了一块竞价池，原因如下：</p>
<ol>
<li>网关池划分： 网关作为业务对外流量的统一入口，提现出非常重要的地步，故而单独分配了一块资源池，同时需要与负载均衡器的配置，对机器权限的控制力度也是和普通业务池是不同的。</li>
<li>业务池划分：这个就什么好说的，用于承载你的业务，跑长期运行的服务。</li>
<li>竞价池划分： 这里需要配合 <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank" rel="external">cluster autoscaler</a> 一起使用更佳，用于跑一些离线任务，随用随起，用完即毁，方便至极。当然如何你使用的是 EKS 直接可以配合 <a href="https://aws.amazon.com/cn/fargate/" target="_blank" rel="external">fargate</a> 来直接使用。</li>
</ol>
<hr>
<p>Cluster Autoscaler：</p>
<ul>
<li><a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/cluster-autoscaler.html#ca-ng-considerations" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/cluster-autoscaler.html#ca-ng-considerations</a></li>
<li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank" rel="external">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</a></li>
</ul>
<h3 id="未来展望"><a href="#未来展望" class="headerlink" title="未来展望"></a>未来展望</h3><p>目前，我们通过 <a href="http://getkong.org" target="_blank" rel="external">kong</a> 来实现了对外流量的管理与控制，但对内部服务之间的访问控制还未开始，也缺乏一些服务治理的手段，当然，使用 <a href="http://getkong.org" target="_blank" rel="external">kong</a> 也可以达到，但这样我们就会过渡依赖这个集中网关了，一旦故障，那是灾难性的，<code>service mesh</code> 的到来正式解决此类问题，让服务治理变的更游刃有余，后面我们会上 <a href="https://istio.io" target="_blank" rel="external">isito</a> 来实现诸如灰度、蓝绿发布以及对内部服务调用限流、监控等控制能力的加强。</p>
<h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><ul>
<li><a href="https://docs.aws.amazon.com/" target="_blank" rel="external">https://docs.aws.amazon.com/</a></li>
<li><a href="http://getkong.org" target="_blank" rel="external">http://getkong.org</a></li>
<li><a href="https://eksctl.io/" target="_blank" rel="external">https://eksctl.io/</a></li>
<li><a href="https://eksworkshop.com" target="_blank" rel="external">https://eksworkshop.com</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[分析并解决AWS服务从ec2迁移至Kubernetes后延迟增加的问题]]></title>
      <url>http://team.jiunile.com/blog/2020/01/k8s-k8s-in-aws-latency.html</url>
      <content type="html"><![CDATA[<h2 id="问题概要"><a href="#问题概要" class="headerlink" title="问题概要"></a>问题概要</h2><p>上周我们将一个微服务迁移到中央平台上，包括CI/CD，Kubernetes运行时，metric和其他一些程序。这次实验是为了之后一个月里大概150个微服务的迁移作准备，所有这些服务支撑着西班牙在线市场的运营。</p>
<p>当我们将应用程序部署到Kubernetes上，并且将一些生产流量导入其中之后，事情开始有些不妙了。Kubernetes上的请求延迟比EC2上的高10倍左右。除非我们能找到解决方案，不然这会是微服务迁移的最大障碍，甚至可能彻底摧毁整个项目。<br><a id="more"></a></p>
<h2 id="为什么Kubernetes上的延时比EC2高那么多？"><a href="#为什么Kubernetes上的延时比EC2高那么多？" class="headerlink" title="为什么Kubernetes上的延时比EC2高那么多？"></a>为什么Kubernetes上的延时比EC2高那么多？</h2><p>为了找到系统瓶颈，我们收集了整个请求路径的metric。架构很简单，一个API网关（Zuul）将请求路由到EC2或者Kubernetes的微服务里。在Kubernetes上，我们使用NGINX Ingress控制器，后台是常规的Deployment运行一个基于Spring的JVM应用程序。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">                                  EC2</span><br><span class="line">                            +---------------+</span><br><span class="line">                            |  +---------+  |</span><br><span class="line">                            |  |         |  |</span><br><span class="line">                       +-------&gt; BACKEND |  |</span><br><span class="line">                       |    |  |         |  |</span><br><span class="line">                       |    |  +---------+  |                   </span><br><span class="line">                       |    +---------------+</span><br><span class="line">             +------+  |</span><br><span class="line">Public       |      |  |</span><br><span class="line">      -------&gt; ZUUL +--+</span><br><span class="line">traffic      |      |  |              Kubernetes</span><br><span class="line">             +------+  |    +-----------------------------+</span><br><span class="line">                       |    |  +-------+      +---------+ |</span><br><span class="line">                       |    |  |       |  xx  |         | |</span><br><span class="line">                       +-------&gt; NGINX +------&gt; BACKEND | |</span><br><span class="line">                            |  |       |  xx  |         | |</span><br><span class="line">                            |  +-------+      +---------+ |</span><br><span class="line">                            +-----------------------------+</span><br></pre></td></tr></table></figure></p>
<p>问题看上去是后台的上游延迟（上图用xx表示）。当应用程序部署在EC2上时，响应时间大概20ms。在Kubernetes上则需要100～200ms。</p>
<p>我们很快排除了运行时变更的影响。JVM版本是一致的。容器化的影响也被排除了，因为EC2上也是运行在容器里。也和压力无关，因为即使每秒只有1个请求仍然能看到很高的延时。也不是GC的影响。</p>
<p>一个Kubernetes管理员问应用程序是否有外部的依赖，比如以前DNS解析曾经导致过类似的问题，这是目前为止最可能的猜想。</p>
<h2 id="猜想1：DNS解析"><a href="#猜想1：DNS解析" class="headerlink" title="猜想1：DNS解析"></a>猜想1：DNS解析</h2><p>每次请求里，我们的应用程序会向AWS ElasticSearch实例（域名类似 elastic.spain.adevinta.com）发送1～3次请求。我们在容器内放置了一个shell脚本可以验证这个域名从DNS解析需要多长时间。</p>
<p>容器内的DNS查询：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@be-851c76f696-alf8z /]<span class="comment"># while true; do dig "elastic.spain.adevinta.com" | grep time; sleep 2; done</span></span><br><span class="line">;; Query time: 22 msec</span><br><span class="line">;; Query time: 22 msec</span><br><span class="line">;; Query time: 29 msec</span><br><span class="line">;; Query time: 21 msec</span><br><span class="line">;; Query time: 28 msec</span><br><span class="line">;; Query time: 43 msec</span><br><span class="line">;; Query time: 43 msec</span><br></pre></td></tr></table></figure></p>
<p>运行着对应相同用程序的EC2实例里的同样的查询：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bash-4.4<span class="comment"># while true; do dig "elastic.spain.adevinta.com" | grep time; sleep 2; done</span></span><br><span class="line">;; Query time: 77 msec</span><br><span class="line">;; Query time: 0 msec</span><br><span class="line">;; Query time: 0 msec</span><br><span class="line">;; Query time: 0 msec</span><br><span class="line">;; Query time: 0 msec</span><br></pre></td></tr></table></figure></p>
<p>大概30ms的解析时间，似乎我们的应用程序在和ElasticSearch通信时增加了DNS解析的额外消耗。</p>
<p>但是这里有两点很奇怪：</p>
<ul>
<li>在Kubernetes里已经有很多应用程序和AWS资源通信，但是并没有这个问题。</li>
<li>我们知道JVM实现了内存内的DNS缓存。查看这些镜像的配置，在 $JAVA_HOME/jre/lib/security/java.security里配置了TTL为 networkaddress.cache.ttl=10。JVM应该能够缓存10秒内的所有DNS查询。</li>
</ul>
<p>为了确认是DNS的影响，我们决定避免DNS解析并且查看问题是否会消失。首先尝试让应用程序直接和Elasticsearch的IP通信，而不是域名。这要求代码变更并且重新部署，因此我们只是简单地在 /etc/hosts文件里添加了域名和IP的映射：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">34.55.5.111 elastic.spain.adevinta.com</span><br></pre></td></tr></table></figure></p>
<p>这样容器可以立刻解析IP。我们确实观察到了延时的改善，但是离我们的最终目标还是很远。即使DNS解析足够快，但是真实原因还是没有找到。</p>
<h2 id="网络plumbing"><a href="#网络plumbing" class="headerlink" title="网络plumbing"></a>网络plumbing</h2><p>我们决定在容器里执行 tcpdump，这样可以看到网络到底干了些什么。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@be-851c76f696-alf8z /]<span class="comment"># tcpdump -leni any -w capture.pcap</span></span><br></pre></td></tr></table></figure></p>
<p>然后发送了一些请求并且下载了capture文件（ <code>kubectl cpmy-service:/capture.pcap capture.pcap</code>）在<a href="https://wiki.wireshark.org/FrontPage" target="_blank" rel="external">Wireshark</a>里查看。</p>
<p>DNS查询看上去很正常（除了一些细节，之后会提到）但是我们的服务处理请求的时候很奇怪。下图是capture的截图，显示一个请求从开始到响应的全过程。<br><img src="/images/wireshark.jpg" alt="Wireshark"></p>
<p>第一列是packet序号。我用不同的颜色标示不同的TCP流。</p>
<p>绿色的流从<code>packet 328</code>开始，显示客户端（172.17.22.150）开启了容器（172.17.36.147）的TCP连接。最初的握手（328-330）之后，<code>packet 331</code>开始 <code>HTTP GET/v1/..</code>，这是对我们自己服务的入站请求。整个流程花了<strong>1ms</strong>。</p>
<p>灰色流从<code>packet 339</code>开始，展示了我们的服务发送一个HTTP请求给Elasticsearch实例（这里看不到TCP握手过程因为它使用了一个已有的TCP连接）。这里花了<strong>18ms</strong>。</p>
<p>至此都没有什么问题，所花的时间和预计差不多（<strong>～20-30ms</strong>）。</p>
<p>但是在这两次交互之间，紫色部分花了<strong>86ms</strong>。这里发生了什么？在<code>packet 333</code>， 我们的服务发送了HTTP GET到 <code>/latest/meta-data/iam/security-credentials</code>，之后，在同一个TCP连接里，另一个GET发送到 <code>/latest/meta-data/iam/security-credentials/arn:..</code>。</p>
<p>我们发现每次请求里都会这样做。DNS解析在容器里确实有一点慢（解释很有意思，我会在另一篇文章里介绍）。但是高延迟的实际原因是每次请求里对<code>AWS Instance Metadata</code>的查询。</p>
<h2 id="猜想2：AWS调用"><a href="#猜想2：AWS调用" class="headerlink" title="猜想2：AWS调用"></a>猜想2：AWS调用</h2><p>这两个endpoint都是<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials" target="_blank" rel="external"><code>AWS Instance Metadata API</code></a>的一部分。我们的微服务从Elasticsearch里读取时会用到这个服务。这两个调用都是基础的授权工作流。</p>
<p>第一个请求里查询的endpoint得到和该实例相关联的IAM角色。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># curl http://169.254.169.254/latest/meta-data/iam/security-credentials/</span></span><br><span class="line">arn:aws:iam::&lt;account_id&gt;:role/some_role</span><br></pre></td></tr></table></figure></p>
<p>第二个请求查询第二个endpoint得到该实例的临时credential。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># curl http://169.254.169.254/latest/meta-data/iam/security-credentials/arn:aws:iam::&lt;account_id&gt;:role/some_role</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="string">"Code"</span>: <span class="string">"Success"</span>,</span><br><span class="line">	<span class="string">"LastUpdated"</span>: <span class="string">"2012-04-26T16:39:16Z"</span>,</span><br><span class="line">	<span class="string">"Type"</span>: <span class="string">"AWS-HMAC"</span>,</span><br><span class="line">    <span class="string">"AccessKeyId"</span>: <span class="string">"ASIAIOSFODNN7EXAMPLE"</span>,</span><br><span class="line">    <span class="string">"SecretAccessKey"</span>: <span class="string">"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"</span>,</span><br><span class="line">    <span class="string">"Token"</span>: <span class="string">"token"</span>,</span><br><span class="line">    <span class="string">"Expiration"</span>: <span class="string">"2017-05-17T15:09:54Z"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>客户端可以在短时间内使用它们，并且需要周期性地（在 Expiration之前）去获取新的credencial。模型很简单：AWS为了安全考虑经常轮询临时密钥，但是客户端可以将密钥缓存几分钟来弥补获得新credencial所带来的性能影响。</p>
<p>AWS Java SDK应该处理这些，但是，因为某种原因，它没有这么做。</p>
<p>在GitHub issue里搜索后找到了<a href="https://github.com/aws/aws-sdk-java/issues/1921" target="_blank" rel="external">#1921</a>，里面有我们需要的线索。</p>
<p>AWS SDK在下面两种情况的某一种满足时就会刷新credential：</p>
<ul>
<li>Expiration在 EXPIRATION_THRESHOLD内，硬编码为15分钟。</li>
<li>前一次刷新credential的尝试所花时间大于 REFRESH_THRESHOLD，硬编码为60分钟。</li>
</ul>
<p>我们需要查看得到的证书里的实际过期时间，因此运行了两个 cURL命令调用AWS API，一次从容器里，一次从EC2实例里。从容器里获得的证书过期时间短得多，是15分钟。</p>
<p>问题变得清晰了：我们的服务在第一个请求里会获取临时credential。因为它有15分钟的过期时间，在下一次请求里，AWS SDK会重新刷新credential。每次请求都会这样。</p>
<h2 id="为什么credential过期时间变短了？"><a href="#为什么credential过期时间变短了？" class="headerlink" title="为什么credential过期时间变短了？"></a>为什么credential过期时间变短了？</h2><p>AWS Intance Metadata Service设计上是在EC2实例里使用，而不是Kubernetes上。我们希望应用程序保留相同的接口。因此使用了<a href="https://github.com/uswitch/kiam" target="_blank" rel="external">Kiam</a>，在每个Kubernetes节点上运行一个agent，允许用户（部署应用程序到集群里的工程师）将IAM角色关联到Pod容器上，就像它是个EC2实例一样。它会截获发送到AWS Instance Metadata服务的调用，并且使用agent提前从AWS获取并放在缓存里的内容响应。从应用程序的角度来看，和运行在EC2上没什么区别。</p>
<p>Kiam给Pod提供的正是短期的credencial，这有道理，因为它假定Pod的平均生命周期比EC2实例要短。默认值就是15分钟。</p>
<p>但是如果两处都使用默认值就有问题了。提供给应用程序的证书过期时间为15分钟。AWS Java SDK会强制刷新任何过期时间少于15分钟的证书。</p>
<p>结果就是每个请求都会强制刷新临时证书，这需要两次调用AWS API，给每次请求都带来了巨大的延迟。之后我们找到了<a href="https://github.com/aws/aws-sdk-java/issues/1893" target="_blank" rel="external">AWS Java SDK的一个功能请求</a>，里面提到了同样的问题。</p>
<p>解决办法很简单，我们重新配置了Kiam，请求更长过期时间的credencial。当这一变更生效后，请求就不用每次都调用AWS Metadata服务了，而且延迟比EC2还要小。</p>
<h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>从我们迁移的经验里，最经常遇到的问题不是Kubernetes的bug或者平台的问题。也不是微服务本身的问题。问题通常只是因为集成。我们将以前从来没有一起集成过的复杂系统混合在一起，并且期望它们组成单个的大系统。可移动组件越多，可能发生问题的地方就越多。</p>
<p>在这个问题里，高延迟并不是因为bug或者Kubernetes、Kiam、AWS Java SDK或我们自己微服务本身有什么问题。它是Kiam和AWS Java SDK里两个独立的默认值组合在一起导致的问题。独立来看，两个默认值都没什么问题：AWS Java SDK强制credential刷新策略和Kiam比较低的默认过期时间。但是组合起来就导致了问题。<strong>两个单独看都是正确的决定合在一起并不一定是正确的</strong>。</p>
<blockquote>
<p>如果使用的是aws的eks，现在可以不用使用kiam这个插件来实现role访问aws服务，aws eks 实现了service accounts 与 role 绑定，具体参考：<a href="https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html" target="_blank" rel="external">https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/iam-roles-for-service-accounts.html</a></p>
</blockquote>
<p>来源：kubernetes-added-a-0-to-my-latency</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[LPR是什么，央行给有放贷的人带来什么新年礼物]]></title>
      <url>http://team.jiunile.com/blog/2020/01/lpr.html</url>
      <content type="html"><![CDATA[<h2 id="part-1"><a href="#part-1" class="headerlink" title="part 1"></a>part 1</h2><p><code>每一个背负了房贷的人，绝对想不到，进入2020年头的时候，央行会送来一份大礼。</code></p>
<p>2019年12月28日，中国人民银行（央行）发布年内第30号公告。公告中的文字非常专业、晦涩，不是金融专业出身的人，会觉得云里雾里，也体会不到这则公告的史诗级作用。</p>
<p>第一：<strong>从2020年1月1日开始，商业银行不得和买房人，签订参考贷款基准利率的浮动利率贷款合同。</strong></p>
<p>第二：<strong>从2020年3月1日开始，商业银行必须和存量房贷的借款人，废除原有房贷合同，让借款人重新二选一，要么：固定利率；要么：「LPR利率+基点加成」模式。</strong></p>
<a id="more"></a>
<p>也就是说，从今年的3月1日开始至8月份的5个月时间里，当初和买房人签订了房贷合同的银行，会联系买房人，协商废除原有的「基准利率+浮动比例」利率合同，再二选一。</p>
<p>一定要记住，无论银行怎么巧舌如簧，口吐莲花：</p>
<p><code>不要选择固定利率模式，选择「LPR利率+基点加成」模式！</code></p>
<p>央行公告中说得很清楚，这种变更，一辈子只有一次机会，选错了的话，是无法更改的。</p>
<p>不要小看两种利率模式的差别，我们可以大致估算下，在某些年份，利率差距可能达到1-5个百分点。如果房贷余额有300万元的话，每年还款差异可能达到3-15万元。</p>
<p>在过去几年，数百万的年轻人夫妻，掏空了父母辈的钱包，背负了房贷，不敢辞职和跳槽，过上了不敢消费的佛系生活。</p>
<p>翻阅了央行的定期报告，截止2019年8月份，个人贷款余额总量在29万亿元。你的房贷，就在其中。</p>
<p>现在，央行要给你松绑了，卸下重担了，就看你3个月之后，要不要接收这份大礼，改变一家子的命运了。</p>
<h2 id="part-2"><a href="#part-2" class="headerlink" title="part 2"></a>part 2</h2><p>央行在2019年8月份，进行房贷变革的细节。</p>
<p><strong>变革前，房贷利率=央行基准利率+浮动比例。</strong></p>
<p>基准利率由央行统一公布，商业银行无权控制，只能被动跟随。如果央行加息了，或者降息了，那么在来年的固定日期1月1日，购房者的基准利率就统一改变。当前的基准利率是4.90%。</p>
<p>商业银行能够改变的就是“浮动比例”。有些城市的银行货币宽松，鼓励首套房置业，地方政府财源依赖于土地出让金，那么可能在基准利率的基础上打折扣，但是大多数城市，是没有浮动比例，或者上浮的。</p>
<p>对购房者而言，和银行签订的“浮动比例”一旦确认，是无法变更的。</p>
<p><strong>变革后，房贷利率=贷款市场报价（LPR）利率+基点加成。</strong></p>
<p>LPR利率，专业说法是“全国银行间同业拆借中心”，统计18家银行报出的各自的1年期和5年期以上贷款利率，剔除最低价、最高价后，计算算术平均价。</p>
<p>说得更加直白一点</p>
<p><code>LPR利率更加市场化，在反映全社会无风险利率的程度上，更加逼真。</code></p>
<p>当经济过热时，通胀时，利率高；</p>
<p>当经济过冷时，通缩时，利率低。</p>
<p>这种市场化的利率，将贷款人和借款人之间的利益、风险，均衡了下。</p>
<p>央行高屋建瓴，将1年期LPR利率，用于实体经济；将5年期LPR利率，用于了房地产市场。<br><img src="/images/other/1.jpg" alt="1"></p>
<p>当需要给实体经济降息时，就调低1年期LPR利率，不让漫灌的大水进入房地产行业。</p>
<p>当需要单独对房地产调控时，只需要提高5年期LPR的利率，就不会对实体经济产生成本压力。</p>
<p>但是从长期来看，<strong>5年期LPR利率，一定会向1年期LPR利率进行回归和靠拢的。</strong></p>
<p>2019年11月20日，央行授权全国银行间同业拆借中心公布了最新一期贷款市场报价利率（LPR）。1年期LPR利率为4.15%，5年期以上LPR利率为4.80%。同10月21日公布的上期数值相比，均下调了5个基点。</p>
<p>特别地，5年期LPR利率4.80%，比原有的贷款基准利率4.90%，低了10个基点，也就是0.1个百分点。</p>
<p>如果2020年1月以后买房子，签订的是变革后的房贷利率模式，未来LPR利率长期下行，那么买房人和贷款机构的房贷利率，也必然下行，这样的话，买房人承担的利率成本，和全社会的利率成本大致相当。</p>
<p>如果是2019年8月份之前买的房子，买房人和贷款机构签订的是基准利率，或者固定利率，那么未来社会无风险利率下行后，买房人承担的压力，将前所未有的大。这批买房人，完全将自己的人生上贡给了银行帝国，和后来轻装上阵的买房者，泾渭分明，社会的不公平性就拉大了。</p>
<p>有些人可能会说，变革前后，二种利率的差异，就在10个基点，0.1个百分点，差异不大。</p>
<p>你要这么说，只能证明，<code>对我们国家未来利率下行趋势的力量，一无所知！</code></p>
<h2 id="part-3"><a href="#part-3" class="headerlink" title="part 3"></a>part 3</h2><p>你也许会问：</p>
<p><strong>央行和放贷银行，居然给我们买房者这么好的福利，会把吃进肚子里的肉，吐了出来，它们图什么呢？</strong></p>
<p>呵呵。先让明哥笑几声。</p>
<p>如果你能这么想，说明你真的太天真了。</p>
<p>央行和放贷银行，给借款人松绑，绝对不是为了把吃进肚子里的肉吐出来，而是担心未来进入低利率时代以后，借款人弃房而去，那样会引发全社会系统性的大风险。</p>
<p>因为，它们是极其专业和前瞻的机构，已经提前预测到了：</p>
<p><code>在未来5-20年的时间长度内，中国一定会步发达国家的后尘，进入低利率，甚至是负利率时代。</code></p>
<p>这是坏事吗？绝对不是，这是任何经济体发展到一定水平后，必然要进入的常态。</p>
<p>当经济体的生产力已经充分发达，人口不再继续增长，物质充分丰富以后，一个市场的消费能力是有限的。也就是说，企业生产出来的实体商品、提供的服务和虚拟商品，终将超过该国或者全球所有人口的消费潜力。</p>
<p>假如没有突破性的科技进展，那么企业家没有多余利润可图，将不再扩大生产，不将利润再投资，不向银行间接贷款融资，不向股票市场直接股权融资，社会的GDP原地踏步，年轻人薪资20年不上涨。</p>
<p>这样将导致，社会上的存款资金和可投资资金，将无处可去。商业银行收储了海量的居民存款，找不到适格的企业去发放贷款，自然也赚取不了息差。</p>
<p>整体社会的利率，将显著下行。</p>
<p>统计过，2020年初，全球26个国家或经济体当中，有20个国家或经济体利率处于下降趋势。</p>
<p>欧盟央行：-0.5%；</p>
<p>日本央行：-0.1%；</p>
<p>德国10年期国债：-0.675%。</p>
<p>2019年8月5号，丹麦的第三大银行日德兰银行，推出了人类历史上首笔负利率按揭贷款业务，房贷利率为-0.5%。什么意思呢？</p>
<p>如果你借丹麦这家银行100万元去买房，一年后你只需还99.5万元就可以了。</p>
<p>这家银行向买房人发放贷款，贷款买房人，最终还的钱，比当初银行借出的钱，还要少！</p>
<p>听起来，像不像天方夜谭？绝对不是，这在西方发达经济体之中，已经成为了常识。</p>
<p>2019年8月6号，瑞士银行宣布对50万欧元以下的存款账户收取年费，并且不支付利息。</p>
<p>同样地，由于欧盟组织金融体系的利率是-0.5%，这意味着，一个人在欧盟所属的银行存款1万欧元，一年后，只能取回9950欧元。<br><img src="/images/other/2.jpg" alt="2"></p>
<p>2019年11月21日，已经卸任了中国人民银行行长职位的周小川，在创新经济论坛上表示：</p>
<p><strong>中国可以尽量避免快速地进入到负利率时代。</strong></p>
<p>言下之意，我们社会可以推迟进入到负利率时代的时间长度，延迟它的到来，但是改变不了历史归途。</p>
<p>在2017年之前的20年时间里，我们已经习惯了房价的上涨、物价的上涨，人民币购买力的贬值。其根本原因在于，我们国家依然处于发展阶段，各行各业的商品和服务，还有着巨大的空白市场去等着发掘。</p>
<p>房贷利率5.53%、民间借贷利率10-30%、P2P网贷利率20-50%。银行理财产品收益率低于6%，都没有大妈大爷看得上。</p>
<p>从2017年开始，高收益的幻象，逐一破灭了。房价下跌、P2P平台爆雷、上市公司多元化扩张一地鸡毛。那些追逐高收益的人，不仅没有得到高利率，连自己的本金都损失掉了。</p>
<p>股市收益率预期降低、年轻人就业心态放平、中年人只求保住饭碗，这才是未来我国经济的「新常态」。</p>
<p>进入低利率时代后，我们去银行存钱，不仅没有利息，还要倒付费用给银行来负责保管我们的资产数据。</p>
<p>为什么呢？</p>
<p>因为银行收储之后，将居民资产保管在银行内部，它放贷不出去，没有人来借款，自然赚取不了利息差带来的利润。</p>
<p>所以，负利率时代到来以后，谁放钱在银行，谁就要支付给银行一定的费用。</p>
<p>如果借款人发现，自己的存款没有利息，全社会的无风险利率都接近于零，企业不愿意去投资，社会GDP增速原地踏步，自己反而要承担4.9%的利率，那这种负担，绝对是压力山大，不可承受的。</p>
<p>等到那个时候，别说奢望房价上涨成为了白日梦，每年下跌2-10%，都是很有可能的，结果还要凭空承担-4.9%的利率亏损，傻子都知道，应该抛弃房子，直接断供。</p>
<p>如果大批量的买房人，集体断供，最终会引发什么后果呢？那就是2008年美国次贷危机的翻版，银行将倒闭，经济继续萧条，年轻人失业。那种后果不是每个社会都能承受的。</p>
<p>对于那些在2020年，就选择了「LPR利率+基点加成」模式的买房人而言，会张灯结彩、敲锣打鼓地迎接低利率时代、甚至是负利率时代的到来。</p>
<p>理想情况下，假如5年期LPR利率降低到了零，那么当初的买房人承担的利率就只是当初的「基点加成」，无论是上浮还是下浮，都可以认为轻如鸿毛。</p>
<p>这个时候，原有传统模式买房人，承担的利率是4.90%，接受了新模式的买房人，几乎没有房贷利率！</p>
<p>年化4.90%利率的差别，如果房贷余额还剩下200万，那就是年化9.8万元的差距。</p>
<p>这钱不香吗？为什么要凭空贡献给银行呢？</p>
<h2 id="part-4"><a href="#part-4" class="headerlink" title="part 4"></a>part 4</h2><p>站在2020年的门槛上，央行一眼洞穿了未来20年的经济轨迹，参考了西方发达国家的成熟经验，引导各大商业银行贷款人，和借款人，重新签订锚定于LPR利率的贷款合同。</p>
<p>这不仅是给予借款人的大红包，也是站在全社会的角度，未雨绸缪，将收益和风险，在借款人和贷款机构之间分摊，促进整个社会的稳定运行。</p>
<p>但是，央行出台的房贷改革新政，让贷款人和借款人重新签订利率合同，充满着大量的金融术语和晦涩的概念。</p>
<p>商业银行的目的是为了逐利。它们作为贷款人，赚取利息差，让借款人承担的利率越高，银行的利润就越高。<br><img src="/images/other/3.jpg" alt="3"></p>
<p>收取智商税的最佳方式是，制造信息不对称，利用自己的信息优势，来收割对方。</p>
<p>商业银行一定会巧舌如簧、口吐莲花一般，故意将两种利率模式，说得借款人如云山雾里，眼冒金星。</p>
<p>站在银行的角度来说，它们希望借款人像无头苍蝇一样，跟着他们的节奏走，不知不觉就躺在砧板上，摆好了待宰的体位。</p>
<p>只有明哥，惦记着你的钱包。</p>
<p>千言万语，汇成一句话！</p>
<p><code>从2020年3月1日开始的5个月时间里，当银行联系你重新协商房贷利率时，一定要坚持「LPR利率+基点加成」模式，并且利率一年自动更新一次。</code></p>
<p>这个机会，涉及到你家庭数十万，甚至上百万的资金，是用于自己，还是贡献给银行。</p>
<p>可千万别选错了，因为央行只给了一次机会，没有后悔药可吃！</p>
<p>来源：明哥在路上</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes coredns域名解析5秒记录]]></title>
      <url>http://team.jiunile.com/blog/2019/12/k8s-coredns-debug.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>近期线上 <code>k8s</code> 时不时就会出现一些内部服务间的调用超时问题，通过日志可以得知超时的原因都是出现在域名解析上，并且都是 <code>k8s</code> 内部的域名解析超时，于是直接先将内部域名替换成 <code>k8s service</code> 的 IP，观察一段时间发现没有超时的情况发生了，但是由于使用 <code>service IP</code> 不是长久之计，所以还要去找解决办法。<br><a id="more"></a></p>
<h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p>一开始运维同事在调用方 pod 中使用ab工具对目标服务进行了多次压测，并没有发现有超时的请求，我介入之后分析ab这类 http 压测工具应该都会有 dns 缓存，而我们主要是要测试 dns 服务的性能，于是直接动手撸了一个压测工具只做域名解析，代码如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"context"</span></span><br><span class="line">	<span class="string">"flag"</span></span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"net"</span></span><br><span class="line">	<span class="string">"sync/atomic"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> host <span class="keyword">string</span></span><br><span class="line"><span class="keyword">var</span> connections <span class="keyword">int</span></span><br><span class="line"><span class="keyword">var</span> duration <span class="keyword">int64</span></span><br><span class="line"><span class="keyword">var</span> limit <span class="keyword">int64</span></span><br><span class="line"><span class="keyword">var</span> timeoutCount <span class="keyword">int64</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="comment">// os.Args = append(os.Args, "-host", "www.baidu.com", "-c", "200", "-d", "30", "-l", "5000")</span></span><br><span class="line"></span><br><span class="line">	flag.StringVar(&amp;host, <span class="string">"host"</span>, <span class="string">""</span>, <span class="string">"Resolve host"</span>)</span><br><span class="line">	flag.IntVar(&amp;connections, <span class="string">"c"</span>, <span class="number">100</span>, <span class="string">"Connections"</span>)</span><br><span class="line">	flag.Int64Var(&amp;duration, <span class="string">"d"</span>, <span class="number">0</span>, <span class="string">"Duration(s)"</span>)</span><br><span class="line">	flag.Int64Var(&amp;limit, <span class="string">"l"</span>, <span class="number">0</span>, <span class="string">"Limit(ms)"</span>)</span><br><span class="line">	flag.Parse()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> count <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">var</span> errCount <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">	pool := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;, connections)</span><br><span class="line">	exit := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line">	<span class="keyword">var</span> (</span><br><span class="line">		min <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">		max <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">		sum <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		time.Sleep(time.Second * time.Duration(duration))</span><br><span class="line">		exit &lt;- <span class="literal">true</span></span><br><span class="line">	&#125;()</span><br><span class="line">endD:</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> pool &lt;- <span class="literal">nil</span>:</span><br><span class="line">			<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">				<span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">					&lt;-pool</span><br><span class="line">				&#125;()</span><br><span class="line">				resolver := &amp;net.Resolver&#123;&#125;</span><br><span class="line">				now := time.Now()</span><br><span class="line">				_, err := resolver.LookupIPAddr(context.Background(), host)</span><br><span class="line">				use := time.Since(now).Nanoseconds() / <span class="keyword">int64</span>(time.Millisecond)</span><br><span class="line">				<span class="keyword">if</span> min == <span class="number">0</span> || use &lt; min &#123;</span><br><span class="line">					min = use</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">if</span> use &gt; max &#123;</span><br><span class="line">					max = use</span><br><span class="line">				&#125;</span><br><span class="line">				sum += use</span><br><span class="line">				<span class="keyword">if</span> limit &gt; <span class="number">0</span> &amp;&amp; use &gt;= limit &#123;</span><br><span class="line">					timeoutCount++</span><br><span class="line">				&#125;</span><br><span class="line">				atomic.AddInt64(&amp;count, <span class="number">1</span>)</span><br><span class="line">				<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">					fmt.Println(err.Error())</span><br><span class="line">					atomic.AddInt64(&amp;errCount, <span class="number">1</span>)</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;()</span><br><span class="line">		<span class="keyword">case</span> &lt;-exit:</span><br><span class="line">			<span class="keyword">break</span> endD</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">"request count：%d\nerror count：%d\n"</span>, count, errCount)</span><br><span class="line">	fmt.Printf(<span class="string">"request time：min(%dms) max(%dms) avg(%dms) timeout(%dn)\n"</span>, min, max, sum/count, timeoutCount)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译好<a href="http://www.jiunile.com/k8s/ab-dns" target="_blank" rel="external">二进制</a>程序直接丢到对应的 pod 容器中进行压测：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 200个并发,持续30秒</span></span><br><span class="line"></span><br><span class="line">$ ./ab-dns -host &#123;service&#125;.&#123;namespace&#125; -c 200 <span class="_">-d</span> 30</span><br></pre></td></tr></table></figure></p>
<p>这次可以发现最大耗时有5s多，多次测试结果都是类似：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ./ab-dns -host s-inno-bpm.inno-ci -c 200 -d 30</span></span><br><span class="line">request count：109061</span><br><span class="line">error count：0</span><br><span class="line">request time：min(1ms) max(5082ms) avg(53ms) timeout(0n)</span><br></pre></td></tr></table></figure></p>
<p>而我们内部服务间 HTTP 调用的超时一般都是设置在3s左右，以此推断出与线上的超时情况应该是同一种情况，在并发高的情况下会出现部分域名解析超时而导致 HTTP 请求失败。</p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>起初一直以为是 <code>coredns</code> 的问题，于是找运维升级了下 <code>coredns</code> 版本再进行压测，发现问题还是存在，说明不是版本的问题，难道是 <code>coredns</code> 本身的性能就差导致的？想想也不太可能啊，才 200 的并发就顶不住了那性能也未免太弱了吧，结合之前的压测数据，平均响应都挺正常的(53ms)，但是就有个别请求会延迟，而且都是 5 秒左右，所以就又带着k8s dns 5s的关键字去 google 搜了一下，这不搜不知道一搜吓一跳啊，原来是 k8s 里的一个大坑啊(其实和 k8s 没有太大的关系，只是 k8s 层面没有提供解决方案)。</p>
<h2 id="5s-超时原因"><a href="#5s-超时原因" class="headerlink" title="5s 超时原因"></a>5s 超时原因</h2><p>linux 中 <code>glibc</code> 的 resolver 的缺省超时时间是 5s，而导致超时的原因是内核 <code>conntrack</code> 模块的 bug。</p>
<blockquote>
<p>Weave works 的工程师 Martynas Pumputis 对这个问题做了很详细的分析：<a href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts" target="_blank" rel="external">https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts</a></p>
</blockquote>
<p>这里再引用下 <a href="https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/" target="_blank" rel="external">https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/</a> 文章中的解释：</p>
<blockquote>
<p>DNS client (glibc 或 musl libc) 会并发请求 A 和 AAAA 记录，跟 DNS Server 通信自然会先 connect (建立 fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议， connect 时并不会发包，也就不会创建 conntrack 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，send 时各自发的包它们源 Port 相同(因为用的同一个 socket 发送)，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们分别创建 conntrack 表项，而集群内请求 kube-dns 或 coredns 都是访问的 CLUSTER-IP，报文最终会被 DNAT 成一个 endpoint 的 POD IP，当两个包恰好又被 DNAT 成同一个 POD IP 时，它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 dns 的 pod 副本只有一个实例的情况就很容易发生(始终被 DNAT 成同一个 POD IP)，现象就是 dns 请求超时，client 默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 dns 请求有 5s 的延时。</p>
</blockquote>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="方案（一）：使用-TCP-协议发送-DNS-请求"><a href="#方案（一）：使用-TCP-协议发送-DNS-请求" class="headerlink" title="方案（一）：使用 TCP 协议发送 DNS 请求"></a>方案（一）：使用 TCP 协议发送 DNS 请求</h3><p>通过 <code>resolv.conf</code> 的 <code>use-vc</code> 选项来开启 TCP 协议, 修改 <code>/etc/resolv.conf</code> 文件，在最后加入一行文本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver ...</span><br><span class="line">search ...</span><br><span class="line">options ndots:5 use-vc</span><br></pre></td></tr></table></figure></p>
<p>经过测试，确实没有出现 5s 的超时问题了，但是部分请求耗时还是比较高，在 4s 左右，而且平均耗时比 UPD 协议的还高，效果并不好。</p>
<h3 id="方案（二）：避免相同五元组-DNS-请求的并发"><a href="#方案（二）：避免相同五元组-DNS-请求的并发" class="headerlink" title="方案（二）：避免相同五元组 DNS 请求的并发"></a>方案（二）：避免相同五元组 DNS 请求的并发</h3><p>通过 <code>resolv.conf</code> 的 <code>single-request-reopen</code> 和 <code>single-request</code> 选项来避免：</p>
<ul>
<li>single-request-reopen (glibc&gt;=2.9) 发送 A 类型请求和 AAAA 类型请求使用不同的源端口。这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突。</li>
<li>single-request (glibc&gt;=2.10) 避免并发，改为串行发送 A 类型和 AAAA 类型请求，没有了并发，从而也避免了冲突。</li>
</ul>
<p>修改 /etc/resolv.conf 文件，在最后加入一行文本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver ...</span><br><span class="line">search ...</span><br><span class="line">options ndots:5 timeout:2 single-request-reopen</span><br></pre></td></tr></table></figure></p>
<p>通过压测结果可以看到 <code>single-request-reopen</code> 和 <code>single-request</code> 选项确实可以显著的降低域名解析耗时。</p>
<h2 id="关于方案（一）和方案（二）的实施步骤和缺点"><a href="#关于方案（一）和方案（二）的实施步骤和缺点" class="headerlink" title="关于方案（一）和方案（二）的实施步骤和缺点"></a>关于方案（一）和方案（二）的实施步骤和缺点</h2><p>其实就是要给容器的 <code>/etc/resolv.conf</code> 文件添加选项，目前有两个方案比较合适：</p>
<ol>
<li><p>通过修改 pod 的 postStart hook 来设置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span></span><br><span class="line"><span class="attr">  postStart:</span></span><br><span class="line"><span class="attr">    exec:</span></span><br><span class="line"><span class="attr">      command:</span></span><br><span class="line"><span class="bullet">        -</span> /bin/sh</span><br><span class="line"><span class="bullet">        -</span> -c</span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/echo 'options single-request-reopen' &gt;&gt; /etc/resolv.conf"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>通过修改 pod 的 template.spec.dnsConfig 来设置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">template:</span></span><br><span class="line"><span class="attr">  spec:</span></span><br><span class="line"><span class="attr">    dnsConfig:</span></span><br><span class="line"><span class="attr">      options:</span></span><br><span class="line"><span class="attr">        - name:</span> single-request-reopen</span><br></pre></td></tr></table></figure>
</li>
</ol>
<blockquote>
<p><code>注: 需要 k8s 版本&gt;=1.9</code></p>
</blockquote>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>不支持 <code>alpine</code> 基础镜像的容器，因为 <code>apline</code> 底层使用的 <code>musl libc</code> 库并不支持这些 <code>resolv.conf</code> 选项，所以如果使用 <code>alpine</code> 基础镜像构建的应用，还是无法规避超时的问题。</p>
<h2 id="方案（三）：本地-DNS-缓存"><a href="#方案（三）：本地-DNS-缓存" class="headerlink" title="方案（三）：本地 DNS 缓存"></a>方案（三）：本地 DNS 缓存</h2><p>其实 k8s 官方也意识到了这个问题比较常见，给出了 coredns 以 cache 模式作为 daemonset 部署的解决方案: <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns</a></p>
<p>大概原理就是：</p>
<blockquote>
<p>本地 DNS 缓存以 DaemonSet 方式在每个节点部署一个使用 hostNetwork 的 Pod，创建一个网卡绑上本地 DNS 的 IP，本机的 Pod 的 DNS 请求路由到本地 DNS，然后取缓存或者继续使用 TCP 请求上游集群 DNS 解析 (由于使用 TCP，同一个 socket 只会做一遍三次握手，不存在并发创建 conntrack 表项，也就不会有 conntrack 冲突)</p>
</blockquote>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><ol>
<li><p>获取当前 <code>kube-dns service</code> 的 <code>clusterIP</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get svc kube-dns -o jsonpath=<span class="string">"&#123;.spec.clusterIP&#125;"</span></span><br><span class="line"></span><br><span class="line">10.96.0.10</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载官方提供的 yaml 模板进行关键字替换</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ wget -O nodelocaldns.yaml <span class="string">"https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"</span> &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__DNS__SERVER__/10.96.0.10/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__LOCAL__DNS__/169.254.20.10/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__DNS__DOMAIN__/cluster.local/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__CLUSTER__DNS__/10.96.0.10/g'</span> nodelocaldns.yaml &amp;&amp; \</span><br><span class="line">$ sed -i <span class="string">'s/__PILLAR__UPSTREAM__SERVERS__/\/etc\/resolv.conf/g'</span> nodelocaldns.yaml</span><br></pre></td></tr></table></figure>
</li>
<li><p>最终 yaml 文件如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> node-local-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kube-dns-upstream</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> kube-dns</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">    kubernetes.io/name: <span class="string">"KubeDNSUpstream"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> dns</span><br><span class="line"><span class="attr">      port:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">      protocol:</span> UDP</span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">    - name:</span> dns-tcp</span><br><span class="line"><span class="attr">      port:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">      protocol:</span> TCP</span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> kube-dns</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> node-local-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  Corefile:</span> <span class="string">|</span><br><span class="line">    cluster.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache &#123;</span><br><span class="line">                success 9984 30</span><br><span class="line">                denial 9984 5</span><br><span class="line">        &#125;</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . 10.96.0.10 &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">        &#125;</span><br><span class="line">    in-addr.arpa:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . 10.96.0.10 &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        &#125;</span><br><span class="line">    ip6.arpa:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . 10.96.0.10 &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        &#125;</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">        loop</span><br><span class="line">        bind 169.254.20.10 10.96.0.10</span><br><span class="line">        forward . /etc/resolv.conf &#123;</span><br><span class="line">                force_tcp</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        &#125;</span><br><span class="line">---</span><br><span class="line"></span><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> DaemonSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> node-local-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> node-local-dns</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">    rollingUpdate:</span></span><br><span class="line"><span class="attr">      maxUnavailable:</span> <span class="number">10</span>%</span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> node-local-dns</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> node-local-dns</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      priorityClassName:</span> system-node-critical</span><br><span class="line"><span class="attr">      serviceAccountName:</span> node-local-dns</span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      dnsPolicy:</span> Default <span class="comment"># Don't use cluster DNS.</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">"CriticalAddonsOnly"</span></span><br><span class="line"><span class="attr">          operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> node-cache</span><br><span class="line"><span class="attr">          image:</span> k8s.gcr.io/k8s-dns-node-cache:<span class="number">1.15</span><span class="number">.7</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">25</span>m</span><br><span class="line"><span class="attr">              memory:</span> <span class="number">5</span>Mi</span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line">            [</span><br><span class="line">              <span class="string">"-localip"</span>,</span><br><span class="line">              <span class="string">"169.254.20.10,10.96.0.10"</span>,</span><br><span class="line">              <span class="string">"-conf"</span>,</span><br><span class="line">              <span class="string">"/etc/Corefile"</span>,</span><br><span class="line">              <span class="string">"-upstreamsvc"</span>,</span><br><span class="line">              <span class="string">"kube-dns-upstream"</span>,</span><br><span class="line">            ]</span><br><span class="line"><span class="attr">          securityContext:</span></span><br><span class="line"><span class="attr">            privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">              name:</span> dns</span><br><span class="line"><span class="attr">              protocol:</span> UDP</span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">53</span></span><br><span class="line"><span class="attr">              name:</span> dns-tcp</span><br><span class="line"><span class="attr">              protocol:</span> TCP</span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">9253</span></span><br><span class="line"><span class="attr">              name:</span> metrics</span><br><span class="line"><span class="attr">              protocol:</span> TCP</span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line"><span class="attr">              path:</span> /health</span><br><span class="line"><span class="attr">              port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - mountPath:</span> /run/xtables.lock</span><br><span class="line"><span class="attr">              name:</span> xtables-lock</span><br><span class="line"><span class="attr">              readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">            - name:</span> config-volume</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/coredns</span><br><span class="line"><span class="attr">            - name:</span> kube-dns-config</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/kube-dns</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> xtables-lock</span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> /run/xtables.lock</span><br><span class="line"><span class="attr">            type:</span> FileOrCreate</span><br><span class="line"><span class="attr">        - name:</span> kube-dns-config</span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> kube-dns</span><br><span class="line"><span class="attr">            optional:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - name:</span> config-volume</span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> node-local-dns</span><br><span class="line"><span class="attr">            items:</span></span><br><span class="line"><span class="attr">              - key:</span> Corefile</span><br><span class="line"><span class="attr">                path:</span> Corefile.base</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>通过 yaml 可以看到几个细节：</p>
<ol>
<li>部署类型是使用的 <code>DaemonSet</code>，即在每个 <code>k8s node</code> 节点上运行一个 dns 服务</li>
<li><code>hostNetwork</code> 属性为 <code>true</code>，即直接使用 node 物理机的网卡进行端口绑定，这样在此 node 节点中的 pod 可以直接访问 dns 服务，不通过 service 进行转发，也就不会有 DNAT</li>
<li><code>dnsPolicy</code> 属性为 <code>Default</code>，不使用 <code>cluster DNS</code>，在解析外网域名时直接使用本地的 DNS 设置</li>
<li>绑定在 node 节点 <code>169.254.20.10</code> 和 <code>10.96.0.10</code> IP 上，这样节点下面的 pod 只需要将 dns 设置为<code>169.254.20.10</code> 即可直接访问宿主机上的 dns 服务。</li>
</ol>
<h3 id="实施"><a href="#实施" class="headerlink" title="实施"></a>实施</h3><ol>
<li><p>通过修改 <code>pod</code> 的 <code>template.spec.dnsConfig</code> 来设置，并将 <code>dnsPolicy</code> 设置为 <code>None</code></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line"><span class="attr">  spec:</span></span><br><span class="line"><span class="attr">    dnsConfig:</span></span><br><span class="line">	  nameservers:</span><br><span class="line"><span class="bullet">        -</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line"><span class="attr">      searches:</span></span><br><span class="line"><span class="bullet">        -</span> public.svc.cluster.local</span><br><span class="line">		- svc.cluster.local</span><br><span class="line">		- cluster.local</span><br><span class="line"><span class="attr">      options:</span></span><br><span class="line"><span class="attr">        - name:</span> ndots</span><br><span class="line"><span class="attr">          value:</span> <span class="string">"5"</span></span><br><span class="line"><span class="attr">    dnsPolicy:</span> None</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改默认的 cluster-dns，在 node 节点上将 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 文件中的 <code>--cluster-dns</code> 参数值修改为 <code>169.254.20.10</code>，然后重启 <code>kubelet</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>参考：</p>
<ul>
<li>monkeywie.github.io</li>
<li>imroc.io</li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[白话Kubernetes核心组件及原理]]></title>
      <url>http://team.jiunile.com/blog/2019/12/k8s-k8s-intro.html</url>
      <content type="html"><![CDATA[<h2 id="Kubernetes是什么？"><a href="#Kubernetes是什么？" class="headerlink" title="Kubernetes是什么？"></a>Kubernetes是什么？</h2><p>Kubernetes其实就是一个集群，从我们此前运维的角度来理解，它就是一个集群，组合多台主机的资源（内存、CPU、磁盘等）整合成一个大的资源池并统一对外提供计算存储等能力的集群。我们找很多台主机，每台主机上面安装Kubernetes的相关程序，而不同的主机程序之间相互通信，从而完成彼此之间的协调，并且通过这些应用程序之间的协同工作，把多个主机当成一个主机来使用，形成一个集群，仅此而已，但是在kubernetes集群当中主机是分角色的，即所谓的有中心结点架构的集群系统，master/nodes模型, 由一组节点用于master不需要太多，一般高可用的话需要三个（根据集群规模来判断），nodes节点（worker节点）就是干活的，Kubernetes上提供的各种资源服务，运行在node节点上面。<br><a id="more"></a></p>
<h2 id="用户如何在kubernetes运行容器，逻辑过程是什么？"><a href="#用户如何在kubernetes运行容器，逻辑过程是什么？" class="headerlink" title="用户如何在kubernetes运行容器，逻辑过程是什么？"></a>用户如何在kubernetes运行容器，逻辑过程是什么？</h2><p>用户把创建启动容器的请求首先发给master，具体来说是发给了master节点上面的<code>API Server</code>组件， <code>API Server</code>通过调度器，按照预设的调度算法及策略，去分析各node节点上面现有的可用资源状态，然后找一个最佳适配，来运行用户所请求容器的结点，并把它调度上去（这里需要与node节点上面的<code>kubelet</code>交互），并由这个node节点上面本地的docker或其它容器引擎负责把这个容器启动起来，要启动容器，需要有镜像，镜像在哪里呢？在仓库上面，node上面启动容器是会先检查本地是否有镜像（根据镜像拉取策略），如果没有会docker pull下来，然后再启动，kubernetes自身并没有托管自动所依赖的每一个容器镜像，而是需要到仓库中去下载的，仓库可以是私有的，也可以是公有的。</p>
<p>集群在master节点上面提供一个<code>API Server</code>组件，它负责接受请求，解析请求，处理请求的，至于当用户请求的是创建一个容器，最好不要运行在master节点上面，而应该运行在node节点上面，确定哪个node更合适，这个时间就需要<code>scheder</code>调度器，它负责监控每个node节点上面总可用的计算、内存、存储等资源，并根据用户所请求创建的这个容器所需要的资源，docker容器可以做资源限制 ，但在kubernetes上面不但可以设定容器使用资源的上限（阀值），还可以设定资源使用的下限（资源请求量），调度器就是根据容器的最低需求来进行评估，哪一个节点最合适；当然了，资源的评估不只是一个维度，而是从多个维度考虑，这都是调度器<code>scheduler</code>根据调度策略和算法需要考虑的，如果在一个node上面把容器启动起来了，我们还需要对容器中的应用程序的健康状态做监测，我们不但能根据容器中应用程序是否运行判断它的健康状况，还可以根据额外的健康状态探测方式来探测，我们叫做可用性探测机制来探测服务的可用性。如果一旦容器中的应用挂了，我们又需要确保容器中的一个容器要运行，此时怎么办？node节点之上有一个应用程序，这个应用程序就是<code>kebulet</code>，这个应用程序确保容器始终处于健康状态，如果出现问题，它就会通知<code>API Server</code>，然后重新调度；但是有一点，很不幸，这个node节点如果宕机了，那么此前拖管在此node上面的所有容器就挂了，我们知道kubernetes具有自愈的能力，无论是单个容器，还是node节点上面的所有容器，一旦容器不见了，是不需要人工参与的，kubernetes会使用新的个体来取代它，它会在其它node上面创建出来一模一样的容器出来。如何确保这个容器是健康的呢？以及一旦出问题就可以及时被发现呢? 其实kubernetes是通过控制器组件来负责监控它所管理的每一个容器的健康状态，一旦发现不健康了，控制器向master上面 <code>API server</code>发请求，容器挂了一个，你帮我重新调度再启动一个，这里控制器需要在本地不停的loop循环，周期性，持续性的探测所管理的容器的健康状况，一旦不健康，或者不符合用户所定义（期望）的目标，此是调度器就会向用户期待的状态向前移，确保符合用户期望的状态；其实在kubernetes集群中我们有很多很多的控制器，假设我们有一个控制器挂了呢，用于监控容器健康的控制器不健康了，容器的健康状态就无法保证，怎么办？我们在master节点上面有一个控制器管理器，控制器管理器负责控制监控每个控制器的健康状况，控制器管理器如果出现问题怎么办，因此在这里，我们需要对控制器管理器做冗余。</p>
<p>以上我们通过在集群上面创建一个容器的例子，讲解了<code>API Server</code>、<code>scheduler</code>、<code>控制器</code>、<code>控制管理器</code>等。</p>
<h2 id="什么是Pod？"><a href="#什么是Pod？" class="headerlink" title="什么是Pod？"></a>什么是Pod？</h2><p>Pod，英文意思是豆荚。大家都知道这种植物，一个豆荚中有几个豆粒。</p>
<p>Kubernetes上面运行的最小单元是<code>Pod</code>,  kubernetes并不直接调度容器的运行，而调度的目标是<code>Pod</code>，<code>Pod</code>可以理解为容器的外壳，给容器做了一层抽象的封装，因此<code>Pod</code>成为了Kubernetes集群之上最小的调度单位（逻辑单元），<code>Pod</code>内部主要是用来放容器的，<code>Pod</code>有一个特点，一个<code>Pod</code>中可以运行多个容器，多个容器共享同一个底层的网络命名空间（底层的<code>net</code>, <code>uts</code>, <code>IPC</code>三个网络命名空间），另外三个命名空间相互隔离（<code>User</code>, <code>mnt</code>, <code>pid</code>），这样一来，同一个<code>Pod</code>上面的多个容器，每个容器上面跑应用程序 ，对外更像是同一台“虚拟机”，这也是kubernetes组织容器的一个非常精巧的办法，基于此我们可以构建较为精细的容器间通信，并且同一个Pod上面的容器，还共享第二种资源，叫做存储卷，存储卷不属于容器，属于<code>Pod</code>，<code>Pod</code>的磁盘，相同<code>Pod</code>的容器共享。</p>
<p>各个node节点主要是用来运行<code>Pod</code>的，一般说来，一个<code>Pod</code>上面只放一个容器，除非有特别紧密的关系，需要放在同一个<code>Pod</code>上面，否则，不要放在同一<code>Pod</code>上面；如果确实有需要，将多个容器需要放在一个<code>Pod</code>中，通常有一个容器是主容器，其它的容器为辅助主容器，辅助容器中的应用程序主要是为了完成更多功能来辅佐主容器工作，这里我们调度器也是调度的<code>Pod</code>, node节点上面也是<code>Pod</code>, <code>Pod</code>是一个原子单元，也就意味着一个<code>Pod</code>中有一个容器，还是有多个容器，一旦被调度之后，相同<code>Pod</code>上面的容器，只能在同一个node节点上面。</p>
<p>创建<code>Pod</code>时，可以直接创建，并且自主管理的，但它仍然要提交给<code>API Server</code>，由<code>API Server</code>接收以后，通过调度器调度到指定的Node节点上，而node节点启动此Pod，此后如果pod上面的容器出现故障，需要重要重启容器，需要<code>kubelet</code>完成，但是node节点故障了，节点就消失了。还有一种Pod的创建方式，是通过控制器来创建的，后面为讲什么是控制器，它的作用是什么？</p>
<h2 id="Node节点是做什么的？"><a href="#Node节点是做什么的？" class="headerlink" title="Node节点是做什么的？"></a>Node节点是做什么的？</h2><p>刚才说了node是kubernetes集群中的工作节点，负责运行由master节点上面指派的各种任务，而最核心的任务是以Pod的形式运行容器的，理解上讲node可以是任何形式的资源设备，只要有传统意义上的内存、CPU、存储资源即可，并且可以安装上Kubernetes集群的应用程序 ，都可以做为k8s集群的一个份子来工作，它是承载资源的。</p>
<p>这样一来终端用户不需要关心应用程序（Pod）在哪个Node节点上面，它就这样脱离了终端用户的视线，终端用户也无需关注应用程序部署在哪个node节点上面的pod，从而真正意义上实现了资源池，从而进行统一管理。</p>
<h2 id="什么是标签，标签选择器是做什么的？"><a href="#什么是标签，标签选择器是做什么的？" class="headerlink" title="什么是标签，标签选择器是做什么的？"></a>什么是标签，标签选择器是做什么的？</h2><p>如何让一个控制器管理指定的Pod，例如，我们创建了5个Pod，Pod中运行tomcat容器，我们让一个控制器来管理这一组Pod，为了让Pod能够实现被控制器管理识别，我们需要在Pod上面附加一些元数据（标签），用标签来识别Pod，在创建Pod的时候，或者人为的打上一个标签，让控制器能够识别出标签，进而识别出Pod。我们前面创建了5个 Pod，我们在每一个pod上面加一个标签app, 标签的值叫tomcat (<code>标签：值====&gt; app:tomcat</code>)，我们想把这一类找出来，怎么找，我们先找拥有key是app，并且值是tomcat的pod分拣出来。标签是Kubernetes大规模集群管理、分类、识别资源使用的，标签是非常非常重要的凭证，我们是如何把我们感兴趣的标签找到的呢，我们有一个标签选择器/挑选器（selector）组件，标签选择器，简单来讲就是根据标签，过滤符合条件的资源对象的机制，其实标签不只是Pod有，很多其它资源都有，因此这种选择器叫做标签选择器，而不叫pod标签选择器，Kubernetes是Restfull 风格的API，通过http或者https对外提供服务，所以所有Restfull对外提供的服务资源都称为对象，所有的对象都可以拥有标签，所有的标签都可以使用标签选择器来选择，只不过pod是其中一种比较重要的。</p>
<h2 id="什么是控制器，控制器是做什么的，有哪些控制器？"><a href="#什么是控制器，控制器是做什么的，有哪些控制器？" class="headerlink" title="什么是控制器，控制器是做什么的，有哪些控制器？"></a>什么是控制器，控制器是做什么的，有哪些控制器？</h2><p>我们刚才讲Pod的时候 ，讲到了创建Pod时，一种是直接创建Pod，Pod删除后，不会自动创建，还有一种创建Pod的方式，是通过控制器创建的Pod，这种<code>控制器管理的Pod</code>， 正是控制器管理器机制的使用。在Kubernetes设计中，Pod完全可以叫做有生命周期的对象，而后由调度器将其调度至集群中的某节点，运行以后，任务终止也就被删除停掉了，但是有一些任务，比如nginx，或者运行一个tomcat，他们是做为守护进程来运行的，这种程序，我们要确保这种pod随时运行，一旦出现故障，需要第一时间发现，要么取代它，要么重启它，要么重建一个新的pod，这种靠人的右眼是无法保证的，而Kubernetes提供了具备这种工作能力的组件叫<code>Pod控制器</code>。</p>
<p>Pod控制器最早的一种叫<code>ReplicationController</code> (副本控制器，早期版本的控制器，也称为Pod控制器)， 当我们启动一个pod时，一个不够了，可以再启动一个副本，控制器就是控制同一类资源对象的副本，一旦副本数量少了，就会自动加一个，能够定义要求的副本数，多了就删除，精确符合人们期望的数量，它还可以实现滚动更新，它允许临时添加副本，然后把旧版本的去掉，实现滚动更新；它也允许回滚操作，后来的版本中新加了<code>ReplicaSetController</code>（副本集控制器），而<code>ReplicaSetController</code>也不直接使用，而是有一个声明式更新的控制器叫<code>Deployment</code>，用它来进行管理控制， 我们使用的最多的也是<code>Deployment</code>控制器，而<code>Deployment</code>控制器只能管理哪些无状态的应用，哪么有状态的应用如何控制管理呢？我们使用新的控制器，叫<code>StatefulSet</code>有状态副本集，另外如果我们需要在每个node上面运行一个Pod，而不是随意运行，我们还需要一个<code>DaemonSet</code>，如果我们运行作业，还需要Job, 周期性作业，<code>Cronjob</code>,  这些是常见的Pod控制器；后面的这些控制器都是实现一种特定的应用管理，比如临时运行一个容器去完成删除日志的功能，这个运行完就删除了，我们就可以使用Job控制器管理Pod, 但是如果Job没有运行完挂了，需要重新启动，如果运行完，就删除了，再比如nginx一直需要处于运行状态，就不能使用Job控制器，所以说这么多的控制器是用于确保不同类型的Pod资源，来符合用户所期望的方式来运行，像<code>Deployment</code>控制器还支持二级控制器，叫<code>HPA</code>，叫水平Pod，自动伸缩控制器，比如我们一个控制器控制两个副本在运行，但在资源利用率高的时候，可以自动的伸缩控制，就是使用<code>HPA</code>进行控制，一旦利用率低了，可以自动减少，但要符合我们预期的最小值。</p>
<h2 id="Serveice是什么，为什么需要Service？"><a href="#Serveice是什么，为什么需要Service？" class="headerlink" title="Serveice是什么，为什么需要Service？"></a>Serveice是什么，为什么需要Service？</h2><p>到这里我们想到一个问题，Pod是由生命周期的，万一Pod所在Node节点宕机了，Pod有可能需要在其它的 Node节点上面重新创建，而重新创建完成后的pod，跟之前的不是一个，只不过是应用程序一样而已，提供相同的服务，由于每个pod中容器的IP地址就不一样，这样一来就有一个问题，我们客户端怎么去访问这些Pod呢？是利用服务发现机制，首先客户端每一次去访问服务时，客户端是不知道后端的服务是谁的（不知道pod的存在），他需要找一个地方问一句，发现一下，有没有这种服务，这些服务是Pod启动的时候注册到一个类似总线地址上，客户端直接去总线位置去问，有没有，有的话，给一个Pod地址，客户端与Pod地址进行通信；因此尽可能降低这种复杂度，Kubernetes为每一组提供相同功能的Pod和客户端之间添加了一个中间层，这个中间层是固定的，这个中间层就叫service，只要service不删除，它就是固定的，名称也是固定的，当客户端需要访问时，只需要在客户端写上service 主机名|服务器|地址即可，也不需要发现，而这个服务service 是一个调度器，不但提供一个固定稳定的访问入口，只要不删除，它就是稳定的，客户端只需要写service名称即可，服务再把请求代理到后面的pod上面，那么Pod宕机了，新创建的pod会被service立即给关联进来；还会把新加的pod作为service后面的可用资源对象之一，怎么实现的呢？我们知道 客户端与服务器通信是通过<code>IP：Port</code>或者<code>域名:Port</code>形式，而service与后面的pod不是依靠<code>IP:Port</code>的形式（因为pod的主机名和IP经常要变），而是通过Pod上面固定的标签来识别，只要是相同标签的pod，不管主机名和IP怎么变，都会被service通过标签识别，service是通过标签选择器来关联pod的；这样一来，只要pod属于这个标签选择器，就能立即被service能选中，并且做为service后端组件存在，关联进来以后，再动态探测这个pod的IP地址是什么，端口是什么，并做为自己后端可调度的服务器，资源对象， 最后，客户端是通过service代理至后端pod进行通信；意味着客户端看到的地址就是service的地址，而在kubernetes集群上service可不是什么应用程序 ，也不是一个实体组件，它只不过是一个<code>iptables DNAT</code>规则；我们创建一个<code>DNAT</code> 规则 ，我们所有到达xxx地址的，都统统被目标地址转换成yyy地址，<code>DNAT</code>规则只是一个规则 ，而service地址，事实上并没有配置到任何一张网卡上，是不存在的，它仅仅出在规则中，可以ping通的，并且可以做请求中转，能ping通是因为有<code>TCP/IP</code>协议栈。这个IP地址，仅出现在规则中，更重要的是service做为Kubernetes中的对象来讲，它有名称，就相当于这个服务的名字，名称可以被解析，就是把service名称解析成IP，名称解析靠DNS，没错，我们安装完k8s后，第一件事，就是让部署一个<code>DNS Pod</code>，以确保service被解析，这个Pod是Kubernetes自身的服务就需要的pod，所以我们称之为基础性的系统架构级的pod或对象，而且称他们叫集群的附件。</p>
<h2 id="集群附件DNS"><a href="#集群附件DNS" class="headerlink" title="集群附件DNS"></a>集群附件DNS</h2><p>DNS附件只是Kubernetes集群中纵多附件中的一个，并且这种DNS有一个很有意义的特点，可以动态的创建，动态的改变，动态的更新，动态的变动，比如，你更新了service名称，DNS中的记录即就会被改变，再比如我们手动修改了service IP ,他会自动触发DNS 服务中的解析记录的更改，所以以后客户端访问service时，可以直接访问服务的名称 ，而由集群中专门的DNS服务来负责解析，解析的是service的地址，不是pod地址，再由service代理访问pod，刚才也说了，这种代理是端口代理，由DNAT实现，可不能忘记service后面中两个或者多个pod，这里的DNAT就是多个目标了，多目标调度，对于linux来讲，大家知道对于iptables来讲，已经把负载均衡的功能主要交给了IPVS，因此如果service背后的同一个服务有多个Pod，并且由DNAT来实现，可能在调度效果上并不尽人意，因此在 Kubernetes 1.11版本以后，已经把iptables规则改成了IPVS规则，也就相当于，当你创建一条service规则时，就创建了一条IPVS规则 ，只不过是NAT模型的IPVS规则，因此还支持用户可以指定任意的调度算法，如轮询、加权、最小连接等，所以你就会发现LVS是我们的基础性服务，以上就是我们所讲的service组件。</p>
<p>来源：Linux点滴运维实践</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes修改主机ip]]></title>
      <url>http://team.jiunile.com/blog/2019/12/k8s-kubeadm-edit-hostip.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于外部等不确定因素印象，kubernetes集群中的主机IP修改了，会导致集群受影响，故出以下教程来保证集群的稳定性。此教程适用于kubeadm安装的kubernetes集群，使用版本kubeadm1.15+ </p>
<a id="more"></a>
<h2 id="修改master主机ip"><a href="#修改master主机ip" class="headerlink" title="修改master主机ip"></a>修改master主机ip</h2><h3 id="方式一：通过kubeadm命令进行调整"><a href="#方式一：通过kubeadm命令进行调整" class="headerlink" title="方式一：通过kubeadm命令进行调整"></a>方式一：通过kubeadm命令进行调整</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kubeadm.conf 配置文件如下</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.3 <span class="comment">#--&gt;这里改成你集群对应的版本</span></span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers </span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成/etc/kubernetes目录下的conf文件</span></span><br><span class="line">kubeadm init phase kubeconfig all --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下文件</span></span><br><span class="line"><span class="comment">#admin.conf	</span></span><br><span class="line"><span class="comment">#controller-manager.conf	</span></span><br><span class="line"><span class="comment">#kubelet.conf	</span></span><br><span class="line"><span class="comment">#scheduler.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成manifests目录下的yaml配置</span></span><br><span class="line">kubeadm init phase control-plane all --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下文件</span></span><br><span class="line"><span class="comment">#manifests/kube-apiserver.yaml</span></span><br><span class="line"><span class="comment">#manifests/kube-controller-manager.yaml</span></span><br><span class="line"><span class="comment">#manifests/kube-scheduler.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成etcd.yaml,如果是外部etcd则略过</span></span><br><span class="line">kubeadm init phase etcd <span class="built_in">local</span> --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下文件，外部etcd则不会影响</span></span><br><span class="line"><span class="comment">#manifests/etcd.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新生成组件配置</span></span><br><span class="line">kubeadm init phase addon all --config=/root/kubeadm.conf</span><br><span class="line"><span class="comment">#运行上述命令会影响以下组件</span></span><br><span class="line"><span class="comment">#kube-proxy</span></span><br><span class="line"><span class="comment">#coredns</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#备份/etc/kubernetes/pki目录</span></span><br><span class="line">cp -r /etc/kubernetes/pki /etc/kubernetes/pki-old</span><br><span class="line">rm -rf /etc/kubernetes/pki/apiserver.*  /etc/kubernetes/pki/front-proxy-client* </span><br><span class="line">rm -rf /etc/kubernetes/pki/etcd/healthcheck-client* /etc/kubernetes/pki/etcd/peer* /etc/kubernetes/pki/etcd/server*</span><br><span class="line"><span class="comment">#重新生成证书</span></span><br><span class="line">kubeadm init phase certs all --config=/root/kubeadm.conf</span><br></pre></td></tr></table></figure>
<h3 id="方式二：通过sed-命令进行调整"><a href="#方式二：通过sed-命令进行调整" class="headerlink" title="方式二：通过sed 命令进行调整"></a>方式二：通过sed 命令进行调整</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改/etc/kubernetes 目录下的配置文件</span></span><br><span class="line">sed -i <span class="string">'s/oldip/newip/g'</span> `grep <span class="string">"oldip"</span> -rl /etc/kubernetes`</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改kube-proxy configmap配置</span></span><br><span class="line">kubectl  get cm kube-proxy -n kube-system -o yaml &gt; proxy.yaml</span><br><span class="line">sed -i <span class="string">'s/oldip/newip/g'</span> proxy.yaml</span><br><span class="line">kubectl apply <span class="_">-f</span> proxy.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#重新renew证书</span></span><br><span class="line">kubeadm alpha certs renew all --config=/root/kubeadm.conf</span><br></pre></td></tr></table></figure>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在 Kubernetes 中配置 Container Capabilities]]></title>
      <url>http://team.jiunile.com/blog/2019/12/capabilities.html</url>
      <content type="html"><![CDATA[<p>我们在使用 Kubernetes 过程中，偶尔会遇到如下所示的一段配置：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">  capabilities:</span></span><br><span class="line"><span class="attr">    drop:</span></span><br><span class="line"><span class="bullet">    -</span> ALL</span><br><span class="line"><span class="attr">    add:</span></span><br><span class="line"><span class="bullet">    -</span> NET_BIND_SERVICE</span><br></pre></td></tr></table></figure></p>
<p>实际上这是配置对应的容器的 <code>Capabilities</code>，在我们使用 <code>docker run</code> 的时候可以通过 <code>--cap-add</code> 和 <code>--cap-drop</code> 命令来给容器添加 <code>Linux Capabilities</code>。对于大部分同学可能又要疑问 <code>Linux Capabilities</code> 是什么呢？<br><a id="more"></a></p>
<h2 id="Linux-Capabilities"><a href="#Linux-Capabilities" class="headerlink" title="Linux Capabilities"></a>Linux Capabilities</h2><p>要了解 <code>Linux Capabilities</code>，这就得从 Linux 的权限控制发展来说明。在 Linux 2.2 版本之前，当内核对进程进行权限验证的时候，Linux 将进程划分为两类：特权进程（UID=0，也就是超级用户）和非特权进程（UID!=0），特权进程拥有所有的内核权限，而非特权进程则根据进程凭证（effective UID, effective GID，supplementary group 等）进行权限检查。</p>
<p>比如我们以常用的 <code>passwd</code> 命令为例，修改用户密码需要具有 root 权限，而普通用户是没有这个权限的。但是实际上普通用户又可以修改自己的密码，这是怎么回事呢？在 Linux 的权限控制机制中，有一类比较特殊的权限设置，比如 SUID(Set User ID on execution)，允许用户以可执行文件的 owner 的权限来运行可执行文件。因为程序文件 <code>/bin/passwd</code> 被设置了 <code>SUID</code> 标识，所以普通用户在执行 passwd 命令时，进程是以 passwd 的所有者，也就是 root 用户的身份运行，从而就可以修改密码了。</p>
<p>但是使用 <code>SUID</code> 却带来了新的安全隐患，当我们运行设置了 <code>SUID</code> 的命令时，通常只是需要很小一部分的特权，但是 <code>SUID</code> 却给了它 root 具有的全部权限，一旦 被设置了 <code>SUID</code> 的命令出现漏洞，是不是就很容易被利用了。</p>
<p>为此 Linux 引入了 <code>Capabilities</code> 机制来对 root 权限进行了更加细粒度的控制，实现按需进行授权，这样就大大减小了系统的安全隐患。</p>
<h3 id="什么是-Capabilities"><a href="#什么是-Capabilities" class="headerlink" title="什么是 Capabilities"></a>什么是 Capabilities</h3><p>从内核 2.2 开始，Linux 将传统上与超级用户 root 关联的特权划分为不同的单元，称为 <code>capabilites</code>。<code>Capabilites</code> 每个单元都可以独立启用和禁用。这样当系统在作权限检查的时候就变成了：在执行特权操作时，如果进程的有效身份不是 <strong>root</strong>，就去检查是否具有该特权操作所对应的 <strong>capabilites</strong>，并以此决定是否可以进行该特权操作。比如如果我们要设置系统时间，就得具有 <code>CAP_SYS_TIME</code> 这个 capabilites。下面是从 <a href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank" rel="external">capabilities man page</a> 中摘取的 capabilites 列表：<br><img src="/images/capabilities.png" alt="capabilities"></p>
<h3 id="如何使用-Capabilities"><a href="#如何使用-Capabilities" class="headerlink" title="如何使用 Capabilities"></a>如何使用 Capabilities</h3><p>我们可以通过 <code>getcap</code> 和 <code>setcap</code> 两条命令来分别查看和设置程序文件的 <code>capabilities</code> 属性。比如当前我们是<code>zuiapp</code> 这个用户，使用 <code>getcap</code> 命令查看 <code>ping</code> 命令目前具有的 <code>capabilities</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ ll /bin/ping</span><br><span class="line">-rwxr-xr-x 1 root root 62088 11月  7 2016 /bin/ping</span><br><span class="line">❯ <span class="built_in">getcap</span> /bin/ping</span><br><span class="line">/bin/ping = <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw+p</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到具有 <code>cap_net_admin</code> 这个属性，所以我们现在可以执行 <code>ping</code> 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ ping team.jiunile.com</span><br><span class="line">PING icyxp.github.io (185.199.109.153): 56 data bytes</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=0 ttl=58 time=46.651 ms</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=1 ttl=58 time=35.604 ms</span><br></pre></td></tr></table></figure></p>
<p>但是如果我们把命令的 <code>capabilities</code> 属性移除掉：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❯ sudo <span class="built_in">setcap</span> <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw-p /bin/ping</span><br><span class="line">❯ <span class="built_in">getcap</span> /bin/ping</span><br><span class="line">/bin/ping =</span><br></pre></td></tr></table></figure></p>
<p>这个时候我们执行 <code>ping</code> 命令可以发现已经没有权限了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ ping team.jiunile.com</span><br><span class="line">ping: socket: Operation not permitted</span><br></pre></td></tr></table></figure></p>
<p>因为 ping 命令在执行时需要访问网络，所需的 <code>capabilities</code> 为 <code>cap_net_admin</code> 和 <code>cap_net_raw</code>，所以我们可以通过 <code>setcap</code> 命令可来添加它们：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❯ sudo <span class="built_in">setcap</span> <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw+p /bin/ping</span><br><span class="line">❯ <span class="built_in">getcap</span> /bin/ping</span><br><span class="line">/bin/ping = <span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw+p</span><br><span class="line">❯ ping team.jiunile.com</span><br><span class="line">PING icyxp.github.io (185.199.109.153): 56 data bytes</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=0 ttl=58 time=46.651 ms</span><br><span class="line">64 bytes from 185.199.109.153: icmp_seq=1 ttl=58 time=35.604 ms</span><br></pre></td></tr></table></figure></p>
<p>命令中的 <code>p</code> 表示 <code>Permitted</code> 集合(接下来会介绍)，<code>+</code> 号表示把指定的 <code>capabilities</code> 添加到这些集合中，<code>-</code> 号表示从集合中移除。</p>
<p>对于可执行文件的属性中有三个集合来保存三类 <code>capabilities</code>，它们分别是：</p>
<ul>
<li>Permitted：在进程执行时，Permitted 集合中的 capabilites 自动被加入到进程的 Permitted 集合中。</li>
<li>Inheritable：Inheritable 集合中的 capabilites 会与进程的 Inheritable 集合执行与操作，以确定进程在执行 execve 函数后哪些 capabilites 被继承。</li>
<li>Effective：Effective 只是一个 bit。如果设置为开启，那么在执行 execve 函数后，Permitted 集合中新增的 capabilities 会自动出现在进程的 Effective 集合中。</li>
</ul>
<p>对于进程中有五种 <code>capabilities</code> 集合类型，相比文件的 <code>capabilites</code>，进程的 <code>capabilities</code> 多了两个集合，分别是 <code>Bounding</code> 和 <code>Ambient</code>。</p>
<p>我们可以通过下面的命名来查看当前进程的 <code>capabilities</code> 信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ cat /proc/7029/status | grep <span class="string">'Cap'</span>  <span class="comment">#7029为PID</span></span><br><span class="line">CapInh:	0000000000000000</span><br><span class="line">CapPrm:	0000000000000000</span><br><span class="line">CapEff:	0000000000000000</span><br><span class="line">CapBnd:	0000001fffffffff</span><br><span class="line">CapAmb:	0000000000000000</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以使用 <code>capsh</code> 命令把它们转义为可读的格式，这样基本可以看出进程具有的 <code>capabilities</code> 了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">❯ capsh --decode=0000001fffffffff</span><br><span class="line">0x0000001fffffffff=<span class="built_in">cap</span>_chown,<span class="built_in">cap</span>_dac_override,<span class="built_in">cap</span>_dac_<span class="built_in">read</span>_search,<span class="built_in">cap</span>_fowner,<span class="built_in">cap</span>_fsetid,<span class="built_in">cap</span>_<span class="built_in">kill</span>,<span class="built_in">cap</span>_setgid,<span class="built_in">cap</span>_setuid,<span class="built_in">cap</span>_setpcap,<span class="built_in">cap</span>_linux_immutable,<span class="built_in">cap</span>_net_<span class="built_in">bind</span>_service,<span class="built_in">cap</span>_net_broadcast,<span class="built_in">cap</span>_net_admin,<span class="built_in">cap</span>_net_raw,<span class="built_in">cap</span>_ipc_lock,<span class="built_in">cap</span>_ipc_owner,<span class="built_in">cap</span>_sys_module,<span class="built_in">cap</span>_sys_rawio,<span class="built_in">cap</span>_sys_chroot,<span class="built_in">cap</span>_sys_ptrace,<span class="built_in">cap</span>_sys_pacct,<span class="built_in">cap</span>_sys_admin,<span class="built_in">cap</span>_sys_boot,<span class="built_in">cap</span>_sys_nice,<span class="built_in">cap</span>_sys_resource,<span class="built_in">cap</span>_sys_time,<span class="built_in">cap</span>_sys_tty_config,<span class="built_in">cap</span>_mknod,<span class="built_in">cap</span>_lease,<span class="built_in">cap</span>_audit_write,<span class="built_in">cap</span>_audit_control,<span class="built_in">cap</span>_setfcap,<span class="built_in">cap</span>_mac_override,<span class="built_in">cap</span>_mac_admin,<span class="built_in">cap</span>_syslog,35,36</span><br></pre></td></tr></table></figure></p>
<h2 id="Docker-Container-Capabilities"><a href="#Docker-Container-Capabilities" class="headerlink" title="Docker Container Capabilities"></a>Docker Container Capabilities</h2><p>我们说 Docker 容器本质上就是一个进程，所以理论上容器就会和进程一样会有一些默认的开放权限，默认情况下 Docker 会删除必须的 <code>capabilities</code> 之外的所有 <code>capabilities</code>，因为在容器中我们经常会以 root 用户来运行，使用 <code>capabilities</code> 现在后，容器中的使用的 root 用户权限就比我们平时在宿主机上使用的 root 用户权限要少很多了，这样即使出现了安全漏洞，也很难破坏或者获取宿主机的 root 权限，所以 Docker 支持 <code>Capabilities</code> 对于容器的安全性来说是非常有必要的。</p>
<p>不过我们在运行容器的时候可以通过指定 <code>--privileded</code> 参数来开启容器的超级权限，这个参数一定要慎用，因为他会获取系统 root 用户所有能力赋值给容器，并且会扫描宿主机的所有设备文件挂载到容器内部，所以是非常危险的操作。</p>
<p>但是如果你确实需要一些特殊的权限，我们可以通过 <code>--cap-add</code> 和 <code>--cap-drop</code> 这两个参数来动态调整，可以最大限度地保证容器的使用安全。下面表格中列出的 <code>Capabilities</code> 是 Docker 默认给容器添加的，我们可以通过 <code>--cap-drop</code> 去除其中一个或者多个：<br><img src="/images/docker-add-capabilities.png" alt="docker-add-capabilities"></p>
<p>下面表格中列出的 <code>Capabilities</code> 是 Docker 默认删除的，我们可以通过<code>--cap-add</code>添加其中一个或者多个：<br><img src="/images/docker-drop-capabilities.png" alt="docker-drop-capabilities"></p>
<blockquote>
<p><code>--cap-add</code>和<code>--cap-drop</code> 这两参数都支持<code>ALL</code>值，比如如果你想让某个容器拥有除了<code>MKNOD</code>之外的所有内核权限，那么可以执行下面的命令： <code>$ sudo docker run --cap-add=ALL --cap-drop=MKNOD ...</code></p>
</blockquote>
<p>比如现在我们需要修改网络接口数据，默认情况下是没有权限的，因为需要的 <code>NET_ADMIN</code> 这个 <code>Capabilities</code> 默认被移除了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ docker run -it --rm busybox /bin/sh</span><br><span class="line">/ <span class="comment"># ip link add dummy0 type dummy</span></span><br><span class="line">ip: RTNETLINK answers: Operation not permitted</span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>所以在不使用 <code>--privileged</code> 的情况下（不建议）我们可以使用 <code>--cap-add=NET_ADMIN</code> 将这个 <code>Capabilities</code> 添加回来：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">❯ docker run -it --rm --cap-add=NET_ADMIN busybox /bin/sh</span><br><span class="line">/ <span class="comment"># ip link add dummy0 type dummy</span></span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到已经 OK 了。</p>
<h2 id="Kubernetes-配置-Capabilities"><a href="#Kubernetes-配置-Capabilities" class="headerlink" title="Kubernetes 配置 Capabilities"></a>Kubernetes 配置 Capabilities</h2><p>上面我介绍了在 Docker 容器下如何来配置 <code>Capabilities</code>，在 Kubernetes 中也可以很方便的来定义，我们只需要添加到 Pod 定义的 <code>spec.containers.sercurityContext.capabilities</code>中即可，也可以进行 <code>add</code> 和 <code>drop</code> 配置，同样上面的示例，我们要给 busybox 容器添加 <code>NET_ADMIN</code> 这个 <code>Capabilities</code>，对应的 YAML 文件可以这样定义：(cpb-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> cpb-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> cpb</span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> sleep</span><br><span class="line"><span class="bullet">    -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      capabilities:</span></span><br><span class="line"><span class="attr">        add:</span> <span class="comment"># 添加</span></span><br><span class="line"><span class="bullet">        -</span> NET_ADMIN</span><br><span class="line"><span class="attr">        drop:</span>  <span class="comment"># 删除</span></span><br><span class="line"><span class="bullet">        -</span> KILL</span><br></pre></td></tr></table></figure></p>
<p>我们在 <code>securityContext</code> 下面添加了 <code>capabilities</code> 字段，其中添加了 <code>NET_ADMIN</code> 并且删除了 <code>KILL</code> 这个默认的容器 <code>Capabilities</code>，这样我们就可以在 Pod 中修改网络接口数据了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❯ kubectl apply <span class="_">-f</span> cpb-demo.yaml</span><br><span class="line">❯ kubectl get pods</span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">cpb-demo                  1/1     Running   0          2m9s</span><br><span class="line">❯ kubectl <span class="built_in">exec</span> -it cpb-demo /bin/sh</span><br><span class="line">/ <span class="comment"># ip link add dummy0 type dummy</span></span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p>在 Kubernetes 中通过 <code>sercurityContext.capabilities</code> 进行配置容器的 <code>Capabilities</code>，当然最终还是通过 Docker 的 <code>libcontainer</code> 去借助 <code>Linux kernel capabilities</code> 实现的权限管理。</p>
<h3 id="应用一：在kubernetes容器中使用perf命令"><a href="#应用一：在kubernetes容器中使用perf命令" class="headerlink" title="应用一：在kubernetes容器中使用perf命令"></a>应用一：在kubernetes容器中使用perf命令</h3><p>在容器中运行 <code>perf</code> 会报如下错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">┌─Error:───────────────────────────────────────────────────────────┐</span><br><span class="line">│No permission to <span class="built_in">enable</span> cycles event.                             │</span><br><span class="line">│                                                                  │</span><br><span class="line">│You may not have permission to collect system-wide stats.         │</span><br><span class="line">│                                                                  │</span><br><span class="line">│Consider tweaking /proc/sys/kernel/perf_event_paranoid,           │</span><br><span class="line">│<span class="built_in">which</span> controls use of the performance events system by            │</span><br><span class="line">│unprivileged users (without CAP_SYS_ADMIN).                       │</span><br><span class="line">│                                                                  │</span><br><span class="line">│The current value is 3:                                           │</span><br><span class="line">│                                                                  │</span><br><span class="line">│  -1: Allow use of (almost) all events by all users               │</span><br><span class="line">│&gt;= 0: Disallow raw tracepoint access by users without CAP_IOC_LOCK│</span><br><span class="line">│&gt;= 1: Disallow CPU event access by users without CAP_SYS_ADMIN    │</span><br><span class="line">│&gt;= 2: Disallow kernel profiling by users without C                │</span><br><span class="line">│                                                                  │</span><br><span class="line">│                                                                  │</span><br><span class="line">│Press any key...                                                  │</span><br><span class="line">└──────────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure></p>
<p>提示没有权限，需要更改 <code>/proc/sys/kernel/perf_event_paranoid</code> 文件，然后尝试执行:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 0 &gt; /proc/sys/kernel/perf_event_paranoid</span><br><span class="line">bash: /proc/sys/kernel/perf_event_paranoid: Read-only file system</span><br></pre></td></tr></table></figure></p>
<p>解决办法，修改容器的deployment文件，加入 <code>SYS_ADMIN</code><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line"><span class="attr">    capabilities:</span></span><br><span class="line"><span class="attr">        add:</span> [<span class="string">"SYS_ADMIN"</span>]</span><br></pre></td></tr></table></figure></p>
<p>参考链接:</p>
<ul>
<li><a href="https://www.cnblogs.com/sparkdev/p/11417781.html" target="_blank" rel="external">https://www.cnblogs.com/sparkdev/p/11417781.html</a></li>
<li><a href="https://hustcat.github.io/docker-config-capabilities/" target="_blank" rel="external">https://hustcat.github.io/docker-config-capabilities/</a></li>
<li><a href="https://blog.csdn.net/WaltonWang/article/details/62226738" target="_blank" rel="external">https://blog.csdn.net/WaltonWang/article/details/62226738</a></li>
<li><a href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank" rel="external">http://man7.org/linux/man-pages/man7/capabilities.7.html</a></li>
<li>www.qikqiak.com</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[golang如何取消子 goroutine]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-cancellation-of-goroutines.html</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前写了个工具，用于检测gitlab runner是否能承受住当前runner job的构建，根据Prometheus的监控，在资源使用过载的情况下，就临期启动服务器加入到集群中用于分担runner job构建时的压力。在运行一段时间后发现内存有时占用有点高（<code>goroutine</code>过多），于是就有了下面一步步的优化。<br> <a id="more"></a></p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先我们一开始有以下一段代码逻辑：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"sync"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">    wg sync.WaitGroup</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">	<span class="comment">//假设这个任务要干1000次，一次任务需要做2秒完成</span></span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-time.After(<span class="number">2</span> * time.Second):</span><br><span class="line">            fmt.Println(<span class="string">"Doing some work "</span>, i)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line"></span><br><span class="line">    wg.Add(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> work()</span><br><span class="line">    wg.Wait()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Finished. I'm going home"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing some work  0</span><br><span class="line">Doing some work  1</span><br><span class="line">Doing some work  2</span><br><span class="line">Doing some work  3</span><br><span class="line">...</span><br><span class="line">Doing some work  999</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>现在我们假设下我们调用的<code>work</code>这个方式是来自用户的交互或者一个http请求，我们可能不想一直等待直到<code>goroutine</code>完成，因此，常见的做法是采用超时机制，代码如下：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"log"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-time.After(<span class="number">2</span> * time.Second):</span><br><span class="line">            fmt.Println(<span class="string">"Doing some work "</span>, i)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line"></span><br><span class="line">    ch := <span class="built_in">make</span>(<span class="keyword">chan</span> error, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        ch &lt;- work()</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> err := &lt;-ch:</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            log.Fatal(<span class="string">"Something went wrong :("</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">case</span> &lt;-time.After(<span class="number">4</span> * time.Second):</span><br><span class="line">        fmt.Println(<span class="string">"等的不耐烦了，就这样吧..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Finished. I'm going home"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果如下:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing some work  0</span><br><span class="line">Doing some work  1</span><br><span class="line">Life is to short to wait that long</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>现在情况比上一个好一些，main的执行不在需要等待work完成。</p>
<p>但上述代码还存在一些问题，比如这段代码写在一个http服务中，即使利用超时机制不等待<code>work</code>的完成，但<code>work</code> 这个<code>goroutine</code>还是会在后台一直运行并消耗资源。这时候就需要想个办法来取消这个子<code>goroutine</code>。于是我想到了<code>context</code> 这个包，于是又有了如下的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;sync&quot;</span><br><span class="line">    &quot;time&quot;</span><br><span class="line"></span><br><span class="line">    &quot;golang.org/x/net/context&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">var (</span><br><span class="line">    wg sync.WaitGroup</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func work(ctx context.Context) error &#123;</span><br><span class="line">    defer wg.Done()</span><br><span class="line"></span><br><span class="line">    for i := 0; i &lt; 1000; i++ &#123;</span><br><span class="line">        select &#123;</span><br><span class="line">        case &lt;-time.After(2 * time.Second):</span><br><span class="line">            fmt.Println(&quot;Doing some work &quot;, i)</span><br><span class="line"></span><br><span class="line">        // we received the signal of cancelation in this channel    </span><br><span class="line">        case &lt;-ctx.Done():</span><br><span class="line">            fmt.Println(&quot;Cancel the context &quot;, i)</span><br><span class="line">            return ctx.Err()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func main() &#123;   </span><br><span class="line">    ctx, cancel := context.WithTimeout(context.Background(), 4*time.Second)</span><br><span class="line">    defer cancel()</span><br><span class="line"></span><br><span class="line">    fmt.Println(&quot;Hey, I&apos;m going to do some work&quot;)</span><br><span class="line"></span><br><span class="line">    wg.Add(1)</span><br><span class="line">    go work(ctx)</span><br><span class="line">    wg.Wait()</span><br><span class="line"></span><br><span class="line">    fmt.Println(&quot;Finished. I&apos;m going home&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing some work  0</span><br><span class="line">Cancel the context  1</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>这看上去非常的好，代码看上去也易于管理超时，现在我们确保函数正常工作也不会浪费任何资源。现在为了让例子更加真实，我们在实际的http服务中来进行模拟。</p>
<p>以下是http server代码，模拟有部分概率会有慢响应：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="comment">// Lazy and Very Random Server </span></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"math/rand"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    http.HandleFunc(<span class="string">"/"</span>, LazyServer)</span><br><span class="line">    http.ListenAndServe(<span class="string">":1111"</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// sometimes really fast server, sometimes really slow server</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LazyServer</span><span class="params">(w http.ResponseWriter, req *http.Request)</span></span> &#123;</span><br><span class="line">    headOrTails := rand.Intn(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> headOrTails == <span class="number">0</span> &#123;</span><br><span class="line">        time.Sleep(<span class="number">6</span> * time.Second)</span><br><span class="line">        fmt.Fprintf(w, <span class="string">"Go! slow %v"</span>, headOrTails)</span><br><span class="line">        fmt.Printf(<span class="string">"Go! slow %v"</span>, headOrTails)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fmt.Fprintf(w, <span class="string">"Go! quick %v"</span>, headOrTails)</span><br><span class="line">    fmt.Printf(<span class="string">"Go! quick %v"</span>, headOrTails)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用curl来请求查看结果；<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://localhost:1111/</span><br><span class="line">Go! quick 1</span><br><span class="line">$ curl http://localhost:1111/</span><br><span class="line">Go! quick 1</span><br><span class="line">$ curl http://localhost:1111/</span><br><span class="line">*some seconds later*</span><br><span class="line">Go! slow 0</span><br></pre></td></tr></table></figure></p>
<p>现在，我们将在<code>goroutine</code>中向该服务器发出http请求，但是如果服务器速度较慢，我们将取消该请求并快速返回，以便我们可以管理取消并释放连接。 代码如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"io/ioutil"</span></span><br><span class="line">    <span class="string">"net/http"</span></span><br><span class="line">    <span class="string">"sync"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"golang.org/x/net/context"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">    wg sync.WaitGroup</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// main is not changed</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    ctx, cancel := context.WithTimeout(context.Background(), <span class="number">2</span>*time.Second)</span><br><span class="line">    <span class="keyword">defer</span> cancel()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line"></span><br><span class="line">    wg.Add(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">go</span> work(ctx)</span><br><span class="line">    wg.Wait()</span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">"Finished. I'm going home"</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work</span><span class="params">(ctx context.Context)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">    tr := &amp;http.Transport&#123;&#125;</span><br><span class="line">    client := &amp;http.Client&#123;Transport: tr&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// anonymous struct to pack and unpack data in the channel</span></span><br><span class="line">    c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">        r   *http.Response</span><br><span class="line">        err error</span><br><span class="line">    &#125;, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    req, _ := http.NewRequest(<span class="string">"GET"</span>, <span class="string">"http://localhost:1111"</span>, <span class="literal">nil</span>)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        resp, err := client.Do(req)</span><br><span class="line">        fmt.Println(<span class="string">"Doing http request is a hard job"</span>)</span><br><span class="line">        pack := <span class="keyword">struct</span> &#123;</span><br><span class="line">            r   *http.Response</span><br><span class="line">            err error</span><br><span class="line">        &#125;&#123;resp, err&#125;</span><br><span class="line">        c &lt;- pack</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">        tr.CancelRequest(req)</span><br><span class="line">        &lt;-c <span class="comment">// Wait for client.Do</span></span><br><span class="line">        fmt.Println(<span class="string">"Cancel the context"</span>)</span><br><span class="line">        <span class="keyword">return</span> ctx.Err()</span><br><span class="line">    <span class="keyword">case</span> ok := &lt;-c:</span><br><span class="line">        err := ok.err</span><br><span class="line">        resp := ok.r</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Println(<span class="string">"Error "</span>, err)</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">defer</span> resp.Body.Close()</span><br><span class="line">        out, _ := ioutil.ReadAll(resp.Body)</span><br><span class="line">        fmt.Printf(<span class="string">"Server Response: %s\n"</span>, out)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing http request is a hard job</span><br><span class="line">Server Response: Go! quick 1</span><br><span class="line">Finished. I'</span>m going home</span><br><span class="line"></span><br><span class="line">$ go run work.go</span><br><span class="line">Hey, I<span class="string">'m going to do some work</span><br><span class="line">Doing http request is a hard job</span><br><span class="line">Cancel the context</span><br><span class="line">Finished. I'</span>m going home</span><br></pre></td></tr></table></figure></p>
<p>如您在输出中所看到的，我们避免了服务器的缓慢响应。在客户端中，tcp连接已取消，因此不会忙于等待响应缓慢，因此我们不会浪费资源。</p>
<p>还有一个例子，有一个常驻的任务，即要控制<code>goroutine</code>的增长，又需要防止在<code>goroutine</code>超时后<code>goroutine</code>在后台运行造成资源的浪费，让我们来看下如何实现：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">	<span class="string">"golang.org/x/net/context"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">//var wg sync.WaitGroup</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work1</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//defer wg.Done()</span></span><br><span class="line">	ch &lt;- <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//任务超过3秒就超时</span></span><br><span class="line">	ctx, cancel := context.WithTimeout(context.Background(), <span class="number">3</span>*time.Second)</span><br><span class="line">	<span class="keyword">defer</span> cancel()</span><br><span class="line">	chT := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> <span class="built_in">close</span>(chT)</span><br><span class="line">		<span class="comment">//具体的任务，这里模拟做的任务需要5秒完成</span></span><br><span class="line">		time.Sleep(time.Second * <span class="number">5</span>)</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> &lt;-chT:</span><br><span class="line">		fmt.Println(<span class="string">"job1 finsh..."</span>, runtime.NumGoroutine())</span><br><span class="line">	<span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">		fmt.Println(<span class="string">"job1 timeout..."</span>, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Println(<span class="string">"job1 exit.."</span>)</span><br><span class="line">	&lt;-ch  <span class="comment">//释放chanel</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">work2</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//defer wg.Done()</span></span><br><span class="line">	ch &lt;- <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">	ctx, cancel := context.WithTimeout(context.Background(), <span class="number">3</span>*time.Second)</span><br><span class="line">	<span class="keyword">defer</span> cancel()</span><br><span class="line">	chT := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">defer</span> <span class="built_in">close</span>(chT)</span><br><span class="line">		<span class="comment">//具体的任务，这里模拟做的任务需要1秒完成</span></span><br><span class="line">		time.Sleep(time.Second * <span class="number">1</span>)</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> &lt;-chT:</span><br><span class="line">		fmt.Println(<span class="string">"job2 finsh..."</span>, runtime.NumGoroutine())</span><br><span class="line">	<span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">		fmt.Println(<span class="string">"job2 timeout..."</span>, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&lt;-ch <span class="comment">//释放chanel</span></span><br><span class="line">	fmt.Println(<span class="string">"job2 exit.."</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">"Hey, I'm going to do some work"</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//控制goroutine数量</span></span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//永久运行</span></span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="comment">//因为是永久运行，所以这里的sync.Waitgroup可以不再需要</span></span><br><span class="line">		<span class="comment">//wg.Add(2)</span></span><br><span class="line">		<span class="keyword">go</span> work1(ch)</span><br><span class="line">		<span class="keyword">go</span> work2(ch)</span><br><span class="line">		time.Sleep(<span class="number">2</span> * time.Second)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//wg.Wait()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>希望以上例子可以给你带来一些帮助！Happy coding gophers!.</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 程序占用大量内存问题分析]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-debug-memory.html</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在运行一段时间的go程序后内存竟然达到4G左右，几乎可以肯定是由于某段方法操作不规范引起的问题，于是对go程序进行分析<br><a id="more"></a></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>分析内存光靠手撕代码是比较困难的，总要借助一些工具。Golang <code>pprof</code>是Go官方的<code>profiling</code>工具，非常强大，使用起来也很方便。</p>
<p>代码改造：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> _ <span class="string">"net/http/pprof"</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">	http.ListenAndServe(<span class="string">"0.0.0.0:8888"</span>, <span class="literal">nil</span>)</span><br><span class="line"> &#125;()</span><br></pre></td></tr></table></figure></p>
<p>改造后程序启动后，浏览器中输入<a href="http://ip:8899/debug/pprof/就可以看到一个汇总分析页面，显示如下信息：" target="_blank" rel="external">http://ip:8899/debug/pprof/就可以看到一个汇总分析页面，显示如下信息：</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/debug/pprof/</span><br><span class="line"></span><br><span class="line">profiles:</span><br><span class="line">0    block</span><br><span class="line">32    goroutine</span><br><span class="line">552    heap</span><br><span class="line">0    mutex</span><br><span class="line">51    threadcreate</span><br><span class="line"></span><br><span class="line">full goroutine stack dump</span><br></pre></td></tr></table></figure></p>
<p>点击heap，在汇总分析页面的最上方可以看到如下图所示，红色箭头所指的就是当前已经使用的堆内存是25M！！<br><img src="/images/go/debug_memory_1.png" alt="内存分析"></p>
<p>接下来我们需要借助<code>go tool pprof</code>来分析：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go tool pprof -inuse_space http://本机Ip:8888/debug/pprof/heap</span><br></pre></td></tr></table></figure></p>
<p>这个命令进入后，是一个类似<code>gdb</code>的交互式界面，输入 <code>top</code> 命令可以前10大的内存分配，<code>flat</code> 是堆栈中当前层的inuse内存值，cum是堆栈中本层级的累计inuse内存值（包括调用的函数的inuse内存值，上面的层级）<br><img src="/images/go/debug_memory_2.png" alt="内存分析"></p>
<p>可以看到，<code>bytes.makeSlice</code>这个内置方法竟然使用了24M内存，继续往下看，可以看到<code>ReadFrom</code>这个方法，搜了一下，发现 <code>ioutil.ReadAll()</code> 里会调用 <code>bytes.Buffer.ReadFrom</code>, 而 <code>bytes.Buffer.ReadFrom</code> 会进行 <code>makeSlice</code>。再回头看一下<code>io/ioutil.readAll</code>的代码实现，<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">readAll</span><span class="params">(r io.Reader, capacity <span class="keyword">int64</span>)</span> <span class="params">(b []<span class="keyword">byte</span>, err error)</span></span> &#123;</span><br><span class="line">    buf := bytes.NewBuffer(<span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">0</span>, capacity))</span><br><span class="line">    <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        e := <span class="built_in">recover</span>()</span><br><span class="line">        <span class="keyword">if</span> e == <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> panicErr, ok := e.(error); ok &amp;&amp; panicErr == bytes.ErrTooLarge &#123;</span><br><span class="line">            err = panicErr</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">panic</span>(e)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line">    _, err = buf.ReadFrom(r)</span><br><span class="line">    <span class="keyword">return</span> buf.Bytes(), err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// bytes.MinRead = 512</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadAll</span><span class="params">(r io.Reader)</span> <span class="params">([]<span class="keyword">byte</span>, error)</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> readAll(r, bytes.MinRead)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，<code>ioutil.ReadAll</code> 每次都会分配初始化一个大小为 <code>bytes.MinRead</code> 的 buffer ，<code>bytes.MinRead</code> 在 Golang 里是一个常量，值为 <code>512</code> 。就是说每次调用 <code>ioutil.ReadAll</code> 都会分配一块大小为 <code>512</code> 字节的内存，目前看起来是正常的，但我们再看一下<code>ReadFrom</code>的实现，<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ReadFrom reads data from r until EOF and appends it to the buffer, growing</span></span><br><span class="line"><span class="comment">// the buffer as needed. The return value n is the number of bytes read. Any</span></span><br><span class="line"><span class="comment">// error except io.EOF encountered during the read is also returned. If the</span></span><br><span class="line"><span class="comment">// buffer becomes too large, ReadFrom will panic with ErrTooLarge.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(b *Buffer)</span> <span class="title">ReadFrom</span><span class="params">(r io.Reader)</span> <span class="params">(n <span class="keyword">int64</span>, err error)</span></span> &#123;</span><br><span class="line">    b.lastRead = opInvalid</span><br><span class="line">    <span class="comment">// If buffer is empty, reset to recover space.</span></span><br><span class="line">    <span class="keyword">if</span> b.off &gt;= <span class="built_in">len</span>(b.buf) &#123;</span><br><span class="line">        b.Truncate(<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> free := <span class="built_in">cap</span>(b.buf) - <span class="built_in">len</span>(b.buf); free &lt; MinRead &#123;</span><br><span class="line">            <span class="comment">// not enough space at end</span></span><br><span class="line">            newBuf := b.buf</span><br><span class="line">            <span class="keyword">if</span> b.off+free &lt; MinRead &#123;</span><br><span class="line">                <span class="comment">// not enough space using beginning of buffer;</span></span><br><span class="line">                <span class="comment">// double buffer capacity</span></span><br><span class="line">                newBuf = makeSlice(<span class="number">2</span>*<span class="built_in">cap</span>(b.buf) + MinRead)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">copy</span>(newBuf, b.buf[b.off:])</span><br><span class="line">            b.buf = newBuf[:<span class="built_in">len</span>(b.buf)-b.off]</span><br><span class="line">            b.off = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        m, e := r.Read(b.buf[<span class="built_in">len</span>(b.buf):<span class="built_in">cap</span>(b.buf)])</span><br><span class="line">        b.buf = b.buf[<span class="number">0</span> : <span class="built_in">len</span>(b.buf)+m]</span><br><span class="line">        n += <span class="keyword">int64</span>(m)</span><br><span class="line">        <span class="keyword">if</span> e == io.EOF &#123;</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> n, e</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> n, <span class="literal">nil</span> <span class="comment">// err is EOF, so return nil explicitly</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个函数主要作用就是从 <code>io.Reader</code> 里读取的数据放入 <code>buffer</code> 中，如果 buffer 空间不够，就按照每次 <code>2x + MinRead</code> 的算法递增，这里 <code>MinRead</code> 的大小也是 512 Bytes ，也就是说如果我们一次性读取的文件过大，就会导致所使用的内存倍增，假设我们的爬虫文件总共有500M,那么所用的内存就有500M * 2 + 512B，况且爬虫文件中还带了那么多log文件，那看看crawlab源码中是哪一段使用<code>ioutil.ReadAll</code>读了爬虫文件，定位到了这里：<br><img src="/images/go/debug_memory_3.jpg" alt="内存分析"></p>
<p>这里直接将全部的文件内容，以二进制的形式读了进来，导致内存倍增，令人窒息的操作。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>其实在读大文件的时候，把文件内容全部读到内存，直接就翻车了，正确是处理方法有两种</p>
<h3 id="流式处理"><a href="#流式处理" class="headerlink" title="流式处理"></a>流式处理</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadFile</span><span class="params">(filePath <span class="keyword">string</span>, handle <span class="keyword">func</span>(<span class="keyword">string</span>)</span>) <span class="title">error</span></span> &#123;</span><br><span class="line">    f, err := os.Open(filePath)</span><br><span class="line">    <span class="keyword">defer</span> f.Close()</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    buf := bufio.NewReader(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        line, err := buf.ReadLine(<span class="string">"\n"</span>)</span><br><span class="line">        line = strings.TrimSpace(line)</span><br><span class="line">        handle(line)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> err == io.EOF&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="分片处理"><a href="#分片处理" class="headerlink" title="分片处理"></a>分片处理</h3><blockquote>
<p>当读取的是二进制文件，没有换行符的时候，使用这种方案比较合适</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadBigFile</span><span class="params">(fileName <span class="keyword">string</span>, handle <span class="keyword">func</span>([]<span class="keyword">byte</span>)</span>) <span class="title">error</span></span> &#123;</span><br><span class="line">    f, err := os.Open(fileName)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"can't opened this file"</span>)</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> f.Close()</span><br><span class="line">    s := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">4096</span>)</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">switch</span> nr, err := f.Read(s[:]); <span class="literal">true</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> nr &lt; <span class="number">0</span>:</span><br><span class="line">            fmt.Fprintf(os.Stderr, <span class="string">"cat: error reading: %s\n"</span>, err.Error())</span><br><span class="line">            os.Exit(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">case</span> nr == <span class="number">0</span>: <span class="comment">// EOF</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">        <span class="keyword">case</span> nr &gt; <span class="number">0</span>:</span><br><span class="line">            handle(s[<span class="number">0</span>:nr])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>参考：</strong></p>
<ul>
<li>juejin.im/post/5d5be347f265da03b94ff66b</li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[慎用<-time.After()，容易导致内存泄漏]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-select-timer.html</url>
      <content type="html"><![CDATA[<h2 id="问题代码"><a href="#问题代码" class="headerlink" title="问题代码"></a>问题代码</h2><a id="more"></a>
<blockquote>
<p>顺带说明下select 监听是如何工作的</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//go start</span></span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> &#123;</span><br><span class="line">			timerC := time.After(<span class="number">2</span> * time.Second)</span><br><span class="line">			<span class="comment">//timerC 每次都是重新创建的，什么意思呢？简单说来，当 select 成功监听 ch 并进入它的处理分支，下次循环 timerC 重新创建了，时间肯定就重置了。</span></span><br><span class="line">			<span class="keyword">select</span> &#123;</span><br><span class="line">			<span class="comment">//如果有多个 case 都可以运行，select 会随机公平选择出一个执行。其余的则不会执行</span></span><br><span class="line">			<span class="keyword">case</span> num := &lt;-ch:</span><br><span class="line">				fmt.Println(<span class="string">"get num is"</span>, num)</span><br><span class="line">			<span class="keyword">case</span> &lt;-timerC:</span><br><span class="line">				<span class="comment">//等价于 case &lt;-time.After(2 * time.Second)</span></span><br><span class="line">				fmt.Println(<span class="string">"time's up!!!"</span>)</span><br><span class="line">				<span class="comment">//done&lt;-true</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">5</span>; i++ &#123;</span><br><span class="line">		ch &lt;- i</span><br><span class="line">		time.Sleep(time.Second * <span class="number">2</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h2><p>以上代码会导致内存泄漏，其罪魁祸首是<code>&lt;-time.After()</code>，在官方文档中有此说明：如果定时器没有到达定时时间，<code>gc</code> 就不会启动垃圾回收。标准库文档中有说明：</p>
<blockquote>
<p>The underlying Timer is not recovered by the garbage collector until the timer fires</p>
</blockquote>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>在不使用 <code>time.After</code> 来实现超时的前提下，可通过创建 <code>timer</code> 配合 <code>reset</code> 来实现超时机制，具体代码示例如下：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">	idleDuration := <span class="number">2</span> * time.Second</span><br><span class="line">	idleDelay := time.NewTimer(idleDuration)</span><br><span class="line">	<span class="keyword">defer</span> idleDelay.Stop()</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		idleDelay.Reset(idleDuration)</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> num := &lt;-ch:</span><br><span class="line">			fmt.Println(<span class="string">"get num is"</span>, num)</span><br><span class="line">		<span class="keyword">case</span> &lt;-idleDelay.C:</span><br><span class="line">			fmt.Println(<span class="string">"time's up!!!"</span>)</span><br><span class="line">			<span class="comment">//done&lt;-ture</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">....</span><br></pre></td></tr></table></figure></p>
<p><strong>参考：</strong></p>
<ul>
<li>studygolang.com/articles/22617</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 陷阱系列]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-trap.html</url>
      <content type="html"><![CDATA[<h2 id="陷阱一-interface问题"><a href="#陷阱一-interface问题" class="headerlink" title="陷阱一 interface问题"></a>陷阱一 interface问题</h2><a id="more"></a>
<h3 id="问题代码"><a href="#问题代码" class="headerlink" title="问题代码"></a>问题代码</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//DetaildError ...</span></span><br><span class="line"><span class="keyword">type</span> DetaildError <span class="keyword">struct</span> &#123;</span><br><span class="line">	code    <span class="keyword">int</span></span><br><span class="line">	message <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e DetaildError)</span> <span class="title">Error</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> fmt.Sprintf(<span class="string">"Error occured at (%d,%s)"</span>, e.code, e.message)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">handle</span><span class="params">(x <span class="keyword">int</span>)</span> *<span class="title">DetaildError</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> x != <span class="number">1</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> &amp;DetaildError&#123;code: <span class="number">1000</span>, message: <span class="string">"whoami?"</span>&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> err error</span><br><span class="line">	err = handle(<span class="number">0</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"I am Error 1 of %s\n"</span>, err)</span><br><span class="line">	&#125;</span><br><span class="line">	err = handle(<span class="number">1</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"I am Error 2 of %s\n"</span>, err)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I am Error 1 of Error occured at (1000,whoami?)</span><br><span class="line">I am Error 2 of &lt;nil&gt;</span><br></pre></td></tr></table></figure>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><blockquote>
<p>err 是一个接口，接口在Go中保存了两个值，一个是<code>类型T</code>，一个<code>值V</code><br>只有当<code>T</code>和<code>V</code> <strong>同时</strong> 为<code>nil</code>时，接口才是<code>nil</code><br>在Go中，接口是隐式实现，<strong>因此当我们用一个接口类型去接收一个nil结构体的时候，那么这个接口将不再是nil</strong><br>此时的err值为<code>(T=*DetaildError, V=nil)</code>，不满足接口为nil条件</p>
</blockquote>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><h4 id="修正一：将var-err-Error-修改为接收Struct-而不是接口"><a href="#修正一：将var-err-Error-修改为接收Struct-而不是接口" class="headerlink" title="修正一：将var err Error 修改为接收Struct 而不是接口"></a>修正一：将var err Error 修改为接收Struct 而不是接口</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> err *DetaildError</span><br></pre></td></tr></table></figure>
<h4 id="修正二：将handle方法的返回类型为interface"><a href="#修正二：将handle方法的返回类型为interface" class="headerlink" title="修正二：将handle方法的返回类型为interface"></a>修正二：将handle方法的返回类型为interface</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">handle</span><span class="params">(x <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> x != <span class="number">1</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> &amp;DetaildError&#123;code: <span class="number">1000</span>, message: <span class="string">"whoami?"</span>&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>参考：</strong></p>
<ul>
<li>mp.weixin.qq.com/s/0bJOzNxoQhdVjFOunhmVKQ</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[控制 Goroutine 的并发数量]]></title>
      <url>http://team.jiunile.com/blog/2019/09/go-control-goroutine-number.html</url>
      <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在 Go 语言中创建协程（Goroutine）的成本非常低，因此稍不注意就可能创建出大量的协程，一方面会造成资源不断增长负载变高，另一方面不容易控制这些协程的状态。</p>
<p>不过，“能力越大，越需要克制”。网络上已经存在一些讲控制 Goroutine 数目的文章，本文通过图示的方式再简单总结一下其基本理念，以便于记忆。</p>
<a id="more"></a>
<h2 id="Goroutine-数量不受控示例"><a href="#Goroutine-数量不受控示例" class="headerlink" title="Goroutine 数量不受控示例"></a>Goroutine 数量不受控示例</h2><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"sync"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	jobsCount := <span class="number">10</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; jobsCount; j++ &#123;</span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">go</span> do(j)</span><br><span class="line">		fmt.Printf(<span class="string">"index: %d,goroutine Num: %d \n"</span>, j, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Println(<span class="string">"done!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">do</span><span class="params">(i <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">"hello %d!\n"</span>, i)</span><br><span class="line">	time.Sleep(time.Second * <span class="number">2</span>) <span class="comment">// 刻意睡 2 秒钟，模拟耗时</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码假设有 <code>jobsCount</code> 个任务，通过 <code>for-range 给每个任务创建了一个 Goroutine</code>。为了让主协程等待所有的子协程执行完毕后再退出，使用 <code>sync.WaitGroup</code> 监控所有协程的状态，从而保证主协程结束时所有的子协程已经退出。为了说明问题，上面的代码还输出了 <code>runtime.NumGoroutine()</code> 的值用以表征协程的数量。</p>
<p>运行上面的代码，可以得到类似下面的输出。从下面的输出中我们可以得到两点信息：① 协程的执行顺序是随机的（比如 hello 3 在 hello 6 后面出现）；② 协程的数量递增，最后到了 11 个之多。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">index: 0,goroutine Num: 2</span><br><span class="line">hello 0!</span><br><span class="line">index: 1,goroutine Num: 3</span><br><span class="line">index: 2,goroutine Num: 4</span><br><span class="line">hello 1!</span><br><span class="line">index: 3,goroutine Num: 5</span><br><span class="line">index: 4,goroutine Num: 6</span><br><span class="line">index: 5,goroutine Num: 7</span><br><span class="line">index: 6,goroutine Num: 8</span><br><span class="line">hello 6!</span><br><span class="line">hello 3!</span><br><span class="line">index: 7,goroutine Num: 9</span><br><span class="line">hello 7!</span><br><span class="line">index: 8,goroutine Num: 10</span><br><span class="line">index: 9,goroutine Num: 11</span><br><span class="line">hello 2!</span><br><span class="line">hello 9!</span><br><span class="line">hello 8!</span><br><span class="line">hello 4!</span><br><span class="line"><span class="keyword">done</span>!</span><br><span class="line">hello 5!</span><br></pre></td></tr></table></figure></p>
<h3 id="Goroutine-数量不受控制的图示"><a href="#Goroutine-数量不受控制的图示" class="headerlink" title="Goroutine 数量不受控制的图示"></a>Goroutine 数量不受控制的图示</h3><p>我们应该怎么理解例一的代码呢？</p>
<p>假如 CPU 只有 <strong>两个</strong> 核，下图展示了为每个 job 创建一个 goroutine 的情况（换句话说，goroutine 的数量是不受控制的）。此种情况虽然生成了很多的 goroutine，但是 <strong><code>每个 CPU 核上同一时间只能执行一个 goroutine</code></strong> ；当 job 很多且生成了相应数目的 goroutine 后，会出现很多等待执行的 goroutine，从而造成资源上的浪费。</p>
<p><img src="/images/go/figure-goroutine-controll-1.png" alt="goroutine"></p>
<h2 id="Goroutine-数量受控制示例"><a href="#Goroutine-数量受控制示例" class="headerlink" title="Goroutine 数量受控制示例"></a>Goroutine 数量受控制示例</h2><h3 id="Goroutine-数量受到限制的图示"><a href="#Goroutine-数量受到限制的图示" class="headerlink" title="Goroutine 数量受到限制的图示"></a>Goroutine 数量受到限制的图示</h3><p>给每个 job 生成一个 goroutine 的方式显得粗暴了很多，那么可以通过什么样的方式控制 goroutine 的数目呢？其实上面的代码通过一个 for-range 循环完成了两件事情：①为每个 job 创建 goroutine；②把任务相关的标识传给相应的 goroutine 执行。为了控制 goroutine 的数目，可以通过 buffered channel 给每个 goroutine 传递任务相关的信息 或者 可以把上面的两个过程拆分开：a）先通过一个 for-range 循环创建指定数目的 goroutine，b）然后通过 channel/buffered channel 给每个 goroutine 传递任务相关的信息（这里的channel是否缓冲无所谓，主要用到的是 channel 的线程安全特性）。如下图所示。</p>
<p><img src="/images/go/figure-goroutine-controll-2.png" alt="goroutine"></p>
<h3 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h3><p>方式一：通过有阻塞的 <code>buffer channel</code> 来控制 <code>goroutine</code> 增长<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"sync"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	jobsCount := <span class="number">10</span></span><br><span class="line">	ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>, <span class="number">2</span>) <span class="comment">//通过 channel 控制 goroutine 数量，防止无休止增长</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; jobsCount; j++ &#123;</span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line">		ch &lt;- <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">go</span> do(ch, j)</span><br><span class="line">		fmt.Printf(<span class="string">"index: %d,goroutine Num: %d \n"</span>, j, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	wg.Wait()</span><br><span class="line">	fmt.Println(<span class="string">"done!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">do</span><span class="params">(ch <span class="keyword">chan</span> <span class="keyword">bool</span>, i <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">"hello %d!\n"</span>, i)</span><br><span class="line">	time.Sleep(time.Second * <span class="number">2</span>) <span class="comment">// 刻意睡 2 秒钟，模拟耗时</span></span><br><span class="line"></span><br><span class="line">	&lt;-ch  <span class="comment">//必须在任务完成后从channel取出，通过channel来阻塞</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>方式二：通过拆任务实现：</p>
<ul>
<li>a）先通过一个 for-range 循环创建指定数目的 goroutine</li>
<li>b）然后通过 channel/buffered channel 给每个 goroutine 传递任务相关的信息</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"runtime"</span></span><br><span class="line">	<span class="string">"sync"</span></span><br><span class="line">	<span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	jobsCount := <span class="number">10</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> jobsChan = <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// a) 生成指定数目的 goroutine，每个 goroutine 消费 jobsChan 中的数据</span></span><br><span class="line">	poolCount := <span class="number">2</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; poolCount; i++ &#123;</span><br><span class="line">		<span class="keyword">go</span> job(jobsChan)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// b) 把 job 依次推送到 jobsChan 供 goroutine 消费</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; jobsCount; i++ &#123;</span><br><span class="line">		jobsChan &lt;- i</span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line">		fmt.Printf(<span class="string">"index: %d,goroutine Num: %d\n"</span>, i, runtime.NumGoroutine())</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	wg.Wait()</span><br><span class="line">	</span><br><span class="line">	fmt.Println(<span class="string">"done!"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">job</span><span class="params">(jobsChan <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> j := <span class="keyword">range</span> jobsChan &#123;</span><br><span class="line">		fmt.Printf(<span class="string">"hello %d\n"</span>, j)</span><br><span class="line">		time.Sleep(time.Second * <span class="number">2</span>) <span class="comment">// 刻意睡 2 秒钟，模拟耗时</span></span><br><span class="line">		wg.Done()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行上面的代码可以得到下面类似的输出（可以看到 goroutine 的数量控制在了 3 个）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hello 0!</span><br><span class="line">index: 0,goroutine Num: 2</span><br><span class="line">index: 1,goroutine Num: 3</span><br><span class="line">hello 1!</span><br><span class="line">hello 2!</span><br><span class="line">index: 2,goroutine Num: 3</span><br><span class="line">index: 3,goroutine Num: 3</span><br><span class="line">hello 3!</span><br><span class="line">index: 4,goroutine Num: 3</span><br><span class="line">hello 4!</span><br><span class="line">index: 5,goroutine Num: 3</span><br><span class="line">hello 5!</span><br><span class="line">index: 6,goroutine Num: 3</span><br><span class="line">index: 7,goroutine Num: 3</span><br><span class="line">hello 7!</span><br><span class="line">hello 6!</span><br><span class="line">index: 8,goroutine Num: 3</span><br><span class="line">index: 9,goroutine Num: 3</span><br><span class="line">hello 8!</span><br><span class="line">hello 9!</span><br><span class="line"><span class="keyword">done</span>!</span><br></pre></td></tr></table></figure></p>
<p>参考：</p>
<ul>
<li>jingwei.link</li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Podman、Skopeo和Buildah下一代容器代替Docker]]></title>
      <url>http://team.jiunile.com/blog/2019/09/containers-podman-skopeo-buildah.html</url>
      <content type="html"><![CDATA[<h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>在Docker实践中，很多人应该都遇到过开机重启时，由于Docker守护程序在占用多核CPU使用100%C使用的情况，此时所有容器都无法正常工作，所有服务都不能用。解决唯一方法只能杀掉所有容器并重启守护进程，才能恢复。经过了解该问题是由于Docker守护进程引起，而且Docker守护进程是以root特权权限启动的，是一个安全问题，那么有什么方法解决呢？本文介绍一下基于CRI 等标准（Docker新架构也符合CRI标准）的新一代容器工具<code>Podman</code>、<code>Skopero</code>和<code>Buiddah</code>套件。</p>
<h2 id="OCI"><a href="#OCI" class="headerlink" title="OCI"></a>OCI</h2><p>为了防止容器被Docker一家垄断，巨头们（谷歌，Redhat、微软、IBM、Intel、思科）聚在一起决定要成立一个组织（OCI），大家一起商量指定了一套规范（CRI、CNI），大家一致统一只兼容符合这套规范的工具。Docker虽然心有不甘但是毕竟胳膊拧不过大腿，只能该架构兼容规范。</p>
<p>在该规范的指导下就有了本文的三个主人公，这三个工具都是符合OCI计划下的工具（<a href="https://github.com/containers" target="_blank" rel="external">https://github.com/containers</a>）。他们主要是由RedHat推动，三者各司其职，配合完成Docker所有的功能和新扩展功能，并且对docker的问题进行了改良：包括不需要守护程序或访问有root权限的组；容器架构基于fork/exec模型创建容器，更加安全可靠；所以是更先进、高效和安全的下一代容器容器工具。</p>
<a id="more"></a>
<h2 id="Podman"><a href="#Podman" class="headerlink" title="Podman"></a>Podman</h2><p><img src="/images/containers/podman.jpg" alt="podman"></p>
<p><a href="https://github.com/containers/libpod" target="_blank" rel="external">Podman</a>是该工具套件的核心，用来替换Docker中了大多数子命令（RUN，PUSH，PULL等）。Podman无需守护进程，使用用户命名空间来模拟容器中的root，无需连接到具有root权限的套接字保证容器的体系安全。</p>
<p>Podman专注于维护和修改OCI镜像的所有命令和功能，例如拉动和标记。它还允许我们创建，运行和维护从这些镜像创建的容器。</p>
<h2 id="Buildah"><a href="#Buildah" class="headerlink" title="Buildah"></a>Buildah</h2><p><a href="https://github.com/containers/buildah" target="_blank" rel="external">Buildah</a>用来构建OCI图像。虽然Podman也可以用户构建Docker镜像，但是构建速度超慢，并且默认情况下使用vfs存储驱动程序会耗尽大量磁盘空间。 buildah bud（使用Dockerfile构建）则会非常快，并使用覆盖存储驱动程序。</p>
<p><img src="/images/containers/buildah.jpg" alt="buildah"></p>
<p>Buildah专注于构建OCI镜像。 Buildah的命令复制了Dockerfile中的所有命令。可以使用Dockerfiles构建镜像，并且不需要任何root权限。 Buildah的最终目标是提供更低级别的coreutils界面来构建图像。Buildah也支持非Dockerfiles构建镜像，可以允许将其他脚本语言集成到构建过程中。 Buildah遵循一个简单的fork-exec模型，不以守护进程运行，但它基于golang中的综合API，可以存储到其他工具中。</p>
<h2 id="Skopeo"><a href="#Skopeo" class="headerlink" title="Skopeo"></a>Skopeo</h2><p><a href="https://github.com/containers/skopeo" target="_blank" rel="external">Skopeo</a>是一个工具，允许我们通过推，拉和复制镜像来处理Docker和OC镜像。</p>
<p><img src="/images/containers/skopeo.jpg" alt="skopeo"></p>
<h2 id="三个工具对比"><a href="#三个工具对比" class="headerlink" title="三个工具对比"></a>三个工具对比</h2><p>Buildah构建容器，Podman运行容器，Skopeo传输容器镜像。这些都是由Github容器组织维护的开源工具（<a href="https://github.com/containers" target="_blank" rel="external">https://github.com/containers</a>）。这些工具都不需要运行守护进程，并且大多数情况下也不需要root访问权限。</p>
<p>Podman和Buildah之间的一个主要区别是他们的容器概念。 Podman允许用户创建”传统容器”。虽然Buildah容器实际上只是为了允许将内容添加回容器图像而创建的。一种简单方法是buildah run命令模拟Dockerfile中的RUN命令，而podman run命令模拟功能中的docker run命令。</p>
<p>总之，Buildah是创建OCI镜像的有效方式，而Podman允许我们使用熟悉的容器cli命令在生产环境中管理和维护这些镜像和容器。</p>
<h2 id="容器迁移"><a href="#容器迁移" class="headerlink" title="容器迁移"></a>容器迁移</h2><h3 id="套件安装"><a href="#套件安装" class="headerlink" title="套件安装"></a>套件安装</h3><p>各大Linux发行版都提供了二进制安装包， 可以使用发行版的系统包管理工具一键安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Fedora, CentOS：sudo yum -y install podman</span><br><span class="line">Arch &amp; Manjaro Linux： sudo pacman -S podman</span><br></pre></td></tr></table></figure></p>
<p>Ubuntu不支持一键安装，需要先添加第三方私有ppa仓库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update -qq</span><br><span class="line">sudo apt-get install -qq -y software-properties-common uidmap</span><br><span class="line">sudo add-apt-repository -y ppa:projectatomic/ppa</span><br><span class="line">sudo apt-get update -qq</span><br><span class="line">sudo apt-get -qq -y install podman</span><br></pre></td></tr></table></figure></p>
<h2 id="实践迁移"><a href="#实践迁移" class="headerlink" title="实践迁移"></a>实践迁移</h2><p>安装了套件的三个工具后，就可以对docker实例进行迁移了。</p>
<blockquote>
<p>替换cron或者CI作业（脚本）中的所有docker实例，把docker替换为podman，可以使用后面提到的别名的方式。</p>
</blockquote>
<h3 id="1、停止和删除所有的运行的docker。"><a href="#1、停止和删除所有的运行的docker。" class="headerlink" title="1、停止和删除所有的运行的docker。"></a>1、停止和删除所有的运行的docker。</h3><p>为了确保没有有差错，可以使用sysdig来捕获系统中docker的引用，看看是否还有其他东西在调用docker。</p>
<h3 id="2、删除docker"><a href="#2、删除docker" class="headerlink" title="2、删除docker"></a>2、删除docker</h3><p>现在就可以删除docker了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum remove docker || apt remove -y docker-ce</span><br></pre></td></tr></table></figure></p>
<h3 id="3、环境清理"><a href="#3、环境清理" class="headerlink" title="3、环境清理"></a>3、环境清理</h3><p>最后清理下docker文件，我们建个一目录docker备份目录，把以下目录mv到备份目录即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/docker/*，/etc/default/docker和/var/lib/ docker中的任何遗留文件</span><br></pre></td></tr></table></figure></p>
<p>删除docker组<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delgroup docker</span><br></pre></td></tr></table></figure></p>
<p>现在可以吧docker别名成podman来无缝使用了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> docker = podman</span><br></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了符合CIR标准的 cri-o 容器套件Podman，Skopeo和Buildah。该新一代容器套件架构基于*nix传统的fork-exec模型，解决了由于docker守护程序导致的启动和安全问题，提提高了容器的性能和安全。</p>
<p>来源：</p>
<ul>
<li>zhuanlan.zhihu.com</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubelet 状态更新机制]]></title>
      <url>http://team.jiunile.com/blog/2019/08/k8s-kubelet-sync-node-status.html</url>
      <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。</p>
<ol>
<li>kubelet 自身会定期更新状态到 apiserver，通过参数<code>--node-status-update-frequency</code>指定上报频率，默认是 10s 上报一次。</li>
<li>kube-controller-manager 会每隔<code>--node-monitor-period</code>时间去检查 kubelet 的状态，默认是 5s。</li>
<li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>notready</code> 状态，这段时长通过<code>--node-monitor-grace-period</code>参数配置，默认 40s。</li>
<li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>unhealthy</code>状态，这段时长通过<code>--node-startup-grace-period</code>参数配置，默认 1m0s。</li>
<li>当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过<code>--pod-eviction-timeout</code>参数配置，默认 5m0s。</li>
</ol>
<blockquote>
<p>kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果<code>--node-status-update-frequency</code>设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。<br><a id="more"></a><br>Kubelet在更新状态失败时，会进行<code>nodeStatusUpdateRetry</code>次重试，默认为<strong><code>5 次</code></strong>。</p>
</blockquote>
<p>Kubelet 会在函数<code>tryUpdateNodeStatus</code>中尝试进行状态更新。Kubelet 使用了 Golang 中的<code>http.Client()</code>方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。</p>
<p>因此，在<code>nodeStatusUpdateRetry * --node-status-update-frequency</code>时间后才会更新一次节点状态。</p>
<p>同时，Kubernetes 的 controller manager 将尝试每<code>--node-monitor-period</code>时间周期内检查<code>nodeStatusUpdateRetry</code>次。在<code>--node-monitor-grace-period</code>之后，会认为节点 unhealthy，然后会在<code>--pod-eviction-timeout</code>后删除 Pod。</p>
<p>kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>对于这些参数的配置，需要根据不通的集群规模场景来进行配置。</p>
<h3 id="社区默认的配置"><a href="#社区默认的配置" class="headerlink" title="社区默认的配置"></a>社区默认的配置</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">10s</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">5s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">40s</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">5m</td>
</tr>
</tbody>
</table>
<h3 id="快速更新和快速响应"><a href="#快速更新和快速响应" class="headerlink" title="快速更新和快速响应"></a>快速更新和快速响应</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">4s</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">2s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">20s</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">30s</td>
</tr>
</tbody>
</table>
<p>在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，<code>--pod-eviction-timeout</code> 在 30s 之后发生，Kubelet将尝试每4秒更新一次状态。因此，在Kubernetes控制器管理器考虑节点的不健康状态之前，它将是 (20s / 4s * 5) = 25 次尝试，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。</p>
<p>如果环境有1000个节点，那么每分钟将有(60s / 4s * 1000) = 15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。</p>
<blockquote>
<p>如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。</p>
</blockquote>
<h3 id="中等更新和平均响应"><a href="#中等更新和平均响应" class="headerlink" title="中等更新和平均响应"></a>中等更新和平均响应</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">20s</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">5s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">2m</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">1m</td>
</tr>
</tbody>
</table>
<p>这种场景下会，Pod 将在 3m 被驱逐。 Kubelet将尝试每20秒更新一次状态。因此，在Kubernetes控制器管理器考虑节点的不健康状态之前，它将是 (2m <em> 60 / 20s </em> 5) = 30 次尝试</p>
<p>如果有 1000 个节点，1分钟之内就会有 (60s / 20s * 1000) = 3000 次的节点状态更新操作。</p>
<h3 id="低更新和慢响应"><a href="#低更新和慢响应" class="headerlink" title="低更新和慢响应"></a>低更新和慢响应</h3><table>
<thead>
<tr>
<th>参数</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">值</th>
</tr>
</thead>
<tbody>
<tr>
<td>–node-status-update-frequency</td>
<td style="text-align:left">kubelet 间隔多少时间向apiserver上报node status信息</td>
<td style="text-align:left">1m</td>
</tr>
<tr>
<td>–node-monitor-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间后从apiserver同步node status信息</td>
<td style="text-align:left">5s</td>
</tr>
<tr>
<td>–node-monitor-grace-period</td>
<td style="text-align:left">kube-controller-manager 间隔多少时间之后，把node状态设置为NotReady</td>
<td style="text-align:left">5m</td>
</tr>
<tr>
<td>–pod-eviction-timeout</td>
<td style="text-align:left">kube-controller-manager 在第一次kubelet notReady事件之后的多少时间后，开始驱逐pod。并不是CM把node状态设置为notready之后再等待<code>pod-eviction-timeout</code>时间</td>
<td style="text-align:left">1m</td>
</tr>
</tbody>
</table>
<p>这种场景下会，Pod 将在 6m 被驱逐。 Kubelet将尝试每1分钟更新一次状态。因此，在Kubernetes控制器管理器考虑节点的不健康状态之前，它将是 (5m / 1m * 5) = 25 次尝试</p>
<p>如果有 1000 个节点，1分钟之内就会有 (1m / 60s * 1000) = 1000 次的节点状态更新操作。</p>
<p>参考链接：</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md" target="_blank" rel="external">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md</a></li>
<li><a href="https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md" target="_blank" rel="external">https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md</a></li>
</ul>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Calico 3.5+ 根据节点标签分配 IP 地址]]></title>
      <url>http://team.jiunile.com/blog/2019/08/k8s-calico-assigning-ip.html</url>
      <content type="html"><![CDATA[<h2 id="关于-IP-地址的分配"><a href="#关于-IP-地址的分配" class="headerlink" title="关于 IP 地址的分配"></a>关于 IP 地址的分配</h2><p>Calico 能够进行配置，为不同拓扑指定 IP 地址池。例如可能希望某些机架、地区、或者区域能够从同一个 IP 池中获取地址。这对于降低路由数量或者配合防火墙策略的要求会很有帮助。</p>
<p><a href="https://docs.projectcalico.org/v3.5/reference/cni-plugin/configuration#ipam" target="_blank" rel="external">cni 插件配置参考中的 IP 地址管理章节</a>中包含了三种分配 IP 地址的方式。Kubernetes 注解方式只能用于 Namespace 或者 Pod 一级。剩下的只有两个办法，CNI 配置或者是基于节点选择器的 IP 池，相对于 CNI 配置的方式来说，节点选择器方案省去了修改本地文件的麻烦。</p>
<p>在更高层次上，基于节点选择器的 IP 地址分配方法就是给节点设置标签，然后用节点选择器选择对应的 IP 地址池进行分配。后面的内容中将给出一个详细的例子，用这种方式来设置一种机架亲和方式的 IP 地址分配方案。</p>
<blockquote>
<p>如果 Calico 无法根据上述顺序来决定一个 IP 地址池，或者在选定的地址池中找不到可用的 IP 地址，那么这一工作负载就不会分到 IP 地址，无法启动。为了防止这种情况的发生，我们建议所有节点至少有一个合适的地址池。<br><a id="more"></a></p>
<h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p><strong><code>这一功能需要 Calico 在 ETCD 模式下工作</code></strong></p>
</blockquote>
<h2 id="示例：Kubernetes"><a href="#示例：Kubernetes" class="headerlink" title="示例：Kubernetes"></a>示例：Kubernetes</h2><p>本例中，我们会创建一个集群，其中包含四个节点，分布在两个机架上，每个机架各两台。示意如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">       -------------------</span><br><span class="line">       |    router       |</span><br><span class="line">       -------------------</span><br><span class="line">       |                 |</span><br><span class="line">---------------   ---------------</span><br><span class="line">| rack-0      |   | rack-1      |</span><br><span class="line">---------------   ---------------</span><br><span class="line">| kube-node-0 |   | kube-node-2 |</span><br><span class="line">- - - - - - - -   - - - - - - - -</span><br><span class="line">| kube-node-1 |   | kube-node-3 |</span><br><span class="line">- - - - - - - -   - - - - - - - -</span><br></pre></td></tr></table></figure></p>
<p>Pod IP 地址范围为 <code>192.168.0.0/16</code>，我们进行如下设计：保留 <code>192.168.0.0/24</code> 给 <code>rack-0</code>,  <code>192.168.1.0/24</code> 给 <code>rack-1</code>。</p>
<p>要设置一个没有缺省地址池的的 Calico，首先运行 <code>calicoctl get ippool -o wide</code>，会看到已经创建了一个 <code>192.168.0.0/16</code> 的地址池：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME                  CIDR             NAT    IPIPMODE   DISABLED   SELECTOR</span><br><span class="line">default-ipv4-ippool   192.168.0.0/16   <span class="literal">true</span>   Always     <span class="literal">false</span>      all()</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>删除缺省地址池<br><code>default-ipv4-ippool</code> 地址池已经存在，并占据了整个 <code>/16</code> 块，因此必须删除：<code>calicoctl delete ippools default-ipv4-ippool</code></p>
</li>
<li><p>给 Node 打标签。<br>要给特定节点分配地址池，节点必须用标签进行标识：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl label nodes kube-node-0 rack=0</span><br><span class="line">kubectl label nodes kube-node-1 rack=0</span><br><span class="line">kubectl label nodes kube-node-2 rack=1</span><br><span class="line">kubectl label nodes kube-node-3 rack=1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>为每个机架创建地址池</strong></p>
<p>rack-0-ippool<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">calicoctl create -f -&lt;&lt;EOF</span><br><span class="line"><span class="attr">apiVersion:</span> projectcalico.org/v3</span><br><span class="line"><span class="attr">kind:</span> IPPool</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> rack<span class="bullet">-0</span>-ippool</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  cidr:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line"><span class="attr">  ipipMode:</span> Always</span><br><span class="line"><span class="attr">  natOutgoing:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  nodeSelector:</span> rack == <span class="string">"0"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>rack-1-ippool<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">calicoctl create -f -&lt;&lt;EOF</span><br><span class="line"><span class="attr">apiVersion:</span> projectcalico.org/v3</span><br><span class="line"><span class="attr">kind:</span> IPPool</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> rack<span class="bullet">-1</span>-ippool</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  cidr:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.0</span>/<span class="number">24</span></span><br><span class="line"><span class="attr">  ipipMode:</span> Always</span><br><span class="line"><span class="attr">  natOutgoing:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  nodeSelector:</span> rack == <span class="string">"1"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>现在就创建了两个地址池，使用 <code>calicoctl get ippool -o wide</code> 进行查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NAME                  CIDR             NAT    IPIPMODE   DISABLED   SELECTOR</span><br><span class="line">rack-1-ippool         192.168.0.0/24   <span class="literal">true</span>   Always     <span class="literal">false</span>      rack == <span class="string">"0"</span></span><br><span class="line">rack-2-ippool         192.168.1.0/24   <span class="literal">true</span>   Always     <span class="literal">false</span>      rack == <span class="string">"1"</span></span><br></pre></td></tr></table></figure></p>
<ol>
<li>检查地址池的工作状态<br>创建一个 Nginx 的 Deployment，其中包含五个副本，保证分配到每一个节点上。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image nginx --replicas 5</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>检查新的 Pod 是否已经根据所在机架获得了应有的 IP 地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAME                   READY   STATUS    RESTARTS   AGE    IP             NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-5c7588df-prx4z   1/1     Running   0          6m3s   192.168.0.64   kube-node-0   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df<span class="_">-s</span>7qw6   1/1     Running   0          6m7s   192.168.0.129  kube-node-1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df-w7r7g   1/1     Running   0          6m3s   192.168.1.65   kube-node-2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df-62lnf   1/1     Running   0          6m3s   192.168.1.1    kube-node-3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-5c7588df-pnsvv   1/1     Running   0          6m3s   192.168.1.64   kube-node-2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，IP 地址的是根据节点（所在的机架）来选择了对应的地址池进行分配的。</p>
<p>参考地址</p>
<ul>
<li><a href="https://docs.projectcalico.org/v3.5/usage/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.5</a></li>
<li><a href="https://docs.projectcalico.org/v3.6/networking/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.6</a></li>
<li><a href="https://docs.projectcalico.org/v3.7/networking/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.7</a></li>
<li><a href="https://docs.projectcalico.org/v3.8/networking/assigning-ip-addresses-topology" target="_blank" rel="external">calico 3.8</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[镜像漏洞检测工具 -- Trivy]]></title>
      <url>http://team.jiunile.com/blog/2019/08/security-trivy.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>道路千万条，安全第一条； 镜像不规范，同事两行泪。</p>
</blockquote>
<p><a href="https://github.com/aquasecurity/trivy" target="_blank" rel="external">Trivy</a> 是一个面向镜像的漏洞检测工具，具备如下特点：</p>
<ol>
<li>开源</li>
<li>免费</li>
<li>易用</li>
<li>准确度高</li>
<li>CI 友好</li>
</ol>
<p>相对于老前辈 <a href="https://github.com/coreos/clair" target="_blank" rel="external">Clair</a>，Trivy 的使用非常直观方便，适用于更多的场景。</p>
<p>下面是官方出具的对比表格：</p>
<table>
<thead>
<tr>
<th>扫描器</th>
<th style="text-align:center">操作系统</th>
<th style="text-align:center">依赖检测</th>
<th style="text-align:center">适用性</th>
<th style="text-align:center">准确度</th>
<th style="text-align:center">CI 友好</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trivy</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◎</td>
<td style="text-align:center">◯</td>
</tr>
<tr>
<td>Clair</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">△</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">△</td>
</tr>
<tr>
<td>Anchore Engine</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">△</td>
<td style="text-align:center">△</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">△</td>
</tr>
<tr>
<td>Quay</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td>MicroScanner</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
</tr>
<tr>
<td>Docker Hub</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td>GCR</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">◯</td>
<td style="text-align:center">×</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="MacOS"><a href="#MacOS" class="headerlink" title="MacOS"></a>MacOS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew tap knqyf263/trivy</span><br><span class="line">$ brew install knqyf263/trivy/trivy</span><br></pre></td></tr></table></figure>
<h3 id="RHEL-CentOS"><a href="#RHEL-CentOS" class="headerlink" title="RHEL/CentOS"></a>RHEL/CentOS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/yum.repos.d/trivy.repo</span><br><span class="line">[trivy]</span><br><span class="line">name=Trivy repository</span><br><span class="line">baseurl=https://aquasecurity.github.io/trivy-repo/rpm/releases/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">$ sudo yum -y update</span><br><span class="line">$ sudo yum -y install trivy</span><br></pre></td></tr></table></figure>
<p>or<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.0.15/trivy_0.0.15_Linux-64bit.rpm</span><br></pre></td></tr></table></figure></p>
<h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><p>这个工具的最大闪光点就是提供了很多适合用在自动化场景的用法。详细使用帮助可参考官方文档：<a href="https://github.com/aquasecurity/trivy" target="_blank" rel="external">Trivy</a></p>
<h3 id="扫描镜像："><a href="#扫描镜像：" class="headerlink" title="扫描镜像："></a>扫描镜像：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy centos</span><br></pre></td></tr></table></figure>
<h3 id="扫描镜像文件"><a href="#扫描镜像文件" class="headerlink" title="扫描镜像文件"></a>扫描镜像文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker save ruby:2.3.0-alpine3.9 -o ruby-2.3.0.tar</span><br><span class="line">$ trivy --input ruby-2.3.0.tar</span><br></pre></td></tr></table></figure>
<h3 id="根据严重程度进行过滤"><a href="#根据严重程度进行过滤" class="headerlink" title="根据严重程度进行过滤"></a>根据严重程度进行过滤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy --severity HIGH,CRITICAL ruby:2.3.0</span><br></pre></td></tr></table></figure>
<h3 id="忽略未修复问题"><a href="#忽略未修复问题" class="headerlink" title="忽略未修复问题"></a>忽略未修复问题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy --ignore-unfixed ruby:2.3.0</span><br></pre></td></tr></table></figure>
<h3 id="忽略特定问题"><a href="#忽略特定问题" class="headerlink" title="忽略特定问题"></a>忽略特定问题</h3><p>使用 <code>.trivyignore</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat .trivyignore</span><br><span class="line"><span class="comment"># Accept the risk</span></span><br><span class="line">CVE-2018-14618</span><br><span class="line"></span><br><span class="line"><span class="comment"># No impact in our settings</span></span><br><span class="line">CVE-2019-1543</span><br><span class="line"></span><br><span class="line">$ trivy python:3.4-alpine3.9</span><br></pre></td></tr></table></figure></p>
<h3 id="使用-JSON-输出结果"><a href="#使用-JSON-输出结果" class="headerlink" title="使用 JSON 输出结果"></a>使用 JSON 输出结果</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ trivy <span class="_">-f</span> json dustise/translat-chatbot:20190428-5</span><br></pre></td></tr></table></figure>
<h3 id="定义返回值"><a href="#定义返回值" class="headerlink" title="定义返回值"></a>定义返回值</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ trivy --exit-code 0 --severity MEDIUM,HIGH ruby:2.3.0</span><br><span class="line">$ trivy --exit-code 1 --severity CRITICAL ruby:2.3.0</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>相对于其它同类工具，Trivy 非常适合自动化操作，从 CircleCI 之类的公有服务，到企业内部使用的 Jenkins、Gitlab 等私有工具，或者作为开发运维人员的自测环节，都有 Trivy 的用武之地。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Go 语言的错误处理：从拒绝到接受]]></title>
      <url>http://team.jiunile.com/blog/2019/08/go-errors-in-go.html</url>
      <content type="html"><![CDATA[<h3 id="学习如何在go中不在担心并且爱上错误处理"><a href="#学习如何在go中不在担心并且爱上错误处理" class="headerlink" title="学习如何在go中不在担心并且爱上错误处理"></a>学习如何在go中不在担心并且爱上错误处理</h3><p>正如一位英国诗人所说的，“犯错是人，宽恕是神”。错误处理是编程实践中非常重要的一部分，但在很多流行语言中并没有对它给予足够的重视。</p>
<p>作为众多语言的鼻祖，C 语言从一开始就没有一个完善的错误处理和异常机制。在 C 语言中，错误处理完全由程序员来负责，要么通过设置一个错误码，或者程序直接就崩溃了（segment fault）。</p>
<p>虽然异常处理机制早在 C 语言发明之前就出现了（最早由 LISP 1.5在1962年支持），但直到19世纪80年代它才流行开来。C++ 和 Java 让程序员熟悉了 <code>try...catch</code> 这一模式，所有的解释型语言也沿用了它。</p>
<p>尽管在语法上略有差异（比如是用 <code>try</code> 还是 <code>begin</code>），我之前遇到的每一种语言在一开始学习的时候都不会让你注意到错误处理的概念。通常，在你刚开始写着玩的时候根本用不到它，只有当你开始写一个真正的项目时才会意识到需要有错误处理。至少对于我而言，一直如此。</p>
<p>然后我遇到了 Golang ：一开始大家都是从《a Tour of Go》来认识它的。<br><a id="more"></a></p>
<p>在学习《a Tour of Go》的过程中，不断的有 <code>err</code> 这样代表错误的变量映入眼帘。不管一个 Go 项目有多大，一种模式总是存在：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f, err := os.Open(filename)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  <span class="comment">// 在这里处理错误</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据 Go 的惯例，每一个会出现错误的函数都需要在最后一个返回值中定义它（Go 允许多返回值），程序员需要在每一次调用函数后都对返回的错误进行处理，因此就出现了随处可见的 <code>if err != nil</code> 代码片段。</p>
<p>一开始，每个函数调用后都进行一次错误检查让人感觉很崩溃。对于许多 Go 新手来说，这非常痛苦。在我们刚接触到错误处理的时候就开始为它这种繁琐的处理方式感到厌恶了。</p>
<p>有一个著名的用于处理悲痛和失去的模型，它是由美籍瑞士心理学家 Elisabeth Kübler-Ross 在1969年提出。它包含了五个阶段：拒绝，愤怒，讨价还价，失落，接受。虽然起初它主要是用于解决跟死亡和伤痛有关的问题，但事实已经证明，它在处理当一个人遇到重大变故而产生内心抵抗时都是有效的。学习一门新的编程语言显然属于这一范畴。</p>
<p>在我拥抱 Go 的错误处理模式的过程中，我经历了所有这五个阶段，下面我就跟你分享一下我的旅程。</p>
<p>那么，一切都从拒绝开始说起吧。</p>
<h3 id="拒绝"><a href="#拒绝" class="headerlink" title="拒绝"></a>拒绝</h3><p>“一定是哪里出错了，不应该出现这么多错误检查…”</p>
<p>这是我刚开始写 Go 代码时的想法。我下意识的想找 Go 里的异常机制，但我没找到。Go 有意地去掉了对异常的支持。</p>
<p>使用异常的一个问题是你永远都不知道一个函数是否会抛异常。当然了，Java 通过 <code>throws</code> 关键字来显式声明了一个函数可能会抛出异常，这解决了异常不明确的问题，但同时也使得代码变得非常啰嗦。有人说我可以在文档中把这个问题说清楚，但文档也不是银弹，通常更新不及时的文档是大部分项目永远的痛。</p>
<p>Go 中的错误处理机制体现了一致性：每个可能产生错误的函数都应在最后一个返回值中返回一个 <code>error</code> 类型。</p>
<p>如果你正常处理了错误，那一切相安无事，代码会继续运行。在某些情况下，如果你觉得没有必要进行错误检查，你完全可以忽略它。当出现错误时，其它的返回值默认则是零值，这样有些错误你完全不必理会。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Let's convert a string into int64.</span></span><br><span class="line"><span class="comment">// We don't care whether strconv.ParseInt returns an error</span></span><br><span class="line"><span class="comment">// as the first returned value will be 0 if it fails to convert.</span></span><br><span class="line">i, _ := strconv.ParseInt(strVal, <span class="number">10</span>, <span class="number">64</span>)</span><br><span class="line">log.Printf(<span class="string">"Parsed value is: %d"</span>, i)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>例如上面的 ParseInt 函数，当转换错误时 i 默认为0，因为对于转换的错误你即使不做处理也是 ok 的，但是对于后续的处理逻辑则无法区分是因为转换错误导致 i 等于0，还是本身转换的内容就是0这两种情况。</p>
</blockquote>
<p>但是如果你仅仅是调用这个函数，并没有处理它的返回值，那么你很容易忘记这个函数可能还会返回一个错误。因此，在使用之前在文档中查看一下这个函数是否会返回错误永远都是明智的做法。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// http.ListenAndServe returns an error, but we don't check for it.</span></span><br><span class="line"><span class="comment">// Since we don't use returned values further, this code will compile.</span></span><br><span class="line">http.ListenAndServe(<span class="string">":8080"</span>, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// However, it is still better to check the returned error for consistency.</span></span><br><span class="line">err := http.ListenAndServe(<span class="string">":8080"</span>, <span class="literal">nil</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">  log.Fatalf(<span class="string">"Can't start the server: %s"</span>, err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="愤怒"><a href="#愤怒" class="headerlink" title="愤怒"></a>愤怒</h3><p>有许多编程语言都有所谓“正常”的错误处理（类似于 <code>try catch</code>），那我为什么要用这种奇怪的像垃圾碎片一样的“把错误作为函数结果”来返回呢？</p>
<p>作为作者，这两种处理机制我都使用过。Go 不仅仅是将那些我习以为常的 exception 当成 error 来替换，这让我对这门语言感到愤慨。然而更好的做法是将这种 error 视为方法是否被成功执行的指示。</p>
<p>如果你曾经在 Rails 中用过 <code>active record</code>，你可能会熟悉这样的代码：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">user = User.new(user_params)</span><br><span class="line"><span class="keyword">if</span> user.save</span><br><span class="line">  head <span class="symbol">:ok</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  render <span class="symbol">json:</span> user.errors, <span class="symbol">status:</span> <span class="number">422</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><code>if</code> 后面的 <code>user.save</code> 是一个 bool 值，它表示是否成功保存了用户的实例。<code>user.errors</code> 则返回了可能发生的 error 列表结果。当时保存用户实例失败的时候， <code>user</code> 就会包含 <code>errors</code>，这种方法经常被批评为反模式。</p>
<p>然而，Go 语言自带报告方法”失败细节“的内置模式，并且还没有什么副作用。毕竟，Go 的 error 只是一个含有单个方法的接口：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> error <span class="keyword">interface</span> &#123;</span><br><span class="line">  Error() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以任意去集成这个接口。如果想要提供一些验证错误的信息，可以定义如下的结构体类型：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> ValidationErr <span class="keyword">struct</span> &#123;</span><br><span class="line">  <span class="comment">// We will store validation errors here.</span></span><br><span class="line">  <span class="comment">// The key is a field name, and the value is a slice of validation messages.</span></span><br><span class="line">  ErrorMessages <span class="keyword">map</span>[<span class="keyword">string</span>][]<span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *ValidationErr)</span> <span class="title">Error</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> FormatErrors(e.ErrorMessages)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于如何格式化错误信息并不是本文的重点， 所以我省去了 <code>FormatErrors</code> 方法的具体实现。我们只说如何将错误信息合并成单个的字符串。</p>
<p>现在假设我们用 Go 语言写了一个类似 Rails 的框架，<code>actions handler</code> 的处理方式就像这样：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(a * Action)</span> <span class="title">Handle</span><span class="params">()</span></span> &#123;</span><br><span class="line">  user := NewUser(a.Params[<span class="string">"user"</span>])</span><br><span class="line">  <span class="keyword">if</span> err := user.Save(); err == <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="comment">// No errors, yay! Respond with 200.</span></span><br><span class="line">    a.Respond(<span class="number">200</span>, <span class="string">"OK"</span>, <span class="string">"text/plain"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> verr, ok := err.(*ValidationErr); ok &#123;</span><br><span class="line">    <span class="comment">// err was successfully typecast to ValidationErr.</span></span><br><span class="line">    <span class="comment">// Let's respond with 422.</span></span><br><span class="line">    resp, _ := json.Marshal(verr.ErrorMessages)</span><br><span class="line">    a.Respond(<span class="number">422</span>, <span class="keyword">string</span>(resp), <span class="string">"application/json"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Unexpected error, respond with 500.</span></span><br><span class="line">    a.Respond(<span class="number">500</span>, err.Error(), <span class="string">"text/plain"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>就这样，错误验证是函数返回的合法部分，我们减少了 <code>user.Save</code> 方法的副作用。所有非预期的错误都是在显式的进行处理，而不是隐藏在框架里面。如果还出现问题，我们可以在处理后续逻辑之前采取其他必要的措施。</p>
<p>返回错误的时候如果有额外的信息，这总归是好的。许多流行的 Go 包都会用他们自己实现的 error 接口，比如我的 imgproxy 也不例外。此处，我用了自定义的 imgproxyError 结构体，它来告诉 HTTP handler 应该返回什么状态码，返回给上层调用者什么消息，在日志中应该打印什么信息。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> imgproxyError <span class="keyword">struct</span> &#123;</span><br><span class="line">  StatusCode    <span class="keyword">int</span></span><br><span class="line">  Message       <span class="keyword">string</span></span><br><span class="line">  PublicMessage <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *imgproxyError)</span> <span class="title">Error</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> e.Message</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来演示一下我是如何用这种方式的:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ierr, ok := err.(*imgproxyError); ok &#123;</span><br><span class="line">  respondWithError(ierr)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  msg := fmt.Sprintf(<span class="string">"Unexpected error: %s"</span>, err)</span><br><span class="line">  respondWithError(&amp;imgproxyError&#123;<span class="number">500</span>, msg, <span class="string">"Internal error"</span>&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而在之前，我所做的就是检查错误类型是否是我所定义的类型，不是我定义的类型说明不是预期的错误。那么就将它转化成 imgproxyError 实例，以此来告诉 HTTP handler 去响应500的状态码并让程序在日志中打印错误信息。</p>
<p>这里有必要说一个 Go 中类型转换的注意事项，毕竟它总是让新手困扰。你可以通过两种方式进行类型转换，不过建议最好还是用相对安全的方式：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Unsafe. If err is not *imgproxyError, Go will panic.</span></span><br><span class="line">ierr := err.(*imgproxyError)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Safe way. ok indicates if interface was typecast successfully or not.</span></span><br><span class="line"><span class="comment">// Go will not panic even if the interface represents the wrong type.</span></span><br><span class="line">ierr, ok := err.(*imgproxyError)</span><br></pre></td></tr></table></figure>
<p>现在，我们可以看到 Go 惯用的错误处理可以非常的灵活，接下来就可以进入下一个环节 - 讨价还价。</p>
<h3 id="讨价还价"><a href="#讨价还价" class="headerlink" title="讨价还价"></a>讨价还价</h3><p>“哪里出现错误，哪里处理错误”，这种错误处理的方式依旧对我来说很陌生，也许我能做些什么让它更像我喜欢的语言。</p>
<p>在代码每个可能出现的地方都进行错误处理是一件很麻烦的事情。很多时候，我们都想把错误提升到某些可以批量或集中处理的地方。这种方式，最显而易见的就是函数嵌套调用，在最上层处理掉来自底层的方法所产生的错误。</p>
<p>看一下这个公认的函数调用函数的例子，期望在最顶层处理掉所有的error：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"errors"</span></span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The principal function to be called where all errors will end up.</span></span><br><span class="line"><span class="comment">// Takes a numeric string, logs a square root of that number.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  f, err := StringToSqrt(str)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    HandleError(err) <span class="comment">// a function where he handle all errors</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, f)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Tries to parse a float64 out of a string and returns its square root.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  f, err := strconv.ParseFloat(str, <span class="number">64</span>)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, err</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  f, err = Sqrt(f)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, err</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> f, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Calculates a square root of the float.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calc sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这是 Go 语言惯用的方式，是的，看起来又臭又长。好在写 Go 语言的人好像也承认了这个问题。目前他们正在就 Go 2 的错误检查和处理问题发起讨论。官方错误处理草案引入了一个新的 construct <code>check ... handle</code>，关于它是如何工作的，草案是这么说的：</p>
<ul>
<li>check 语句适用于 error 类型的表达式或者函数返回以 error 类型值结尾的函数调用。如果 error 非 nil，check 语句将会返回闭包方法的结果，而这个闭包方法是通过 error 值调用处理程序链触发的。</li>
<li>handle 语句定义的代码块就是 handler，用来处理 check 语句检测到的 error。handler 中的 return 语句会导致闭包函数立刻返回给定的返回值。只有闭包函数没有结果或使用命名结果的时候， 才允许不带返回值。在后一种情况下，函数返回那些结果的当前值。</li>
</ul>
<p>依旧是 square 的例子，现在用另一种方式来进行错误处理。Go 2 已经发布，官方建议的写法如下：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"errors"</span></span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  handle err &#123; HandleError(err) &#125; <span class="comment">// where the magic happens</span></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, check StringToSqrt(str))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  handle err &#123; <span class="keyword">return</span> <span class="number">0</span>, err &#125; <span class="comment">// no need to explicitly if...else</span></span><br><span class="line">  <span class="keyword">return</span> check math.Sqrt(check strconv.ParseFloat(str, <span class="number">64</span>)), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calculate sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看上去好一些了，但是距离真正用 Go 2 做实际开发仍旧有一段距离。</p>
<p>与此同时，其实我们可以用另一种错误处理的方式，他可以显著减少 <code>if ... else</code> 语句，并且允许出现单点的 error。我叫这种方法为“Panic 驱动的错误处理”。</p>
<p>为了做到“Panic 驱动”，将依赖内置于 Go 语言的三个关键词：defer，panic，recover。这里稍微回顾一下他们分时是什么：</p>
<ul>
<li>defer 将函数 push 到本函数返回后执行的堆栈中，当你需要一些清理时候会派上用场。在我们的这个 case 里面，什么时候会用到 defer 呢？就是从 panic 中 recover 的时候，需要用到 defer。</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Foo</span><span class="params">()</span></span> &#123;</span><br><span class="line">f, _ := os.Open(<span class="string">"filename"</span>)</span><br><span class="line"><span class="comment">// defer ensures that f.Close() will be executed when Foo returns.</span></span><br><span class="line"><span class="keyword">defer</span> f.Close()</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>panic 会停止普通的程序流控制并开始 panicking。当函数开始 panic，程序的正常执行会被中止，程序开始调用堆栈执行所有的 defer 方法，同时在当前的 goroutine 的 root goroutine 程序开始崩溃。</li>
<li>recover 重新获取正在 panic 的 goroutine 的控制，并返回触发 panic 的 interface。recover 仅在 defer 中有效，在其他地方将返回 nil。</li>
</ul>
<p>BTW，纯粹的讲，下面的代码不代表最常见的 Go。灵感来自于 Gin 的源码（Gin 是当前比较流行的Go 领域的 web 框架）我自己并没有完全想的出它。 在 Gin 框架里面，如果一个 critical error 发生了，你可以在 handler 程序中调用 panic，然后 Gin 会 recover，打印错误日志并且返回500状态码。</p>
<p>由 Panic 驱动错误处理的想法很简单：只要嵌套调用返回 error 引发的 panic（译者注：checkErr 封装作为 reference，有多处地方调用），在 recover 的时候有单独的地方进行错误处理：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"errors"</span></span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// A simple helper to panic on errors.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErr</span><span class="params">(err error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="comment">// It is important to defer the anonymous function that wraps around error handling.</span></span><br><span class="line">  <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> r := <span class="built_in">recover</span>(); r != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> err, ok := r.(error); ok &#123;</span><br><span class="line">        <span class="comment">// Recover returned an error, handle it somehow.</span></span><br><span class="line">        HandleError(err)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Recover returned something that is not an error, so "re-panic".</span></span><br><span class="line">        <span class="built_in">panic</span>(r)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A call that starts a chain of events that might go wrong</span></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, StringToSqrt(str))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line">  f, err := strconv.ParseFloat(str, <span class="number">64</span>)</span><br><span class="line">  checkErr(err)</span><br><span class="line"></span><br><span class="line">  f, err = Sqrt(f)</span><br><span class="line">  checkErr(err)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> f</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calculate sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>的确， 这看起来不像其他语言上的 <code>try catch</code>，但是却让我们将错误处理这样的责任移动到对应的调用链上。</p>
<p>在 imgproxy 这个模块里面，我用这种方式实现当达到 timeout 就停止图片加载。回到之前说的，如果在每个方法中达到timeout就要进行 timeout error 的处理，这是很让人烦恼的，现在，我可以在任何地方用一行代码进行 timeout 的 check。</p>
<p>关于 error 的内容，我们也同样希望能添加更多的信息，但是golang的标准错误类型并没有提供堆栈跟踪信息。好在可以直接用 github.com/pkg/errors 来替换内置的 errors 包。你只需要用 <code>import “github.com/pkg/errors”</code> 替换 <code>import “errors”</code>，然后你的 errors 就可以包含堆栈跟踪信息了。注意现在起，你可不是在处理默认的 error 类型。下面就是标准类库的替代方案所建议的：</p>
<ul>
<li><code>func New(message string)</code> 是类似于内置 errors 包的同名函数。它实现并返回了包含堆栈信息的 error 类型</li>
<li><code>func WithMessage(err error,message string)</code> 将你的 error 封装到另一个类型里面， 并且这个类型包含了一些额外的信息。</li>
<li><code>fuc WithStack(err error) error</code> 封装了你的 error 到另一个类型， 这个类型包含了堆栈信息。当你用第三方包时，相当当前类型的 error 添加到第三方包的 error；或者想要添加堆栈信息到第三方包的 error。</li>
<li><code>func Wrap(err error,messag string) error</code> 是 WithStack+WitchMessage 的缩写。</li>
</ul>
<p>试着用刚才说的方法改进一下之前的代码：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">"log"</span></span><br><span class="line">  <span class="string">"math"</span></span><br><span class="line">  <span class="string">"strconv"</span></span><br><span class="line"></span><br><span class="line">  <span class="string">"github.com/pkg/errors"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErr</span><span class="params">(err error, msg <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(errors.WithMessage(err, msg))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErrWithStack</span><span class="params">(err error, msg <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(errors.Wrap(err, msg))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LogSqrt</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">  <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> r := <span class="built_in">recover</span>(); r != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> err, ok := r.(error); ok &#123;</span><br><span class="line">        <span class="comment">// Print the error to the log before handling.</span></span><br><span class="line">        <span class="comment">// %+v prints formatted error with additional messages and a stack trace:</span></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        <span class="comment">// Failed to sqrt: Can't calc sqrt of a negative number</span></span><br><span class="line">        <span class="comment">// main.main /app/main.go:14</span></span><br><span class="line">        <span class="comment">// runtime.main /goroot/libexec/src/runtime/proc.go:198</span></span><br><span class="line">        <span class="comment">// runtime.goexit /goroot/libexec/src/runtime/asm_amd64.s:2361</span></span><br><span class="line">        log.Printf(<span class="string">"%+v"</span>, err)</span><br><span class="line">        HandleError(err)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">panic</span>(r)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;()</span><br><span class="line"></span><br><span class="line">  log.Printf(<span class="string">"Sqrt of %s is %f"</span>, str, StringToSqrt(str))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StringToSqrt</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line">  f, err := strconv.ParseFloat(str, <span class="number">64</span>)</span><br><span class="line">  checkErrWithStack(err, <span class="string">"Failed to parse"</span>)</span><br><span class="line"></span><br><span class="line">  f, err = Sqrt(f)</span><br><span class="line">  checkErr(err, <span class="string">"Failed to sqrt"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> f</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Sqrt</span><span class="params">(f <span class="keyword">float64</span>)</span> <span class="params">(<span class="keyword">float64</span>, error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">    <span class="comment">// We use New from https://github.com/pkg/errors,</span></span><br><span class="line">    <span class="comment">// so our error will contain a stack trace.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, errors.New(<span class="string">"Can't calc sqrt of a negative number"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> math.Sqrt(f), <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>重要提示</strong>：也许你已经注意到了，errors.WithMessage 和 errors.WithStack 将 github.com/pkg/errors 封装进了定义类型里面。 这同时意味着你不能对自己的 error 实现直接的进行类型转化了。为了能将 github.com/pkg/errors 类型转化成你自己的 error 类型，首先需要用errors.Cause 对 github.com/pkg/errors 进行解包：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">err := PerformValidation()</span><br><span class="line"><span class="keyword">if</span> verr, ok := errors.Cause(err).(*ValidationErr); ok &#123;</span><br><span class="line">  <span class="comment">// Do something with the validation error</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在看似有强大的机制在一个地方集中处理相关的错误。但是别高兴的太早，Go 语言中最强大的就是 goroutine，goroutine 在并发的情况下，这种方法将会失败。</p>
<p>接下来我们就讲讲这种让人沮丧的时刻 - 失落。</p>
<h3 id="失落"><a href="#失落" class="headerlink" title="失落"></a>失落</h3><p>我努力的在我的代码中采用集中处理错误的方式，但是当我在使用 goroutines 的时候它却失效了。这种错误处理机制变得毫无意义。。。</p>
<p>不要 panic，将 panic 留给你的代码。在 goroutines 中的固定位置处理错误依然可行，此处我将使用不止一种方法（实际上是两种）。</p>
<h3 id="Channels-和-sync-WaitGroup"><a href="#Channels-和-sync-WaitGroup" class="headerlink" title="Channels 和 sync.WaitGroup"></a>Channels 和 sync.WaitGroup</h3><p>你可以将 Go 的 channel 和内置的 sync.Waitgroup 结合起来使用，这样就可以在特定的 channel 中处理相应的 errors，同时在异步进程处理完成后可以一个个的处理它们。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">errCh := <span class="built_in">make</span>(<span class="keyword">chan</span> error, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"><span class="comment">// We will launch two goroutines.</span></span><br><span class="line">wg.Add(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #1</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="comment">// We are done on return</span></span><br><span class="line">  <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If any error has occurred, put it into the channel.</span></span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    errCh &lt;- err</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #2</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">defer</span> wg.Done()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    errCh &lt;- err</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wait till all goroutines are done and close the channel.</span></span><br><span class="line">wg.Wait()</span><br><span class="line"><span class="built_in">close</span>(errCh)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Loop over the channel to collect all errors.</span></span><br><span class="line"><span class="keyword">for</span> err := <span class="keyword">range</span> errCh &#123;</span><br><span class="line">  HandleErr(err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当你需要在多个 goroutines 中收集所有错误时，这种方法将非常有用。</p>
<p>通常情况下，我们很少需要处理每个一个错误。多数情况下，要么全处理要么不处理：我们需要知道是否其中一些 goroutines 失败了。因此，我们准备使用 Golang 官方的子代码库中的 errgroup 包。下面代码展示了如何使用它：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> g errgroup.Group</span><br><span class="line"></span><br><span class="line"><span class="comment">// g.Go takes a function that returns error.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #1</span></span><br><span class="line">g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">  <span class="comment">// If any error has occurred, return it.</span></span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Goroutine #2</span></span><br><span class="line">g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err := dangerous.Action(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// g.Wait waits till all goroutines are done</span></span><br><span class="line"><span class="comment">// and returns only the first error.</span></span><br><span class="line"><span class="keyword">if</span> err := g.Wait(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">  HandleErr(err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只有 errgroup.Group 内部启动的 subroutines 中的第一个非零错误（如果有）才会被返回。而所有繁重的工作都是在幕后完成的。</p>
<h3 id="开始你自己的-PanicGroup"><a href="#开始你自己的-PanicGroup" class="headerlink" title="开始你自己的 PanicGroup"></a>开始你自己的 PanicGroup</h3><p>正如之前提到的，所有的 goroutines 在他们自己的范围里发生 panic。如果你想在 goroutines 中使用“panic 驱动错误处理”模式，你还需要做一点点其他的工作。糟糕的是 errgroup 不会有所帮助。然而，没有任何人阻止我们实现一遍我们自己的 PanicGroup！下面试一下完整的实现：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> PanicGroup <span class="keyword">struct</span> &#123;</span><br><span class="line">  wg      sync.WaitGroup</span><br><span class="line">  errOnce sync.Once</span><br><span class="line">  err     error</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *PanicGroup)</span> <span class="title">Wait</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">  g.wg.Wait()</span><br><span class="line">  <span class="keyword">return</span> g.err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *PanicGroup)</span> <span class="title">Go</span><span class="params">(f <span class="keyword">func</span>()</span>)</span> &#123;</span><br><span class="line">  g.wg.Add(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> g.wg.Done()</span><br><span class="line">    <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line">      <span class="keyword">if</span> r := <span class="built_in">recover</span>(); r != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> err, ok := r.(error); ok &#123;</span><br><span class="line">          <span class="comment">// 我们仅仅需要第一个错误, sync.Onece 在这里很有帮助.</span></span><br><span class="line">          g.errOnce.Do(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">            g.err = err</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="built_in">panic</span>(r)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    f()</span><br><span class="line">  &#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在，我们可以像下面这样，使用我们自己的 PanicGroup：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkErr</span><span class="params">(err error)</span></span> &#123;</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="built_in">panic</span>(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Foo</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">var</span> g PanicGroup</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Goroutine #1</span></span><br><span class="line">  g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 如果在这里发生了任何错误, panic.</span></span><br><span class="line">    checkErr(dangerous.Action())</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Goroutine #2</span></span><br><span class="line">  g.Go(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    checkErr(dangerous.Action())</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> err := g.Wait(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    HandleErr(err)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以， 当我们需要处理多个 goroutines，并且每个 goroutines 还需要抛出它自定义的 panic 时， 我们仍然可以通过上面的方式， 来保证代码清晰，简练。</p>
<h3 id="接受-并且完美"><a href="#接受-并且完美" class="headerlink" title="接受(并且完美)"></a>接受(并且完美)</h3><p>感谢您看完了我的文章。现在，我们就能了解到为什么 Go 语言里的错误处理是这个样子，什么才是大家最关心的问题，以及当 Go 2 仅仅出现一点点苗头的时候，我们怎么去克服这些困难。我们的”疗法”很完整。</p>
<p>当浏览完我所有的5个悲伤的阶段，我意识到，Go 里面的错误处理不应该被当成一种痛苦，反而相对于流程控制而言，是一种强大的，灵活的工具。</p>
<p>无论任何时候，在错误刚刚出现的后面，通过 <code>if err != nil</code> 来处理是一种完美的选择。如果你需要在一个地方集中处理所有的错误，将错误向上逐层返回到调用者。在这一点上，为错误添加上下文将是有益的，因此您不会忘记正在发生的事情并且可以正确处理每种错误。</p>
<p>如果您需要在发生错误后完全停止程序流程，请随便使用我所描述的“panic 驱动错误处理”，并且不要忘记通过 Twitter 与我分享您的经验。</p>
<p>最后一个要点，请记住，当事情真的发生了错误，保证总会有 <code>log.Fatal</code> 去记录一下。</p>
<p>来源：evilmartians.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一份快速实用的 tcpdump 命令参考手册]]></title>
      <url>http://team.jiunile.com/blog/2019/06/tcpdump.html</url>
      <content type="html"><![CDATA[<h2 id="tcpdump-简介"><a href="#tcpdump-简介" class="headerlink" title="tcpdump 简介"></a>tcpdump 简介</h2><p>对于 <code>tcpdump</code> 的使用，大部分管理员会分成两类。有一类管理员，他们熟知  <code>tcpdump</code> 和其中的所有标记；另一类管理员，他们仅了解基本的使用方法，剩下事情都要借助参考手册才能完成。出现这种情况的原因在于， <code>tcpdump</code> 是一个相当高级的命令，使用的时候需要对网络的工作机制有相当深入的了解。</p>
<p>在今天的文章中，我想提供一个快速但相当实用的 <code>tcpdump</code> 参考。我会谈到基本的和一些高级的使用方法。我敢肯定我会忽略一些相当酷的命令，欢迎你补充在评论部分。</p>
<p>在我们深入了解以前，最重要的是了解  <code>tcpdump</code> 是用来做什么的。 tcpdump 命令用来保存和记录网络流量。你可以用它来观察网络上发生了什么，并可用来解决各种各样的问题，包括和网络通信无关的问题。除了网络问题，我经常用 <code>tcpdump</code> 解决应用程序的问题。如果你发现两个应用程序之间无法很好工作，可以用  <code>tcpdump</code>  观察出了什么问题。 <code>tcpdump</code> 可以用来抓取和读取数据包，特别是当通信没有被加密的时候。</p>
<a id="more"></a>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p>了解 <code>tcpdump</code> ，首先要知道 <code>tcpdump</code> 中使用的标记（flag）。在这个章节中，我会涵盖到很多基本的标记，这些标记在很多场合下会被用到。</p>
<h3 id="不转换主机名、端口号等"><a href="#不转换主机名、端口号等" class="headerlink" title="不转换主机名、端口号等"></a>不转换主机名、端口号等</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -n</span></span><br></pre></td></tr></table></figure>
<p>通常情况下， tcpdump  会尝试查找和转换主机名和端口号。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p>你可以通过 <code>-n</code> 标记关闭这个功能。我个人总是使用这个标记，因为我喜欢使用 IP 地址而不是主机名，主机名和端口号的转换经常会带来困扰。但是，知道利用  <code>tcpdump</code>  转换或者不转换的功能还是相当有用的，特别是有些时候，知道源流量（source traffic）来自哪个服务器是相当重要的。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -n</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:23:47.934665 IP 10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], seq 2546457621:2546457817, ack 1824684201, win 355, options [nop,nop,TS val 621010158 ecr 621010055], length 196</span><br></pre></td></tr></table></figure></p>
<h3 id="增加详细信息"><a href="#增加详细信息" class="headerlink" title="增加详细信息"></a>增加详细信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -v</span></span><br></pre></td></tr></table></figure>
<p>增加一个简单 <code>-v</code> 标记，输出会包含更多信息，例如一个 IP 包的生存时间(ttl, time to live)、长度和其他的选项。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p><code>tcpdump</code>  的详细信息有三个等级，你可以通过在命令行增加 <code>v</code> 标记的个数来获取更多的信息。通常我在使用 <code>tcpmdump</code> 的时候，总是使用最高等级的详细信息，因为我希望看到所有信息，以免后面会用到。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -vvv -c 1</span></span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:36:13.873456 IP (tos 0x10, ttl 64, id 121, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    blog.ssh &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0x0dfd), seq 2546458841:2546458973, ack 1824684869, win 355, options [nop,nop,TS val 621196643 ecr 621196379], length 132</span><br></pre></td></tr></table></figure></p>
<h3 id="指定网络接口"><a href="#指定网络接口" class="headerlink" title="指定网络接口"></a>指定网络接口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -i eth0</span></span><br></pre></td></tr></table></figure>
<p>通常情况下，如果不指定网络接口， <code>tcpdump</code>  在运行时会选择编号最低的网络接口，一般情况下是 <code>eth0</code>，不过因系统不同可能会有所差异。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p>你可以用 <code>-i</code> 标记来指定网络接口。在大多数 Linux 系统上，<code>any</code> 这一特定的网络接口名用来让  <code>tcpdump</code>  监听所有的接口。我发现这在排查服务器（拥有多个网络接口）的问题特别有用，尤其是牵扯到路由的时候。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -i any</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">16:45:59.312046 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2547763641:2547763837, ack 1824693949, win 355, options [nop,nop,TS val 621343002 ecr 621342962], length 196</span><br></pre></td></tr></table></figure></p>
<h3 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -w /path/to/file</span></span><br></pre></td></tr></table></figure>
<p><code>tcpdump</code>  运行结果会输出在屏幕上。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump</span></span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">16:15:05.051896 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2546456553:2546456749, ack 1824683693, win 355, options [nop,nop,TS val 620879437 ecr 620879348], length 196</span><br></pre></td></tr></table></figure></p>
<p>但很多时候，你希望把  <code>tcpdump</code>  的输出结果保存在文件中，最简单的方法就是利用 <code>-w</code> 标记。如果你后续还会检查这些网络数据，这样做就特别有用。将这些数据存成一个文件的好处，就是你可以多次读取这个保存下来的文件，并且可以在这个网络流量的快照上使用其它标记或者过滤器（我们后面会讨论到）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -w /var/tmp/tcpdata.pcap</span></span><br><span class="line"> tcpdump : listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">1 packet captured</span><br><span class="line">2 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure></p>
<p>通常这些数据被缓存而不会被写入文件，直到你用 <code>CTRL+C</code> 结束 <code>tcpdump</code> 命令的时候。</p>
<h3 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -r /path/to/file</span></span><br></pre></td></tr></table></figure>
<p>一旦你将输出存成文件，就必然需要读取这个文件。要做到这点，你只需要在 <code>-r</code> 标记后指定这个文件的存放路径。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -r /var/tmp/tcpdata.pcap </span></span><br><span class="line">reading from file /var/tmp/tcpdata.pcap, link-type EN10MB (Ethernet)</span><br><span class="line">16:56:01.610473 IP blog.ssh &gt; 10.0.3.1.32855: Flags [P.], seq 2547766673:2547766805, ack 1824696181, win 355, options [nop,nop,TS val 621493577 ecr 621493478], length 132</span><br></pre></td></tr></table></figure></p>
<p>一个小提醒，如果你熟悉 <a href="https://www.wireshark.org/" target="_blank" rel="external">wireshark</a> 这类网络诊断工具，也可以利用它们来读取  <code>tcpdump</code>  保存的文件。</p>
<h3 id="指定抓包大小"><a href="#指定抓包大小" class="headerlink" title="指定抓包大小"></a>指定抓包大小</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -s 100</span></span><br></pre></td></tr></table></figure>
<p>较新版本的  <code>tcpdump</code>  通常可以截获 <strong>65535</strong> 字节，但某些情况下你不需要截获默认大小的数据包。运行  <code>tcpdump</code>  时，你可以通过 <code>-s</code> 标记来指定快照长度。</p>
<h3 id="指定抓包数量"><a href="#指定抓包数量" class="headerlink" title="指定抓包数量"></a>指定抓包数量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -c 10</span></span><br></pre></td></tr></table></figure>
<p><code>tcpdump</code>  会一直运行，直至你用 <code>CTRL+C</code> 让它退出。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  host google.com</span></span><br><span class="line"> tcpdump : verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes</span><br><span class="line">^C</span><br><span class="line">0 packets captured</span><br><span class="line">4 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure></p>
<p>你也可以通过 <code>-c</code> 标记后面加上抓包的数量，让  <code>tcpdump</code> 在抓到一定数量的数据包后停止操作。当你不希望看到  <code>tcpdump</code>  的输出大量出现在屏幕上，以至于你无法阅读的时候，就会希望使用这个标记。当然，通常更好的方法是借助过滤器来截获特定的流量。</p>
<h3 id="基础知识汇总"><a href="#基础知识汇总" class="headerlink" title="基础知识汇总"></a>基础知识汇总</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 100 -s 100</span></span><br></pre></td></tr></table></figure>
<p>你可以将以上这些基础的标记组合起来使用，来让  <code>tcpdump</code>  提供你所需要的信息。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -w /var/tmp/tcpdata.pcap -i any -c 10 -vvv</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">10 packets captured</span><br><span class="line">10 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br><span class="line"><span class="comment"># tcpdump -r /var/tmp/tcpdata.pcap -nvvv -c 5</span></span><br><span class="line">reading from file /var/tmp/tcpdata.pcap, link-type LINUX_SLL (Linux cooked)</span><br><span class="line">17:35:14.465902 IP (tos 0x10, ttl 64, id 5436, offset 0, flags [DF], proto TCP (6), length 104)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1b51 (incorrect -&gt; 0x72bc), seq 2547781277:2547781329, ack 1824703573, win 355, options [nop,nop,TS val 622081791 ecr 622081775], length 52</span><br><span class="line">17:35:14.466007 IP (tos 0x10, ttl 64, id 52193, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x4950), seq 1, ack 52, win 541, options [nop,nop,TS val 622081791 ecr 622081791], length 0</span><br><span class="line">17:35:14.470239 IP (tos 0x10, ttl 64, id 5437, offset 0, flags [DF], proto TCP (6), length 168)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1b91 (incorrect -&gt; 0x98c3), seq 52:168, ack 1, win 355, options [nop,nop,TS val 622081792 ecr 622081791], length 116</span><br><span class="line">17:35:14.470370 IP (tos 0x10, ttl 64, id 52194, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x48da), seq 1, ack 168, win 541, options [nop,nop,TS val 622081792 ecr 622081792], length 0</span><br><span class="line">17:35:15.464575 IP (tos 0x10, ttl 64, id 5438, offset 0, flags [DF], proto TCP (6), length 104)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1b51 (incorrect -&gt; 0xc3ba), seq 168:220, ack 1, win 355, options [nop,nop,TS val 622082040 ecr 622081792], length 52</span><br></pre></td></tr></table></figure></p>
<h2 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h2><p>介绍完基础的标记后，我们该介绍过滤器了。 <code>tcpdump</code>  可以通过各式各样的表达式，来过滤所截取或者输出的数据。我在这篇文章里会给出一些简单的例子，以便让你们了解语法规则。你们可以查询  <code>tcpdump</code>  帮助中的 <a href="http://www.tcpdump.org/manpages/pcap-filter.7.html" target="_blank" rel="external">pcap-filter</a> 章节，了解更为详细的信息。</p>
<h3 id="查找特定主机的流量"><a href="#查找特定主机的流量" class="headerlink" title="查找特定主机的流量"></a>查找特定主机的流量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 3 host 10.0.3.1</span></span><br></pre></td></tr></table></figure>
<p>运行上述命令， <code>tcpdump</code>  会像前面一样把结果输出到屏幕上，不过只会显示源 IP 或者目的 IP 地址是 <code>10.0.3.1</code> 的数据包。通过增加主机 <code>10.0.3.1</code> 参数，我们可以让  <code>tcpdump</code>  过滤掉源和目的地址不是 <code>10.0.3.1</code> 的数据包。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 host 10.0.3.1</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">17:54:15.067496 IP (tos 0x10, ttl 64, id 5502, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0x9f75), seq 2547785621:2547785753, ack 1824705637, win 355, options [nop,nop,TS val 622366941 ecr 622366923], length 132</span><br><span class="line">17:54:15.067613 IP (tos 0x10, ttl 64, id 52315, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x7c34), seq 1, ack 132, win 540, options [nop,nop,TS val 622366941 ecr 622366941], length 0</span><br><span class="line">17:54:15.075230 IP (tos 0x10, ttl 64, id 5503, offset 0, flags [DF], proto TCP (6), length 648)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1d71 (incorrect -&gt; 0x3443), seq 132:728, ack 1, win 355, options [nop,nop,TS val 622366943 ecr 622366941], length 596</span><br></pre></td></tr></table></figure></p>
<h3 id="只显示源地址为特定主机的流量"><a href="#只显示源地址为特定主机的流量" class="headerlink" title="只显示源地址为特定主机的流量"></a>只显示源地址为特定主机的流量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 3 src host 10.0.3.1</span></span><br></pre></td></tr></table></figure>
<p>前面的例子显示了源和目的地址是 <code>10.0.3.1</code> 的流量，而上面的命令只显示数据包源地址是 <code>10.0.3.1</code> 的流量。这是通过在 <code>host</code> 前面增加 <code>src</code> 参数来实现的。这个额外的过滤器告诉  <code>tcpdump</code>  查找特定的源地址。 反过来通过 <code>dst</code> 过滤器，可以指定目的地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 src host 10.0.3.1</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">17:57:12.194902 IP (tos 0x10, ttl 64, id 52357, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x1707), seq 1824706545, ack 2547787717, win 540, options [nop,nop,TS val 622411223 ecr 622411223], length 0</span><br><span class="line">17:57:12.196288 IP (tos 0x10, ttl 64, id 52358, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x15c5), seq 0, ack 325, win 538, options [nop,nop,TS val 622411223 ecr 622411223], length 0</span><br><span class="line">17:57:12.197677 IP (tos 0x10, ttl 64, id 52359, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.32855 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x1491), seq 0, ack 633, win 536, options [nop,nop,TS val 622411224 ecr 622411224], length 0</span><br><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 dst host 10.0.3.1</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">17:59:37.266838 IP (tos 0x10, ttl 64, id 5552, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0x586d), seq 2547789725:2547789857, ack 1824707577, win 355, options [nop,nop,TS val 622447491 ecr 622447471], length 132</span><br><span class="line">17:59:37.267850 IP (tos 0x10, ttl 64, id 5553, offset 0, flags [DF], proto TCP (6), length 392)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1c71 (incorrect -&gt; 0x462e), seq 132:472, ack 1, win 355, options [nop,nop,TS val 622447491 ecr 622447491], length 340</span><br><span class="line">17:59:37.268606 IP (tos 0x10, ttl 64, id 5554, offset 0, flags [DF], proto TCP (6), length 360)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.32855: Flags [P.], cksum 0x1c51 (incorrect -&gt; 0xf469), seq 472:780, ack 1, win 355, options [nop,nop,TS val 622447491 ecr 622447491], length 308</span><br></pre></td></tr></table></figure></p>
<h3 id="过滤源和目的端口"><a href="#过滤源和目的端口" class="headerlink" title="过滤源和目的端口"></a>过滤源和目的端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 3 port 22 and port 60738</span></span><br></pre></td></tr></table></figure>
<p>通过类似 <code>and</code> 操作符，你可以在  <code>tcpdump</code>  上使用更为复杂的过滤器描述。这个就类似 <code>if</code> 语句，你就这么想吧。这个例子中，我们使用 <code>and</code> 操作符告诉  <code>tcpdump</code>  只输出端口号是 <code>22</code> 和 <code>60738</code> 的数据包。这点在分析网络问题的时候很有用，因为可以通过这个方法来关注某一个特定会话（session）的数据包。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 port 22 and port 60738</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:05:54.069403 IP (tos 0x10, ttl 64, id 64401, offset 0, flags [DF], proto TCP (6), length 104)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [P.], cksum 0x1b51 (incorrect -&gt; 0x5b3c), seq 917414532:917414584, ack 1550997318, win 353, options [nop,nop,TS val 622541691 ecr 622538903], length 52</span><br><span class="line">18:05:54.072963 IP (tos 0x10, ttl 64, id 13601, offset 0, flags [DF], proto TCP (6), length 184)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.60738: Flags [P.], cksum 0x1ba1 (incorrect -&gt; 0xb0b1), seq 1:133, ack 52, win 355, options [nop,nop,TS val 622541692 ecr 622541691], length 132</span><br><span class="line">18:05:54.073080 IP (tos 0x10, ttl 64, id 64402, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x1e3b), seq 52, ack 133, win 353, options [nop,nop,TS val 622541692 ecr 622541692], length 0</span><br></pre></td></tr></table></figure></p>
<p>你可以用两种方式来表示 <code>and</code> 操作符，<code>and</code> 或者 <code>&amp;&amp;</code> 都可以。我个人倾向于两个都使用，特别要记住在使用 <code>&amp;&amp;</code> 的时候，要用单引号或者双引号包住表达式。在 BASH 中，你可以使用 <code>&amp;&amp;</code> 运行一个命令，该命令成功后再执行后面的命令。通常，最好将表达式用引号包起来，这样会避免不预期的结果，特别当过滤器中有一些特殊字符的时候。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 3 'port 22 &amp;&amp; port 60738'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:06:16.062818 IP (tos 0x10, ttl 64, id 64405, offset 0, flags [DF], proto TCP (6), length 88)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [P.], cksum 0x1b41 (incorrect -&gt; 0x776c), seq 917414636:917414672, ack 1550997518, win 353, options [nop,nop,TS val 622547190 ecr 622541776], length 36</span><br><span class="line">18:06:16.065567 IP (tos 0x10, ttl 64, id 13603, offset 0, flags [DF], proto TCP (6), length 120)</span><br><span class="line">    10.0.3.246.22 &gt; 10.0.3.1.60738: Flags [P.], cksum 0x1b61 (incorrect -&gt; 0xaf2d), seq 1:69, ack 36, win 355, options [nop,nop,TS val 622547191 ecr 622547190], length 68</span><br><span class="line">18:06:16.065696 IP (tos 0x10, ttl 64, id 64406, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.60738 &gt; 10.0.3.246.22: Flags [.], cksum 0x1b1d (incorrect -&gt; 0xf264), seq 36, ack 69, win 353, options [nop,nop,TS val 622547191 ecr 622547191], length 0</span><br></pre></td></tr></table></figure></p>
<h3 id="查找两个端口号的流量"><a href="#查找两个端口号的流量" class="headerlink" title="查找两个端口号的流量"></a>查找两个端口号的流量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 20 'port 80 or port 443'</span></span><br></pre></td></tr></table></figure>
<p>你可以用 <code>or</code> 或者 <code>||</code> 操作符来过滤结果。在这个例子中，我们使用 <code>or</code> 操作符去截获发送和接收端口为 <code>80</code> 或 <code>443</code> 的数据流。这在 Web 服务器上特别有用，因为服务器通常有两个开放的端口，端口号 <code>80</code> 表示 <code>http</code> 连接，<code>443</code> 表示 <code>https</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 20 'port 80 or port 443'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:24:28.817940 IP (tos 0x0, ttl 64, id 39930, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.1.50524 &gt; 10.0.3.246.443: Flags [S], cksum 0x1b25 (incorrect -&gt; 0x8611), seq 3836995553, win 29200, options [mss 1460,sackOK,TS val 622820379 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:24:28.818052 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 40)</span><br><span class="line">    10.0.3.246.443 &gt; 10.0.3.1.50524: Flags [R.], cksum 0x012c (correct), seq 0, ack 3836995554, win 0, length 0</span><br><span class="line">18:24:32.721330 IP (tos 0x0, ttl 64, id 48510, offset 0, flags [DF], proto TCP (6), length 475)</span><br><span class="line">    10.0.3.1.60374 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1cc4 (incorrect -&gt; 0x3a4e), seq 580573019:580573442, ack 1982754038, win 237, options [nop,nop,TS val 622821354 ecr 622815632], length 423</span><br><span class="line">18:24:32.721465 IP (tos 0x0, ttl 64, id 1266, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.246.80 &gt; 10.0.3.1.60374: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x45d7), seq 1, ack 423, win 243, options [nop,nop,TS val 622821355 ecr 622821354], length 0</span><br><span class="line">18:24:32.722098 IP (tos 0x0, ttl 64, id 1267, offset 0, flags [DF], proto TCP (6), length 241)</span><br><span class="line">    10.0.3.246.80 &gt; 10.0.3.1.60374: Flags [P.], cksum 0x1bda (incorrect -&gt; 0x855c), seq 1:190, ack 423, win 243, options [nop,nop,TS val 622821355 ecr 622821354], length 189</span><br><span class="line">18:24:32.722232 IP (tos 0x0, ttl 64, id 48511, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.60374 &gt; 10.0.3.246.80: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x4517), seq 423, ack 190, win 245, options [nop,nop,TS val 622821355 ecr 622821355], length 0</span><br></pre></td></tr></table></figure></p>
<h3 id="查找两个特定端口和来自特定主机的数据流"><a href="#查找两个特定端口和来自特定主机的数据流" class="headerlink" title="查找两个特定端口和来自特定主机的数据流"></a>查找两个特定端口和来自特定主机的数据流</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 20 '(port 80 or port 443) and host 10.0.3.169'</span></span><br></pre></td></tr></table></figure>
<p>前面的例子用来排查多端口的协议问题，是非常有效的。如果 Web 服务器的数据流量相当大， <code>tcpdump</code>  的输出可能有点混乱。我们可以通过增加 <code>host</code> 参数进一步限定输出。在这种情况下，我们通过把 <code>or</code> 表达式放在括号中来保持 <code>or</code> 描述。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 20 '(port 80 or port 443) and host 10.0.3.169'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:38:05.551194 IP (tos 0x0, ttl 64, id 63169, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.169.33786 &gt; 10.0.3.246.443: Flags [S], cksum 0x1bcd (incorrect -&gt; 0x0d96), seq 4173164403, win 29200, options [mss 1460,sackOK,TS val 623024562 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:38:05.551310 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 40)</span><br><span class="line">    10.0.3.246.443 &gt; 10.0.3.169.33786: Flags [R.], cksum 0xa64a (correct), seq 0, ack 4173164404, win 0, length 0</span><br><span class="line">18:38:05.717130 IP (tos 0x0, ttl 64, id 51574, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.169.35629 &gt; 10.0.3.246.80: Flags [S], cksum 0x1bcd (incorrect -&gt; 0xdf7c), seq 1068257453, win 29200, options [mss 1460,sackOK,TS val 623024603 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:38:05.717255 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.80 &gt; 10.0.3.169.35629: Flags [S.], cksum 0x1bcd (incorrect -&gt; 0xed80), seq 2992472447, ack 1068257454, win 28960, options [mss 1460,sackOK,TS val 623024603 ecr 623024603,nop,wscale 7], length 0</span><br><span class="line">18:38:05.717474 IP (tos 0x0, ttl 64, id 51575, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.169.35629 &gt; 10.0.3.246.80: Flags [.], cksum 0x1bc5 (incorrect -&gt; 0x8c87), seq 1, ack 1, win 229, options [nop,nop,TS val 623024604 ecr 623024603], length 0</span><br></pre></td></tr></table></figure></p>
<p>在一个过滤器中，你可以多次使用括号。在下面的例子中，下面命令可以限定截获满足如下条件的数据包：发送或接收端口号为 <code>80</code> 或 <code>443</code>，主机来源于 <code>10.0.3.169</code> 或者 <code>10.0.3.1</code>，且目的地址是 <code>10.0.3.246</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 20 '((port 80 or port 443) and (host 10.0.3.169 or host 10.0.3.1)) and dst host 10.0.3.246'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">18:53:30.349306 IP (tos 0x0, ttl 64, id 52641, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [S], cksum 0x1b25 (incorrect -&gt; 0x4890), seq 3026316656, win 29200, options [mss 1460,sackOK,TS val 623255761 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:53:30.349558 IP (tos 0x0, ttl 64, id 52642, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x3454), seq 3026316657, ack 3657995297, win 229, options [nop,nop,TS val 623255762 ecr 623255762], length 0</span><br><span class="line">18:53:30.354056 IP (tos 0x0, ttl 64, id 52643, offset 0, flags [DF], proto TCP (6), length 475)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1cc4 (incorrect -&gt; 0x10c2), seq 0:423, ack 1, win 229, options [nop,nop,TS val 623255763 ecr 623255762], length 423</span><br><span class="line">18:53:30.354682 IP (tos 0x0, ttl 64, id 52644, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.1.35407 &gt; 10.0.3.246.80: Flags [.], cksum 0x1b1d (incorrect -&gt; 0x31e6), seq 423, ack 190, win 237, options [nop,nop,TS val 623255763 ecr 623255763], length 0</span><br></pre></td></tr></table></figure></p>
<h2 id="理解输出结果"><a href="#理解输出结果" class="headerlink" title="理解输出结果"></a>理解输出结果</h2><p>打开  <code>tcpdump</code>  的所有选项去截获网络流量是相当困难的，但一旦你拿到这些数据你就要对它进行解读。在这个章节，我们将涉及如何判断源/目的 IP 地址，源/目的端口号，以及 <code>TCP</code> 协议类型的数据包。当然这些是相当基础的，你从  <code>tcpdump</code>  里面获取的信息也远不止这些。不过这篇文章主要是粗略的介绍，我们会关注在这些基础知识上。我建议你们可以通过<a href="http://www.tcpdump.org/manpages/" target="_blank" rel="external">帮助页</a>获取更为详细的信息。</p>
<h3 id="判断源和目的地址"><a href="#判断源和目的地址" class="headerlink" title="判断源和目的地址"></a>判断源和目的地址</h3><p>判断源和目的地址和端口号相当简单。</p>
<p>从上面的输出，我们可以看到源 IP 地址是 <code>10.0.3.246</code>，源端口号是 <code>56894</code>， 目的 IP 地址是 <code>192.168.0.92</code>，端口号是 <code>22</code>。一旦你理解  <code>tcpdump</code>  格式后，这些信息很容易判断。如果你还没有猜到格式，你可以按照 <code>src-ip.src-port &gt; dest-ip.dest-port: Flags[S]</code> 格式来分析。源地址位于 <code>&gt;</code> 前面，后面则是目的地址。你可以把 <code>&gt;</code> 想象成一个指向目的地址的箭头符号。</p>
<h3 id="判断数据包类型"><a href="#判断数据包类型" class="headerlink" title="判断数据包类型"></a>判断数据包类型</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x0388), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989005 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure>
<p>从上面的例子，我们可以判断这个数据包是一个 <code>SYN</code> 数据包。我们是通过  <code>tcpdump</code>  输出中的 <code>[S]</code> 标记字段得出这个结论，不同类型的数据包有不同类型的标记。不需要深入了解 <code>TCP</code> 协议中的数据包类型，你就可以通过下面的速查表来加以判断。</p>
<ul>
<li>[S] – SYN (开始连接)</li>
<li>[.] – 没有标记</li>
<li>[P] – PSH (数据推送)</li>
<li>[F] – FIN (结束连接)</li>
<li>[R] – RST (重启连接)</li>
</ul>
<p>在这个版本的  <code>tcpdump</code>  输出中，<code>[S.]</code> 标记代表这个数据包是 <code>SYN-ACK</code> 数据包。</p>
<h3 id="不好的例子"><a href="#不好的例子" class="headerlink" title="不好的例子"></a>不好的例子</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">15:15:43.323412 IP (tos 0x0, ttl 64, id 51051, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x0388), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989005 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:15:44.321444 IP (tos 0x0, ttl 64, id 51052, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x028e), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989255 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:15:46.321610 IP (tos 0x0, ttl 64, id 51053, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.56894 &gt; 192.168.0.92.22: Flags [S], cksum 0xcf28 (incorrect -&gt; 0x009a), seq 682725222, win 29200, options [mss 1460,sackOK,TS val 619989755 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure>
<p>上面显示了一个不好的通信例子，在这个例子中“不好”，代表通信没有建立起来。我们可以看到 <code>10.0.3.246</code> 发出一个 <code>SYN</code> 数据包给 主机 <code>192.168.0.92</code>，但是主机并没有应答。</p>
<h3 id="好的例子"><a href="#好的例子" class="headerlink" title="好的例子"></a>好的例子</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">15:18:25.716453 IP (tos 0x10, ttl 64, id 53344, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.0.3.246.34908 &gt; 192.168.0.110.22: Flags [S], cksum 0xcf3a (incorrect -&gt; 0xc838), seq 1943877315, win 29200, options [mss 1460,sackOK,TS val 620029603 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:18:25.716777 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    192.168.0.110.22 &gt; 10.0.3.246.34908: Flags [S.], cksum 0x594a (correct), seq 4001145915, ack 1943877316, win 5792, options [mss 1460,sackOK,TS val 18495104 ecr 620029603,nop,wscale 2], length 0</span><br><span class="line">15:18:25.716899 IP (tos 0x10, ttl 64, id 53345, offset 0, flags [DF], proto TCP (6), length 52)</span><br><span class="line">    10.0.3.246.34908 &gt; 192.168.0.110.22: Flags [.], cksum 0xcf32 (incorrect -&gt; 0x9dcc), ack 1, win 229, options [nop,nop,TS val 620029603 ecr 18495104], length 0</span><br></pre></td></tr></table></figure>
<p>好的例子应该向上面这样，我们看到典型的 TCP 3次握手。第一数据包是 <code>SYN</code> 包，从主机 <code>10.0.3.246</code> 发送给 主机<code>192.168.0.110</code>，第二个包是 <code>SYN-ACK</code> 包，主机<code>192.168.0.110</code> 回应 <code>SYN</code> 包。最后一个包是一个 <code>ACK</code> 或者 <code>SYN – ACK – ACK</code> 包，是主机 <code>10.0.3.246</code> 回应收到了 <code>SYN – ACK</code> 包。从上面看到一个 TCP/IP 连接成功建立。</p>
<h2 id="数据包检查"><a href="#数据包检查" class="headerlink" title="数据包检查"></a>数据包检查</h2><h3 id="用十六进制和-ASCII-码打印数据包"><a href="#用十六进制和-ASCII-码打印数据包" class="headerlink" title="用十六进制和 ASCII 码打印数据包"></a>用十六进制和 ASCII 码打印数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 1 -XX 'port 80 and host 10.0.3.1'</span></span><br></pre></td></tr></table></figure>
<p>排查应用程序网络问题的通常做法，就是用  <code>tcpdump</code>  的 <code>-XX</code> 标记打印出 16 进制和 ASCII 码格式的数据包。这是一个相当有用的命令，它可以让你看到源地址，目的地址，数据包类型以及数据包本身。但我不是这个命令输出的粉丝，我认为它太难读了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 1 -XX 'port 80 and host 10.0.3.1'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">19:51:15.697640 IP (tos 0x0, ttl 64, id 54313, offset 0, flags [DF], proto TCP (6), length 483)</span><br><span class="line">    10.0.3.1.45732 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1ccc (incorrect -&gt; 0x2ce8), seq 3920159713:3920160144, ack 969855140, win 245, options [nop,nop,TS val 624122099 ecr 624117334], length 431</span><br><span class="line">        0x0000:  0000 0001 0006 fe0a e2d1 8785 0000 0800  ................</span><br><span class="line">        0x0010:  4500 01e3 d429 4000 4006 49f5 0a00 0301  E....)@.@.I.....</span><br><span class="line">        0x0020:  0a00 03f6 b2a4 0050 e9a8 e3e1 39ce d0a4  .......P....9...</span><br><span class="line">        0x0030:  8018 00f5 1ccc 0000 0101 080a 2533 58f3  ............%3X.</span><br><span class="line">        0x0040:  2533 4656 4745 5420 2f73 6f6d 6570 6167  %3FVGET./somepag</span><br><span class="line">        0x0050:  6520 4854 5450 2f31 2e31 0d0a 486f 7374  e.HTTP/1.1..Host</span><br><span class="line">        0x0060:  3a20 3130 2e30 2e33 2e32 3436 0d0a 436f  :.10.0.3.246..Co</span><br><span class="line">        0x0070:  6e6e 6563 7469 6f6e 3a20 6b65 6570 2d61  nnection:.keep<span class="_">-a</span></span><br><span class="line">        0x0080:  6c69 7665 0d0a 4361 6368 652d 436f 6e74  live..Cache-Cont</span><br><span class="line">        0x0090:  726f 6c3a 206d 6178 2d61 6765 3d30 0d0a  rol:.max-age=0..</span><br><span class="line">        0x00a0:  4163 6365 7074 3a20 7465 7874 2f68 746d  Accept:.text/htm</span><br><span class="line">        0x00b0:  6c2c 6170 706c 6963 6174 696f 6e2f 7868  l,application/xh</span><br><span class="line">        0x00c0:  746d 6c2b 786d 6c2c 6170 706c 6963 6174  tml+xml,applicat</span><br><span class="line">        0x00d0:  696f 6e2f 786d 6c3b 713d 302e 392c 696d  ion/xml;q=0.9,im</span><br><span class="line">        0x00e0:  6167 652f 7765 6270 2c2a 2f2a 3b71 3d30  age/webp,*/*;q=0</span><br><span class="line">        0x00f0:  2e38 0d0a 5573 6572 2d41 6765 6e74 3a20  .8..User-Agent:.</span><br><span class="line">        0x0100:  4d6f 7a69 6c6c 612f 352e 3020 284d 6163  Mozilla/5.0.(Mac</span><br><span class="line">        0x0110:  696e 746f 7368 3b20 496e 7465 6c20 4d61  intosh;.Intel.Ma</span><br><span class="line">        0x0120:  6320 4f53 2058 2031 305f 395f 3529 2041  c.OS.X.10_9_5).A</span><br><span class="line">        0x0130:  7070 6c65 5765 624b 6974 2f35 3337 2e33  ppleWebKit/537.3</span><br><span class="line">        0x0140:  3620 284b 4854 4d4c 2c20 6c69 6b65 2047  6.(KHTML,.like.G</span><br><span class="line">        0x0150:  6563 6b6f 2920 4368 726f 6d65 2f33 382e  ecko).Chrome/38.</span><br><span class="line">        0x0160:  302e 3231 3235 2e31 3031 2053 6166 6172  0.2125.101.Safar</span><br><span class="line">        0x0170:  692f 3533 372e 3336 0d0a 4163 6365 7074  i/537.36..Accept</span><br><span class="line">        0x0180:  2d45 6e63 6f64 696e 673a 2067 7a69 702c  -Encoding:.gzip,</span><br><span class="line">        0x0190:  6465 666c 6174 652c 7364 6368 0d0a 4163  deflate,sdch..Ac</span><br><span class="line">        0x01a0:  6365 7074 2d4c 616e 6775 6167 653a 2065  cept-Language:.e</span><br><span class="line">        0x01b0:  6e2d 5553 2c65 6e3b 713d 302e 380d 0a49  n-US,en;q=0.8..I</span><br><span class="line">        0x01c0:  662d 4d6f 6469 6669 6564 2d53 696e 6365  f-Modified-Since</span><br><span class="line">        0x01d0:  3a20 5375 6e2c 2031 3220 4f63 7420 3230  :.Sun,.12.Oct.20</span><br><span class="line">        0x01e0:  3134 2031 393a 3430 3a32 3020 474d 540d  14.19:40:20.GMT.</span><br><span class="line">        0x01f0:  0a0d 0a                                  ...</span><br></pre></td></tr></table></figure></p>
<h3 id="只打印-ASCII-码格式的数据包"><a href="#只打印-ASCII-码格式的数据包" class="headerlink" title="只打印 ASCII 码格式的数据包"></a>只打印 ASCII 码格式的数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  tcpdump  -nvvv -i any -c 1 -A 'port 80 and host 10.0.3.1'</span></span><br></pre></td></tr></table></figure>
<p>我倾向于只打印 ASCII 格式数据，这可以帮助我快速定位数据包中发送了什么，哪些是正确的，哪些是错误的。你可以通过 <code>-A</code> 标记来实现这一点。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 1 -A 'port 80 and host 10.0.3.1'</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">19:59:52.011337 IP (tos 0x0, ttl 64, id 53757, offset 0, flags [DF], proto TCP (6), length 406)</span><br><span class="line">    10.0.3.1.46172 &gt; 10.0.3.246.80: Flags [P.], cksum 0x1c7f (incorrect -&gt; 0xead1), seq 1552520173:1552520527, ack 428165415, win 237, options [nop,nop,TS val 624251177 ecr 624247749], length 354</span><br><span class="line">E.....@.@.Ln</span><br><span class="line">...</span><br><span class="line">....\.P\.....I<span class="string">'...........</span><br><span class="line">%5Q)%5C.GET /newpage HTTP/1.1</span><br><span class="line"> </span><br><span class="line">Host: 10.0.3.246</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8</span><br><span class="line">User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36</span><br><span class="line">Accept-Encoding: gzip,deflate,sdch</span><br><span class="line">Accept-Language: en-US,en;q=0.8</span></span><br></pre></td></tr></table></figure></p>
<p>从上面的输出，你可以看到我们成功获取了一个 http 的 <code>GET</code> 请求包。如果网络通信没有被加密，用人类可阅读的格式打出包中数据，对于解决应用程序的问题是很有帮助。如果你排查一个网络通信被加密的问题，打印包中数据就不是很有用。不过如果你有证书的话，你还是可以使用 <code>ssldump</code> 或者 <code>wireshark</code>。</p>
<h2 id="非-TCP-数据流"><a href="#非-TCP-数据流" class="headerlink" title="非 TCP 数据流"></a>非 TCP 数据流</h2><p>虽然这篇文章主要采用 TCP 传输来讲解  <code>tcpdump</code> ，但是  <code>tcpdump</code>  绝对不是只能抓 TCP 数据包。它还可以用来获取其他类型的数据包，例如 ICMP、 UDP 和 ARP 包。下面是一些简单的例子，说明  <code>tcpdump</code>  可以截获非 TCP 数据包。</p>
<h3 id="ICMP-数据包"><a href="#ICMP-数据包" class="headerlink" title="ICMP 数据包"></a>ICMP 数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 2 icmp</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">20:11:24.627824 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.0.3.169 &gt; 10.0.3.246: ICMP <span class="built_in">echo</span> request, id 15683, seq 1, length 64</span><br><span class="line">20:11:24.627926 IP (tos 0x0, ttl 64, id 31312, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.0.3.246 &gt; 10.0.3.169: ICMP <span class="built_in">echo</span> reply, id 15683, seq 1, length 64</span><br></pre></td></tr></table></figure>
<h3 id="UDP-数据包"><a href="#UDP-数据包" class="headerlink" title="UDP 数据包"></a>UDP 数据包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tcpdump -nvvv -i any -c 2 udp</span></span><br><span class="line">tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">20:12:41.726355 IP (tos 0xc0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 76)</span><br><span class="line">    10.0.3.246.123 &gt; 198.55.111.50.123: [bad udp cksum 0x43a9 -&gt; 0x7043!] NTPv4, length 48</span><br><span class="line">        Client, Leap indicator: clock unsynchronized (192), Stratum 2 (secondary reference), poll 6 (64s), precision -22</span><br><span class="line">        Root Delay: 0.085678, Root dispersion: 57.141830, Reference-ID: 199.102.46.75</span><br><span class="line">          Reference Timestamp:  3622133515.811991035 (2014/10/12 20:11:55)</span><br><span class="line">          Originator Timestamp: 3622133553.828614115 (2014/10/12 20:12:33)</span><br><span class="line">          Receive Timestamp:    3622133496.748308420 (2014/10/12 20:11:36)</span><br><span class="line">          Transmit Timestamp:   3622133561.726278364 (2014/10/12 20:12:41)</span><br><span class="line">            Originator - Receive Timestamp:  -57.080305658</span><br><span class="line">            Originator - Transmit Timestamp: +7.897664248</span><br><span class="line">20:12:41.748948 IP (tos 0x0, ttl 54, id 9285, offset 0, flags [none], proto UDP (17), length 76)</span><br><span class="line">    198.55.111.50.123 &gt; 10.0.3.246.123: [udp sum ok] NTPv4, length 48</span><br><span class="line">        Server, Leap indicator:  (0), Stratum 3 (secondary reference), poll 6 (64s), precision -20</span><br><span class="line">        Root Delay: 0.054077, Root dispersion: 0.058944, Reference-ID: 216.229.0.50</span><br><span class="line">          Reference Timestamp:  3622132887.136984840 (2014/10/12 20:01:27)</span><br><span class="line">          Originator Timestamp: 3622133561.726278364 (2014/10/12 20:12:41)</span><br><span class="line">          Receive Timestamp:    3622133618.830113530 (2014/10/12 20:13:38)</span><br><span class="line">          Transmit Timestamp:   3622133618.830129086 (2014/10/12 20:13:38)</span><br><span class="line">            Originator - Receive Timestamp:  +57.103835195</span><br><span class="line">            Originator - Transmit Timestamp: +57.103850722</span><br></pre></td></tr></table></figure>
<p>如果你觉得有好例子进一步说明  <code>tcpdump</code>  命令，请在评论中补充。</p>
<p>来源：bencane.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Calico 问题排障]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-calico-troubleshooting.html</url>
      <content type="html"><![CDATA[<h2 id="故障一"><a href="#故障一" class="headerlink" title="故障一"></a>故障一</h2><h3 id="问题表现"><a href="#问题表现" class="headerlink" title="问题表现"></a>问题表现</h3><p>集群中有台node服务器因为资源达到上限出现假死现状，重启后发现calico node 无法启动成功，提示如下信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:Readiness probe failed: caliconode is not ready: BIRD is not ready: BGP not established with 172.18.0.1</span><br></pre></td></tr></table></figure></p>
<p>使用<code>calicoctl node status</code> 命令查看node状态信息提示如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+--------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |              INFO              |</span><br><span class="line">+--------------+-------------------+-------+----------+--------------------------------+</span><br><span class="line">| 172.18.0.1   | node-to-node mesh | start | 01:56:41 | Connect Socket: Network        |</span><br><span class="line">|              |                   |       |          | unreachable                    |</span><br><span class="line">+--------------+-------------------+-------+----------+--------------------------------+</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h3><p>在BGP网络中出现了一个未知的IP地址172.18.0.1，我们集群中的机器都是10开头的网络地址，所以登录有问题的机器上查看对应的网络信息 <code>ip a</code>，结果如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">14: br-3a10a9384428: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:3a:f3:45:18 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-3a10a9384428</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:3aff:fef3:4518/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>后来咨询了相关同事，结果是同事在node机器上用docker启动了<a href="https://github.com/google/cadvisor" target="_blank" rel="external"><code>advisor</code></a> 这个容器监控程序，所以会产生一块虚拟网卡出来。</p>
<h3 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h3><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>将<code>advisor</code> 改成使用二进制文件启动，不使用容器启动，则不会产生虚拟网络设备</p>
<h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p>调整calicao 网络插件的网卡发现机制，修改<code>IP_AUTODETECTION_METHOD</code>对应的value值。官方提供的yaml文件中，ip识别策略（IPDETECTMETHOD）没有配置，即默认为first-found，这会导致一个网络异常的ip作为nodeIP被注册，从而影响<code>node-to-node mesh</code>。我们可以修改成<code>can-reach</code>或者<code>interface</code>的策略，尝试连接某一个Ready的node的IP，以此选择出正确的IP。</p>
<ul>
<li><p><strong>can-reach</strong> 使用您的本地路由来确定将使用哪个IP地址到达提供的目标。可以使用IP地址和域名。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using IP addresses</span></span><br><span class="line">IP_AUTODETECTION_METHOD=can-reach=8.8.8.8</span><br><span class="line">IP6_AUTODETECTION_METHOD=can-reach=2001:4860:4860::8888</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using domain names</span></span><br><span class="line">IP_AUTODETECTION_METHOD=can-reach=www.google.com</span><br><span class="line">IP6_AUTODETECTION_METHOD=can-reach=www.google.com</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>interface</strong> 使用提供的接口正则表达式（golang语法）枚举匹配的接口并返回第一个匹配接口上的第一个IP地址。列出接口和IP地址的顺序取决于系统。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Valid IP address on interface eth0, eth1, eth2 etc.</span></span><br><span class="line">IP_AUTODETECTION_METHOD=interface=eth.*</span><br><span class="line">IP6_AUTODETECTION_METHOD=interface=eth.*</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="故障二"><a href="#故障二" class="headerlink" title="故障二"></a>故障二</h2><h3 id="问题表现-1"><a href="#问题表现-1" class="headerlink" title="问题表现"></a>问题表现</h3><p>将某台新机器（aws美东区域）加到容器集群之后，发现该节点没法加入到容器集群里面。</p>
<p>该节点的calico-node启动后不久反复的crash重启。crash前的log如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl logs -f calico-node-wm6bb  -n kube-system -c calico-node</span></span><br><span class="line">Skipping datastore connection <span class="built_in">test</span></span><br><span class="line">Using autodetected IPv4 address 10.12.13.12/24 on matching interface eth0</span><br></pre></td></tr></table></figure></p>
<p>恰巧新加坡和美东区域各有一台闲置机器，尝试将这两台机器加入calico网络。发现新加坡机器成功加入到calico网络，而美东机器加入calico失败，且失败表现相同。</p>
<p>这究竟是什么鬼原因？</p>
<h3 id="问题排查-1"><a href="#问题排查-1" class="headerlink" title="问题排查"></a>问题排查</h3><h4 id="排除ETCD连接问题"><a href="#排除ETCD连接问题" class="headerlink" title="排除ETCD连接问题"></a>排除ETCD连接问题</h4><p>calico-node启动阶段会访问etcd获取集群网络配置，所以首先怀疑会不会是节点连接etcd失败。</p>
<p>在问题节点上，尝试访问etcd：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  curl --cacert /etc/cni/net.d/calico-tls/etcd-ca --cert /etc/cni/net.d/calico-tls/etcd-cert --key /etc/cni/net.d/calico-tls/etcd-key https://[ETCD服务IP]:2379/health</span></span><br><span class="line">&#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>发现可以连上etcd。那么可以排除etcd连接问题。</p>
<h4 id="排除AWS路由表限制"><a href="#排除AWS路由表限制" class="headerlink" title="排除AWS路由表限制"></a>排除AWS路由表限制</h4><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-route-tables" target="_blank" rel="external">AWS EC2路由表有50条的数量限制</a> ，这有可能会限制集群的节点数上限。<a href="https://docs.projectcalico.org/v3.2/reference/public-cloud/aws" target="_blank" rel="external">但根据calico官方文档，calico应该不受aws 50节点的限制</a> :<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No 50 Node Limit: Calico allows you to surpass the 50 node <span class="built_in">limit</span>, <span class="built_in">which</span> exists as a consequence of the AWS 50 route <span class="built_in">limit</span> when using the VPC routing table.</span><br></pre></td></tr></table></figure></p>
<p>而且，我们的集群节点数已经超出50（目前节点数53）。</p>
<p>至此，排除AWS路由表原因。</p>
<h4 id="排除aws-security-groups影响"><a href="#排除aws-security-groups影响" class="headerlink" title="排除aws security groups影响"></a>排除aws security groups影响</h4><p>仔细看了<a href="https://docs.projectcalico.org/v2.6/reference/public-cloud/aws" target="_blank" rel="external">calico的aws部署说明，其中提到需要配置aws安全组，允许BGP和IPIP通信</a>，会不会因为公司aws美东区域安全组配置的原因？</p>
<p>查看美东那两台问题机器的security-groups是 DY-Default-10.12，而其他机器的 security-groups是DY-Default-10.12。很失望。可以排除aws security group的影响。</p>
<h4 id="最终原因"><a href="#最终原因" class="headerlink" title="最终原因"></a>最终原因</h4><p>至此，问题原因仍毫无头绪。只好找来<a href="https://github.com/projectcalico/node/blob/4db4e815e47885db77957e113a18269fa1ce0ffd/pkg/startup/startup.go#L233" target="_blank" rel="external">calico node的启动代码</a>来看看。</p>
<p>期间发现，calico node启动时是依据<code>CALICO_STARTUP_LOGLEVEL</code>环境变量来设置log级别。考虑到出问题的calico node输出的log实在是太少，修改calico node的daemonset配置，将log等级设置成最低的DEBUG级别：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - env:</span></span><br><span class="line"><span class="attr">    - name:</span> CALICO_STARTUP_LOGLEVEL</span><br><span class="line"><span class="attr">      value:</span> DEBUG</span><br></pre></td></tr></table></figure></p>
<p>重启问题节点的calico-node，果然输出了更多的log：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl logs -f calico-node-jz7cz -n kube-system -c calico-node </span></span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=info msg=<span class="string">"Early log level set to debug"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=info msg=<span class="string">"NODENAME environment not specified - check HOSTNAME"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=info msg=<span class="string">"Loading config from environment"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=debug msg=<span class="string">"Using datastore type 'etcdv2'"</span> </span><br><span class="line">Skipping datastore connection <span class="built_in">test</span></span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=debug msg=<span class="string">"Validate name: AMZ-IAD12-OpsResPool-13-33"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:16Z"</span> level=debug msg=<span class="string">"Get Key: /calico/v1/host/AMZ-IAD12-OpsResPool-13-33/metadata"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=info msg=<span class="string">"Building new node resource"</span> Name=AMZ-IAD12-OpsResPool-13-33 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=info msg=<span class="string">"Initialise BGP data"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Querying interface addresses"</span> Interface=eth0 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Found valid IP address and network"</span> CIDR=10.12.13.33/24 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Check interface"</span> Name=eth0 </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Check address"</span> CIDR=10.12.13.33/24 </span><br><span class="line">Using autodetected IPv4 address 10.12.13.33/24 on matching interface eth0</span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=info msg=<span class="string">"Node IPv4 changed, will check for conflicts"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Listing all host metadatas"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Parse host directories."</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Get Node key from /calico/v1/host/AMZ-IAD12-Coupon-35-221/metadata"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Get Node key from /calico/v1/host/AMZ-IAD12-Coupon-35-222/metadata"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:17Z"</span> level=debug msg=<span class="string">"Get Node key from /calico/v1/host/AMZ-IAD12-OpsResPool-13-31/metadata"</span> </span><br><span class="line"></span><br><span class="line">.... 省略类似<span class="built_in">log</span> ....</span><br><span class="line"></span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/ip_addr_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/network_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/ip_addr_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/network_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:26Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-OR39-CTR-135-60/as_num"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:27Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:47:27Z"</span> level=debug msg=<span class="string">"Get Key: /calico/v1/host/AMZ-OR39-CTR-135-60/orchestrator_refs"</span> </span><br><span class="line"></span><br><span class="line">.... 省略类似<span class="built_in">log</span> ....</span><br><span class="line"></span><br><span class="line">time=<span class="string">"2018-10-31T07:48:12Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/ip_addr_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/network_v4"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/ip_addr_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/network_v6"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:13Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-62/as_num"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:14Z"</span> level=debug msg=<span class="string">"Key not found error"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:14Z"</span> level=debug msg=<span class="string">"Get Key: /calico/v1/host/AMZ-SIN8-OpsResPool-33-62/orchestrator_refs"</span> </span><br><span class="line">time=<span class="string">"2018-10-31T07:48:14Z"</span> level=debug msg=<span class="string">"Get Key: /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-63/ip_addr_v4"</span></span><br></pre></td></tr></table></figure></p>
<p>输出完上面的log后，calico-node就被重启了，是什么原因呢？</p>
<p>根据calico node的代码，<a href="https://github.com/projectcalico/node/blob/4db4e815e47885db77957e113a18269fa1ce0ffd/pkg/startup/startup.go#L1003" target="_blank" rel="external">如果calico node启动失败，那退出前会先打一行log：Terminating</a>，但在上面的log中并未发现有Terminating。</p>
<p>从log看出，calico-node终止前是在查询etcd中的节点数据。于是又试了试在问题节点上查询etcd：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># etcdctl  --ca-file=/var/lib/kubernetes/ca.pem --cert-file=/var/lib/kubernetes/kubernetes.pem --key-file=/var/lib/kubernetes/kubernetes-key.pem --endpoints=https://[etcd服务ip]:2379 get /calico/bgp/v1/host/AMZ-SIN8-OpsResPool-33-63/ip_addr_v4</span></span><br><span class="line">10.8.33.63</span><br></pre></td></tr></table></figure></p>
<p>看结果很正常，绝不会因此而出错。</p>
<p>纠结之际，突然想到：会不会calico-node启动超时了？</p>
<h3 id="问题处理-1"><a href="#问题处理-1" class="headerlink" title="问题处理"></a>问题处理</h3><p>马上看了下calico-node daemonset的livenessProbe：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line"><span class="attr">  failureThreshold:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  httpGet:</span></span><br><span class="line"><span class="attr">    path:</span> /liveness</span><br><span class="line"><span class="attr">    port:</span> <span class="number">9099</span></span><br><span class="line"><span class="attr">    scheme:</span> HTTP</span><br><span class="line"><span class="attr">  initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  timeoutSeconds:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>尝试着把initialDelaySeconds加到60秒，failureThreshold加到10。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line"><span class="attr">  failureThreshold:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  httpGet:</span></span><br><span class="line"><span class="attr">    path:</span> /liveness</span><br><span class="line"><span class="attr">    port:</span> <span class="number">9099</span></span><br><span class="line"><span class="attr">    scheme:</span> HTTP</span><br><span class="line"><span class="attr">  initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">  periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  timeoutSeconds:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>改完重启了问题节点的calico-node。等待了数分钟，发现居然启动成功了。</p>
<p>至此，问题原因就明确了：<strong><code>由于集群节点越来越多，calico-node启动所需时间也随着变长了，超出了liveness probe的重启时间限制，从而被k8s干掉重启</code></strong>。</p>
<p>那么为什么新加坡区域的节点启动成功，而美东的节点启动失败？</p>
<p>原因应该是，我们的master与etcd都在新加坡区域，因此新加坡节点从etcd获取数据较快，calico-node启动速度也就更快，可以在限定的时间内启动完毕。而美东节点访问新加坡etcd的延迟较长，因此美东calico-node启动更慢。</p>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[谈谈kubernetes Runtime]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-k8s-runtime.html</url>
      <content type="html"><![CDATA[<h2 id="白话kubernetes-Runtime"><a href="#白话kubernetes-Runtime" class="headerlink" title="白话kubernetes Runtime"></a>白话kubernetes Runtime</h2><blockquote>
<p>回想最开始接触 k8s 的时候, 经常搞不懂 CRI 和 OCI 的联系和区别, 也不知道为啥要垫那么多的 “shim”(尤其是 containerd-shim 和 dockershim 这两个完全没啥关联的东西还恰好都叫 shim). 所以嘛, 这篇就写一写 k8s 的 runtime 部分, 争取一篇文章把下面这张 Landscape 里的核心项目给白话明白</p>
</blockquote>
<p><img src="/images/k8s/bf52b77fly1g0wkhtwdlij217c0si44d.jpg" alt="landscape"></p>
<h2 id="典型的-Runtime-架构"><a href="#典型的-Runtime-架构" class="headerlink" title="典型的 Runtime 架构"></a>典型的 Runtime 架构</h2><p>我们从最常见的 runtime 方案 Docker 说起, 现在 Kubelet 和 Docker 的集成还是挺啰嗦的:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws7jziucj21hy0dmtb0.jpg" alt="典型的runtime架构"><br><a id="more"></a><br>当 Kubelet 想要创建一个容器时, 有这么几步:</p>
<ol>
<li>Kubelet 通过 <strong>CRI 接口</strong>(gRPC) 调用 dockershim, 请求创建一个容器. <strong>CRI</strong> 即容器运行时接口(Container Runtime Interface), 这一步中, Kubelet 可以视作一个简单的 CRI Client, 而 dockershim 就是接收请求的 Server. 目前 dockershim 的代码其实是内嵌在 Kubelet 中的, 所以接收调用的凑巧就是 Kubelet 进程;</li>
<li>dockershim 收到请求后, 转化成 Docker Daemon 能听懂的请求, 发到 Docker Daemon 上请求创建一个容器;</li>
<li>Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程: containerd 中了, 因此 Docker Daemon 仍然不能帮我们创建容器, 而是要请求 containerd 创建一个容器;</li>
<li>containerd 收到请求后, 并不会自己直接去操作容器, 而是创建一个叫做 containerd-shim 的进程, 让 containerd-shim 去操作容器. 这是因为容器进程需要一个父进程来做诸如收集状态, 维持 stdin 以及 fd 打开等工作. 而假如这个父进程就是 containerd, 那每次 containerd 挂掉或升级, 整个宿主机上所有的容器都得退出了. 而引入了 containerd-shim 就规避了这个问题(containerd 和 shim 并不需要是父子进程关系, 当 containerd 退出或重启时, shim 会 re-parent 到 systemd 这样的 1 号进程上);</li>
<li>我们知道创建容器需要做一些设置 namespaces 和 cgroups, 挂载 root filesystem 等等操作, 而这些事该怎么做已经有了公开的规范了, 那就是 <a href="https://github.com/opencontainers/runtime-spec" target="_blank" rel="external">OCI(Open Container Initiative, 开放容器标准)</a>. 它的一个参考实现叫做 <a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runc</a>. 于是, containerd-shim 在这一步需要调用 <code>runc</code> 这个命令行工具, 来启动容器;</li>
<li><code>runc</code> 启动完容器后本身会直接退出, containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程;</li>
</ol>
<p>这个过程乍一看像是在搞我们: Docker Daemon 和 dockershim 看上去就是两个不干活躺在中间划水的啊, Kubelet 为啥不直接调用 containerd 呢?</p>
<p>当然是可以的, 不过咱们先不提那个, 先看看为什么现在的架构如此繁冗.</p>
<h2 id="小插曲-容器历史小叙-不负责任版"><a href="#小插曲-容器历史小叙-不负责任版" class="headerlink" title="小插曲: 容器历史小叙(不负责任版)"></a>小插曲: 容器历史小叙(不负责任版)</h2><p>其实 k8s 最开始的 Runtime 架构远没这么复杂: kubelet 想要创建容器直接跟 Docker Daemon 说一声就行, 而那时也不存在 containerd, Docker Daemon 自己调一下 <code>libcontainer</code> 这个库把容器跑起来, 整个过程就搞完了.</p>
<p>而熟悉容器和容器编排历史的读者老爷应该知道, 这之后就是容器圈的一系列政治斗争, 先是大佬们认为运行时标准不能被 Docker 一家公司控制, 于是就撺掇着搞了开放容器标准 OCI. Docker 则把 <code>libcontainer</code> 封装了一下, 变成 runC 捐献出来作为 OCI 的参考实现.</p>
<p>再接下来就是 <a href="https://github.com/rkt/rkt" target="_blank" rel="external">rkt</a> 想从 docker 那边分一杯羹, 希望 k8s 原生支持 rkt 作为运行时, 而且 PR 还真的合进去了. 维护过这块业务同时接两个需求方的读者老爷应该都知道类似的事情有多坑, k8s 中负责维护 kubelet 的小组 sig-node 也是被狠狠坑了一把.</p>
<p>大家一看这么搞可不行, 今天能有 rkt, 明天就能有更多幺蛾子出来, 这么搞下去我们小组也不用干活了, 整天搞兼容性的 bug 就够呛. 于是乎, k8s 1.5 推出了 CRI 机制, 即容器运行时接口(Container Runtime Interface), k8s 告诉大家, 你们想做 Runtime 可以啊, 我们也资瓷欢迎, 实现这个接口就成, 成功反客为主.</p>
<p>不过 CRI 本身只是 k8s 推的一个标准, 当时的 k8s 尚未达到如今这般武林盟主的地位, 容器运行时当然不能说我跟 k8s 绑死了只提供 CRI 接口, 于是就有了 shim(垫片) 这个说法, 一个 shim 的职责就是作为 Adapter 将各种容器运行时本身的接口适配到 k8s 的 CRI 接口上.</p>
<p>接下来就是 Docker 要搞 Swarm 进军 PaaS 市场, 于是做了个架构切分, 把容器操作都移动到一个单独的 Daemon 进程 containerd 中去, 让 Docker Daemon 专门负责上层的封装编排. 可惜 Swarm 在 k8s 面前实在是不够打, 惨败之后 Docker 公司就把 <a href="https://github.com/containerd/containerd" target="_blank" rel="external">containerd 项目</a>捐给 CNCF 缩回去安心搞 Docker 企业版了.</p>
<p>最后就是我们在上一张图里看到的这一坨东西了, 尽管现在已经有 CRI-O, containerd-plugin 这样更精简轻量的 Runtime 架构, dockershim 这一套作为经受了最多生产环境考验的方案, 迄今为止仍是 k8s 默认的 runtime 实现.</p>
<p>了解这些具体的架构有时能在 debug 时候帮我们一些忙, 但更重要的是它们能作为一个例子, 帮助我们更好地理解整个 k8s runtime 背后的设计逻辑, 我们这就言归正传.</p>
<h2 id="OCI-CRI-与被滥用的名词-“Runtime”"><a href="#OCI-CRI-与被滥用的名词-“Runtime”" class="headerlink" title="OCI, CRI 与被滥用的名词 “Runtime”"></a>OCI, CRI 与被滥用的名词 “Runtime”</h2><p>OCI, 也就是前文提到的”开放容器标准”其实就是一坨文档, 其中主要规定了两点:</p>
<ol>
<li>容器镜像要长啥样, 即 <a href="https://github.com/opencontainers/image-spec" target="_blank" rel="external">ImageSpec</a>. 里面的大致规定就是你这个东西需要是一个压缩了的文件夹, 文件夹里以 xxx 结构放 xxx 文件;</li>
<li>容器要需要能接收哪些指令, 这些指令的行为是什么, 即 <a href="https://github.com/opencontainers/runtime-spec" target="_blank" rel="external">RuntimeSpec</a>. 这里面的大致内容就是”容器”要能够执行 “create”, “start”, “stop”, “delete” 这些命令, 并且行为要规范.</li>
</ol>
<p><a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runC</a> 为啥叫参考实现呢, 就是它能按照标准将符合标准的容器镜像运行起来(当然, 这里为了易读性略去了很多细节, 要了解详情建议点前文的链接读文档)</p>
<p>标准的好处就是方便搞创新, 反正只要我符合标准, 生态圈里的其它工具都能和我一起愉快地工作(…当然 OCI 这个标准本身制订得不怎么样, 真正工程上还是要做一些 adapter 的), 那我的镜像就可以用任意的工具去构建, 我的”容器”就不一定非要用 namespace 和 cgroups 来做隔离. 这就让各种虚拟化容器可以更好地参与到游戏当中, 我们暂且不表.</p>
<p>而 CRI 更简单, 单纯是一组 gRPC 接口, 扫一眼 <a href="https://github.com/kubernetes/kubernetes/blob/8327e433590f9e867b1e31a4dc32316685695729/pkg/kubelet/apis/cri/services.go" target="_blank" rel="external">kubelet/apis/cri/services.go</a> 就能归纳出几套核心接口:</p>
<ul>
<li>一套针对容器操作的接口, 包括创建,启停容器等等;</li>
<li>一套针对镜像操作的接口, 包括拉取镜像删除镜像等;</li>
<li>还有一套针对 PodSandbox (容器沙箱环境) 的操作接口, 我们之后再说;</li>
</ul>
<p>现在我们可以找到很多符合 OCI 标准或兼容了 CRI 接口的项目, 而这些项目就大体构成了整个 Kuberentes 的 Runtime 生态:</p>
<ul>
<li>OCI Compatible: <a href="https://github.com/opencontainers/runc" target="_blank" rel="external">runC</a>, <a href="https://github.com/kata-containers/kata-containers" target="_blank" rel="external">Kata</a>(以及它的前身 <a href="https://github.com/hyperhq/runv" target="_blank" rel="external">runV</a> 和 <a href="https://github.com/clearcontainers/runtime" target="_blank" rel="external">Clear Containers</a>), <a href="https://github.com/google/gvisor" target="_blank" rel="external">gVisor</a>. 其它比较偏门的还有 Rust 写的 <a href="https://github.com/oracle/railcar" target="_blank" rel="external">railcar</a></li>
<li>CRI Compatible: Docker(借助 dockershim), <a href="https://github.com/containerd/containerd" target="_blank" rel="external">containerd</a>(借助 CRI-containerd), <a href="https://github.com/kubernetes-sigs/cri-o" target="_blank" rel="external">CRI-O</a>, <a href="https://github.com/kubernetes/frakti" target="_blank" rel="external">frakti</a>, etc.</li>
</ul>
<p>最开始 k8s 的时候我经常弄不清 OCI 和 CRI 的区别与联系, 其中一大原因就是社区里糟糕的命名: 这上面的项目统统可以称为容器运行时(Container Runtime), 彼此之间区分的办法就是给”容器运行时”这个词加上各种定语和从句来进行修饰. Dave Cheney 有条推说:</p>
<blockquote>
<p>Good naming is like a good joke. If you have to explain it, it’s not funny.</p>
</blockquote>
<p>显然 Container Runtime 在这里就不是一个好名字了, 我们接下来换成一个在这篇文章的语境中更准确的说法: <strong>cri-runtime</strong> 和 <strong>oci-runtime</strong>. 通过这个粗略的分类, 我们其实可以总结出整个 runtime 架构万变不离其宗的三层抽象:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Orchestration API -&gt; Container API -&gt; Kernel API</span><br></pre></td></tr></table></figure></p>
<p>这其中 k8s 已经是 Orchestration API 的事实标准, 而在 k8s 中, Container API 的接口标准就是 CRI, 由 cri-runtime 实现, Kernel API 的规范是 OCI, 由 oci-runtime 实现.</p>
<p>根据这个思路, 我们就很容易理解下面这两种东西:</p>
<ul>
<li>各种更为精简的 cri-runtime</li>
<li>各种”强隔离”容器方案</li>
</ul>
<h2 id="containerd-和-CRI-O"><a href="#containerd-和-CRI-O" class="headerlink" title="containerd 和 CRI-O"></a>containerd 和 CRI-O</h2><p>我们在第一节就看到现在的 runtime 实在是有点复杂了, 而复杂是万恶之源, 于是就有了直接拿 containerd 做 oci-runtime 的方案. 当然, 除了 k8s 之外, containerd 还要接诸如 Swarm 等调度系统, 因此它不会去直接实现 CRI, 这个适配工作当然就要交给一个 shim 了.</p>
<p>containerd 1.0 中, 对 CRI 的适配通过一个单独的进程 <code>CRI-containerd</code> 来完成:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws7vtgq2j21de0cmgmo.jpg" alt="containerd 1.0"></p>
<p>containerd 1.1 中做的又更漂亮一点, 砍掉了 CRI-containerd 这个进程, 直接把适配逻辑作为插件放进了 containerd 主进程中:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws87espcj21ao0d00tz.jpg" alt="containerd 1.1"></p>
<p>但在 containerd 做这些事情之情, 社区就已经有了一个更为专注的 cri-runtime: <a href="https://github.com/kubernetes-sigs/cri-o" target="_blank" rel="external">CRI-O</a>, 它非常纯粹, 就是兼容 CRI 和 OCI, 做一个 k8s 专用的运行时:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws8dqqsoj21am0d0q4q.jpg" alt="CRI-O"></p>
<p>其中 <code>conmon</code> 就对应 containerd-shim, 大体意图是一样的.</p>
<p>CRI-O 和 (直接调用)containerd 的方案比起默认的 dockershim 确实简洁很多, 但没啥生产环境的验证案例, 我所知道的仅仅是 containerd 在 GKE 上是 beta 状态. 因此假如你对 docker 没有特殊的政治恨意, 大可不必把 dockershim 这套换掉.</p>
<h2 id="强隔离容器-Kata-gVisor-firecracker"><a href="#强隔离容器-Kata-gVisor-firecracker" class="headerlink" title="强隔离容器: Kata, gVisor, firecracker"></a>强隔离容器: Kata, gVisor, firecracker</h2><p>一直以来 k8s 都有一个被诟病的点: 难以实现真正的多租户.</p>
<p>为什么这么说呢, 我们先考虑一下什么样是理想的多租户状态:</p>
<blockquote>
<p>理想来说, 平台的各个租户(tenant)之间应该无法感受到彼此的存在, 表现得就像每个租户独占这整个平台一样. 具体来说, 我不能看到其它租户的资源, 我的资源跑满了不能影响其它租户的资源使用, 我也无法从网络或内核上攻击其它租户.</p>
</blockquote>
<p>k8s 当然做不到, 其中最大的两个原因是:</p>
<ul>
<li>kube-apiserver 是整个集群中的单例, 并且没有多租户概念</li>
<li>默认的 oci-runtime 是 runC, 而 runC 启动的容器是共享内核的</li>
</ul>
<p>对于第二个问题, 一个典型的解决方案就是提供一个新的 OCI 实现, 用 VM 来跑容器, 实现内核上的硬隔离. <a href="https://github.com/hyperhq/runv" target="_blank" rel="external">runV</a> 和 <a href="https://github.com/clearcontainers/runtime" target="_blank" rel="external">Clear Containers</a> 都是这个思路. 因为这两个项目做得事情是很类似, 后来就合并成了一个项目 <a href="https://github.com/kata-containers/kata-containers" target="_blank" rel="external">Kata Container</a>. Kata 的一张图很好地解释了基于虚拟机的容器与基于 namespaces 和 cgroups 的容器间的区别:</p>
<p><img src="/images/k8s/bf52b77fly1g0wnfdxo9fj20hs0anmyb.jpg" alt="Kata Container"></p>
<blockquote>
<p>当然, 没有系统是完全安全的, 假如 hypervisor 存在漏洞, 那么用户仍有可能攻破隔离. 但所有的事情都要对比而言, 在共享内核的情况下, 暴露的攻击面是非常大的, 做安全隔离的难度就像在美利坚和墨西哥之间修 The Great Wall, 而当内核隔离之后, 只要守住 hypervisor 这道关子就后顾无虞了</p>
</blockquote>
<p>嗯, 一个 VM 里跑一个容器, 听上去隔离性很不错, 但不是说虚拟机又笨重又不好管理才切换到容器的吗, 怎么又要走回去了?</p>
<p>Kata 告诉你, 虚拟机没那么邪恶, 只是以前没玩好:</p>
<ul>
<li><strong>不好管理</strong>是因为没有遵循”不可变基础设施”, 大家都去虚拟机上这摸摸那碰碰, 这台装 Java 8 那台装 Java 6, Admin 是要 angry 的. Kata 则支持 OCI 镜像, 完全可以用上 Dockerfile + 镜像, 让不好管理成为了过去时;</li>
<li><strong>笨重</strong>是因为之前要虚拟化整个系统, 现在我们只着眼于虚拟化应用, 那就可以裁剪掉很多功能, 把 VM 做得很轻量, 因此即便用虚拟机来做容器, Kata 还是可以将容器启动时间压缩得非常短, 启动后在内存上和IO 上的 overhead 也尽可能去优化;</li>
</ul>
<p>不过话说回来, k8s 上的调度单位是 Pod, 是容器组啊, Kata 这样一个虚拟机里一个容器, 同一个 Pod 间的容器还怎么做 namespace 的共享?</p>
<p>这就要说回我们前面讲到的 CRI 中针对 PodSandbox (容器沙箱环境) 的操作接口了. 第一节中, 我们刻意简化了场景, 只考虑创建一个<strong>容器</strong>, 而没有讨论创建一个<strong>Pod</strong>. 大家都知道, 真正启动 Pod 里定义的容器之前, kubelet 会先启动一个 infra 容器, 并执行 /pause 让 infra 容器的主进程永远挂起. 这个容器存在的目的就是维持住整个 pod 的各种 namespace, 真正的业务容器只要加入 infra 容器的 network 等 namespace 就能实现对应 namespace 的共享. 而 infra 容器创造的这个共享环境则被抽象为 <strong>PodSandbox</strong>. 每次 kubelet 在创建 Pod 时, 就会先调用 CRI 的 <code>RunPodSandbox</code> 接口启动一个沙箱环境, 再调用 <code>CreateContainer</code> 在沙箱中创建容器.</p>
<p>这里就已经说出答案了, 对于 <strong>Kata Container</strong> 而言, 只要在 <code>RunPodSandbox</code> 调用中创建一个 VM, 之后再往 VM 中添加容器就可以了. 最后运行 Pod 的样子就是这样的:</p>
<p><img src="/images/k8s/bf52b77fly1g0ws90i7ttj21mq0d440r.jpg" alt="Kata Container"></p>
<p>说完了 Kata, 其实 gVisor 和 firecracker 都不言自明了, 大体上都是类似的, 只是:</p>
<ul>
<li><a href="https://github.com/google/gvisor" target="_blank" rel="external">gVisor</a> 并不会去创建一个完整的 VM, 而是实现了一个叫 “Sentry” 的用户态进程来处理容器的 syscall, 而拦截 syscall 并重定向到 Sentry 的过程则由 KVM 或 ptrace 实现.</li>
<li><a href="https://github.com/firecracker-microvm/firecracker" target="_blank" rel="external">firecracker</a> 称自己为 microVM, 即轻量级虚拟机, 它本身还是基于 KVM 的, 不过 KVM 通常使用 QEMU 来虚拟化除CPU和内存外的资源, 比如IO设备,网络设备. firecracker 则使用 rust 实现了最精简的设备虚拟化, 为的就是压榨虚拟化的开销, 越轻量越好.</li>
</ul>
<h2 id="安全容器与-Serverless"><a href="#安全容器与-Serverless" class="headerlink" title="安全容器与 Serverless"></a>安全容器与 Serverless</h2><p>你可能觉得安全容器对自己而言没什么用: 大不了我给每个产品线都部署 k8s, 机器池也都隔离掉, 从基础设施的层面就隔离掉嘛.</p>
<p>这么做当然可以, 但同时也要知道, 这种做法最终其实是以 IaaS 的方式在卖资源, 是做不了真正的 PaaS 乃至 Serverless 的.</p>
<p>Serverless 要做到所有的用户容器或函数按需使用计算资源, 那必须满足两点:</p>
<ul>
<li><strong>多租户强隔离</strong>: 用户的容器或函数都是按需启动按秒计费, 我们可不能给每个用户预先分配一坨隔离的资源,因此我们要保证整个 Platform 是多租户强隔离的;</li>
<li><strong>极度轻量</strong>: Serverless 的第一个特点是运行时沙箱会更频繁地创建和销毁, 第二个特点是切分的粒度会非常非常细, 细中细就是 FaaS, 一个函数就要一个沙箱. 因此就要求两点: <pre><code>1. 沙箱启动删除必须飞快; 
2. 沙箱占用的资源越少越好. 
</code></pre>这两点在 long-running, 粒度不大的容器运行环境下可能不明显, 但在 Serverless 环境下就会急剧被放大. 这时候去做MicroVM 的 ROI 就比以前要高很多. 想想, 用传统的 KVM 去跑 FaaS, 那还不得亏到姥姥家了?</li>
</ul>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>这次的内容是越写越多, 感觉怎么都写不完的样子, rkt, lxd 其实都还没涉及, 这里就提供下类比, 大家可以自行做拓展阅读: rkt 跟 docker 一样是一个容器引擎, 特点是无 daemon, 目前项目基本不活跃了; lxc 是 docker 最早使用的容器工具集, 位置可以类比 runc, 提供跟 kernel 打交道的库&amp;命令行工具, lxd 则是基于 lxc 的一个容器引擎, 只不过大多数容器引擎的目标是容器化应用, lxd 的目标则是容器化操作系统.</p>
<p>来源：aleiwu.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[如何在线关闭一个tcp socket连接]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-online-close-tcp-socket.html</url>
      <content type="html"><![CDATA[<p>你可能会说，简单，<code>netstat -antp</code>找到连接，<code>kill</code>掉这个进程就行了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -antp|grep 6789</span></span><br><span class="line">tcp        0      0 1.1.1.1:59950      1.1.1.2:6789        ESTABLISHED 45059/ceph-fuse</span><br><span class="line"><span class="comment"># kill 45059</span></span><br></pre></td></tr></table></figure>
<p>连接确实关掉了，进程也跟着一起杀死了。达不到“在线”的要求。</p>
<p>有没有办法不杀死进程，但还是可以关闭socket连接呢？</p>
<p>我们知道，在编码的时候，要关闭一个socket，只要调用 close 函数就可以了，但是进程在运行着呢，怎么让它调用 close 呢？<br><a id="more"></a></p>
<p>在<a href="https://superuser.com/" target="_blank" rel="external">superuser</a>上看到一个很棒的方法，原理就是 <code>gdb attach</code> 到进程上下文，然后 <code>call close($fd)</code>。</p>
<p>1、 使用 <code>netstat</code> 找到进程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -antp|grep 6789</span></span><br><span class="line">tcp        0      0 1.1.1.1:59950      1.1.1.2:6789        ESTABLISHED 45059/ceph-fuse</span><br></pre></td></tr></table></figure></p>
<p>如上，进程pid为45059。</p>
<p>2、 使用 lsof 找到进程45059打开的所有文件描述符，并找到对应的socket连接<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lsof -np 45059</span><br><span class="line">COMMAND     PID USER   FD   TYPE             DEVICE SIZE/OFF       NODE NAME</span><br><span class="line">ceph-fuse 45059 root  rtd    DIR                8,2     4096          2 /</span><br><span class="line">ceph-fuse 45059 root  txt    REG                8,2  6694144    1455967 /usr/bin/ceph-fuse</span><br><span class="line">ceph-fuse 45059 root  mem    REG                8,2   510416    2102312 /usr/lib64/libfreeblpriv3.so</span><br><span class="line">...</span><br><span class="line">ceph-fuse 45059 root   12u  IPv4         1377072656      0t0        TCP 1.1.1.1:59950-&gt;1.1.1.2:smc-https (ESTABLISHED)</span><br></pre></td></tr></table></figure></p>
<p>其中 <code>12u</code> 就是上面对应socket连接的文件描述符。</p>
<p>3、 gdb 连接到进程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gdb -p 45059</span><br></pre></td></tr></table></figure></p>
<p>4、 关闭socket连接<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(gdb) call close(12u)</span><br></pre></td></tr></table></figure></p>
<p>socket连接就可以关闭了，但是进程 45059 还是好好着的。</p>
<p>你可能会问，什么时候会用到这个特性呢？场景还是比较多的，比如你想测试下应用是否会自动重连mysql，通过这个办法就可以比较方便的测试了。</p>
<p>Ref:</p>
<ul>
<li><a href="https://superuser.com/questions/127863/manually-closing-a-port-from-commandline" target="_blank" rel="external">Manually closing a port from commandline</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kuberenetes 升级后新加入节点报错]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-kubeadm-up-node-join-failed.html</url>
      <content type="html"><![CDATA[<h2 id="问题简述"><a href="#问题简述" class="headerlink" title="问题简述"></a>问题简述</h2><blockquote>
<p>原本用<code>kubeadm</code>安装的的kubernetes 1.11.x集群升级到1.12.x 后（使用<code>kubeadm upgrade</code>升级）发现无法将新的node加入到集群中，会出现以下报错信息</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[preflight] running pre-flight checks</span><br><span class="line">    [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_sh ip_vs ip_vs_rr ip_vs_wrr] or no <span class="built_in">builtin</span> kernel ipvs support: map[ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125; ip_vs:&#123;&#125;]</span><br><span class="line">you can solve this problem with following methods:</span><br><span class="line"> 1. Run <span class="string">'modprobe -- '</span> to load missing kernel modules;</span><br><span class="line">2. Provide the missing <span class="built_in">builtin</span> kernel ipvs support</span><br><span class="line"></span><br><span class="line">[discovery] Trying to connect to API Server <span class="string">"172.19.170.254:6443"</span></span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from <span class="string">"https://172.19.170.254:6443"</span></span><br><span class="line">[discovery] Requesting info from <span class="string">"https://172.19.170.254:6443"</span> again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server <span class="string">"172.19.170.254:6443"</span></span><br><span class="line">[discovery] Successfully established connection with API Server <span class="string">"172.19.170.254:6443"</span></span><br><span class="line">[kubelet] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.12"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">configmaps <span class="string">"kubelet-config-1.12"</span> is forbidden: User <span class="string">"system:bootstrap:y1zgt7"</span> cannot get configmaps <span class="keyword">in</span> the namespace <span class="string">"kube-system"</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="修复方法"><a href="#修复方法" class="headerlink" title="修复方法"></a>修复方法</h2><blockquote>
<p>在master节点上执行步骤1-4，在从节点（将要加入集群的机器）上执行步骤5</p>
</blockquote>
<h3 id="步骤一"><a href="#步骤一" class="headerlink" title="步骤一"></a>步骤一</h3><p><strong>从现有的”ConfigMap kubelet-config-1.11” 创建一个新的ConfigMap “kubelet-config-1.12”</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get cm --all-namespaces</span><br><span class="line">$ kubectl -n kube-system get cm kubelet-config-1.11 -o yaml --export &gt; kubelet-config-1.12-cm.yaml</span><br><span class="line">$ vim kubelet-config-1.12-cm.yaml       <span class="comment">#modify at the bottom:</span></span><br><span class="line">                                        <span class="comment">#name: kubelet-config-1.12</span></span><br><span class="line">                                        <span class="comment">#delete selfLink</span></span><br><span class="line">$ kubectl -n kube-system create <span class="_">-f</span> kubelet-config-1.12-cm.yaml</span><br></pre></td></tr></table></figure>
<h3 id="步骤二：获取令牌前缀"><a href="#步骤二：获取令牌前缀" class="headerlink" title="步骤二：获取令牌前缀"></a>步骤二：获取令牌前缀</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm token list           <span class="comment">#if no output, then create a token:</span></span><br><span class="line">$ kubeadm token create</span><br><span class="line">TOKEN                       ...     ...</span><br><span class="line">a0b1c2.svn4my9ifft4zxgg     ...     ...</span><br><span class="line"><span class="comment"># Token prefix is "a0b1c2"</span></span><br></pre></td></tr></table></figure>
<h3 id="步骤三"><a href="#步骤三" class="headerlink" title="步骤三"></a>步骤三</h3><p><strong>从现有角色“kubeadm：kubelet-config-1.11”创建一个新角色“kubeadm：kubelet-config-1.12”</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get roles --all-namespaces</span><br><span class="line">$ kubectl -n kube-system get role kubeadm:kubelet-config-1.11 -o yaml &gt; kubeadm:kubelet-config-1.12-role.yaml</span><br><span class="line">$ vim kubeadm\:kubelet-config-1.12-role.yaml    <span class="comment">#modify the following:</span></span><br><span class="line">                                                <span class="comment">#name: kubeadm:kubelet-config-1.12</span></span><br><span class="line">                                                <span class="comment">#resourceNames: kubelet-config-1.12</span></span><br><span class="line">                                                <span class="comment">#delete creationTimestamp, resourceVersion, selfLink, uid (because --export option is not supported)    </span></span><br><span class="line">$ kubectl -n kube-system create <span class="_">-f</span> kubeadm\:kubelet-config-1.12-role.yaml</span><br></pre></td></tr></table></figure>
<h3 id="步骤四"><a href="#步骤四" class="headerlink" title="步骤四"></a>步骤四</h3><p><strong>从现有角色绑定 “kubeadm：kubelet-config-1.11” 创建一个新角色绑定 “kubeadm：kubelet-config-1.12” </strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get rolebindings --all-namespaces</span><br><span class="line">$ kubectl -n kube-system get rolebinding kubeadm:kubelet-config-1.11 -o yaml &gt; kubeadm:kubelet-config-1.12-rolebinding.yaml</span><br><span class="line">$ vim kubeadm\:kubelet-config-1.12-rolebinding.yaml     <span class="comment">#modify the following:</span></span><br><span class="line">                                                            <span class="comment">#metadata/name: kubeadm:kubelet-config-1.12</span></span><br><span class="line">                                                            <span class="comment">#roleRef/name: kubeadm:kubelet-config-1.12</span></span><br><span class="line">                                                            <span class="comment">#delete creationTimestamp, resourceVersion, selfLink, uid (because --export option is not supported)</span></span><br><span class="line">- apiGroup: rbac.authorization.k8s.io                       <span class="comment">#add these 3 lines as another group in "subjects:" at the bottom, with the 6 character token prefix from STEP 2</span></span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:bootstrap:a0b1c2 </span><br><span class="line">$ kubectl -n kube-system create <span class="_">-f</span> kubeadm\:kubelet-config-1.12-rolebinding.yaml</span><br></pre></td></tr></table></figure>
<h3 id="步骤5：从工作节点启动kubeadm-join"><a href="#步骤5：从工作节点启动kubeadm-join" class="headerlink" title="步骤5：从工作节点启动kubeadm join"></a>步骤5：从工作节点启动kubeadm join</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo kubeadm join --token &lt;token&gt; &lt;master-IP&gt;:6443 --discovery-token-ca-cert-hash sha256:&lt;key-value&gt; </span><br><span class="line"><span class="comment"># If you receive 2 ERRORS, run kubeadm join again with the following options:</span></span><br><span class="line">$ sudo kubeadm join --token &lt;token&gt; &lt;master-IP&gt;:6443 --discovery-token-ca-cert-hash sha256:&lt;key-value&gt; --ignore-preflight-errors=FileAvailable--etc-kubernetes-bootstrap-kubelet.conf,FileAvailable--etc-kubernetes-pki-ca.crt</span><br></pre></td></tr></table></figure>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm1.14 证书调整]]></title>
      <url>http://team.jiunile.com/blog/2019/05/k8s-kubeadm14-ca-upgrade.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>kubeadm部署的kubernets证书一直都是个诟病，默认都只有一年有效期，kubeadm 1.14.x安装后有部分证书还是一年有效期，但个别证书已修改为10年有效期，但对我们使用来说，一年有效期还是一个比较的坑，需要进行调整。</p>
</blockquote>
<h3 id="修改kubeadm-1-14-x源码，调整证书过期时间"><a href="#修改kubeadm-1-14-x源码，调整证书过期时间" class="headerlink" title="修改kubeadm 1.14.x源码，调整证书过期时间"></a>修改kubeadm 1.14.x源码，调整证书过期时间</h3><h4 id="kubeadm1-14-x-安装过后crt证书如下所示"><a href="#kubeadm1-14-x-安装过后crt证书如下所示" class="headerlink" title="kubeadm1.14.x 安装过后crt证书如下所示"></a>kubeadm1.14.x 安装过后crt证书如下所示</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/etc/kubernetes/pki/apiserver.crt</span><br><span class="line">/etc/kubernetes/pki/front-proxy-ca.crt         #10年有效期</span><br><span class="line">/etc/kubernetes/pki/ca.crt                     #10年有效期</span><br><span class="line">/etc/kubernetes/pki/apiserver-etcd-client.crt</span><br><span class="line">/etc/kubernetes/pki/front-proxy-client.crt     #10年有效期</span><br><span class="line">/etc/kubernetes/pki/etcd/server.crt</span><br><span class="line">/etc/kubernetes/pki/etcd/ca.crt                #10年有效期</span><br><span class="line">/etc/kubernetes/pki/etcd/peer.crt              #10年有效期</span><br><span class="line">/etc/kubernetes/pki/etcd/healthcheck-client.crt</span><br><span class="line">/etc/kubernetes/pki/apiserver-kubelet-client.crt</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>如上所示，除了标注说明的证书为10年有效期，其余都是1年有效期，我们查看下原先调整证书有效期的源码，克隆kubernetes 源码，切换到1.14.1 tag 查看：<br>代码目录： <code>staging/src/k8s.io/client-go/util/cert/cert.go</code><br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> duration365d = time.Hour * <span class="number">24</span> * <span class="number">365</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewSelfSignedCACert</span><span class="params">(cfg Config, key crypto.Signer)</span> <span class="params">(*x509.Certificate, error)</span></span> &#123;</span><br><span class="line">    now := time.Now()</span><br><span class="line">    tmpl := x509.Certificate&#123;</span><br><span class="line">        SerialNumber: <span class="built_in">new</span>(big.Int).SetInt64(<span class="number">0</span>),</span><br><span class="line">        Subject: pkix.Name&#123;</span><br><span class="line">            CommonName:   cfg.CommonName,</span><br><span class="line">            Organization: cfg.Organization,</span><br><span class="line">        &#125;,</span><br><span class="line">        NotBefore:             now.UTC(),</span><br><span class="line">        <span class="comment">//这里已经调整为10年有效期</span></span><br><span class="line">        NotAfter:              now.Add(duration365d * <span class="number">10</span>).UTC(),</span><br><span class="line">        KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,</span><br><span class="line">        BasicConstraintsValid: <span class="literal">true</span>,</span><br><span class="line">        IsCA:                  <span class="literal">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;tmpl, &amp;tmpl, key.Public(), key)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x509.ParseCertificate(certDERBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如上所示，通过<code>NewSelfSignedCACert</code>这个方法签发的证书都默认为10年有效期了，但这个只影响部分证书，但这样还没满足我们的需求，个别证书的有效期调整，在经过对源码的分析后，找到了如下的逻辑：</p>
<p>发现部分证书是通过<code>NewSignedCert</code>这个方法签发，而这个方法签发的证书默认只有一年有效期，查看代码逻辑：<br>代码: <code>cmd/kubeadm/app/util/pkiutil/pki_helpers.go</code><br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> duration365d = time.Hour * <span class="number">24</span> * <span class="number">365</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewSignedCert</span><span class="params">(cfg *certutil.Config, key crypto.Signer, caCert *x509.Certificate, caKey crypto.Signer)</span> <span class="params">(*x509.Certificate, error)</span></span> &#123;</span><br><span class="line">    serial, err := rand.Int(rand.Reader, <span class="built_in">new</span>(big.Int).SetInt64(math.MaxInt64))</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cfg.CommonName) == <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">"must specify a CommonName"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cfg.Usages) == <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">"must specify at least one ExtKeyUsage"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    certTmpl := x509.Certificate&#123;</span><br><span class="line">        Subject: pkix.Name&#123;</span><br><span class="line">            CommonName:   cfg.CommonName,</span><br><span class="line">            Organization: cfg.Organization,</span><br><span class="line">        &#125;,</span><br><span class="line">        DNSNames:     cfg.AltNames.DNSNames,</span><br><span class="line">        IPAddresses:  cfg.AltNames.IPs,</span><br><span class="line">        SerialNumber: serial,</span><br><span class="line">        NotBefore:    caCert.NotBefore,</span><br><span class="line">        <span class="comment">// 只有一年有效期</span></span><br><span class="line">        NotAfter:     time.Now().Add(duration365d).UTC(),</span><br><span class="line">        KeyUsage:     x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature,</span><br><span class="line">        ExtKeyUsage:  cfg.Usages,</span><br><span class="line">    &#125;</span><br><span class="line">    certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;certTmpl, caCert, key.Public(), caKey)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x509.ParseCertificate(certDERBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>至此，调整<code>NewSignedCert</code>这个方法，重新进行编译，将证书有效期调整为你想要的任何时间。</p>
<p>如何重新编译kubeadm源码，请参考之前的文章，链接如下：<a href="http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ca-upgdate.html">Kubeadm证书过期时间调整</a></p>
<h3 id="如何一键离线安装kubernetes-1-14教程"><a href="#如何一键离线安装kubernetes-1-14教程" class="headerlink" title="如何一键离线安装kubernetes 1.14教程"></a>如何一键离线安装kubernetes 1.14教程</h3><blockquote>
<p>k8s 1.14.x 离线一键安装包教程&amp;&amp;地址：<a href="http://team.jiunile.com/pro/k8s/">kubernetes 1.14 离线安装地址</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kuberenetes 云应用实践]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-cloud.html</url>
      <content type="html"><![CDATA[<h2 id="如何使用云盘"><a href="#如何使用云盘" class="headerlink" title="如何使用云盘"></a>如何使用云盘</h2><h3 id="aws-ebs"><a href="#aws-ebs" class="headerlink" title="aws ebs"></a>aws ebs</h3><a id="more"></a>
<h4 id="设置IAM角色并分配到机器上"><a href="#设置IAM角色并分配到机器上" class="headerlink" title="设置IAM角色并分配到机器上"></a>设置IAM角色并分配到机器上</h4><blockquote>
<p>具有controlplane(控制面板)角色的节点的IAM策略：</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line"><span class="attr">"Statement"</span>: [</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">    <span class="attr">"Action"</span>: [</span><br><span class="line">      <span class="string">"autoscaling:DescribeAutoScalingGroups"</span>,</span><br><span class="line">      <span class="string">"autoscaling:DescribeLaunchConfigurations"</span>,</span><br><span class="line">      <span class="string">"autoscaling:DescribeTags"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeInstances"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeRegions"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeRouteTables"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeSecurityGroups"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeSubnets"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeVolumes"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateSecurityGroup"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateTags"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:ModifyInstanceAttribute"</span>,</span><br><span class="line">      <span class="string">"ec2:ModifyVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:AttachVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:AuthorizeSecurityGroupIngress"</span>,</span><br><span class="line">      <span class="string">"ec2:CreateRoute"</span>,</span><br><span class="line">      <span class="string">"ec2:DeleteRoute"</span>,</span><br><span class="line">      <span class="string">"ec2:DeleteSecurityGroup"</span>,</span><br><span class="line">      <span class="string">"ec2:DeleteVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:DetachVolume"</span>,</span><br><span class="line">      <span class="string">"ec2:RevokeSecurityGroupIngress"</span>,</span><br><span class="line">      <span class="string">"ec2:DescribeVpcs"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:AddTags"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:AttachLoadBalancerToSubnets"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateLoadBalancerPolicy"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateLoadBalancerListeners"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ConfigureHealthCheck"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteLoadBalancerListeners"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeLoadBalancers"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeLoadBalancerAttributes"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DetachLoadBalancerFromSubnets"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeregisterInstancesFromLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ModifyLoadBalancerAttributes"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:RegisterInstancesWithLoadBalancer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:AddTags"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateListener"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:CreateTargetGroup"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteListener"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DeleteTargetGroup"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeListeners"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeLoadBalancerPolicies"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeTargetGroups"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:DescribeTargetHealth"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ModifyListener"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:ModifyTargetGroup"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:RegisterTargets"</span>,</span><br><span class="line">      <span class="string">"elasticloadbalancing:SetLoadBalancerPoliciesOfListener"</span>,</span><br><span class="line">      <span class="string">"iam:CreateServiceLinkedRole"</span>,</span><br><span class="line">      <span class="string">"kms:DescribeKey"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"Resource"</span>: [</span><br><span class="line">      <span class="string">"*"</span></span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>worker角色的节点的IAM策略</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line"><span class="attr">"Statement"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">        <span class="attr">"Action"</span>: [</span><br><span class="line">            <span class="string">"ec2:DescribeInstances"</span>,</span><br><span class="line">            <span class="string">"ec2:DescribeRegions"</span>,</span><br><span class="line">            <span class="string">"ecr:GetAuthorizationToken"</span>,</span><br><span class="line">            <span class="string">"ecr:BatchCheckLayerAvailability"</span>,</span><br><span class="line">            <span class="string">"ecr:GetDownloadUrlForLayer"</span>,</span><br><span class="line">            <span class="string">"ecr:GetRepositoryPolicy"</span>,</span><br><span class="line">            <span class="string">"ecr:DescribeRepositories"</span>,</span><br><span class="line">            <span class="string">"ecr:ListImages"</span>,</span><br><span class="line">            <span class="string">"ecr:BatchGetImage"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"Resource"</span>: <span class="string">"*"</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="配置ClusterID"><a href="#配置ClusterID" class="headerlink" title="配置ClusterID"></a>配置ClusterID</h4><blockquote>
<p>需要在机器上打上标签，如果不配置会产生以下错误</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed to run Kubelet: could not init cloud provider <span class="string">"aws"</span>: AWS cloud failed to find ClusterID</span><br></pre></td></tr></table></figure>
<p>标 签 Key ： <code>kubernetes.io/cluster/CLUSTERID</code><br>标签Value：  <code>owned</code></p>
<p><code>CLUSTERID</code> 可以为任何值</p>
<h4 id="kubernetes配置"><a href="#kubernetes配置" class="headerlink" title="kubernetes配置"></a>kubernetes配置</h4><p>需要kube-apiserver, kube-controller-manager以及kubelet 启动加上 <code>--cloud-provider=aws</code>参数 , <code>注意</code>需要在<strong>所有node</strong>上的kubelet 开启<code>--cloud-provider=aws</code></p>
<p><strong>使用kubeadm 初始化kubernetes设置</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiServerExtraArgs:</span></span><br><span class="line"><span class="attr">  cloud-provider:</span> aws</span><br><span class="line"><span class="attr">controllerManagerExtraArgs:</span></span><br><span class="line"><span class="attr">  cloud-provider:</span> aws</span><br><span class="line"></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line">  <span class="comment">#建议将Kubelet的名称设置为EC2中节点的私有DNS条目（这可确保它与主机名匹配，如本文前面所述）</span></span><br><span class="line"><span class="attr">  name:</span> ip<span class="bullet">-10</span><span class="bullet">-15</span><span class="bullet">-30</span><span class="bullet">-45.</span>cn-northwest<span class="bullet">-1.</span>compute.internal</span><br><span class="line"><span class="attr">  kubeletExtraArgs:</span></span><br><span class="line"><span class="attr">    cloud-provider:</span> aws</span><br></pre></td></tr></table></figure></p>
<p><strong>使用kubeadm 命令安装后的情况下</strong></p>
<ul>
<li><code>kube-apiserver</code> 修改 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>下的启动参数</li>
<li><code>kube-controller-manager</code> 修改 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code>下的启动参数</li>
<li><code>kubelet</code> 修改 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 下的启动参数</li>
</ul>
<h4 id="开始使用EBS"><a href="#开始使用EBS" class="headerlink" title="开始使用EBS"></a>开始使用EBS</h4><h5 id="创建StorageClass"><a href="#创建StorageClass" class="headerlink" title="创建StorageClass"></a>创建StorageClass</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat standard-storageclass.yaml</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> standard</span><br><span class="line"><span class="attr">provisioner:</span> kubernetes.io/aws-ebs</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> gp2</span><br><span class="line"><span class="attr">  zone:</span> ap-northeast<span class="bullet">-1</span>b</span><br><span class="line"><span class="attr">reclaimPolicy:</span> Delete       <span class="comment"># 在删除 Pod 的同时，也删除 EBS 磁盘</span></span><br><span class="line"><span class="comment"># kubectl apply -f standard-storageclass.yaml</span></span><br></pre></td></tr></table></figure>
<h5 id="创建PVC"><a href="#创建PVC" class="headerlink" title="创建PVC"></a>创建PVC</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat nginx-pv-claim.yaml</span></span><br><span class="line"><span class="attr">kind:</span> PersistentVolumeClaim</span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-pv-claim</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> ReadWriteOnce</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">20</span>Gi</span><br><span class="line"><span class="attr">  storageClassName:</span> standard</span><br><span class="line"><span class="comment"># kubectl apply -f nginx-pv-claim.yaml</span></span><br></pre></td></tr></table></figure>
<h5 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat nginx-pv-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-pv</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">    - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> nginx-pv-claim</span><br><span class="line"><span class="attr">      mountPath:</span> /data</span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx-pv-claim</span><br><span class="line"><span class="attr">    persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">      claimName:</span> nginx-pv-claim</span><br><span class="line"><span class="comment"># kubectl apply -f nginx-pv-pod.yaml</span></span><br></pre></td></tr></table></figure>
<h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul>
<li><a href="https://www.kubernetes.org.cn/4078.html" target="_blank" rel="external">https://www.kubernetes.org.cn/4078.html</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#aws" target="_blank" rel="external">https://kubernetes.io/docs/concepts/storage/storage-classes/#aws</a></li>
<li><a href="https://blog.scottlowe.org/2018/09/28/setting-up-the-kubernetes-aws-cloud-provider/" target="_blank" rel="external">https://blog.scottlowe.org/2018/09/28/setting-up-the-kubernetes-aws-cloud-provider/</a></li>
</ul>
<h3 id="阿里云云盘"><a href="#阿里云云盘" class="headerlink" title="阿里云云盘"></a>阿里云云盘</h3><h4 id="创建阿里云Provisioner-控制器"><a href="#创建阿里云Provisioner-控制器" class="headerlink" title="创建阿里云Provisioner 控制器"></a>创建阿里云Provisioner 控制器</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-common</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> cloud</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-efficiency</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> cloud_efficiency</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-ssd</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> cloud_ssd</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> StorageClass</span><br><span class="line"><span class="attr">apiVersion:</span> storage.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-available</span><br><span class="line"><span class="attr">provisioner:</span> alicloud/disk</span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  type:</span> available</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller-runner</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"persistentvolumes"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"delete"</span>]</span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"persistentvolumeclaims"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"update"</span>]</span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">"storage.k8s.io"</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"storageclasses"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line"><span class="attr">  - apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line"><span class="attr">    resources:</span> [<span class="string">"events"</span>]</span><br><span class="line"><span class="attr">    verbs:</span> [<span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"update"</span>, <span class="string">"patch"</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> run-alicloud-disk-controller</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">    name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">    namespace:</span> kube-system</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller-runner</span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> cloud-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  special.keyid: 你阿里云的access key</span><br><span class="line">  special.keysecret: 你阿里云的secret key</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">apiVersion:</span> extensions/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    type:</span> Recreate</span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">        operator:</span> Exists</span><br><span class="line"><span class="attr">        key:</span> node-role.kubernetes.io/master</span><br><span class="line"><span class="attr">      - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">        operator:</span> Exists</span><br><span class="line"><span class="attr">        key:</span> node.cloudprovider.kubernetes.io/uninitialized</span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line">         node-role.kubernetes.io/master: <span class="string">""</span></span><br><span class="line"><span class="attr">      serviceAccount:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> alicloud-disk-controller</span><br><span class="line"><span class="attr">          image:</span> icyboy/alicloud-disk-controller:<span class="number">3652</span>ddf</span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">          - name:</span> ACCESS_KEY_ID</span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              configMapKeyRef:</span></span><br><span class="line"><span class="attr">                name:</span> cloud-config</span><br><span class="line"><span class="attr">                key:</span> special.keyid</span><br><span class="line"><span class="attr">          - name:</span> ACCESS_KEY_SECRET</span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              configMapKeyRef:</span></span><br><span class="line"><span class="attr">                name:</span> cloud-config</span><br><span class="line"><span class="attr">                key:</span> special.keysecret</span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> cloud-config</span><br><span class="line"><span class="attr">              mountPath:</span> /etc/kubernetes/</span><br><span class="line"><span class="attr">            - name:</span> logdir</span><br><span class="line"><span class="attr">              mountPath:</span> /var/log/alicloud/</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> cloud-config</span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> /etc/kubernetes/</span><br><span class="line"><span class="attr">        - name:</span> logdir</span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> /var/log/alicloud/</span><br></pre></td></tr></table></figure>
<h4 id="创建PVC-1"><a href="#创建PVC-1" class="headerlink" title="创建PVC"></a>创建PVC</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> PersistentVolumeClaim</span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> disk</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> ReadWriteOnce</span><br><span class="line"><span class="comment">#alicloud-disk-common     普通云盘</span></span><br><span class="line"><span class="comment">#alicloud-disk-efficiency 高效云盘</span></span><br><span class="line"><span class="comment">#alicloud-disk-ssd        ssd云盘</span></span><br><span class="line"><span class="comment">#alicloud-disk-available</span></span><br><span class="line"><span class="attr">  storageClassName:</span> alicloud-disk-common</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">20</span>Gi</span><br></pre></td></tr></table></figure>
<h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pvc</span></span><br><span class="line">NAME      STATUS    VOLUME                   CAPACITY   ACCESS MODES   STORAGECLASS           AGE</span><br><span class="line">disk      Bound     d-bp1cz8sslda31ld2snbq   20Gi       RWO            alicloud-disk-common   11s</span><br><span class="line"><span class="comment"># kubectl get pv</span></span><br><span class="line">NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM          STORAGECLASS           REASON    AGE</span><br><span class="line">d-bp1cz8sslda31ld2snbq   20Gi       RWO            Delete           Bound     default/disk   alicloud-disk-common             14s</span><br></pre></td></tr></table></figure>
<h4 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h4><ul>
<li><a href="https://yq.aliyun.com/articles/629001" target="_blank" rel="external">https://yq.aliyun.com/articles/629001</a></li>
<li><a href="https://github.com/AliyunContainerService/alicloud-storage-provisioner" target="_blank" rel="external">https://github.com/AliyunContainerService/alicloud-storage-provisioner</a></li>
</ul>
<h2 id="IAM-使用"><a href="#IAM-使用" class="headerlink" title="IAM 使用"></a>IAM 使用</h2><h3 id="kube2iam-in-aws-iam"><a href="#kube2iam-in-aws-iam" class="headerlink" title="kube2iam in aws iam"></a>kube2iam in aws iam</h3><h4 id="通过AWS-IAM创建一个名为kube2iam的策略，策略规则如下"><a href="#通过AWS-IAM创建一个名为kube2iam的策略，策略规则如下" class="headerlink" title="通过AWS IAM创建一个名为kube2iam的策略，策略规则如下"></a>通过AWS IAM创建一个名为kube2iam的<code>策略</code>，策略规则如下</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line">    <span class="attr">"Statement"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"Action"</span>: [</span><br><span class="line">                <span class="string">"sts:AssumeRole"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">            <span class="attr">"Resource"</span>: <span class="string">"arn:aws-cn:iam::955466075186:role/k8s-*"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="通过AWS-IAM创建一个名为：node-k8s-role的角色，并将kube2iam的策略赋予它，注意！！同时将这个角色赋予到机器上"><a href="#通过AWS-IAM创建一个名为：node-k8s-role的角色，并将kube2iam的策略赋予它，注意！！同时将这个角色赋予到机器上" class="headerlink" title="通过AWS IAM创建一个名为：node-k8s-role的角色，并将kube2iam的策略赋予它，注意！！同时将这个角色赋予到机器上"></a>通过AWS IAM创建一个名为：node-k8s-role的<code>角色</code>，并将<code>kube2iam</code>的<code>策略</code>赋予它，<code>注意！！同时将这个角色赋予到机器上</code></h4><h4 id="通过AWS-IAM创建一个名为：k8s-kube2iam-demo的角色，将AmazonS3FullAccess的策略赋予它，同时编辑信任关系，调整为如下："><a href="#通过AWS-IAM创建一个名为：k8s-kube2iam-demo的角色，将AmazonS3FullAccess的策略赋予它，同时编辑信任关系，调整为如下：" class="headerlink" title="通过AWS IAM创建一个名为：k8s-kube2iam-demo的角色，将AmazonS3FullAccess的策略赋予它，同时编辑信任关系，调整为如下："></a>通过AWS IAM创建一个名为：k8s-kube2iam-demo的<code>角色</code>，将<code>AmazonS3FullAccess</code>的<code>策略</code>赋予它，同时编辑信任关系，调整为如下：</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line">  <span class="attr">"Statement"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">      <span class="attr">"Principal"</span>: &#123;</span><br><span class="line">        <span class="attr">"Service"</span>: <span class="string">"ec2.amazonaws.com.cn"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"Action"</span>: <span class="string">"sts:AssumeRole"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"Sid"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">      <span class="attr">"Principal"</span>: &#123;</span><br><span class="line">        <span class="attr">"AWS"</span>: <span class="string">"arn:aws-cn:iam::955466075186:role/node-k8s-role"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"Action"</span>: <span class="string">"sts:AssumeRole"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="部署kube2iam"><a href="#部署kube2iam" class="headerlink" title="部署kube2iam"></a>部署kube2iam</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Source: kube2iam/templates/serviceaccount.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: kube2iam/templates/clusterrole.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">  - apiGroups:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="bullet">      -</span> namespaces</span><br><span class="line"><span class="bullet">      -</span> pods</span><br><span class="line"><span class="attr">    verbs:</span></span><br><span class="line"><span class="bullet">      -</span> list</span><br><span class="line"><span class="bullet">      -</span> watch</span><br><span class="line"><span class="bullet">      -</span> get</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: kube2iam/templates/clusterrolebinding.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> ServiceAccount</span><br><span class="line"><span class="attr">    name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">    namespace:</span> default</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: kube2iam/templates/daemonset.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> extensions/v1beta1</span><br><span class="line"><span class="attr">kind:</span> DaemonSet</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> kube2iam</span><br><span class="line"><span class="attr">    chart:</span> kube2iam<span class="bullet">-1.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    heritage:</span> Tiller</span><br><span class="line"><span class="attr">    release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">  name:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> kube2iam</span><br><span class="line"><span class="attr">        release:</span> ardent-wildebeest</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> kube2iam</span><br><span class="line"><span class="attr">          image:</span> <span class="string">"jtblin/kube2iam:0.10.4"</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">"IfNotPresent"</span></span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--host-interface=cali+"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--node=$(NODE_NAME)"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--host-ip=$(HOST_IP)"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--iptables=true"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--base-role-arn=arn:aws-cn:iam::955466075186:role/node-k8s-role"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--app-port=8181"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"--use-regional-sts-endpoint"</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> HOST_IP</span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> status.podIP</span><br><span class="line"><span class="attr">            - name:</span> NODE_NAME</span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> spec.nodeName</span><br><span class="line"><span class="attr">            - name:</span> AWS_REGION</span><br><span class="line"><span class="attr">              value:</span> cn-northwest<span class="bullet">-1</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - name:</span> http</span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">8181</span></span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> /healthz</span><br><span class="line"><span class="attr">              port:</span> <span class="number">8181</span></span><br><span class="line"><span class="attr">              scheme:</span> HTTP</span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line">            &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">          securityContext:</span></span><br><span class="line"><span class="attr">            privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> ardent-wildebeest-kube2iam</span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line">        []</span><br><span class="line"></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">    type:</span> OnDelete</span><br></pre></td></tr></table></figure>
<h4 id="测试pod"><a href="#测试pod" class="headerlink" title="测试pod"></a>测试pod</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> awscli-deployment</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> awscli</span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      annotations:</span></span><br><span class="line">        iam.amazonaws.com/role: arn:aws-cn:iam::<span class="number">955466075186</span>:role/k8s/k8s-kube2iam-demo</span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> awscli</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> awscli</span><br><span class="line"><span class="attr">        image:</span> mesosphere/aws-cli</span><br><span class="line"><span class="attr">        command:</span> [ <span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>, <span class="string">"--"</span> ]</span><br><span class="line"><span class="attr">        args:</span> [ <span class="string">"while true; do sleep 3000; done;"</span> ]</span><br></pre></td></tr></table></figure>
<h2 id="云应用疑难杂症"><a href="#云应用疑难杂症" class="headerlink" title="云应用疑难杂症"></a>云应用疑难杂症</h2><h3 id="使用云负载均衡器代理APIServer问题"><a href="#使用云负载均衡器代理APIServer问题" class="headerlink" title="使用云负载均衡器代理APIServer问题"></a>使用云负载均衡器代理APIServer问题</h3><blockquote>
<p>阿里云/AWS的负载均衡是四层TCP负责，<strong>不支持后端ECS实例既作为Real Server又作为客户端向所在的负载均衡实例发送请求</strong>。因为返回的数据包只在云服务器内部转发，不经过负载均衡，所以在后端ECS实例上去访问负载均衡的服务地址是不通的。什么意思？就是如果你要使用阿里云的SLB的话，那么你不能在apiserver节点上使用SLB（比如在apiserver 上安装kubectl，然后将apiserver的地址设置为SLB的负载地址使用），因为这样的话就可能造成回环了，所以简单的做法是另外用两个新的节点做HA实例，然后将这两个实例添加到SLB 机器组中。</p>
</blockquote>
<ul>
<li>aws 使用nlb，指定ip</li>
<li>aliyun 在apiserver上搭建keepalived</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kubernetes内部DNS剖析]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-dns.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们可以通过 Service 生成的<code>ClusterIP(VIP)</code>来访问 Pod 提供的服务，但是在使用的时候还有一个问题：我们怎么知道某个应用的 VIP 呢？比如我们有两个应用，一个是 api 应用，一个是 db 应用，两个应用都是通过<code>Deployment</code>进行管理的，并且都通过 Service 暴露出了端口提供服务。api 需要连接到 db 这个应用，我们只知道 db 应用的名称和 db 对应的 Service 的名称，但是并不知道它的 VIP 地址，我们前面的 Service 课程中是不是学习到我们通过<code>ClusterIP</code>就可以访问到后面的<code>Pod</code>服务，如果我们知道了 VIP 的地址是不是就行了？</p>
<h2 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h2><p>我们知道可以从 apiserver 中直接查询获取到对应 service 的后端 Endpoints信息，所以最简单的办法是从 apiserver 中直接查询，如果偶尔一个特殊的应用，我们通过<code>apiserver</code>去查询到<code>Service</code>后面的 <code>Endpoints</code>直接使用是没问题的，但是如果每个应用都在启动的时候去查询依赖的服务，这不但增加了应用的复杂度，这也导致了我们的应用需要依赖<code>Kubernetes</code>了，耦合度太高了，不具有通用性。<br><a id="more"></a></p>
<h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>为了解决上面的问题，在之前的版本中，Kubernetes 采用了环境变量的方法，每个 Pod 启动的时候，会通过环境变量设置所有服务的 IP 和 port 信息，这样 Pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息，这种方法使用起来相对简单，但是有一个很大的问题就是依赖的服务必须在 Pod 启动之前就存在，不然是不会被注入到环境变量中的。比如我们首先创建一个 Nginx 服务：(test-nginx.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-deploy</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> nginx-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> nginx</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Service</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx-service</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> nginx-service</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">5000</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br></pre></td></tr></table></figure></p>
<p>创建上面的服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> <span class="built_in">test</span>-nginx.yaml</span><br><span class="line">deployment.apps <span class="string">"nginx-deploy"</span> created</span><br><span class="line">service <span class="string">"nginx-service"</span> created</span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">...</span><br><span class="line">nginx-deploy-75675f5897-47h4t             1/1       Running   0          53s</span><br><span class="line">nginx-deploy-75675f5897-mmm8w             1/1       Running   0          53s</span><br><span class="line">...</span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">...</span><br><span class="line">nginx-service   ClusterIP   10.107.225.42    &lt;none&gt;        5000/TCP         1m</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到两个 Pod 和一个名为 nginx-service 的服务创建成功了，该 Service 监听的端口是 5000，同时它会把流量转发给它代理的所有 Pod（我们这里就是拥有 app: nginx 标签的两个 Pod）。</p>
<p>现在我们再来创建一个普通的 Pod，观察下该 Pod 中的环境变量是否包含上面的 nginx-service 的服务信息：（test-pod.yaml）<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> test-pod</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> test-service-pod</span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>, <span class="string">"env"</span>]</span><br></pre></td></tr></table></figure></p>
<p>然后创建该测试的 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> <span class="built_in">test</span>-pod.yaml</span><br><span class="line">pod <span class="string">"test-pod"</span> created</span><br></pre></td></tr></table></figure></p>
<p>等 Pod 创建完成后，我们查看日志信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs <span class="built_in">test</span>-pod</span><br><span class="line">...</span><br><span class="line">KUBERNETES_PORT=tcp://10.96.0.1:443</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">HOSTNAME=<span class="built_in">test</span>-pod</span><br><span class="line">HOME=/root</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP_ADDR=10.107.225.42</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP_PORT=5000</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP_PROTO=tcp</span><br><span class="line">KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1</span><br><span class="line">PATH=/usr/<span class="built_in">local</span>/sbin:/usr/<span class="built_in">local</span>/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">NGINX_SERVICE_SERVICE_HOST=10.107.225.42</span><br><span class="line">NGINX_SERVICE_PORT_5000_TCP=tcp://10.107.225.42:5000</span><br><span class="line">KUBERNETES_PORT_443_TCP_PORT=443</span><br><span class="line">KUBERNETES_PORT_443_TCP_PROTO=tcp</span><br><span class="line">NGINX_SERVICE_SERVICE_PORT=5000</span><br><span class="line">NGINX_SERVICE_PORT=tcp://10.107.225.42:5000</span><br><span class="line">KUBERNETES_SERVICE_PORT_HTTPS=443</span><br><span class="line">KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.96.0.1</span><br><span class="line">PWD=/</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到打印了很多环境变量处理，其中就包括我们刚刚创建的 nginx-service 这个服务，有 HOST、PORT、PROTO、ADDR 等，也包括其他已经存在的 Service 的环境变量，现在如果我们需要在这个 Pod 里面访问 nginx-service 的服务，我们是不是可以直接通过 NGINX_SERVICE_SERVICE_HOST 和 NGINX_SERVICE_SERVICE_PORT 就可以了，但是我们也知道如果这个 Pod 启动起来的时候如果 nginx-service 服务还没启动起来，在环境变量中我们是无法获取到这些信息的，当然我们可以通过 <code>initContainer</code>之类的方法来确保 nginx-service 启动后再启动 Pod，但是这种方法毕竟增加了 Pod 启动的复杂性，所以这不是最优的方法。</p>
<h2 id="KubeDNS"><a href="#KubeDNS" class="headerlink" title="KubeDNS"></a>KubeDNS</h2><p>由于上面环境变量这种方式的局限性，我们需要一种更加智能的方案，其实我们可以自己想学一种比较理想的方案：那就是可以直接使用 Service 的名称，因为 Service 的名称不会变化，我们不需要去关心分配的<code>ClusterIP</code>的地址，因为这个地址并不是固定不变的，所以如果我们直接使用 Service 的名字，然后对应的<code>ClusterIP</code>地址的转换能够自动完成就很好了。我们知道名字和 IP 直接的转换是不是和我们平时访问的网站非常类似啊？他们之间的转换功能通过<code>DNS</code>就可以解决了，同样的，<code>Kubernetes</code>也提供了<code>DNS</code><br>的方案来解决上面的服务发现的问题。</p>
<h3 id="kubedns-介绍"><a href="#kubedns-介绍" class="headerlink" title="kubedns 介绍"></a>kubedns 介绍</h3><p>DNS 服务不是一个独立的系统服务，而是作为一种 addon 插件而存在，也就是说不是 Kubernetes 集群必须安装的，当然我们强烈推荐安装，可以将这个插件看成是一种运行在 Kubernetes 集群上的一直比较特殊的应用，现在比较推荐的两个插件：<code>kube-dns</code> 和 <code>CoreDNS</code>。我们在前面使用<code>kubeadm</code>搭建集群的时候直接安装的 kube-dns 插件，如果不记得了可以回头去看一看。当然如果我们想使用<code>CoreDNS</code> 的话也很方便，只需要执行下面的命令即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm init --feature-gates=CoreDNS=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>Kubernetes DNS pod 中包括 3 个容器，可以通过 kubectl 工具查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line">NAME                                    READY     STATUS    RESTARTS   AGE</span><br><span class="line">...</span><br><span class="line">kube-dns-5868f69869-zp5kz               3/3       Running   0          19d</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>READY 一栏可以看到是 3/3，用如下命令可以很清楚的看到 kube-dns 包含的3个容器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod kube-dns-5868f69869-zp5kz -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>kube-dns、dnsmasq-nanny、sidecar 这3个容器分别实现了什么功能?</p>
<ul>
<li>kubedns: kubedns 基于 SkyDNS 库，通过 apiserver 监听 Service 和 Endpoints 的变更事件同时也同步到本地 Cache，实现了一个实时的 Kubernetes 集群内 Service 和 Pod 的 DNS服务发现</li>
<li>dnsmasq: dsnmasq 容器则实现了 DNS 的缓存功能(在内存中预留一块默认大小为 1G 的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubedns 中查询，并把结果缓存起来)，通过监听 ConfigMap 来动态生成配置</li>
<li>sider: sidecar 容器实现了可配置的 DNS 探测，并采集对应的监控指标暴露出来供 prometheus 使用<br><img src="/images/k8s/kubedns.jpg" alt="kubedns"></li>
</ul>
<h3 id="对-Pod-的影响"><a href="#对-Pod-的影响" class="headerlink" title="对 Pod 的影响"></a>对 Pod 的影响</h3><p>DNS Pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 被分配后，kubelet 会将使用 <code>--cluster-dns = &lt;dns-service-ip&gt;</code>参数配置的 DNS 传递给每个容器。DNS 名称也需要域名，本地域可以使用参数<code>--cluster-domain = &lt;default-local-domain&gt;</code>在<code>kubelet</code>中配置。</p>
<p>我们说<code>dnsmasq</code>容器通过监听<code>ConfigMap</code>来动态生成配置，可以自定义存根域和上下游域名服务器。</p>
<p>例如，下面的<code>ConfigMap</code>建立了一个 DNS 配置，它具有一个单独的存根域和两个上游域名服务器：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kube-dns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  stubDomains:</span> <span class="string">|</span><br><span class="line">    &#123;"jiunile.local": ["192.168.3.108"]&#125;</span><br><span class="line"></span><span class="attr">  upstreamNameservers:</span> <span class="string">|</span><br><span class="line">    ["8.8.8.8", "8.8.4.4"]</span></span><br></pre></td></tr></table></figure></p>
<p>如何已使用coredns，则通过修改coredns配置信息来调整，如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  Corefile:</span> <span class="string">|</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">           pods insecure</span><br><span class="line">           # 用于解析外部主机主机（外部服务）</span><br><span class="line">           upstream 114.114.114.114 223.5.5.5</span><br><span class="line">           fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        # 任何不在集群域内的查询将转发到预定义的解析器，默认：/etc/resolv.conf；</span><br><span class="line">        # 在coredns "Deployment"资源中"dnsPolicy"设置为"Default"，即提供dns服务的pod从所在节点继承/etc/resolv.conf，如果节点的上游解析地址与"upstream"一致，则设置任意一个参数即可</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        #proxy . 114.114.114.114 223.5.5.5</span><br><span class="line">        cache 30</span><br><span class="line">        reload</span><br><span class="line">    &#125;</span><br><span class="line">    # 自定义dns记录，对应kube-dns中的stubdomains；</span><br><span class="line">    # 每条记录，单独设置1各zone</span><br><span class="line">    jiunile.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        proxy . 192.168.3.108</span><br><span class="line">    &#125;</span><br><span class="line"></span><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="number">2019</span><span class="bullet">-01</span><span class="bullet">-25</span>T10:<span class="number">19</span>:<span class="number">17</span>Z</span><br><span class="line"><span class="attr">  name:</span> coredns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"2637234"</span></span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/kube-system/configmaps/coredns</span><br><span class="line"><span class="attr">  uid:</span> ac30bd36<span class="bullet">-208</span>a<span class="bullet">-11e9</span><span class="bullet">-999</span>a<span class="bullet">-02390</span>c37278e</span><br></pre></td></tr></table></figure></p>
<p>按如上说明，具有.jiunile.local后缀的 DNS 请求被转发到 DNS 192.168.3.108。Google 公共 DNS 服务器 为上游查询提供服务。下表描述了具有特定域名的查询如何映射到它们的目标 DNS 服务器：<br>| 域名      | 响应查询的服务器  |<br>| :——– | :——– |<br>| kubernetes.default.svc.cluster.local  | kube-dns |<br>| jiunile.local | 自定义 DNS (192.168.3.108) |<br>| widget.com | 上游 DNS (114.114.114.114, 223.5.5.5其中之一) |</p>
<p>另外我们还可以为每个 Pod 设置 DNS 策略。 当前 Kubernetes 支持两种 Pod 特定的 DNS 策略：“Default” 和 “ClusterFirst”。 可以通过<code>dnsPolicy</code>标志来指定这些策略。</p>
<blockquote>
<p><strong><code>注意</code></strong>：Default 不是默认的 DNS 策略。如果没有显式地指定dnsPolicy，将会使用 ClusterFirst</p>
</blockquote>
<ul>
<li>如果<code>dnsPolicy</code>被设置为 “Default”，则名字解析配置会继承自 Pod 运行所在的节点。自定义上游域名服务器和存根域不能够与这个策略一起使用</li>
<li>如果<code>dnsPolicy</code>被设置为 “ClusterFirst”，这就要依赖于是否配置了存根域和上游 DNS 服务器<ul>
<li>未进行自定义配置：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游域名服务器。<ul>
<li>进行自定义配置：如果配置了存根域和上游 DNS 服务器（类似于 前面示例 配置的内容），DNS 查询将基于下面的流程对请求进行路由：<ul>
<li>查询首先被发送到<code>kube-dns</code>中的 DNS 缓存层。</li>
<li>从缓存层，检查请求的后缀，并根据下面的情况转发到对应的 DNS 上：<ul>
<li>具有集群后缀的名字（例如 “.cluster.local”）：请求被发送到 kubedns。</li>
<li>具有存根域后缀的名字（例如 “.jiunile.local”）：请求被发送到配置的自定义 DNS 解析器（例如：监听在 192.168.3.108）。</li>
<li>未能匹配上后缀的名字（例如 “widget.com”）：请求被转发到上游 DNS（例如：Google 公共 DNS 服务器，114.114.114.114 和 223.5.5.5）。<br><img src="/images/k8s/dns.png" alt="dns"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="域名格式"><a href="#域名格式" class="headerlink" title="域名格式"></a>域名格式</h3><p>我们前面说了如果我们建立的 Service 如果支持域名形式进行解析，就可以解决我们的服务发现的功能，那么利用 kubedns 可以将 Service 生成怎样的 DNS 记录呢？</p>
<ul>
<li>普通的 Service：会生成<code>servicename.namespace.svc.cluster.local</code>的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成 <code>servicename.namespace</code>，如果处于同一个命名空间下面，甚至可以只写成 servicename 即可访问</li>
<li>Headless Service：无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过<code>podname.servicename.namespace.svc.cluster.local</code>访问到具体的某一个 Pod。</li>
</ul>
<blockquote>
<p>CoreDNS 实现的功能和 KubeDNS 是一致的，不过<code>CoreDNS</code>的所有功能都集成在了同一个容器中，在最新版的1.11.0版本中官方已经推荐使用 <code>CoreDNS</code>了，大家也可以安装CoreDNS来代替 KubeDNS，其他使用方法都是一致的：<a href="https://coredns.io/" target="_blank" rel="external">https://coredns.io/</a></p>
</blockquote>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>现在我们来使用一个简单 Pod 来测试下 Service 的域名访问：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run --rm -i --tty <span class="built_in">test</span>-dns --image=busybox /bin/sh</span><br><span class="line">If you don<span class="string">'t see a command prompt, try pressing enter.</span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ #</span></span><br></pre></td></tr></table></figure></p>
<p>我们进入到 Pod 中，查看/etc/resolv.conf中的内容，可以看到 nameserver 的地址10.96.0.10，该 IP 地址即是在安装 kubedns 插件的时候集群分配的一个固定的静态 IP 地址，我们可以通过下面的命令进行查看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc kube-dns -n kube-system</span><br><span class="line">NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP   62d</span><br></pre></td></tr></table></figure></p>
<p>也就是说我们这个 Pod 现在默认的 nameserver 就是 kubedns 的地址，现在我们来访问下前面我们创建的 nginx-service 服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># wget -q -O- nginx-service.default.svc.cluster.local</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到上面我们使用 wget 命令去访问 nginx-service 服务的域名的时候被 hang 住了，没有得到期望的结果，这是因为上面我们建立 Service 的时候暴露的端口是 5000：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># wget -q -O- nginx-service.default.svc.cluster.local:5000</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.org/"</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.com/"</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you <span class="keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>加上 5000 端口，就正常访问到服务，再试一试访问：nginx-service.default.svc、nginx-service.default、nginx-service，不出意外这些域名都可以正常访问到期望的结果。</p>
<p>到这里我们是不是就实现了在集群内部通过 Service 的域名形式进行互相通信了，大家下去试着看看访问不同 namespace 下面的服务呢？</p>
<p>来源：阳明的博客</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kuberenetes 操作实践]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-best-practices.html</url>
      <content type="html"><![CDATA[<h2 id="常用操作命令"><a href="#常用操作命令" class="headerlink" title="常用操作命令"></a>常用操作命令</h2><h3 id="常规命令"><a href="#常规命令" class="headerlink" title="常规命令"></a>常规命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置主机名</span></span><br><span class="line">hostnamectl --static <span class="built_in">set</span>-hostname  xxxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取集群加入信息</span></span><br><span class="line">kubeadm token create --print-join-command</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker仓库口令</span></span><br><span class="line">kubectl create secret docker-registry registry-secret --docker-server=registry.cn-shanghai.aliyuncs.com --docker-username=xupeng@patsnap --docker-password=patsnap2019! --docker-email=xupeng@patsnap -n ningbo</span><br><span class="line"></span><br><span class="line"><span class="comment"># etcd3.3 node状态查看</span></span><br><span class="line">ETCDCTL_API=3 etcdctl endpoint health --endpoints <span class="string">"https://10.40.20.41:2379,https://10.40.20.46:2379,https://10.40.20.233:2379"</span>   --cacert=/etc/etcd/ssl/etcd-ca.pem  --cert=/etc/etcd/ssl/etcd.pem   --key=/etc/etcd/ssl/etcd-key.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># etcd key 查看</span></span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">--endpoints=https://127.0.0.1:2379 \</span><br><span class="line">--cacert /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">get /registry/minions/ningbo-db --prefix</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="kubernetes-node-常用操作命令"><a href="#kubernetes-node-常用操作命令" class="headerlink" title="kubernetes node 常用操作命令"></a>kubernetes node 常用操作命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置node 不可调度</span></span><br><span class="line">kubectl cordon k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#驱逐node 上的pod</span></span><br><span class="line">kubectl drain k8s-node-1 --delete-local-data --force --ignore-daemonsets</span><br><span class="line"></span><br><span class="line"><span class="comment">#node 重新加入</span></span><br><span class="line">kubectl uncordon k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除node</span></span><br><span class="line">kubectl delete node k8s-node-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置master不进行调度</span></span><br><span class="line">kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置label</span></span><br><span class="line">kubectl label node k8s-master project=ipms-app</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置role标签</span></span><br><span class="line">kubectl label node k8s-node-01 node-role.kubernetes.io/node=</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除标签</span></span><br><span class="line">kubectl label nodes k8s-master mtype-</span><br></pre></td></tr></table></figure>
<h2 id="如何调整网络插件"><a href="#如何调整网络插件" class="headerlink" title="如何调整网络插件"></a>如何调整网络插件</h2><blockquote>
<p>calico网络插件调整，原本没使用IPIP模式，现在需要使用CrossSubnet</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先删除原本的calico插件</span></span><br><span class="line">kubectl delete <span class="_">-f</span> calico.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改calico.yaml，将CALICO_IPV4POOL_IPIP调整为CrossSubnet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#启用calico插件</span></span><br><span class="line">kubectl apply <span class="_">-f</span> calico.yaml</span><br></pre></td></tr></table></figure>
<h2 id="如何将kubeProxy的iptables修改为ipvs"><a href="#如何将kubeProxy的iptables修改为ipvs" class="headerlink" title="如何将kubeProxy的iptables修改为ipvs"></a>如何将kubeProxy的iptables修改为ipvs</h2><h3 id="1-加载内核模块"><a href="#1-加载内核模块" class="headerlink" title="1. 加载内核模块"></a>1. 加载内核模块</h3><p>查看内核模块是否加载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod|grep ip_vs</span><br></pre></td></tr></table></figure></p>
<p>如果没有加载，使用如下命令加载ipvs相关模块<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br></pre></td></tr></table></figure></p>
<h3 id="2-更改kube-proxy配置"><a href="#2-更改kube-proxy配置" class="headerlink" title="2. 更改kube-proxy配置"></a>2. 更改kube-proxy配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit configmap kube-proxy -n kube-system</span><br></pre></td></tr></table></figure>
<p>找到并修改如下部分的内容<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enableProfiling:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">10256</span></span><br><span class="line"><span class="attr">hostnameOverride:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">iptables:</span></span><br><span class="line"><span class="attr">  masqueradeAll:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  masqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">ipvs:</span></span><br><span class="line"><span class="attr">  excludeCIDRs:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  scheduler:</span> <span class="string">""</span>  <span class="comment">#=========================&gt; 为空表示默认的负载均衡算法为轮询， rr, wrr, lc, wlc, sh, dh, lblc...</span></span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">metricsBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">10249</span></span><br><span class="line"><span class="attr">mode:</span> iptables   <span class="comment">#=========================&gt; 修改此处为ipvs</span></span><br><span class="line"><span class="attr">nodePortAddresses:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">portRange:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">resourceContainer:</span> /kube-proxy</span><br><span class="line"><span class="attr">udpIdleTimeout:</span> <span class="number">250</span>ms</span><br></pre></td></tr></table></figure></p>
<p>编辑完，保存退出</p>
<h3 id="3-删除所有kube-proxy的pod"><a href="#3-删除所有kube-proxy的pod" class="headerlink" title="3. 删除所有kube-proxy的pod"></a>3. 删除所有kube-proxy的pod</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod $(kubectl get pod -n kube-system | grep kube-proxy | awk -F <span class="string">' '</span> <span class="string">'&#123;print $1&#125;'</span>) -n kube-system</span><br></pre></td></tr></table></figure>
<h3 id="4-查看kube-proxy的pod日志"><a href="#4-查看kube-proxy的pod日志" class="headerlink" title="4. 查看kube-proxy的pod日志"></a>4. 查看kube-proxy的pod日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs kube-proxy-xxx -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment">#I0308 02:16:02.980965       1 server_others.go:183] Using ipvs Proxier.</span></span><br><span class="line"><span class="comment">#W0308 02:16:02.991188       1 proxier.go:356] IPVS scheduler not specified, use rr by default</span></span><br><span class="line"><span class="comment">#I0308 02:16:02.991338       1 server_others.go:210] Tearing down inactive rules.</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.022123       1 server.go:448] Version: v1.11.6</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.028801       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_max' to 131072</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029030       1 conntrack.go:52] Setting nf_conntrack_max to 131072</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029208       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029296       1 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029639       1 config.go:102] Starting endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029682       1 controller_utils.go:1025] Waiting for caches to sync for endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029723       1 config.go:202] Starting service config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.029777       1 controller_utils.go:1025] Waiting for caches to sync for service config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.129930       1 controller_utils.go:1032] Caches are synced for endpoints config controller</span></span><br><span class="line"><span class="comment">#I0308 02:16:03.129931       1 controller_utils.go:1032] Caches are synced for service config controller</span></span><br><span class="line"></span><br><span class="line">看到有 Using ipvs Proxier 即表明切换成功.</span><br></pre></td></tr></table></figure>
<h3 id="5-安装ipvsadm"><a href="#5-安装ipvsadm" class="headerlink" title="5. 安装ipvsadm"></a>5. 安装ipvsadm</h3><p>使用ipvsadm查看ipvs相关规则，如果没有这个命令可以直接yum安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ipvsadm</span><br></pre></td></tr></table></figure></p>
<h2 id="kubelet-GC-机制及对应配置"><a href="#kubelet-GC-机制及对应配置" class="headerlink" title="kubelet GC 机制及对应配置"></a>kubelet GC 机制及对应配置</h2><blockquote>
<p>可通过kubelet gc的能力来自动清理无用的container以及image，从而来释放磁盘空间。kubelet会启动两个GC，分别回收container和image。其中container的回收频率为1分钟一次，而image回收频率为2分钟一次。</p>
</blockquote>
<h3 id="容器GC"><a href="#容器GC" class="headerlink" title="容器GC"></a>容器GC</h3><blockquote>
<p>退出的容器也会继续占用系统资源，比如还会在文件系统存储很多数据、docker 应用也要占用 CPU 和内存去维护这些容器。docker 本身并不会自动删除已经退出的容器，因此 kubelet 就负起了这个责任。kubelet 容器的回收是为了删除已经退出的容器以节省节点的空间，提升性能。容器 GC 虽然有利于空间和性能，但是删除容器也会导致错误现场被清理，不利于 debug 和错误定位，因此不建议把所有退出的容器都删除。因此容器的清理需要一定的策略，主要是告诉 kubelet 你要保存多少已经退出的容器。和容器 GC 有关的可以配置的 kubelet 启动参数 <code>/var/lib/kubelet/config.yaml</code></p>
</blockquote>
<ul>
<li><code>MinimumGCAge</code>：container 结束多长时间之后才能够被回收，默认是一分钟</li>
<li><code>MaxPerPodContainerCount</code>：每个 container 最终可以保存多少个已经结束的容器，默认是 1，设置为负数表示不做限制</li>
<li><code>MaxContainerCount</code>：节点上最多能保留多少个结束的容器，默认是 -1，表示不做限制</li>
</ul>
<p>gc的步骤如下：</p>
<ol>
<li>获取可以清除的容器，这些容器都是非活动的，并且创建时间比 gcPolicy.MinAge 要早</li>
<li>通过强制执行 gcPolicy.MaxPerPodContainer，为每个pod删除最老的死亡容器</li>
<li>通过强制执行 gcPolicy.MaxContainers 来移除最老的死亡容器</li>
<li>获取未准备好且不包含容器的可清除沙箱</li>
<li>移除可移除的沙箱</li>
</ol>
<h3 id="镜像GC"><a href="#镜像GC" class="headerlink" title="镜像GC"></a>镜像GC</h3><blockquote>
<p>镜像主要占用磁盘空间，虽然 docker 使用镜像分层可以让多个镜像共享存储，但是长时间运行的节点如果下载了很多镜像也会导致占用的存储空间过多。如果镜像导致磁盘被占满，会造成应用无法正常工作。docker 默认也不会做镜像清理，镜像一旦下载就会永远留在本地，除非被手动删除。其实很多镜像并没有被实际使用，这些不用的镜像继续占用空间是非常大的浪费，也是巨大的隐患，因此 kubelet 也会周期性地去清理镜像。镜像的清理和容器不同，是以占用的空间作为标准的，用户可以配置当镜像占据多大比例的存储空间时才进行清理。清理的时候会优先清理最久没有被使用的镜像，镜像被 pull 下来或者被容器使用都会更新它的最近使用时间。启动 kubelet 的时候，可以配置这些参数控制镜像清理的策略 <code>/var/lib/kubelet/config.yaml</code></p>
</blockquote>
<ul>
<li><code>imageMinimumGCAge</code>：镜像最少多久没有被使用才会被清理</li>
<li><code>imageGCHighThresholdPercent</code>：磁盘使用率的上限，当达到这一使用率的时候会触发镜像清理。默认值为 90%</li>
<li><code>imageGCLowThresholdPercent</code>：磁盘使用率的下限，每次清理直到使用率低于这个值或者没有可以清理的镜像了才会停止.默认值为 80%</li>
</ul>
<p>也就是说，默认情况下，当镜像占满所在盘 90% 容量的时候，kubelet 就会进行清理，一直到镜像占用率低于 80% 为止。</p>
<h2 id="使用kubeadm进行cluster升级"><a href="#使用kubeadm进行cluster升级" class="headerlink" title="使用kubeadm进行cluster升级"></a>使用kubeadm进行cluster升级</h2><blockquote>
<p>使用kubeadm 安装好后kubernetes，后续如何进行升级，升级 kubernetes集群，只能逐版本升级。只能从 1.12 升级到 1.13 而不能从 1.1 直接升级到 1.13，升级步骤： 1.11—&gt;1.12—&gt;1.13—&gt;1.14</p>
</blockquote>
<p>需要注意的地方是，kubernetes从1.11版本开始变化比较大，CoreDNS已作为默认DNS。<br><code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>中的环境变量被分配为三个文件：<code>/var/lib/kubelet/config.yaml</code> (其中cgroup驱动默认cgroupfs)、<code>/var/lib/kubelet/kubeadm-flags.env</code> (cgroup驱动默认systemd，优先权)、<code>/etc/sysconfig/kubelet</code><br>全新安装的kubernetes集群是有网络CNI配置的，升级安装的是没有CNI配置的，配置文件<code>/var/lib/kubelet/kubeadm-flags.env</code>。依赖的镜像tag抬头从<code>gcr.io/google_containers</code>变成<code>k8s.gcr.io</code>，升级基本使用gcr.io/google<em>containers，全新安装则使用<code>k8s.gcr.io</code>&gt;</em>&lt;</p>
<blockquote>
<p>从1.8开始为kube-proxy组件引入了IPVS模式，1.11版本开始正式支持IPVS，默认不开启，1.12以上版本默认开启，不开启则使用iptables模式</p>
</blockquote>
<h3 id="准备升级镜像"><a href="#准备升级镜像" class="headerlink" title="准备升级镜像"></a>准备升级镜像</h3><p>提前准备好升级所需的镜像image，并打成官方标准tag。<br>master节点需要所有镜像，node节点仅需要proxy、pause镜像</p>
<h4 id="在所有Master节点下载各版本镜像"><a href="#在所有Master节点下载各版本镜像" class="headerlink" title="在所有Master节点下载各版本镜像"></a>在所有Master节点下载各版本镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-scheduler:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-controller-manager:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-apiserver:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/etcd:3.2.24</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/pause:3.1</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/coredns:1.2.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-scheduler:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-controller-manager:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-apiserver:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/etcd:3.2.24</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/pause:3.1</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/coredns:1.2.6</span></span><br></pre></td></tr></table></figure>
<h4 id="Node节点下载各版本镜像"><a href="#Node节点下载各版本镜像" class="headerlink" title="Node节点下载各版本镜像"></a>Node节点下载各版本镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.12.7</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/kube-proxy:v1.13.5</span></span><br><span class="line"><span class="comment">#k8s.gcr.io/pause:3.1</span></span><br></pre></td></tr></table></figure>
<h3 id="1-11-6升级到1-12-7"><a href="#1-11-6升级到1-12-7" class="headerlink" title="1.11.6升级到1.12.7"></a>1.11.6升级到1.12.7</h3><h4 id="master节点（使用内部etcd）"><a href="#master节点（使用内部etcd）" class="headerlink" title="master节点（使用内部etcd）"></a>master节点（使用内部etcd）</h4><h5 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在所有master节点上执行以下命令</span></span><br><span class="line">kubectl get configmap -n kube-system kubeadm-config -o yaml &gt; kubeadm-config-cm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#编辑第一台master节点kubeadm-config-cm.yaml配置，修改以下参数</span></span><br><span class="line">api.advertiseAddress <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.advertise-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.initial-advertise-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.initial-cluster <span class="comment">#---&gt; 当前master ip地址和master主机名，例如："ip-172-31-92-42=https://172.31.92.42:2380,ip-172-31-89-186=https://172.31.89.186:2380,ip-172-31-90-42=https://172.31.90.42:2380"</span></span><br><span class="line"></span><br><span class="line">You must also pass an additional argument (initial-cluster-state: existing) to etcd.local.extraArgs.</span><br><span class="line"></span><br><span class="line"><span class="comment">#编辑其余master主机上的kubeadm-config-cm.yaml 修改ClusterConfiguration以下参数：</span></span><br><span class="line">etcd.local.extraArgs.advertise-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.initial-advertise-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-client-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line">etcd.local.extraArgs.listen-peer-urls <span class="comment">#---&gt; 当前master ip地址</span></span><br><span class="line"></span><br><span class="line">You must also modify the ClusterStatus to add a mapping <span class="keyword">for</span> the current host under apiEndpoints.</span><br></pre></td></tr></table></figure>
<h5 id="开始master升级"><a href="#开始master升级" class="headerlink" title="开始master升级"></a>开始master升级</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.12.7 kubelet-1.12.7 kubectl-1.12.7 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line">执行升级，确保上面已经准备好镜像image</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级第一台master节点</span></span><br><span class="line">kubectl apply <span class="_">-f</span> kubeadm-config-cm.yaml --force</span><br><span class="line">kubeadm upgrade apply v1.12.7</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级其余master节点</span></span><br><span class="line">kubectl annotate node &lt;nodename&gt; kubeadm.alpha.kubernetes.io/cri-socket=/var/run/dockershim.sock</span><br><span class="line">kubectl apply <span class="_">-f</span> kubeadm-config-cm.yaml --force</span><br><span class="line">kubeadm upgrade apply v1.12.7</span><br><span class="line"></span><br><span class="line"><span class="comment">#看到如下，说明升级成功</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.12.7"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="master节点（使用外部etcd）"><a href="#master节点（使用外部etcd）" class="headerlink" title="master节点（使用外部etcd）"></a>master节点（使用外部etcd）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在所有master节点上执行以下命令</span></span><br><span class="line">kubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.MasterConfiguration&#125; &gt; kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line">编辑kubeadm-config.yaml, 修改api.advertiseAddress值为当前master的ip地址</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.12.3 kubelet-1.12.7 kubectl-1.12.7 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line">执行升级，确保上面已经准备好镜像image</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#正式升级命令</span></span><br><span class="line">kubeadm upgrade apply v1.12.7 --config kubeadm-config.yaml</span><br><span class="line"><span class="comment">#看到如下，说明升级成功</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.12.3"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="node节点"><a href="#node节点" class="headerlink" title="node节点"></a>node节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级kubernetes的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon node其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain node其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#节点上执行，升级kubelet kubeadm kubectl</span></span><br><span class="line">yum install -y kubeadm-1.12.7 kubelet-1.12.7 kubectl-1.12.7 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级node节点的配置，配置文件/var/lib/kubelet/config.yaml中的cgroupDriver需要保持与docker的Cgroup Driver一致</span></span><br><span class="line">kubeadm upgrade node config --kubelet-version $(kubelet --version | cut <span class="_">-d</span> <span class="string">' '</span> <span class="_">-f</span> 2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon node节点名称</span><br></pre></td></tr></table></figure>
<h3 id="1-12-7升级到1-13-5"><a href="#1-12-7升级到1-13-5" class="headerlink" title="1.12.7升级到1.13.5"></a>1.12.7升级到1.13.5</h3><h4 id="master节点（使用内部etcd）-1"><a href="#master节点（使用内部etcd）-1" class="headerlink" title="master节点（使用内部etcd）"></a>master节点（使用内部etcd）</h4><h5 id="前置准备-1"><a href="#前置准备-1" class="headerlink" title="前置准备"></a>前置准备</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br><span class="line"></span><br><span class="line">1. 移除etcd相关部分</span><br><span class="line">2. 修改apiEndpoints值 Add an entry <span class="keyword">for</span> each of the additional control plane hosts，如下例子：</span><br><span class="line"><span class="comment">#      ip-10-40-40-14.cn-northwest-1.compute.internal:</span></span><br><span class="line"><span class="comment">#        advertiseAddress: 10.40.40.14</span></span><br><span class="line"><span class="comment">#        bindPort: 6443</span></span><br><span class="line"><span class="comment">#      ip-10-40-40-190.cn-northwest-1.compute.internal:</span></span><br><span class="line"><span class="comment">#        advertiseAddress: 10.40.40.190</span></span><br><span class="line"><span class="comment">#        bindPort: 6443</span></span><br><span class="line"><span class="comment">#      ip-10-40-40-195.cn-northwest-1.compute.internal:</span></span><br><span class="line"><span class="comment">#        advertiseAddress: 10.40.40.195</span></span><br><span class="line"><span class="comment">#        bindPort: 6443</span></span><br></pre></td></tr></table></figure>
<h5 id="开始升级master"><a href="#开始升级master" class="headerlink" title="开始升级master"></a>开始升级master</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.13.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级，确保上面已经准备好镜像image</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master 使用以下命令执行</span></span><br><span class="line">kubeadm upgrade apply v1.13.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#其余master使用以下命令执行</span></span><br><span class="line">kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master升级成功信息</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.13.5"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"><span class="comment">#其余master升级成功信息</span></span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.13.5 kubectl-1.13.5</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="master节点（使用外部etcd）-1"><a href="#master节点（使用外部etcd）-1" class="headerlink" title="master节点（使用外部etcd）"></a>master节点（使用外部etcd）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级docker的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon master其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain master其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#先升级kubeadm</span></span><br><span class="line">yum install -y kubeadm-1.13.5</span><br><span class="line"></span><br><span class="line">执行升级，确保上面已经准备好镜像image</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master使用以下命令执行：</span></span><br><span class="line">kubeadm upgrade apply v1.13.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#剩余master使用以下命令执行：</span></span><br><span class="line">kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一台master升级成功信息</span></span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to <span class="string">"v1.13.5"</span>. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets <span class="keyword">in</span> turn.</span><br><span class="line"><span class="comment">#其余master升级成功信息</span></span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br><span class="line"></span><br><span class="line"><span class="comment">#容器进行初始化，查看系统服务pod是否恢复正常运行。</span></span><br><span class="line">kubectl get pod -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.13.5 kubectl-1.13.5</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看master节点版本已成功升级</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon master节点名称</span><br></pre></td></tr></table></figure>
<h4 id="node节点-1"><a href="#node节点-1" class="headerlink" title="node节点"></a>node节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置升级kubernetes的节点为不可调度，并且将剩余的pod驱逐，通过kubectl get nodes命令看到该节点已被标记不可调度</span></span><br><span class="line">kubectl cordon node其中一台的节点名</span><br><span class="line"></span><br><span class="line"><span class="comment">#忽略了所有的daemonset的pod，并且将剩余的pod驱逐</span></span><br><span class="line">kubectl drain node其中一台的节点名 --ignore-daemonsets --delete-local-data</span><br><span class="line"></span><br><span class="line"><span class="comment">#节点上执行，升级kubelet kubeadm kubectl</span></span><br><span class="line">yum install -y kubeadm-1.13.5 kubelet-1.13.5 kubectl-1.13.5 kubernetes-cni-0.7.5-0</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级node节点的配置，配置文件/var/lib/kubelet/config.yaml中的cgroupDriver需要保持与docker的Cgroup Driver一致</span></span><br><span class="line">kubeadm upgrade node config --kubelet-version $(kubelet --version | cut <span class="_">-d</span> <span class="string">' '</span> <span class="_">-f</span> 2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启服务</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复调度</span></span><br><span class="line">kubectl uncordon node节点名称</span><br></pre></td></tr></table></figure>
<h3 id="1-13-5升级到1-14-0"><a href="#1-13-5升级到1-14-0" class="headerlink" title="1.13.5升级到1.14.0"></a>1.13.5升级到1.14.0</h3><h4 id="如果使用的外部的etcd则进行如下操作"><a href="#如果使用的外部的etcd则进行如下操作" class="headerlink" title="如果使用的外部的etcd则进行如下操作"></a>如果使用的外部的etcd则进行如下操作</h4><p>Modify <code>configmap/kubeadm-config</code> for this control plane node by removing the <strong>etcd</strong> section completely</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  ClusterConfiguration:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiServer:</span></span><br><span class="line"><span class="attr">      certSANs:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span></span><br><span class="line"><span class="attr">      extraArgs:</span></span><br><span class="line"><span class="attr">        authorization-mode:</span> Node,RBAC</span><br><span class="line"><span class="attr">      timeoutForControlPlane:</span> <span class="number">4</span>m0s</span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">    certificatesDir:</span> /etc/kubernetes/pki</span><br><span class="line"><span class="attr">    clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">    controlPlaneEndpoint:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span>:<span class="number">6443</span></span><br><span class="line"><span class="attr">    controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">    dns:</span></span><br><span class="line"><span class="attr">      type:</span> CoreDNS</span><br><span class="line"><span class="attr">    etcd:</span></span><br><span class="line"><span class="attr">      local:</span></span><br><span class="line"><span class="attr">        dataDir:</span> /var/lib/etcd</span><br><span class="line"><span class="attr">    imageRepository:</span> k8s.gcr.io</span><br><span class="line"><span class="attr">    kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">    kubernetesVersion:</span> v1<span class="number">.14</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    networking:</span></span><br><span class="line"><span class="attr">      dnsDomain:</span> cluster.local</span><br><span class="line"><span class="attr">      podSubnet:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">      serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span></span><br><span class="line"><span class="attr">    scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="attr">  ClusterStatus:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiEndpoints:</span></span><br><span class="line"><span class="attr">      k8s-master1:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.161</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master2:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.162</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master3:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.163</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">    kind:</span> ClusterStatus</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="string">"2019-05-21T10:08:03Z"</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"209870"</span></span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/kube-system/configmaps/kubeadm-config</span><br><span class="line"><span class="attr">  uid:</span> <span class="number">52419642</span><span class="bullet">-7</span>bb0<span class="bullet">-11e9</span><span class="bullet">-8</span>a89<span class="bullet">-0800270</span>fde1d</span><br></pre></td></tr></table></figure>
<h4 id="master节点"><a href="#master节点" class="headerlink" title="master节点"></a>master节点</h4><h5 id="升级第一个控制平面-mater1"><a href="#升级第一个控制平面-mater1" class="headerlink" title="升级第一个控制平面(mater1)"></a>升级第一个控制平面(mater1)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade apply v1.14.0</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.14.0-0 kubectl-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h5 id="升级其他控制平面-mater2、master3"><a href="#升级其他控制平面-mater2、master3" class="headerlink" title="升级其他控制平面(mater2、master3)"></a>升级其他控制平面(mater2、master3)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.14.0-0 kubectl-1.14.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#master2日志</span></span><br><span class="line">[upgrade] Reading configuration from the cluster...</span><br><span class="line">[upgrade] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[upgrade] Upgrading your Static Pod-hosted control plane instance to version <span class="string">"v1.14.0"</span>...</span><br><span class="line">Static pod: kube-apiserver-k8s-master2 <span class="built_in">hash</span>: ba03afd84d454d318c2cc6e3a6e23f53</span><br><span class="line">Static pod: kube-controller-manager-k8s-master2 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-scheduler-k8s-master2 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">[upgrade/etcd] Upgrading to TLS <span class="keyword">for</span> etcd</span><br><span class="line">Static pod: etcd-k8s-master2 <span class="built_in">hash</span>: d990ad5b88743835159168644453f90b</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/etcd.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: etcd-k8s-master2 <span class="built_in">hash</span>: d990ad5b88743835159168644453f90b</span><br><span class="line">Static pod: etcd-k8s-master2 <span class="built_in">hash</span>: e56ee6ac7c0de512a17ef30c3a44e01c</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=etcd</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"etcd"</span> upgraded successfully!</span><br><span class="line">[upgrade/etcd] Waiting <span class="keyword">for</span> etcd to become available</span><br><span class="line">[upgrade/staticpods] Writing new Static Pod manifests to <span class="string">"/etc/kubernetes/tmp/kubeadm-upgraded-manifests998233672"</span></span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-apiserver.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/kube-apiserver.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-apiserver-k8s-master2 <span class="built_in">hash</span>: ba03afd84d454d318c2cc6e3a6e23f53</span><br><span class="line">Static pod: kube-apiserver-k8s-master2 <span class="built_in">hash</span>: 94e207e0d84e092ae98dc64af5b870ba</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-apiserver</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-apiserver"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-controller-manager.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/kube-controller-manager.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-controller-manager-k8s-master2 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-controller-manager-k8s-master2 <span class="built_in">hash</span>: e45f10af1ae684722cbd74cb11807900</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-controller-manager</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-controller-manager"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-scheduler.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-45-09/kube-scheduler.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-scheduler-k8s-master2 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">Static pod: kube-scheduler-k8s-master2 <span class="built_in">hash</span>: 58272442e226c838b193bbba4c44091e</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-scheduler</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br><span class="line"></span><br><span class="line"><span class="comment">#master3 升级日志</span></span><br><span class="line">[upgrade] Reading configuration from the cluster...</span><br><span class="line">[upgrade] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[upgrade] Upgrading your Static Pod-hosted control plane instance to version <span class="string">"v1.14.0"</span>...</span><br><span class="line">Static pod: kube-apiserver-k8s-master3 <span class="built_in">hash</span>: 556e7d43da7a389c6b0b116ae5a46d97</span><br><span class="line">Static pod: kube-controller-manager-k8s-master3 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-scheduler-k8s-master3 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">[upgrade/etcd] Upgrading to TLS <span class="keyword">for</span> etcd</span><br><span class="line">[upgrade/staticpods] Writing new Static Pod manifests to <span class="string">"/etc/kubernetes/tmp/kubeadm-upgraded-manifests859456185"</span></span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-apiserver.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-48-13/kube-apiserver.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-apiserver-k8s-master3 <span class="built_in">hash</span>: 556e7d43da7a389c6b0b116ae5a46d97</span><br><span class="line">Static pod: kube-apiserver-k8s-master3 <span class="built_in">hash</span>: 1a94c94ecfa9f698cfc902<span class="built_in">fc</span>37c15be9</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-apiserver</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-apiserver"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-controller-manager.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-48-13/kube-controller-manager.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-controller-manager-k8s-master3 <span class="built_in">hash</span>: 0a9f25af4e4ad5e5427feb8295<span class="built_in">fc</span>055a</span><br><span class="line">Static pod: kube-controller-manager-k8s-master3 <span class="built_in">hash</span>: e45f10af1ae684722cbd74cb11807900</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-controller-manager</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-controller-manager"</span> upgraded successfully!</span><br><span class="line">[upgrade/staticpods] Moved new manifest to <span class="string">"/etc/kubernetes/manifests/kube-scheduler.yaml"</span> and backed up old manifest to <span class="string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-05-21-23-48-13/kube-scheduler.yaml"</span></span><br><span class="line">[upgrade/staticpods] Waiting <span class="keyword">for</span> the kubelet to restart the component</span><br><span class="line">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span><br><span class="line">Static pod: kube-scheduler-k8s-master3 <span class="built_in">hash</span>: 8cea5badbe1b177ab58353a73cdedd01</span><br><span class="line">Static pod: kube-scheduler-k8s-master3 <span class="built_in">hash</span>: 58272442e226c838b193bbba4c44091e</span><br><span class="line">[apiclient] Found 3 Pods <span class="keyword">for</span> label selector component=kube-scheduler</span><br><span class="line">[upgrade/staticpods] Component <span class="string">"kube-scheduler"</span> upgraded successfully!</span><br><span class="line">[upgrade] The control plane instance <span class="keyword">for</span> this node was successfully updated!</span><br></pre></td></tr></table></figure>
<h4 id="node节点-node1、node2、node3-…"><a href="#node节点-node1、node2、node3-…" class="headerlink" title="node节点(node1、node2、node3 …)"></a>node节点(node1、node2、node3 …)</h4><h5 id="升级kubeadm"><a href="#升级kubeadm" class="headerlink" title="升级kubeadm"></a>升级kubeadm</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.14.x-0 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<h5 id="驱除节点上的容器并让节点不可调度"><a href="#驱除节点上的容器并让节点不可调度" class="headerlink" title="驱除节点上的容器并让节点不可调度"></a>驱除节点上的容器并让节点不可调度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$WORKERNODE</span> --ignore-daemonsets</span><br><span class="line"><span class="comment">#$WORKERNODE: node1、node2、node3 ...</span></span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-配置"><a href="#升级-kubelet-配置" class="headerlink" title="升级 kubelet 配置"></a>升级 kubelet 配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm upgrade node config --kubelet-version v1.14.0</span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-与-kubectl"><a href="#升级-kubelet-与-kubectl" class="headerlink" title="升级 kubelet 与 kubectl"></a>升级 kubelet 与 kubectl</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.14.x-0 kubectl-1.14.x-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启kubelet</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#让节点重新在线</span></span><br><span class="line">kubectl uncordon <span class="variable">$WORKERNODE</span></span><br></pre></td></tr></table></figure>
<h5 id="验证集群的状态"><a href="#验证集群的状态" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure>
<h3 id="1-14-0升级到1-15-0"><a href="#1-14-0升级到1-15-0" class="headerlink" title="1.14.0升级到1.15.0"></a>1.14.0升级到1.15.0</h3><h4 id="如果使用的外部的etcd则进行如下操作-1"><a href="#如果使用的外部的etcd则进行如下操作-1" class="headerlink" title="如果使用的外部的etcd则进行如下操作"></a>如果使用的外部的etcd则进行如下操作</h4><p>Modify <code>configmap/kubeadm-config</code> for this control plane node by removing the <strong>etcd</strong> section completely</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  ClusterConfiguration:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiServer:</span></span><br><span class="line"><span class="attr">      certSANs:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span></span><br><span class="line"><span class="attr">      extraArgs:</span></span><br><span class="line"><span class="attr">        authorization-mode:</span> Node,RBAC</span><br><span class="line"><span class="attr">      timeoutForControlPlane:</span> <span class="number">4</span>m0s</span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta2</span><br><span class="line"><span class="attr">    certificatesDir:</span> /etc/kubernetes/pki</span><br><span class="line"><span class="attr">    clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">    controlPlaneEndpoint:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.238</span>:<span class="number">6443</span></span><br><span class="line"><span class="attr">    controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">    dns:</span></span><br><span class="line"><span class="attr">      type:</span> CoreDNS</span><br><span class="line"><span class="attr">    etcd:</span></span><br><span class="line"><span class="attr">      local:</span></span><br><span class="line"><span class="attr">        dataDir:</span> /var/lib/etcd</span><br><span class="line"><span class="attr">    imageRepository:</span> k8s.gcr.io</span><br><span class="line"><span class="attr">    kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">    kubernetesVersion:</span> v1<span class="number">.15</span><span class="number">.0</span></span><br><span class="line"><span class="attr">    networking:</span></span><br><span class="line"><span class="attr">      dnsDomain:</span> cluster.local</span><br><span class="line"><span class="attr">      serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span></span><br><span class="line"><span class="attr">    scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="attr">  ClusterStatus:</span> <span class="string">|</span><br><span class="line"></span><span class="attr">    apiEndpoints:</span></span><br><span class="line"><span class="attr">      k8s-master1:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.161</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master2:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.162</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">      k8s-master3:</span></span><br><span class="line"><span class="attr">        advertiseAddress:</span> <span class="number">10.164</span><span class="number">.178</span><span class="number">.163</span></span><br><span class="line"><span class="attr">        bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">    apiVersion:</span> kubeadm.k8s.io/v1beta2</span><br><span class="line"><span class="attr">    kind:</span> ClusterStatus</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="string">"2019-06-25T04:12:53Z"</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm-config</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"1337635"</span></span><br><span class="line"><span class="attr">  selfLink:</span> /api/v1/namespaces/kube-system/configmaps/kubeadm-config</span><br><span class="line"><span class="attr">  uid:</span> <span class="number">81334</span>a15<span class="bullet">-96</span>ff<span class="bullet">-11e9</span>-b14f<span class="bullet">-0800270</span>fde1d</span><br></pre></td></tr></table></figure>
<h4 id="master节点-1"><a href="#master节点-1" class="headerlink" title="master节点"></a>master节点</h4><h5 id="升级第一个控制平面-mater1-1"><a href="#升级第一个控制平面-mater1-1" class="headerlink" title="升级第一个控制平面(mater1)"></a>升级第一个控制平面(mater1)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade apply v1.15.0</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.15.0-0 kubectl-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h5 id="升级其他控制平面-mater2、master3-1"><a href="#升级其他控制平面-mater2、master3-1" class="headerlink" title="升级其他控制平面(mater2、master3)"></a>升级其他控制平面(mater2、master3)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade node</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.15.0-0 kubectl-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h4 id="node节点-node1、node2、node3-…-1"><a href="#node节点-node1、node2、node3-…-1" class="headerlink" title="node节点(node1、node2、node3 …)"></a>node节点(node1、node2、node3 …)</h4><h5 id="升级kubeadm-1"><a href="#升级kubeadm-1" class="headerlink" title="升级kubeadm"></a>升级kubeadm</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.15.x-0 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<h5 id="驱除节点上的容器并让节点不可调度-1"><a href="#驱除节点上的容器并让节点不可调度-1" class="headerlink" title="驱除节点上的容器并让节点不可调度"></a>驱除节点上的容器并让节点不可调度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$WORKERNODE</span> --ignore-daemonsets</span><br><span class="line"><span class="comment">#$WORKERNODE: node1、node2、node3 ...</span></span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-配置-1"><a href="#升级-kubelet-配置-1" class="headerlink" title="升级 kubelet 配置"></a>升级 kubelet 配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm upgrade node</span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-与-kubectl-1"><a href="#升级-kubelet-与-kubectl-1" class="headerlink" title="升级 kubelet 与 kubectl"></a>升级 kubelet 与 kubectl</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.15.x-0 kubectl-1.15.x-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启kubelet</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#让节点重新在线</span></span><br><span class="line">kubectl uncordon <span class="variable">$WORKERNODE</span></span><br></pre></td></tr></table></figure>
<h5 id="验证集群的状态-1"><a href="#验证集群的状态-1" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure>
<h3 id="1-15-0升级到1-16-0"><a href="#1-15-0升级到1-16-0" class="headerlink" title="1.15.0升级到1.16.0"></a>1.15.0升级到1.16.0</h3><h4 id="如果使用的外部的etcd则进行如下操作-2"><a href="#如果使用的外部的etcd则进行如下操作-2" class="headerlink" title="如果使用的外部的etcd则进行如下操作"></a>如果使用的外部的etcd则进行如下操作</h4><p>Modify <code>configmap/kubeadm-config</code> for this control plane node by removing the <strong>etcd</strong> section completely</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改当前节点的configmap/kubeadm-config ClusterConfiguration值</span></span><br><span class="line">kubectl edit configmap -n kube-system kubeadm-config</span><br></pre></td></tr></table></figure>
<h4 id="master节点-2"><a href="#master节点-2" class="headerlink" title="master节点"></a>master节点</h4><h5 id="升级第一个控制平面-mater1-2"><a href="#升级第一个控制平面-mater1-2" class="headerlink" title="升级第一个控制平面(mater1)"></a>升级第一个控制平面(mater1)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.16.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看版本</span></span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line"><span class="comment">#让master1下线</span></span><br><span class="line">kubectl drain <span class="variable">$MASTER</span> --ignore-daemonsets</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看升级计划</span></span><br><span class="line">kubeadm upgrade plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade apply v1.16.0</span><br><span class="line"></span><br><span class="line"><span class="comment">#master 上线</span></span><br><span class="line">kubectl uncordon <span class="variable">$MASTER</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.15.0-0 kubectl-1.15.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h5 id="升级其他控制平面-mater2、master3-2"><a href="#升级其他控制平面-mater2、master3-2" class="headerlink" title="升级其他控制平面(mater2、master3)"></a>升级其他控制平面(mater2、master3)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装对应的组件</span></span><br><span class="line">yum install -y kubeadm-1.16.0-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行升级</span></span><br><span class="line">kubeadm upgrade node</span><br><span class="line"></span><br><span class="line"><span class="comment">#确保容器AGE在一分钟以上，再重启服务</span></span><br><span class="line">yum install -y kubelet-1.16.0-0 kubectl-1.16.0-0 --disableexcludes=kubernetes</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<h4 id="node节点-node1、node2、node3-…-2"><a href="#node节点-node1、node2、node3-…-2" class="headerlink" title="node节点(node1、node2、node3 …)"></a>node节点(node1、node2、node3 …)</h4><h5 id="升级kubeadm-2"><a href="#升级kubeadm-2" class="headerlink" title="升级kubeadm"></a>升级kubeadm</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.16.x-0 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<h5 id="驱除节点上的容器并让节点不可调度-2"><a href="#驱除节点上的容器并让节点不可调度-2" class="headerlink" title="驱除节点上的容器并让节点不可调度"></a>驱除节点上的容器并让节点不可调度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$WORKERNODE</span> --ignore-daemonsets</span><br><span class="line"><span class="comment">#$WORKERNODE: node1、node2、node3 ...</span></span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-配置-2"><a href="#升级-kubelet-配置-2" class="headerlink" title="升级 kubelet 配置"></a>升级 kubelet 配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm upgrade node</span><br></pre></td></tr></table></figure>
<h5 id="升级-kubelet-与-kubectl-2"><a href="#升级-kubelet-与-kubectl-2" class="headerlink" title="升级 kubelet 与 kubectl"></a>升级 kubelet 与 kubectl</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.16.x-0 kubectl-1.16.x-0 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment">#重启kubelet</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#让节点重新在线</span></span><br><span class="line">kubectl uncordon <span class="variable">$WORKERNODE</span></span><br></pre></td></tr></table></figure>
<h5 id="验证集群的状态-2"><a href="#验证集群的状态-2" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure>
<h3 id="1-16-0升级到1-17-0"><a href="#1-16-0升级到1-17-0" class="headerlink" title="1.16.0升级到1.17.0"></a>1.16.0升级到1.17.0</h3><blockquote>
<p>同1.15.0到1.16.0的升级</p>
</blockquote>
<h3 id="1-17-0升级到1-18-0"><a href="#1-17-0升级到1-18-0" class="headerlink" title="1.17.0升级到1.18.0"></a>1.17.0升级到1.18.0</h3><blockquote>
<p>同1.15.0到1.16.0的升级</p>
</blockquote>
<p>参考：<br><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-12/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-12/</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-13/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-13/</a><br><a href="https://github.com/truongnh1992/upgrade-kubeadm-cluster" target="_blank" rel="external">https://github.com/truongnh1992/upgrade-kubeadm-cluster</a></p>
<h2 id="kubeadm-init-config-配置"><a href="#kubeadm-init-config-配置" class="headerlink" title="kubeadm init config 配置"></a>kubeadm init config 配置</h2><h3 id="kubeadm-v1-12"><a href="#kubeadm-v1-12" class="headerlink" title="kubeadm v1.12"></a>kubeadm v1.12</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3</span><br><span class="line"><span class="attr">kind:</span> InitConfiguration</span><br><span class="line"><span class="attr">apiEndpoint:</span></span><br><span class="line"><span class="attr">  advertiseAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">  bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="attr">- groups:</span></span><br><span class="line"><span class="attr">  - system:</span>bootstrappers:kubeadm:default-node-token</span><br><span class="line"><span class="attr">  token:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"><span class="attr">  ttl:</span> <span class="number">24</span>h0m0s</span><br><span class="line"><span class="attr">  usages:</span></span><br><span class="line"><span class="bullet">  -</span> signing</span><br><span class="line"><span class="bullet">  -</span> authentication</span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">  criSocket:</span> /var/run/containerd/containerd.sock</span><br><span class="line"><span class="attr">  name:</span> master</span><br><span class="line"><span class="attr">  taints:</span></span><br><span class="line"><span class="attr">  - effect:</span> NoSchedule</span><br><span class="line"><span class="attr">    key:</span> node-role.kubernetes.io/master</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/apis/kubeadm/v1alpha3/types.go</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">auditPolicy:</span></span><br><span class="line"><span class="attr">  logDir:</span> /var/log/kubernetes/audit</span><br><span class="line"><span class="attr">  logMaxAge:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">certificatesDir:</span> /etc/kubernetes/pki</span><br><span class="line"><span class="attr">clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">controllerManagerExtraArgs:</span></span><br><span class="line"><span class="attr">  address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">schedulerExtraArgs:</span></span><br><span class="line"><span class="attr">  address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line"><span class="attr">    dataDir:</span> /var/lib/etcd</span><br><span class="line"><span class="attr">    image:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">imageRepository:</span> k8s.gcr.io</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.12</span><span class="number">.0</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line"><span class="attr">  dnsDomain:</span> cluster.local</span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span></span><br><span class="line"><span class="attr">unifiedControlPlaneImage:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3</span><br><span class="line"><span class="attr">kind:</span> JoinConfiguration</span><br><span class="line"><span class="attr">apiEndpoint:</span></span><br><span class="line"><span class="attr">  advertiseAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">  bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">caCertPath:</span> /etc/kubernetes/pki/ca.crt</span><br><span class="line"><span class="attr">clusterName:</span> kubernetes</span><br><span class="line"><span class="attr">discoveryFile:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">discoveryTimeout:</span> <span class="number">5</span>m0s</span><br><span class="line"><span class="attr">discoveryToken:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"><span class="attr">discoveryTokenAPIServers:</span></span><br><span class="line"><span class="attr">- kube-apiserver:</span><span class="number">6443</span></span><br><span class="line"><span class="attr">discoveryTokenUnsafeSkipCAVerification:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">  criSocket:</span> /var/run/containerd/containerd.sock</span><br><span class="line"><span class="attr">  name:</span> master</span><br><span class="line"><span class="attr">tlsBootstrapToken:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"><span class="attr">token:</span> abcdef<span class="number">.0123456789</span>abcdef</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">bindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">clientConnection:</span></span><br><span class="line"><span class="attr">  acceptContentTypes:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  burst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  contentType:</span> application/vnd.kubernetes.protobuf</span><br><span class="line"><span class="attr">  kubeconfig:</span> /var/lib/kube-proxy/kubeconfig.conf</span><br><span class="line"><span class="attr">  qps:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">clusterCIDR:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">configSyncPeriod:</span> <span class="number">15</span>m0s</span><br><span class="line"><span class="attr">conntrack:</span></span><br><span class="line"><span class="attr">  max:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  maxPerCore:</span> <span class="number">32768</span></span><br><span class="line"><span class="attr">  min:</span> <span class="number">131072</span></span><br><span class="line"><span class="attr">  tcpCloseWaitTimeout:</span> <span class="number">1</span>h0m0s</span><br><span class="line"><span class="attr">  tcpEstablishedTimeout:</span> <span class="number">24</span>h0m0s</span><br><span class="line"><span class="attr">enableProfiling:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">10256</span></span><br><span class="line"><span class="attr">hostnameOverride:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">iptables:</span></span><br><span class="line"><span class="attr">  masqueradeAll:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  masqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">ipvs:</span></span><br><span class="line"><span class="attr">  excludeCIDRs:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  minSyncPeriod:</span> <span class="number">0</span>s</span><br><span class="line"><span class="attr">  scheduler:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  syncPeriod:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">metricsBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">10249</span></span><br><span class="line"><span class="attr">mode:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">nodePortAddresses:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">portRange:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">resourceContainer:</span> /kube-proxy</span><br><span class="line"><span class="attr">udpIdleTimeout:</span> <span class="number">250</span>ms</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> kubelet.config.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> KubeletConfiguration</span><br><span class="line"><span class="attr">address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line"><span class="attr">  anonymous:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  webhook:</span></span><br><span class="line"><span class="attr">    cacheTTL:</span> <span class="number">2</span>m0s</span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  x509:</span></span><br><span class="line"><span class="attr">    clientCAFile:</span> /etc/kubernetes/pki/ca.crt</span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line"><span class="attr">  mode:</span> Webhook</span><br><span class="line"><span class="attr">  webhook:</span></span><br><span class="line"><span class="attr">    cacheAuthorizedTTL:</span> <span class="number">5</span>m0s</span><br><span class="line"><span class="attr">    cacheUnauthorizedTTL:</span> <span class="number">30</span>s</span><br><span class="line"><span class="attr">cgroupDriver:</span> cgroupfs</span><br><span class="line"><span class="attr">cgroupsPerQOS:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">clusterDNS:</span></span><br><span class="line"><span class="bullet">-</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line"><span class="attr">clusterDomain:</span> cluster.local</span><br><span class="line"><span class="attr">configMapAndSecretChangeDetectionStrategy:</span> Watch</span><br><span class="line"><span class="attr">containerLogMaxFiles:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">containerLogMaxSize:</span> <span class="number">10</span>Mi</span><br><span class="line"><span class="attr">contentType:</span> application/vnd.kubernetes.protobuf</span><br><span class="line"><span class="attr">cpuCFSQuota:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">cpuCFSQuotaPeriod:</span> <span class="number">100</span>ms</span><br><span class="line"><span class="attr">cpuManagerPolicy:</span> none</span><br><span class="line"><span class="attr">cpuManagerReconcilePeriod:</span> <span class="number">10</span>s</span><br><span class="line"><span class="attr">enableControllerAttachDetach:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">enableDebuggingHandlers:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line"><span class="bullet">-</span> pods</span><br><span class="line"><span class="attr">eventBurst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">eventRecordQPS:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  imagefs.available: <span class="number">15</span>%</span><br><span class="line">  memory.available: <span class="number">100</span>Mi</span><br><span class="line">  nodefs.available: <span class="number">10</span>%</span><br><span class="line">  nodefs.inodesFree: <span class="number">5</span>%</span><br><span class="line"><span class="attr">evictionPressureTransitionPeriod:</span> <span class="number">5</span>m0s</span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">fileCheckFrequency:</span> <span class="number">20</span>s</span><br><span class="line"><span class="attr">hairpinMode:</span> promiscuous-bridge</span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">healthzPort:</span> <span class="number">10248</span></span><br><span class="line"><span class="attr">httpCheckFrequency:</span> <span class="number">20</span>s</span><br><span class="line"><span class="attr">imageGCHighThresholdPercent:</span> <span class="number">85</span></span><br><span class="line"><span class="attr">imageGCLowThresholdPercent:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">imageMinimumGCAge:</span> <span class="number">2</span>m0s</span><br><span class="line"><span class="attr">iptablesDropBit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">iptablesMasqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="attr">kubeAPIBurst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">kubeAPIQPS:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">makeIPTablesUtilChains:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">maxOpenFiles:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">110</span></span><br><span class="line"><span class="attr">nodeLeaseDurationSeconds:</span> <span class="number">40</span></span><br><span class="line"><span class="attr">nodeStatusUpdateFrequency:</span> <span class="number">10</span>s</span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="bullet">-999</span></span><br><span class="line"><span class="attr">podPidsLimit:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">port:</span> <span class="number">10250</span></span><br><span class="line"><span class="attr">registryBurst:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">registryPullQPS:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">resolvConf:</span> /etc/resolv.conf</span><br><span class="line"><span class="attr">rotateCertificates:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">runtimeRequestTimeout:</span> <span class="number">2</span>m0s</span><br><span class="line"><span class="attr">serializeImagePulls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">staticPodPath:</span> /etc/kubernetes/manifests</span><br><span class="line"><span class="attr">streamingConnectionIdleTimeout:</span> <span class="number">4</span>h0m0s</span><br><span class="line"><span class="attr">syncFrequency:</span> <span class="number">1</span>m0s</span><br><span class="line"><span class="attr">volumeStatsAggPeriod:</span> <span class="number">1</span>m0s</span><br></pre></td></tr></table></figure>
<h2 id="traefik如何设置路由"><a href="#traefik如何设置路由" class="headerlink" title="traefik如何设置路由"></a>traefik如何设置路由</h2><p><a href="http://yunke.science/2018/03/28/Ingress-traefik/" target="_blank" rel="external">http://yunke.science/2018/03/28/Ingress-traefik/</a></p>
<h2 id="etcd-3-2-x-gt-3-3-x"><a href="#etcd-3-2-x-gt-3-3-x" class="headerlink" title="etcd 3.2.x -&gt; 3.3.x"></a>etcd 3.2.x -&gt; 3.3.x</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先进行备份</span></span><br><span class="line">ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert=/etc/etcd/ssl/etcd-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem snapshot save snapshot.db</span><br><span class="line"></span><br><span class="line"><span class="comment">#停止etcd 状态</span></span><br><span class="line">systemctl stop etcd</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级</span></span><br><span class="line">wget http://mirror.centos.org/centos/7/extras/x86_64/Packages/etcd-3.3.11-2.el7.centos.x86_64.rpm</span><br><span class="line">yum localinstall -y etcd-3.2.22-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动</span></span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://www.jianshu.com/p/aa528c57f3be" target="_blank" rel="external">https://www.jianshu.com/p/aa528c57f3be</a></p>
<h2 id="Coredns-相关操作"><a href="#Coredns-相关操作" class="headerlink" title="Coredns 相关操作"></a>Coredns 相关操作</h2><h3 id="如何使用外部dns解析"><a href="#如何使用外部dns解析" class="headerlink" title="如何使用外部dns解析"></a>如何使用外部dns解析</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> ConfigMap</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> coredns</span><br><span class="line"><span class="attr">  namespace:</span> kube-system</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">      addonmanager.kubernetes.io/mode: EnsureExists</span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  Corefile:</span> <span class="string">|</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">            pods insecure</span><br><span class="line">            # 默认</span><br><span class="line">            upstream</span><br><span class="line">            # 用于解析外部主机主机（外部服务）</span><br><span class="line">            # upstream 114.114.114.114 223.5.5.5</span><br><span class="line">            fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        # 默认使用主机的</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        # 任何不在集群域内的查询将转发到预定义的解析器，默认：/etc/resolv.conf；</span><br><span class="line">        # 在coredns “Deployment”资源中“dnsPolicy“设置为”Default”，即提供dns服务的pod从所在节点继承/etc/resolv.conf，如果节点的上游解析地址与”upstream”一致，则设置任意一个参数即可</span><br><span class="line">        #proxy . 114.114.114.114 223.5.5.5</span><br><span class="line">        cache 30</span><br><span class="line">        loop</span><br><span class="line">        reload</span><br><span class="line">        loadbalance</span><br><span class="line">&#125;</span><br><span class="line">#自定义dns记录，对应kube-dns中的stubdomains；</span><br><span class="line">#每条记录，单独设置1各zone</span><br><span class="line">    patsnap.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        cache 30</span><br><span class="line">        proxy . 192.168.3.108</span><br><span class="line">    &#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="如何给kubernetes-service做cname解析"><a href="#如何给kubernetes-service做cname解析" class="headerlink" title="如何给kubernetes service做cname解析"></a>如何给kubernetes service做cname解析</h3><h4 id="coredns-配置文件"><a href="#coredns-配置文件" class="headerlink" title="coredns 配置文件"></a>coredns 配置文件</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">  "kind":</span> <span class="string">"ConfigMap"</span>,</span><br><span class="line"><span class="attr">  "apiVersion":</span> <span class="string">"v1"</span>,</span><br><span class="line"><span class="attr">  "metadata":</span> &#123;</span><br><span class="line"><span class="attr">    "name":</span> <span class="string">"coredns"</span>,</span><br><span class="line"><span class="attr">    "namespace":</span> <span class="string">"kube-system"</span>,</span><br><span class="line"><span class="attr">    "selfLink":</span> <span class="string">"/api/v1/namespaces/kube-system/configmaps/coredns"</span>,</span><br><span class="line"><span class="attr">    "uid":</span> <span class="string">"aa45aaab-4c79-11e9-9629-00163e022859"</span>,</span><br><span class="line"><span class="attr">    "resourceVersion":</span> <span class="string">"118616"</span>,</span><br><span class="line"><span class="attr">    "creationTimestamp":</span> <span class="string">"2019-03-22T08:08:24Z"</span></span><br><span class="line">  &#125;,</span><br><span class="line"><span class="attr">  "data":</span> &#123;</span><br><span class="line">    //格式化内容如下</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#格式化后的效果：</span></span><br><span class="line"><span class="attr">Corefile:</span></span><br><span class="line">.:<span class="number">53</span> &#123;</span><br><span class="line">    errors</span><br><span class="line">    health</span><br><span class="line">    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">       pods insecure</span><br><span class="line">       upstream</span><br><span class="line">       fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :<span class="number">9153</span></span><br><span class="line">    proxy . /etc/resolv.conf</span><br><span class="line">    cache <span class="number">30</span></span><br><span class="line">    autopath @kubernetes</span><br><span class="line">    reload</span><br><span class="line">    file /etc/coredns/patsnap.io.zone  nexus.patsnap.io &#123;</span><br><span class="line">        upstream</span><br><span class="line">    &#125;</span><br><span class="line">    file /etc/coredns/patsnap.local.zone  patsnap.local &#123;</span><br><span class="line">        upstream</span><br><span class="line">    &#125;</span><br><span class="line">    file /etc/coredns/patsnap.com.zone  nexus.patsnap.com &#123;</span><br><span class="line">        upstream</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">patsnap.com.zone:</span><br><span class="line">;@ 当前域名 nexus.patsnap.com</span><br><span class="line">;<span class="number">900</span> 表示ttl</span><br><span class="line">;ns<span class="bullet">-1304.</span>awsdns<span class="bullet">-35.</span>org. 表示根</span><br><span class="line">;icyboy.jiunile.com. 表示邮箱 icyboy@jiunile.com</span><br><span class="line">@                   <span class="number">900</span>     IN      SOA         ns<span class="bullet">-1304.</span>awsdns<span class="bullet">-35.</span>org. icyboy.jiunile.com. (</span><br><span class="line">    <span class="number">2017033001</span> ; serial</span><br><span class="line">    <span class="number">7200</span>       ; refresh (<span class="number">2</span> hour)</span><br><span class="line">    <span class="number">900</span>        ; retry (<span class="number">15</span> min)</span><br><span class="line">    <span class="number">1209600</span>    ; expire</span><br><span class="line">    <span class="number">86400</span>      ; min TTL (<span class="number">1</span> day)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">;@ 等价于nexus.patsnap.com CNAME 后面的域名必须写全</span><br><span class="line">@                            IN        CNAME       s-ops-maven-nexus.ops-qa.svc.cluster.local.</span><br><span class="line"></span><br><span class="line">patsnap.io.zone:</span><br><span class="line">@                   <span class="number">900</span>      IN      SOA         ns<span class="bullet">-196.</span>awsdns<span class="bullet">-24.</span>com. icyboy.jiunile.com. (</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">    <span class="number">7200</span></span><br><span class="line">    <span class="number">900</span></span><br><span class="line">    <span class="number">1209600</span></span><br><span class="line">    <span class="number">86400</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">;npm 等价于npm.nexus.patsnap.io.</span><br><span class="line">npm                         IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br><span class="line">pypi                        IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br><span class="line"></span><br><span class="line">patsnap.local.zone:</span><br><span class="line">;@ 当前域名 patsnap.local</span><br><span class="line">;<span class="number">900</span> 表示ttl</span><br><span class="line">;<span class="number">192.168</span><span class="number">.3</span><span class="number">.108</span>. 表示根</span><br><span class="line">;icyboy.jiunile.com. 表示邮箱 icyboy@jiunile.com</span><br><span class="line">@                   <span class="number">900</span>     IN      SOA         <span class="number">192.168</span><span class="number">.3</span><span class="number">.108</span>. icyboy.jiunile.com. (</span><br><span class="line">    <span class="number">2019092202</span></span><br><span class="line">    <span class="number">21600</span></span><br><span class="line">    <span class="number">3600</span></span><br><span class="line">    <span class="number">604800</span></span><br><span class="line">    <span class="number">86400</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">；npm.nexus 等价于 npm.nexus.patsnap.local.</span><br><span class="line">npm.nexus                   IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br><span class="line">pypi.nexus                  IN      CNAME       s-ops-npm-pypi-nexus.ops-qa.svc.cluster.local.</span><br></pre></td></tr></table></figure>
<h4 id="coredns-deployment"><a href="#coredns-deployment" class="headerlink" title="coredns deployment"></a>coredns deployment</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"volumes":</span> [</span><br><span class="line">          &#123;</span><br><span class="line"><span class="attr">            "name":</span> <span class="string">"config-volume"</span>,</span><br><span class="line"><span class="attr">            "configMap":</span> &#123;</span><br><span class="line"><span class="attr">              "name":</span> <span class="string">"coredns"</span>,</span><br><span class="line"><span class="attr">              "items":</span> [</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"Corefile"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"Corefile"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"patsnap.io.zone"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"patsnap.io.zone"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"patsnap.local.zone"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"patsnap.local.zone"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line"><span class="attr">                  "key":</span> <span class="string">"patsnap.com.zone"</span>,</span><br><span class="line"><span class="attr">                  "path":</span> <span class="string">"patsnap.com.zone"</span></span><br><span class="line">                &#125;</span><br><span class="line">              ],</span><br><span class="line"><span class="attr">              "defaultMode":</span> <span class="number">420</span></span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>
<p>参考链接：<br><a href="https://www.cnblogs.com/netonline/p/9935228.html" target="_blank" rel="external">https://www.cnblogs.com/netonline/p/9935228.html</a><br><a href="https://yuerblog.cc/2018/12/29/k8s-dns/#post-4008-_Toc533670192" target="_blank" rel="external">https://yuerblog.cc/2018/12/29/k8s-dns/#post-4008-_Toc533670192</a><br><a href="https://github.com/coredns/coredns.io/blob/master/content/blog/custom-dns-and-kubernetes.md" target="_blank" rel="external">https://github.com/coredns/coredns.io/blob/master/content/blog/custom-dns-and-kubernetes.md</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 服务质量 Qos 解析]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-pod-qos.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><code>QoS</code>是 Quality of Service 的缩写，即服务质量。为了实现资源被有效调度和分配的同时提高资源利用率，<code>kubernetes</code>针对不同服务质量的预期，通过 QoS（Quality of Service）来对 pod 进行服务质量管理。对于一个 pod 来说，服务质量体现在两个具体的指标：<code>CPU 和内存</code>。当节点上内存资源紧张时，kubernetes 会根据预先设置的不同 QoS 类别进行相应处理。</p>
<p>QoS 主要分为<code>Guaranteed</code>、<code>Burstable</code> 和 <code>Best-Effort</code>三类，优先级从高到低。</p>
<h2 id="Guaranteed-有保证的"><a href="#Guaranteed-有保证的" class="headerlink" title="Guaranteed(有保证的)"></a>Guaranteed(有保证的)</h2><p>对于绑定 CPU 和具有相对可预测性的工作负载（例如，用来处理请求的 Web 服务）来说，这是一个很好的 QoS 等级。属于该级别的pod有以下两种：</p>
<ul>
<li><strong>Pod中的所有容器都且仅设置了 CPU 和内存的 limits</strong></li>
<li><strong>pod中的所有容器都设置了 CPU 和内存的 requests 和 limits ，且单个容器内的requests==limits（requests不等于0）</strong><a id="more"></a>
<strong>示例1：pod中的所有容器都且仅设置了limits</strong><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>示例2： pod 中的所有容器都设置了 requests 和 limits，且单个容器内的<code>requests==limits</code></strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span>Mi</span><br></pre></td></tr></table></figure></p>
<p>容器foo和bar内resources的requests和limits均相等，该pod的QoS级别属于<code>Guaranteed</code>。</p>
<h2 id="Burstable-不稳定的"><a href="#Burstable-不稳定的" class="headerlink" title="Burstable(不稳定的)"></a>Burstable(不稳定的)</h2><p>这对短时间内需要消耗大量资源或者初始化过程很密集的工作负载非常有用，例如：用来构建 Docker 容器的 Worker 和运行未优化的 JVM 进程的容器都可以使用该 QoS 等级。<strong>pod中只要有一个容器的requests和limits的设置不相同</strong>，该pod的QoS即为<code>Burstable</code>。</p>
<p><strong>示例1：容器foo指定了resource，而容器bar未指定</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">10</span>m</span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br></pre></td></tr></table></figure></p>
<p><strong>示例2：容器foo设置了内存limits，而容器bar设置了CPU limits</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="number">1</span>Gi</span><br><span class="line"></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      limits:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span>m</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong><code>注意：若容器指定了requests而未指定limits，则limits的值等于节点resource的最大值；若容器指定了limits而未指定requests，则requests的值等于limits。</code></strong></p>
</blockquote>
<h2 id="Best-Effort-尽最大努力"><a href="#Best-Effort-尽最大努力" class="headerlink" title="Best-Effort(尽最大努力)"></a>Best-Effort(尽最大努力)</h2><p>这对于可中断和低优先级的工作负载非常有用，例如：迭代运行的幂等优化过程。<strong>如果Pod中所有容器的resources均未设置requests与limits</strong>，该pod的QoS即为<code>Best-Effort</code>。</p>
<p><strong>示例1：容器foo和容器bar均未设置requests和limits</strong><br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">  name:</span> foo</span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">  name:</span> bar</span><br><span class="line"><span class="attr">    resources:</span></span><br></pre></td></tr></table></figure></p>
<h2 id="根据QoS进行资源回收策略"><a href="#根据QoS进行资源回收策略" class="headerlink" title="根据QoS进行资源回收策略"></a>根据QoS进行资源回收策略</h2><p>Kubernetes 通过<code>cgroup</code>给pod设置QoS级别，当资源不足时先<code>kill</code>优先级低的 pod，在实际使用过程中，通过<code>OOM</code>分数值来实现，<code>OOM</code>分数值范围为0-1000。OOM 分数值根据<code>OOM_ADJ</code>参数计算得出。</p>
<p>对于<code>Guaranteed</code>级别的 Pod，OOM_ADJ参数设置成了-998，对于<code>Best-Effort</code>级别的 Pod，OOM_ADJ参数设置成了1000，对于<code>Burstable</code>级别的 Pod，OOM_ADJ参数取值从2到999。</p>
<p>对于 kuberntes 保留资源，比如kubelet，docker，OOM_ADJ参数设置成了-999，表示不会被OOM kill掉。<strong>OOM_ADJ参数设置的越大，计算出来的OOM分数越高，表明该pod优先级就越低，当出现资源竞争时会越早被kill掉</strong>，对于OOM_ADJ参数是-999的表示kubernetes永远不会因为OOM将其kill掉。</p>
<h2 id="QoS-pods被kill掉场景与顺序"><a href="#QoS-pods被kill掉场景与顺序" class="headerlink" title="QoS pods被kill掉场景与顺序"></a>QoS pods被kill掉场景与顺序</h2><ul>
<li><code>Best-Effort pods</code>：系统用完了全部内存时，该类型 pods 会最先被kill掉。</li>
<li><code>Burstable pods</code>：系统用完了全部内存，且没有 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。</li>
<li><code>Guaranteed pods</code>：系统用完了全部内存，且没有 Burstable 与 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。</li>
</ul>
<h2 id="QoS使用建议"><a href="#QoS使用建议" class="headerlink" title="QoS使用建议"></a>QoS使用建议</h2><p>如果资源充足，可将 QoS pods 类型均设置为<code>Guaranteed</code>。用计算资源换业务性能和稳定性，减少排查问题时间和成本。如果想更好的提高资源利用率，<strong>业务服务</strong>可以设置为<code>Guaranteed</code>，而其他服务根据重要程度可分别设置为<code>Burstable</code>或<code>Best-Effort</code>。</p>
<p>在搞清楚服务什么时候会出现故障以及为什么会出现故障之前，都不应该将其部署到生产环境中。可通过一些技术手段（<strong>负载压测等</strong>）来设置应用的资源 limits 和 requests。这将会为你的系统增加弹性能力和可预测性。</p>
<p>在测试过程中，记录服务失败时做了哪些操作是至关重要的。可以将发现的故障模式添加到相关的书籍和文档中，这对分类生产环境中出现的问题很有用。下面是我们在测试过程中发现的一些故障模式：</p>
<ul>
<li>内存缓慢增加</li>
<li>CPU 使用率达到 100%</li>
<li>响应时间太长</li>
<li>请求被丢弃</li>
<li>不同请求的响应时间差异很大</li>
</ul>
<p>你最好将这些发现都收集起来，以备不时之需，因为有一天它们可能会为你或团队节省一整天的时间。</p>
<h2 id="一些有用的工具"><a href="#一些有用的工具" class="headerlink" title="一些有用的工具"></a>一些有用的工具</h2><h3 id="Loader-io"><a href="#Loader-io" class="headerlink" title="Loader.io"></a>Loader.io</h3><p><a href="http://loader.io/" target="_blank" rel="external">Loader.io</a> 是一个在线负载测试工具，它允许你配置负载增加测试和负载不变测试，在测试过程中可视化应用程序的性能和负载，并能快速启动和停止测试。它也会保存测试结果的历史记录，因此在资源限制发生变化时很容易对结果进行比较。<br><img src="/images/k8s/loader.jpg" alt="Loader"></p>
<h3 id="Kubescope-cli"><a href="#Kubescope-cli" class="headerlink" title="Kubescope cli"></a>Kubescope cli</h3><p><a href="https://github.com/hharnisc/kubescope-cli" target="_blank" rel="external">Kubescope cli</a> 是一个可以运行在本地或 Kubernetes 中的工具，可直接从 Docker Daemon 中收集容器指标并可视化。和 <code>cAdvisor</code> 等其他集群指标收集服务一样， <code>kubescope cli</code> 收集指标的周期是 1 秒（而不是 10-15 秒）。如果周期是 10-15 秒，你可能会在测试期间错过一些引发性能瓶颈的问题。如果你使用 cAdvisor 进行测试，每次都要使用新的 Pod 作为测试对象，因为 Kubernetes 在超过资源限制时就会将 Pod 杀死，然后重新启动一个全新的 Pod。而 <code>kubescope cli</code> 就没有这方面的忧虑，它直接从 Docker Daemon 中收集容器指标（你可以自定义收集指标的时间间隔），并使用正则表达式来选择和过滤你想要显示的容器。<br><img src="/images/k8s/kubescope-cli.gif" alt="kubescope-cli"></p>
<p>来源：</p>
<ul>
<li>阳明的博客</li>
<li>Ryan Yang</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[理解 kubernetes 亲和性调度]]></title>
      <url>http://team.jiunile.com/blog/2019/03/k8s-affinity.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一般情况下我们部署的 Pod 是通过集群的自动调度策略来选择节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 Pod 的调度，比如我们内部的一些服务 gitlab 之类的也是跑在<code>Kubernetes</code>集群上的，我们就不希望对外的一些服务和内部的服务跑在同一个节点上了，害怕内部服务对外部的服务产生影响；但是有的时候我们的服务之间交流比较频繁，又希望能够将这两个服务的 Pod 调度到同一个的节点上。这就需要用到 Kubernetes 里面的一个概念：亲和性和反亲和性。</p>
<p>亲和性有分成节点亲和性(<code>nodeAffinity</code>)和 Pod 亲和性(<code>podAffinity</code>)。</p>
<h2 id="nodeSelector"><a href="#nodeSelector" class="headerlink" title="nodeSelector"></a>nodeSelector</h2><p>在了解亲和性之前，我们先来了解一个非常常用的调度方式：nodeSelector。我们知道label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的一个就是 service 通过匹配 label 去匹配 Pod 资源，而 Pod 的调度也可以根据节点的 label 来进行调度。</p>
<p>我们可以通过下面的命令查看我们的 node 的 label：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    147d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    67d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    127d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>现在我们先给节点node02增加一个com=youdianzhishi的标签，命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label nodes node02 com=youdianzhishi</span><br><span class="line">node <span class="string">"node02"</span> labeled</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>我们可以通过上面的<code>--show-labels</code>参数可以查看上述标签是否生效。当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 Pod 的<code>spec</code>字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label 即可。比如，下面的 Pod 我们要强制调度到 node02 这个节点上去，我们就可以使用 nodeSelector 来表示了：(node-selector-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> busybox-pod</span><br><span class="line"><span class="attr">  name:</span> test-busybox</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> sleep</span><br><span class="line"><span class="bullet">    -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    image:</span> busybox</span><br><span class="line"><span class="attr">    imagePullPolicy:</span> Always</span><br><span class="line"><span class="attr">    name:</span> test-busybox</span><br><span class="line"><span class="attr">  nodeSelector:</span></span><br><span class="line"><span class="attr">    com:</span> youdianzhishi</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以通过 describe 命令查看调度结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> node-selector-demo.yaml</span><br><span class="line">pod <span class="string">"test-busybox"</span> created</span><br><span class="line">$ kubectl describe pod <span class="built_in">test</span>-busybox</span><br><span class="line">Name:         <span class="built_in">test</span>-busybox</span><br><span class="line">Namespace:    default</span><br><span class="line">Node:         node02/10.151.30.63</span><br><span class="line">......</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  com=youdianzhishi</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason                 Age   From               Message</span><br><span class="line">  ----    ------                 ----  ----               -------</span><br><span class="line">  Normal  SuccessfulMountVolume  55s   kubelet, node02    MountVolume.SetUp succeeded <span class="keyword">for</span> volume <span class="string">"default-token-n9w2d"</span></span><br><span class="line">  Normal  Scheduled              54s   default-scheduler  Successfully assigned <span class="built_in">test</span>-busybox to node02</span><br><span class="line">  Normal  Pulling                54s   kubelet, node02    pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Pulled                 40s   kubelet, node02    Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Created                40s   kubelet, node02    Created container</span><br><span class="line">  Normal  Started                40s   kubelet, node02    Started container</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到 Events 下面的信息，我们的 Pod 通过默认的 default-scheduler 调度器被绑定到了node02节点。不过需要注意的是<code>nodeSelector</code>属于强制性的，如果我们的目标节点没有可用的资源，我们的 Pod 就会一直处于 Pending 状态，这就是<code>nodeSelector</code>的用法。</p>
<p>通过上面的例子我们可以感受到<code>nodeSelector</code>的方式比较直观，但是还够灵活，控制粒度偏大，接下来我们再和大家了解下更加灵活的方式：节点亲和性(<code>nodeAffinity</code>)。</p>
<h2 id="亲和性和反亲和性调度"><a href="#亲和性和反亲和性调度" class="headerlink" title="亲和性和反亲和性调度"></a>亲和性和反亲和性调度</h2><p>我们了解了 kubernetes 调度器的一个调度流程，我们知道默认的调度器在使用的时候，经过了 predicates 和 priorities 两个阶段，但是在实际的生产环境中，往往我们需要根据自己的一些实际需求来控制 pod 的调度，这就需要用到 nodeAffinity(节点亲和性)、podAffinity(pod 亲和性) 以及 podAntiAffinity(pod 反亲和性)。</p>
<p>亲和性调度可以分成软策略和硬策略两种方式:</p>
<ul>
<li><code>软策略</code>就是如果你没有满足调度要求的节点的话，pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓了的策略</li>
<li><code>硬策略</code>就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不干的策略。</li>
</ul>
<p>对于亲和性和反亲和性都有这两种规则可以设置： <code>preferredDuringSchedulingIgnoredDuringExecution</code>和<code>requiredDuringSchedulingIgnoredDuringExecution</code>，前面的就是软策略，后面的就是硬策略。</p>
<blockquote>
<p>这命名不觉得有点反人类吗？有点无语……</p>
</blockquote>
<h2 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h2><p>节点亲和性主要是用来控制 pod 要部署在哪些主机上，以及不能部署在哪些主机上的。它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。</p>
<p>比如现在我们用一个 Deployment 来管理3个 pod 副本，现在我们来控制下这些 pod 的调度，如下例子：（node-affinity-demo.yaml）<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">            nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">            - matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> kubernetes.io/hostname</span><br><span class="line"><span class="attr">                operator:</span> NotIn</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> node03</span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 软策略</span></span><br><span class="line"><span class="attr">          - weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            preference:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> com</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> youdianzhishi</span><br></pre></td></tr></table></figure></p>
<p>上面这个 pod 首先是要求不能运行在 node03 这个节点上，如果有个节点满足<code>com=youdianzhishi</code>的话就优先调度到这个节点上。</p>
<p>下面是我们测试的节点列表信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>可以看到 node02 节点有<code>com=youdianzhishi</code>这样的 label，按要求会优先调度到这个节点来的，现在我们来创建这个 pod，然后使用descirbe命令查看具体的调度情况是否满足我们的要求。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> node-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods <span class="_">-l</span> app=affinity -o wide</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7b4c946854-5gfln   1/1       Running   0          47s       10.244.4.214   node02</span><br><span class="line">affinity-7b4c946854<span class="_">-l</span>8b47   1/1       Running   0          47s       10.244.4.215   node02</span><br><span class="line">affinity-7b4c946854-r86p5   1/1       Running   0          47s       10.244.4.213   node02</span><br></pre></td></tr></table></figure></p>
<p>从结果可以看出 pod 都被部署到了 node02，其他节点上没有部署 pod，这里的匹配逻辑是 label 的值在某个列表中，现在<code>Kubernetes</code>提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<blockquote>
<p>如果<code>nodeSelectorTerms</code>下面有多个选项的话，满足任何一个条件就可以了；如果<code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 POD。</p>
</blockquote>
<h2 id="podAffinity"><a href="#podAffinity" class="headerlink" title="podAffinity"></a>podAffinity</h2><p>pod 亲和性主要解决 pod 可以和哪些 pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 pod 反亲和性主要是解决 pod 不能和哪些 pod 部署在同一个拓扑域中的问题，它们都是处理的 pod 与 pod 之间的关系，比如一个 pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 pod 在节点上了，那么我就不想和你待在同一个节点上。</p>
<p>由于我们这里只有一个集群，并没有区域或者机房的概念，所以我们这里直接使用主机名来作为拓扑域，把 pod 创建在同一个主机上面。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure></p>
<p>同样，还是针对上面的资源对象，我们来测试下 pod 的亲和性：（pod-affinity-demo.yaml）<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">          - labelSelector:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> app</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> busybox-pod</span><br><span class="line"><span class="attr">            topologyKey:</span> kubernetes.io/hostname</span><br></pre></td></tr></table></figure></p>
<p>上面这个例子中的 pod 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 pod：这个 pod 有一个<code>app=busybox-pod</code>的 label。</p>
<p>我们查看有标签<code>app=busybox-pod</code>的 pod 列表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=busybox-pod</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line"><span class="built_in">test</span>-busybox   1/1       Running   164        7d        10.244.4.205   node02</span><br></pre></td></tr></table></figure></p>
<p>我们看到这个 pod 运行在了 node02 的节点上面，所以按照上面的亲和性来说，上面我们部署的3个 pod 副本也应该运行在 node02 节点上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=affinity</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-564f9d7db9-lzzvq   1/1       Running   0          3m        10.244.4.216   node02</span><br><span class="line">affinity-564f9d7db9-p79cq   1/1       Running   0          3m        10.244.4.217   node02</span><br><span class="line">affinity-564f9d7db9-spfzs   1/1       Running   0          3m        10.244.4.218   node02</span><br></pre></td></tr></table></figure></p>
<p>如果我们把上面的 test-busybox 和 affinity 这个 Deployment 都删除，然后重新创建 affinity 这个资源，看看能不能正常调度呢：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete <span class="_">-f</span> node-selector-demo.yaml</span><br><span class="line">pod <span class="string">"test-busybox"</span> deleted</span><br><span class="line">$ kubectl delete <span class="_">-f</span> pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> deleted</span><br><span class="line">$ kubectl create <span class="_">-f</span> pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods -o wide <span class="_">-l</span> app=affinity</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP        NODE</span><br><span class="line">affinity-564f9d7db9-fbc8w   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-n8gcf   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-qc7x6   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到处于<code>Pending</code>状态了，这是因为现在没有一个节点上面拥有<code>busybox-pod</code>这个 label 的 pod，而上面我们的调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 test-busybox 这个 pod 调度到 node03 这个节点上，看看上面的 affinity 的3个副本会不会也被调度到 node03 这个节点上去？</p>
<p>我们这个地方使用的是<code>kubernetes.io/hostname</code>这个拓扑域，意思就是我们当前调度的 pod 要和目标的 pod 处于同一个主机上面，因为要处于同一个拓扑域下面，为了说明这个问题，我们把拓扑域改成<code>beta.kubernetes.io/os</code>，同样的我们当前调度的 pod 要和目标的 pod 处于同一个拓扑域中，目标的 pod 是不是拥有<code>beta.kubernetes.io/os=linux</code>的标签，而我们这里3个节点都有这样的标签，这也就意味着我们3个节点都在同一个拓扑域中，所以我们这里的 pod 可能会被调度到任何一个节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7d86749984-glkhz                 1/1       Running     0          3m        10.244.2.16    node03</span><br><span class="line">affinity-7d86749984-h4fb9                 1/1       Running     0          3m        10.244.4.219   node02</span><br><span class="line">affinity-7d86749984-tj7k2                 1/1       Running     0          3m        10.244.2.14    node03</span><br></pre></td></tr></table></figure></p>
<h2 id="podAntiAffinity"><a href="#podAntiAffinity" class="headerlink" title="podAntiAffinity"></a>podAntiAffinity</h2><p>这就是 pod 亲和性的用法，而 pod 反亲和性则是反着来的，比如一个节点上运行了某个 pod，那么我们的 pod 则希望被调度到其他节点上去，同样我们把上面的 podAffinity 直接改成 podAntiAffinity，(pod-antiaffinity-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> affinity</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> affinity</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> affinity</span><br><span class="line"><span class="attr">        role:</span> test</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> nginxweb</span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line"><span class="attr">          - labelSelector:</span></span><br><span class="line"><span class="attr">              matchExpressions:</span></span><br><span class="line"><span class="attr">              - key:</span> app</span><br><span class="line"><span class="attr">                operator:</span> In</span><br><span class="line"><span class="attr">                values:</span></span><br><span class="line"><span class="bullet">                -</span> busybox-pod</span><br><span class="line"><span class="attr">            topologyKey:</span> kubernetes.io/hostname</span><br></pre></td></tr></table></figure></p>
<p>这里的意思就是如果一个节点上面有一个<code>app=busybox-pod</code>这样的 pod 的话，那么我们的 pod 就别调度到这个节点上面来，上面我们把<code>app=busybox-pod</code>这个 pod 固定到了 node03 这个节点上面来，所以正常来说我们这里的 pod 不会出现在 node03 节点上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> pod-antiaffinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"affinity"</span> created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-bcbd8854f-br8z8                  1/1       Running     0          5s        10.244.4.222   node02</span><br><span class="line">affinity-bcbd8854f-cdffh                  1/1       Running     0          5s        10.244.4.223   node02</span><br><span class="line">affinity-bcbd8854f-htb52                  1/1       Running     0          5s        10.244.4.224   node02</span><br><span class="line"><span class="built_in">test</span>-busybox                              1/1       Running     0          23m       10.244.2.10    node03</span><br></pre></td></tr></table></figure></p>
<p>这就是 pod 反亲和性的用法。</p>
<h2 id="污点（taints）与容忍（tolerations）"><a href="#污点（taints）与容忍（tolerations）" class="headerlink" title="污点（taints）与容忍（tolerations）"></a>污点（taints）与容忍（tolerations）</h2><p>对于<code>nodeAffinity</code>无论是硬策略还是软策略方式，都是调度 pod 到预期节点上，而<code>Taints</code>恰好与之相反，如果一个节点标记为 Taints ，除非 pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 pod。</p>
<p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 pod，则污点就很有用了，pod 不会再被调度到 taint 标记过的节点。我们使用<code>kubeadm</code>搭建的集群默认就给 master 节点添加了一个污点标记，所以我们看到我们平时的 pod 都没有被调度到 master 上去：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node master</span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/hostname=master</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">......</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      <span class="literal">false</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以使用上面的命令查看 master 节点的信息，其中有一条关于 Taints 的信息：node-role.kubernetes.io/master:NoSchedule，就表示给 master 节点打了一个污点的标记，其中影响的参数是<code>NoSchedule</code>，表示 pod 不会被调度到标记为 taints 的节点，除了 NoSchedule 外，还有另外两个选项：</p>
<ul>
<li><code>PreferNoSchedule</code>：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去</li>
<li><code>NoExecute</code>：该选项意味着一旦 Taint 生效，如该节点内正在运行的 pod 没有对应 Tolerate 设置，会直接被逐出</li>
</ul>
<p>污点 taint 标记节点的命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes node02 <span class="built_in">test</span>=node02:NoSchedule</span><br><span class="line">node <span class="string">"node02"</span> tainted</span><br></pre></td></tr></table></figure></p>
<p>上面的命名将 node02 节点标记为了污点，影响策略是 NoSchedule，只会影响新的 pod 调度，如果仍然希望某个 pod 调度到 taint 节点上，则必须在 Spec 中做出<code>Toleration</code>定义，才能调度到该节点，比如现在我们想要将一个 pod 调度到 master 节点：(taint-demo.yaml)<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> apps/v1beta1</span><br><span class="line"><span class="attr">kind:</span> Deployment</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> taint</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> taint</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> taint</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> nginx</span><br><span class="line"><span class="attr">        image:</span> nginx:<span class="number">1.7</span><span class="number">.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> http</span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node-role.kubernetes.io/master"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>由于 master 节点被标记为了污点节点，所以我们这里要想 pod 能够调度到 master 节点去，就需要增加容忍的声明：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="attr">- key:</span> <span class="string">"node-role.kubernetes.io/master"</span></span><br><span class="line"><span class="attr">  operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">  effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>然后创建上面的资源，查看结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> taint-demo.yaml</span><br><span class="line">deployment.apps <span class="string">"taint"</span> created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS             RESTARTS   AGE       IP             NODE</span><br><span class="line">......</span><br><span class="line">taint-845d8bb4fb-57mhm                    1/1       Running            0          1m        10.244.4.247   node02</span><br><span class="line">taint-845d8bb4fb-bbvmp                    1/1       Running            0          1m        10.244.0.33    master</span><br><span class="line">taint-845d8bb4fb-zb78x                    1/1       Running            0          1m        10.244.4.246   node02</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到有一个 pod 副本被调度到了 master 节点，这就是容忍的使用方法。</p>
<p>对于 tolerations 属性的写法，其中的 key、value、effect 与 Node 的 Taint 设置需保持一致， 还有以下几点说明：</p>
<ul>
<li>1.如果 operator 的值是 Exists，则 value 属性可省略</li>
<li>2.如果 operator 的值是 Equal，则表示其 key 与 value 之间的关系是 equal(等于)</li>
<li>3.如果不指定 operator 属性，则默认值为 Equal</li>
</ul>
<p>另外，还有两个特殊值：</p>
<ul>
<li>1.空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也是是能容忍所有 node 的所有 Taints</li>
<li>2.空的 effect 匹配所有的 effect</li>
</ul>
<p>最后，如果我们要取消节点的污点标记，可以使用下面的命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes node02 <span class="built_in">test</span>-</span><br><span class="line">node <span class="string">"node02"</span> untainted</span><br></pre></td></tr></table></figure></p>
<p>这就是污点和容忍的使用方法。                                                                                             </p>
<p>来源：www.qikqiak.com</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在阿里云使用Kubeadm 1.13.x 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2019/02/k8s-kubeadm-ha-1-13-x.html</url>
      <content type="html"><![CDATA[<h2 id="前言（坑）"><a href="#前言（坑）" class="headerlink" title="前言（坑）"></a>前言（坑）</h2><p><strong>负载均衡问题</strong></p>
<ul>
<li>阿里不支持LVS，没有vip可用，必须通过申请SLB来固定VIP</li>
<li>因Kubernetes apiserver为https协议，阿里SLB中能负载均衡HTTPS的只有TCP方式，但TCP协议不能转发到发起主机（<code>apiserver 需要有回环路访问，简单说就是自己给自己发请求</code>）</li>
</ul>
<p>为了解决kubernets apiserver高可用问题，故用以下方式来解决：</p>
<ul>
<li>申请一个内网的SLB（获取VIP），8443为监听端口，6443为apiserver的后端端口</li>
<li>在每台master机器上搭建keepalived+haproxy，VIP 用SLB的VIP</li>
</ul>
<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">申请阿里云的SLB获取到</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.13.3-0 kubelet-1.13.3-0 kubectl-1.13.3-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy"><a href="#搭建keepalived-haproxy" class="headerlink" title="搭建keepalived + haproxy"></a>搭建keepalived + haproxy</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.183']"</span> \ <span class="comment">#master-1地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.184']"</span> \ <span class="comment">#master-2地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=host --cap-add=NET_ADMIN \</span><br><span class="line"><span class="_">-e</span> KEEPALIVED_INTERFACE=eth0 \  <span class="comment">#网卡名称</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_VIRTUAL_IPS=<span class="string">"#PYTHON2BASH:['172.19.170.100']"</span> \  <span class="comment">#VIP地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_UNICAST_PEERS=<span class="string">"#PYTHON2BASH:['172.19.170.185']"</span> \ <span class="comment">#master-3地址</span></span><br><span class="line"><span class="_">-e</span> KEEPALIVED_PASSWORD=k8s \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line"><span class="_">-d</span> osixia/keepalived:2.0.12</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/haproxy</span><br><span class="line">cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 <span class="built_in">local</span>0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:k8s</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  <span class="comment">#maxconn 50000</span></span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server master-1 172.19.170.183:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-2 172.19.170.184:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server master-3 172.19.170.185:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run <span class="_">-d</span> --name k8s-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/<span class="built_in">local</span>/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="配置阿里云-SLB"><a href="#配置阿里云-SLB" class="headerlink" title="配置阿里云 SLB"></a>配置阿里云 SLB</h3><blockquote>
<p>在阿里云SLB 管理界面添加8443监听端口，使用<code>TCP</code>协议，后端服务器选择一台你即将初始化的master机器，后端服务器端口为6443，健康检查默认配置即可，保存配置。此时你的SLB是不进行工作的，因为后端服务器的6443端口还未监听。</p>
</blockquote>
<h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.13</span><span class="number">.3</span></span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.100</span>:<span class="number">8443</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line"><span class="attr">  certSANs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.183</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.184</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.185</span></span><br><span class="line"><span class="bullet">  -</span> <span class="number">172.19</span><span class="number">.170</span><span class="number">.100</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">16</span></span><br><span class="line"><span class="attr">imageRepository:</span> registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line"><span class="attr">kind:</span> KubeProxyConfiguration</span><br><span class="line"><span class="attr">mode:</span> iptables</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:<span class="number">3.1</span>  k8s.gcr.io/pause:<span class="number">3.1</span></span><br><span class="line">kubeadm init --config $HOME/kubeadm<span class="bullet">-1.</span>yaml</span><br><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -f /etc/kubernetes/admin.conf $&#123;HOME&#125;/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="初始化其它master节点"><a href="#初始化其它master节点" class="headerlink" title="初始化其它master节点"></a>初始化其它master节点</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化master-2</span></span><br><span class="line">ssh master-2 <span class="string">"kubeadm reset -f </span><br><span class="line">rm -rf /etc/kubernetes/pki/</span><br><span class="line">mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化master-3</span></span><br><span class="line">ssh master-3 <span class="string">"kubeadm reset -f </span><br><span class="line">rm -rf /etc/kubernetes/pki/ </span><br><span class="line">mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件到master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件到master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取加入口令</span></span><br><span class="line">JOIN_CMD=`kubeadm token create --print-join-command`</span><br><span class="line"></span><br><span class="line"><span class="comment">#将master-2 加入</span></span><br><span class="line">ssh master-2 <span class="string">"<span class="variable">$&#123;JOIN_CMD&#125;</span> --experimental-control-plane"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将master-3 加入</span></span><br><span class="line">ssh master-3 <span class="string">"<span class="variable">$&#123;JOIN_CMD&#125;</span> --experimental-control-plane"</span></span><br></pre></td></tr></table></figure>
<h3 id="再次配置阿里云-SLB"><a href="#再次配置阿里云-SLB" class="headerlink" title="再次配置阿里云 SLB"></a>再次配置阿里云 SLB</h3><blockquote>
<p>同样进入阿里云SLB管理界面，将其余的两台master机器加入到后端服务器中。这样你的apiserver 就高可用了</p>
</blockquote>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.3.2</span><br><span class="line">docker pull quay.io/calico/node:v3.3.2</span><br><span class="line">docker pull quay.io/calico/cni:v3.3.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.13.3</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.13.3</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.13.3</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.13.3</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.13.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes 调度器介绍]]></title>
      <url>http://team.jiunile.com/blog/2019/02/k8s-scheduler.html</url>
      <content type="html"><![CDATA[<h3 id="kube-scheduler-简介"><a href="#kube-scheduler-简介" class="headerlink" title="kube-scheduler 简介"></a>kube-scheduler 简介</h3><blockquote>
<p><code>kube-scheduler</code>是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。</p>
</blockquote>
<h3 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h3><p>默认情况下，kube-scheduler 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。</p>
<p><code>kube-scheduler</code> 是 kubernetes 的调度器，它的主要作用就是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上去，是一个独立的二进制程序，启动之后会一直监听 API Server，获取到 PodSpec.NodeName 为空的 Pod，对每个 Pod 都会创建一个 binding。<br><img src="/images/k8s/kube-scheduler-structrue.jpg" alt="K8s scheduler structure"><br><a id="more"></a><br>这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：</p>
<ul>
<li>如何保证全部的节点调度的公平性？要知道并不是说有节点资源配置都是一样的</li>
<li>如何保证每个节点都能被分配资源？</li>
<li>集群资源如何能够被高效利用？</li>
<li>集群资源如何才能被最大化使用？</li>
<li>如何保证 Pod 调度的性能和效率？</li>
<li>用户是否可以根据自己的实际需求定制自己的调度策略？</li>
</ul>
<p>考虑到实际环境中的各种复杂情况，kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</p>
<p>kubernetes 调度器的源码位于 kubernetes/pkg/scheduler 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/scheduler</span><br><span class="line">-- scheduler.go         <span class="comment">#调度相关的具体实现</span></span><br><span class="line">|-- algorithm</span><br><span class="line">|   |-- predicates      <span class="comment">#节点筛选策略</span></span><br><span class="line">|   |-- priorities      <span class="comment">#节点打分策略</span></span><br><span class="line">|-- algorithmprovider</span><br><span class="line">|   |-- defaults        <span class="comment">#定义默认的调度器</span></span><br></pre></td></tr></table></figure></p>
<p>其中 Scheduler 创建和运行的核心程序，对应的代码在 pkg/scheduler/scheduler.go，如果要查看<code>kube-scheduler</code>的入口程序，对应的代码在 cmd/kube-scheduler/scheduler.go。</p>
<p>调度主要分为以下几个部分：</p>
<ul>
<li>首先是预选过程，过滤掉不满足条件的节点，这个过程称为<code>Predicates</code></li>
<li>然后是优选过程，对通过的节点按照优先级排序，称之为<code>Priorities</code></li>
<li>最后从中选择优先级最高的节点，如果中间任何一步骤有错误，就直接返回错误</li>
</ul>
<p><code>Predicates</code>阶段首先遍历全部节点，过滤掉不满足条件的节点，属于强制性规则，这一阶段输出的所有满足要求的 Node 将被记录并作为第二阶段的输入，如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试。</p>
<blockquote>
<p>所以我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。</p>
</blockquote>
<p><code>Priorities</code>阶段即再次对节点进行筛选，如果有多个节点都满足条件的话，那么系统会按照节点的优先级(priorites)大小对节点进行排序，最后选择优先级最高的节点来部署 Pod 应用。</p>
<p>下面是调度过程的简单示意图：<br><img src="/images/k8s/kube-scheduler-filter.jpg" alt="K8s scheduler filter"></p>
<p>更详细的流程是这样的：</p>
<ul>
<li>首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源</li>
<li>API Server 收到用户请求后，存储相关数据到 etcd 数据库中</li>
<li>调度器监听 API Server 查看为调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点，这个分配过程就是我们上面提到的两个阶段：<ul>
<li>预选阶段(Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉</li>
<li>优选阶段(Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本分布到不同的主机上，使用最低负载的主机等等策略</li>
</ul>
</li>
<li>经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 binding 操作，然后将结果存储到 etcd 中</li>
<li>最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作</li>
</ul>
<p>其中<code>Predicates</code>过滤有一系列的算法可以使用，我们这里简单列举几个：</p>
<ul>
<li>PodFitsResources：节点上剩余的资源是否大于 Pod 请求的资源</li>
<li>PodFitsHost：如果 Pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配</li>
<li>PodFitsHostPorts：节点上已经使用的 port 是否和 Pod 申请的 port 冲突</li>
<li>PodSelectorMatches：过滤掉和 Pod 指定的 label 不匹配的节点</li>
<li>NoDiskConflict：已经 mount 的 volume 和 Pod 指定的 volume 不冲突，除非它们都是只读的</li>
<li>CheckNodeDiskPressure：检查节点磁盘空间是否符合要求</li>
<li>CheckNodeMemoryPressure：检查节点内存是否够用</li>
</ul>
<p>除了这些过滤算法之外，还有一些其他的算法，更多更详细的我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/predicates.go。</p>
<p>而<code>Priorities</code>优先级是由一系列键值对组成的，键是该优先级的名称，值是它的权重值，同样，我们这里给大家列举几个具有代表性的选项：</p>
<ul>
<li>LeastRequestedPriority：通过计算 CPU 和内存的使用率来决定权重，使用率越低权重越高，当然正常肯定也是资源是使用率越低权重越高，能给别的 Pod 运行的可能性就越大</li>
<li>SelectorSpreadPriority：为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高</li>
<li>ImageLocalityPriority：就是如果在某个节点上已经有要使用的镜像节点了，镜像总大小值越大，权重就越高</li>
<li>NodeAffinityPriority：这个就是根据节点的亲和性来计算一个权重值，后面我们会详细讲解亲和性的使用方法</li>
</ul>
<p>除了这些策略之外，还有很多其他的策略，同样我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/ 了解更多信息。每一个优先级函数会返回一个0-10的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn)</span><br></pre></td></tr></table></figure></p>
<h3 id="自定义调度"><a href="#自定义调度" class="headerlink" title="自定义调度"></a>自定义调度</h3><p>上面就是 kube-scheduler 默认调度的基本流程，除了使用默认的调度器之外，我们也可以自定义调度策略。</p>
<h4 id="调度器扩展"><a href="#调度器扩展" class="headerlink" title="调度器扩展"></a>调度器扩展</h4><p><code>kube-scheduler</code>在启动的时候可以通过 <code>--policy-config-file</code>参数来指定调度策略文件，我们可以根据我们自己的需要来组装<code>Predicates</code>和<code>Priority</code>函数。选择不同的过滤函数和优先级函数、控制优先级函数的权重、调整过滤函数的顺序都会影响调度过程。</p>
<p>下面是官方的 Policy 文件示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"kind"</span> : <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"apiVersion"</span> : <span class="string">"v1"</span>,</span><br><span class="line">    <span class="string">"predicates"</span> : [</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"PodFitsHostPorts"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"PodFitsResources"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"NoDiskConflict"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"NoVolumeZoneConflict"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"MatchNodeSelector"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"HostName"</span>&#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"priorities"</span> : [</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"LeastRequestedPriority"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"BalancedResourceAllocation"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"ServiceSpreadingPriority"</span>, <span class="string">"weight"</span> : 1&#125;,</span><br><span class="line">        &#123;<span class="string">"name"</span> : <span class="string">"EqualPriority"</span>, <span class="string">"weight"</span> : 1&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="多调度器"><a href="#多调度器" class="headerlink" title="多调度器"></a>多调度器</h4><p>如果默认的调度器不满足要求，还可以部署自定义的调度器。并且，在整个集群中还可以同时运行多个调度器实例，通过<code>podSpec.schedulerName</code> 来选择使用哪一个调度器（默认使用内置的调度器）。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  schedulerName:</span> my-scheduler  <span class="comment"># 选择使用自定义调度器 my-scheduler</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx:<span class="number">1.10</span></span><br></pre></td></tr></table></figure></p>
<p>要开发我们自己的调度器也是比较容易的，比如我们这里的 my-scheduler:</p>
<ul>
<li>首先需要通过指定的 API 获取节点和 Pod</li>
<li>然后选择<code>phase=Pending</code>和<code>schedulerName=my-scheduler</code>的pod</li>
<li>计算每个 Pod 需要放置的位置之后，调度程序将创建一个<code>Binding</code>对象</li>
<li>然后根据我们自定义的调度器的算法计算出最适合的目标节点</li>
</ul>
<h4 id="优先级调度"><a href="#优先级调度" class="headerlink" title="优先级调度"></a>优先级调度</h4><p>与前面所讲的调度优选策略中的优先级（Priorities）不同，前面所讲的优先级指的是节点优先级，而我们这里所说的优先级 pod priority 指的是 Pod 的优先级，高优先级的 Pod 会优先被调度，或者在资源不足低情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。</p>
<p>要定义 Pod 优先级，就需要先定义<code>PriorityClass</code>对象，该对象没有 Namespace 的限制：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> PriorityClass</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> high-priority</span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"This priority class should be used for XYZ service pods only."</span></span><br></pre></td></tr></table></figure></p>
<p>其中：</p>
<ul>
<li><code>value</code>为 32 位整数的优先级，该值越大，优先级越高</li>
<li><code>globalDefault</code>用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个<code>PriorityClass</code>将其设置为 true</li>
</ul>
<p>然后通过在 Pod 的<code>spec.priorityClassName</code>中指定已定义的<code>PriorityClass</code>名称即可：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> nginx</span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> nginx</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> nginx</span><br><span class="line"><span class="attr">    image:</span> nginx</span><br><span class="line"><span class="attr">    imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="attr">  priorityClassName:</span> high-priority</span><br></pre></td></tr></table></figure></p>
<p>另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 pending 时，抢占（preemption）逻辑就会被触发。<code>Preemption</code>会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。</p>
<p>现在我们通过下面的图再去回顾下 kubernetes 的调度过程是不是就清晰很多了：<br><img src="/images/k8s/kube-scheduler-detail.png" alt="K8s scheduler detail"></p>
<p>来源: k8s技术圈</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.11.x + etcd集群 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-etcd-ha-1-11-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy + etcd</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.11.5-0 kubelet-1.11.5-0 kubectl-1.11.5-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CADVISOR_ARGS=--cadvisor-port=0"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CADVISOR_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建etcd"><a href="#搭建etcd" class="headerlink" title="搭建etcd"></a>搭建etcd</h2><h3 id="在master-1上执行"><a href="#在master-1上执行" class="headerlink" title="在master-1上执行"></a>在master-1上执行</h3><h4 id="安装cfssl"><a href="#安装cfssl" class="headerlink" title="安装cfssl"></a>安装cfssl</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">wget -O /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">wget -O /bin/cfssl-certinfo  https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line"><span class="keyword">for</span> cfssl <span class="keyword">in</span> `ls /bin/cfssl*`;<span class="keyword">do</span> chmod +x <span class="variable">$cfssl</span>;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<h4 id="配置生成etcd-证书"><a href="#配置生成etcd-证书" class="headerlink" title="配置生成etcd 证书"></a>配置生成etcd 证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置证书</span></span><br><span class="line">mkdir -pv <span class="variable">$HOME</span>/ssl &amp;&amp; <span class="built_in">cd</span> <span class="variable">$HOME</span>/ssl</span><br><span class="line"></span><br><span class="line">cat &gt; ca-config.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"signing"</span>: &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">      <span class="string">"expiry"</span>: <span class="string">"87600h"</span> //10年</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"profiles"</span>: &#123;</span><br><span class="line">      <span class="string">"kubernetes"</span>: &#123;</span><br><span class="line">        <span class="string">"usages"</span>: [</span><br><span class="line">            <span class="string">"signing"</span>,</span><br><span class="line">            <span class="string">"key encipherment"</span>,</span><br><span class="line">            <span class="string">"server auth"</span>,</span><br><span class="line">            <span class="string">"client auth"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; etcd-ca-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"JiangSu"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"SuZhou"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"Etcd Security"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; etcd-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.19.170.183"</span>,</span><br><span class="line">      <span class="string">"172.19.170.184"</span>,</span><br><span class="line">      <span class="string">"172.19.170.185"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"JiangSu"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"SuZhou"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"Etcd Security"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成证书并复制证书至其他etcd节点</span></span><br><span class="line">cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca</span><br><span class="line">cfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动证书</span></span><br><span class="line">mkdir -pv /etc/etcd/ssl</span><br><span class="line">cp etcd*.pem /etc/etcd/ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将证书拷贝到其它节点</span></span><br><span class="line">scp -r /etc/etcd master-2:/etc/</span><br><span class="line">scp -r /etc/etcd master-3:/etc/</span><br></pre></td></tr></table></figure>
<h4 id="安装配置etcd"><a href="#安装配置etcd" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">yum install -y etcd </span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.183:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.183:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd1"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.183:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.183:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span> <span class="comment">#这里要设置为new</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line"><span class="comment">#启动会卡住，等待其它节点的加入</span></span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="在master-2-上执行"><a href="#在master-2-上执行" class="headerlink" title="在master-2 上执行"></a>在master-2 上执行</h3><h4 id="安装配置etcd-1"><a href="#安装配置etcd-1" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">yum install -y etcd </span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.184:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.184:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd2"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.184:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.184:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"existing"</span> <span class="comment">#如果etcd 启动报错，可先把这行注释掉，然后在启动</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="在master-3-上执行"><a href="#在master-3-上执行" class="headerlink" title="在master-3 上执行"></a>在master-3 上执行</h3><h4 id="安装配置etcd-2"><a href="#安装配置etcd-2" class="headerlink" title="安装配置etcd"></a>安装配置etcd</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; /etc/etcd/etcd.conf</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.185:2379"</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd3"</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://127.0.0.1:2379,https://172.19.170.185:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.19.170.183:2380,etcd2=https://172.19.170.184:2380,etcd3=https://172.19.170.185:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"BigBoss"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"existing"</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chown -R etcd.etcd /etc/etcd</span><br><span class="line">systemctl <span class="built_in">enable</span> etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<h3 id="etcd-集群验证"><a href="#etcd-集群验证" class="headerlink" title="etcd 集群验证"></a>etcd 集群验证</h3><blockquote>
<p>随意在任何一台etcd (master) 节点上进行验证</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">etcdctl --endpoints <span class="string">"https://172.19.170.183:2379,https://172.19.170.184:2379,https://172.19.170.185:2379"</span>   --ca-file=/etc/etcd/ssl/etcd-ca.pem  \</span><br><span class="line">--cert-file=/etc/etcd/ssl/etcd.pem   --key-file=/etc/etcd/ssl/etcd-key.pem   cluster-health</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">token=$(kubeadm token generate)</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$token</span></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - <span class="string">"https://172.19.170.183:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.184:2379"</span></span><br><span class="line">    - <span class="string">"https://172.19.170.185:2379"</span></span><br><span class="line">    caFile: /etc/etcd/ssl/etcd-ca.pem</span><br><span class="line">    certFile: /etc/etcd/ssl/etcd.pem</span><br><span class="line">    keyFile: /etc/etcd/ssl/etcd-key.pem</span><br><span class="line">token: <span class="variable">$token</span></span><br><span class="line">tokenTTL: <span class="string">"0"</span></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.2.4</span><br><span class="line">docker pull quay.io/calico/node:v3.2.4</span><br><span class="line">docker pull quay.io/calico/cni:v3.2.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.11.5</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[chrome 访问k8s dashboard 出现ssl证书错误]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-dashboard-chrome-err.html</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>最近上了k8s后，给开发提供dashboard访问，发现有不少开发无法使用chrome访问，出现问题的大部分都是使用的windows系统，主要有以下两类问题，这里记录下如何解决</p>
</blockquote>
<h3 id="ERR-SSL-SERVER-CERT-BAD-FORMAT"><a href="#ERR-SSL-SERVER-CERT-BAD-FORMAT" class="headerlink" title="ERR_SSL_SERVER_CERT_BAD_FORMAT"></a>ERR_SSL_SERVER_CERT_BAD_FORMAT</h3><blockquote>
<p>解决方法：重新安装chrome最新版本解决</p>
</blockquote>
<h3 id="NET-ERR-CERT-INVALID"><a href="#NET-ERR-CERT-INVALID" class="headerlink" title="NET::ERR_CERT_INVALID"></a>NET::ERR_CERT_INVALID</h3><p><img src="/images/k8s/chrome_err.png" alt="NET::ERR_CERT_INVALID"></p>
<blockquote>
<p>解决方法：创建chrome桌面快捷方式，然后到桌面：右键chrome–&gt;属性–&gt;在目标后面添加如下：<code>--disable-infobars --ignore-certificate-errors</code></p>
<p>示例：<code>&quot;C:\Program Files (x86)\Google\Chrome\Application\chrome.exe&quot; --disable-infobars --ignore-certificate-errors</code></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.12.x 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ha-1-12-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.12.3-0 kubelet-1.12.3-0 kubectl-1.12.3-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.183:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.183:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380</span><br><span class="line">      initial-cluster-state: new</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源 </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.184:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.184:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源 </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-2 https://172.19.170.184:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-2执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha3</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.12.3</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.185:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.185:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380,master-3=https://172.19.170.185:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: iptables </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-3 https://172.19.170.185:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-3执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="调整配置，使用vip来进行调度"><a href="#调整配置，使用vip来进行调度" class="headerlink" title="调整配置，使用vip来进行调度"></a>调整配置，使用vip来进行调度</h3><blockquote>
<p>在所有master机器上都执行以下命令</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在master-1 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-2 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-3 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.12.3/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.3.2</span><br><span class="line">docker pull quay.io/calico/node:v3.3.2</span><br><span class="line">docker pull quay.io/calico/cni:v3.3.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.12.3</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.12.3</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.12.3</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.12.3</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.12.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubeadm 1.11.x 部署多Master集群]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ha-1-11-x.html</url>
      <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><table>
<thead>
<tr>
<th style="text-align:left">IP</th>
<th style="text-align:left">Hostname</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">172.19.170.183</td>
<td style="text-align:left">master-1</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.184</td>
<td style="text-align:left">master-2</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.185</td>
<td style="text-align:left">master-3</td>
<td style="text-align:left">私有云安装keepalived + haproxy</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.186</td>
<td style="text-align:left">node-1</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.187</td>
<td style="text-align:left">node-2</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left">172.19.170.100</td>
<td style="text-align:left">vip</td>
<td style="text-align:left">私有云使用keepalive+haproxy搭建，公有云使用内部负载均衡器</td>
</tr>
</tbody>
</table>
<h2 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h2><h3 id="在master-1-机器上执行以下命令："><a href="#在master-1-机器上执行以下命令：" class="headerlink" title="在master-1 机器上执行以下命令："></a>在master-1 机器上执行以下命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@master-2</span><br><span class="line">ssh-copy-id root@master-3</span><br></pre></td></tr></table></figure>
<h3 id="修改hosts信息，在所有master机器上运行"><a href="#修改hosts信息，在所有master机器上运行" class="headerlink" title="修改hosts信息，在所有master机器上运行"></a>修改hosts信息，在所有master机器上运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> &lt;&lt;EOF &gt;&gt; /etc/hosts</span><br><span class="line">172.19.170.183 master-1</span><br><span class="line">172.19.170.184 master-2</span><br><span class="line">172.19.170.185 master-3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="优化所有master-amp-node-节点的机器环境"><a href="#优化所有master-amp-node-节点的机器环境" class="headerlink" title="优化所有master &amp; node 节点的机器环境"></a>优化所有master &amp; node 节点的机器环境</h3><blockquote>
<p><code>在所有master &amp; node 机器上都要执行以下命令</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># 安装4.18版本内核</span></span><br><span class="line"><span class="comment"># 由于最新稳定版4.19内核将nf_conntrack_ipv4更名为nf_conntrack，目前的kube-proxy不支持在4.19版本内核下开启ipvs</span></span><br><span class="line"><span class="comment"># 详情可以查看：https://github.com/kubernetes/kubernetes/issues/70304</span></span><br><span class="line"><span class="comment"># 对于该问题的修复10月30日刚刚合并到代码主干，所以目前还没有包含此修复的kubernetes版本发出</span></span><br><span class="line"><span class="comment"># 可以选择安装4.18版本内核，或者不开启IPVS</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment">############################# start #############################</span></span><br><span class="line"><span class="comment"># 升级内核</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重启以更换内核</span></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启ip_vs</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=<span class="string">"/usr/lib/modules/\`uname -r\`/kernel/net/netfilter/ipvs"</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> \`ls \<span class="variable">$ipvs_modules_dir</span> | sed  -r <span class="string">'s#(.*).ko.*#\1#'</span>\`; <span class="keyword">do</span></span><br><span class="line">    /sbin/modinfo -F filename \<span class="variable">$i</span>  &amp;&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ \$? <span class="_">-eq</span> 0 ]; <span class="keyword">then</span></span><br><span class="line">        /sbin/modprobe \<span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否开启了ip_vs</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# end #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化内核</span></span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">fs.may_detach_mounts = 1</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.inotify.max_user_instances = 8192</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行sysctl报错请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory: https://blog.csdn.net/airuozhaoyang/article/details/40534953</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化文件打开数</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nofile 65536"</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard nproc 65536"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* soft  memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* hard memlock  unlimited"</span>  &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭交换分区</span></span><br><span class="line">swapoff <span class="_">-a</span></span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubernet yum源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装kubernet</span></span><br><span class="line">yum install kubeadm-1.11.5-0 kubelet-1.11.5-0 kubectl-1.11.5-0 --disableexcludes=kubernetes -y</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置docker源</span></span><br><span class="line">cat &gt; /etc/yum.repos.d/docker.repo &lt;&lt;EOF</span><br><span class="line">[docker]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/repo/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.tuna.tsinghua.edu.cn/docker/yum/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装docker-engine</span></span><br><span class="line">yum -y install docker-engine-1.13.1 --disableexcludes=docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet启动配置</span></span><br><span class="line">cgroupDriver=$(docker info|grep Cg)</span><br><span class="line">driver=<span class="variable">$&#123;cgroupDriver##*: &#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"driver is <span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CADVISOR_ARGS=--cadvisor-port=0"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=<span class="variable">$&#123;driver&#125;</span>"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--system-reserved=cpu=200m,memory=250Mi --kube-reserved=cpu=200m,memory=250Mi --eviction-soft=memory.available&lt;500Mi,nodefs.available&lt;2Gi --eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s --eviction-max-pod-grace-period=120 --eviction-hard=memory.available&lt;300Mi,nodefs.available&lt;1Gi --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi --node-status-update-frequency=10s --eviction-pressure-transition-period=30s"</span></span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet \<span class="variable">$KUBELET_KUBECONFIG_ARGS</span> \<span class="variable">$KUBELET_SYSTEM_PODS_ARGS</span> \<span class="variable">$KUBELET_NETWORK_ARGS</span> \<span class="variable">$KUBELET_DNS_ARGS</span> \<span class="variable">$KUBELET_AUTHZ_ARGS</span> \<span class="variable">$KUBELET_CADVISOR_ARGS</span> \<span class="variable">$KUBELET_CGROUP_ARGS</span> \<span class="variable">$KUBELET_CERTIFICATE_ARGS</span> \<span class="variable">$KUBELET_EXTRA_ARGS</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="搭建keepalived-haproxy-公有云不需要搭建"><a href="#搭建keepalived-haproxy-公有云不需要搭建" class="headerlink" title="搭建keepalived + haproxy (公有云不需要搭建)"></a>搭建keepalived + haproxy (公有云不需要搭建)</h2><h3 id="在master-1上配置"><a href="#在master-1上配置" class="headerlink" title="在master-1上配置"></a>在master-1上配置</h3><h4 id="keepalived-安装配置"><a href="#keepalived-安装配置" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node1         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 100        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置"><a href="#haproxy-安装配置" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-2上配置"><a href="#在master-2上配置" class="headerlink" title="在master-2上配置"></a>在master-2上配置</h3><h4 id="keepalived-安装配置-1"><a href="#keepalived-安装配置-1" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node2         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-1"><a href="#haproxy-安装配置-1" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h3 id="在master-3上配置"><a href="#在master-3上配置" class="headerlink" title="在master-3上配置"></a>在master-3上配置</h3><h4 id="keepalived-安装配置-2"><a href="#keepalived-安装配置-2" class="headerlink" title="keepalived 安装配置"></a>keepalived 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        notification_email &#123;</span><br><span class="line">            root@localhost      <span class="comment">#发送邮箱</span></span><br><span class="line">        &#125;</span><br><span class="line">        notification_email_from keepalived@localhost    <span class="comment">#邮箱地址   </span></span><br><span class="line">        smtp_server 127.0.0.1   <span class="comment">#邮件服务器地址</span></span><br><span class="line">        smtp_connect_timeout 30 </span><br><span class="line">        router_id node3         <span class="comment">#主机名，每个节点不同即可</span></span><br><span class="line">    &#125;       </span><br><span class="line">        </span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP        <span class="comment">#在另一个节点上为BACKUP</span></span><br><span class="line">    interface eth0      <span class="comment">#IP地址漂移到的网卡</span></span><br><span class="line">    virtual_router_id 6 <span class="comment">#多个节点必须相同</span></span><br><span class="line">    priority 80        <span class="comment">#优先级，备用节点的值必须低于主节点的值</span></span><br><span class="line">    advert_int 1        <span class="comment">#通告间隔1秒</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS      <span class="comment">#预共享密钥认证</span></span><br><span class="line">        auth_pass 571f97b2  <span class="comment">#密钥</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        172.19.170.100    <span class="comment">#VIP地址</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;    </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived</span><br><span class="line">systemctl start keepalived</span><br></pre></td></tr></table></figure>
<h4 id="haproxy-安装配置-2"><a href="#haproxy-安装配置-2" class="headerlink" title="haproxy 安装配置"></a>haproxy 安装配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum install  -y haproxy</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 <span class="built_in">local</span>2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    mode                    tcp</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line"></span><br><span class="line">frontend kubernetes</span><br><span class="line">    <span class="built_in">bind</span> *:8443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend kubernetes-master</span><br><span class="line"></span><br><span class="line">backend kubernetes-master</span><br><span class="line">    balance roundrobin</span><br><span class="line">    server master-1 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-2 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">    server master-3 172.19.170.183:6443 check maxconn 2000</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure>
<h2 id="搭建kubernetes-master-集群"><a href="#搭建kubernetes-master-集群" class="headerlink" title="搭建kubernetes master 集群"></a>搭建kubernetes master 集群</h2><h3 id="初始化master-1"><a href="#初始化master-1" class="headerlink" title="初始化master-1"></a>初始化master-1</h3><blockquote>
<p> <code>在master-1 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.183</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.183:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.183:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.183:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380</span><br><span class="line">      initial-cluster-state: new</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-1</span><br><span class="line">      - 172.19.170.183</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line">kubeadm init --config <span class="variable">$HOME</span>/kubeadm-1.yaml</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp <span class="_">-f</span> /etc/kubernetes/admin.conf <span class="variable">$&#123;HOME&#125;</span>/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看etcd是否启动，等待返回Running 说明启动成功</span></span><br><span class="line">kubectl get pods -n kube-system 2&gt;&amp;1|grep etcd|awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置其他master</span></span><br><span class="line">ssh master-2 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line">ssh master-3 <span class="string">"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件至 master-2</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-2:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-2:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-2:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-2:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-2:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-2:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-2:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-2:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-2:~/.kube/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝文件至 master-3</span></span><br><span class="line">scp /etc/kubernetes/pki/ca.crt master-3:/etc/kubernetes/pki/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/ca.key master-3:/etc/kubernetes/pki/ca.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.key master-3:/etc/kubernetes/pki/sa.key</span><br><span class="line">scp /etc/kubernetes/pki/sa.pub master-3:/etc/kubernetes/pki/sa.pub</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.crt master-3:/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/front-proxy-ca.key master-3:/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.crt master-3:/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.key master-3:/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:/etc/kubernetes/admin.conf</span><br><span class="line">scp /etc/kubernetes/admin.conf master-3:~/.kube/config</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-2"><a href="#初始化master-2" class="headerlink" title="初始化master-2"></a>初始化master-2</h3><blockquote>
<p> <code>在master-2 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.184</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443 </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.184:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.184:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.184:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-2</span><br><span class="line">      - 172.19.170.184</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-2 https://172.19.170.184:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-2执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-2.yaml</span><br></pre></td></tr></table></figure>
<h3 id="初始化master-3"><a href="#初始化master-3" class="headerlink" title="初始化master-3"></a>初始化master-3</h3><blockquote>
<p> <code>在master-3 上执行</code></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.5</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 172.19.170.185</span><br><span class="line">  <span class="built_in">bind</span>Port: 6443</span><br><span class="line">  controlPlaneEndpoint: 172.19.170.100:8443    </span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- 172.19.170.183</span><br><span class="line">- 172.19.170.184</span><br><span class="line">- 172.19.170.185</span><br><span class="line">- master-1</span><br><span class="line">- master-2</span><br><span class="line">- master-3</span><br><span class="line">- 172.19.170.100</span><br><span class="line">- 127.0.0.1</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: iptables</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: https://127.0.0.1:2379,https://172.19.170.185:2379</span><br><span class="line">      advertise-client-urls: https://172.19.170.185:2379</span><br><span class="line">      listen-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-advertise-peer-urls: https://172.19.170.185:2380</span><br><span class="line">      initial-cluster: master-1=https://172.19.170.183:2380,master-2=https://172.19.170.184:2380,master-3=https://172.19.170.185:2380</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - master-3</span><br><span class="line">      - 172.19.170.185</span><br><span class="line">networking:</span><br><span class="line">  <span class="comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span></span><br><span class="line">  podSubnet: 192.168.0.0/16</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers  <span class="comment"># image的仓库源  </span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-1执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubectl <span class="built_in">exec</span> \</span><br><span class="line">-n kube-system etcd-master-1 -- etcdctl \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--endpoints=https://172.19.170.183:2379 \</span><br><span class="line">member add master-3 https://172.19.170.185:2380</span><br><span class="line"><span class="comment"># 在master-1 执行后会卡在那边，等待其他的etcd节点加入</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 切换至master-3执行 ##################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line">kubeadm alpha phase certs all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig controller-manager --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig scheduler --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">kubeadm alpha phase etcd <span class="built_in">local</span> --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config <span class="variable">$HOME</span>/kubeadm-3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="调整配置，使用vip来进行调度"><a href="#调整配置，使用vip来进行调度" class="headerlink" title="调整配置，使用vip来进行调度"></a>调整配置，使用vip来进行调度</h3><blockquote>
<p>在所有master机器上都执行以下命令</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在master-1 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.183:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-2 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.184:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在master-3 上执行</span></span><br><span class="line">sed -i <span class="string">'s/etcd-servers=https:\/\/127.0.0.1:2379/etcd-servers=https:\/\/172.19.170.183:2379,https:\/\/172.19.170.184:2379,https:\/\/172.19.170.185:2379/g'</span> /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> ~/.kube/config</span><br><span class="line">sed -i <span class="string">'s/172.19.170.185:6443/172.19.170.100:8443/g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################</span></span><br><span class="line"><span class="comment">################## 至此集群搭建完成 #####################</span></span><br><span class="line"><span class="comment">######################################################</span></span><br></pre></td></tr></table></figure>
<h2 id="开启kubernetes-插件"><a href="#开启kubernetes-插件" class="headerlink" title="开启kubernetes 插件"></a>开启kubernetes 插件</h2><blockquote>
<p>在随意一台master执行</p>
</blockquote>
<h3 id="安装calico-网络插件"><a href="#安装calico-网络插件" class="headerlink" title="安装calico 网络插件"></a>安装calico 网络插件</h3><p>确保 <code>Kubernetes controller manager (/etc/kubernetes/manifests/kube-controller-manager.yaml)</code> 设置了以下标志：<code>--cluster-cidr=192.168.0.0/16和--allocate-node-cidrs=true</code>。</p>
<p>提示：在kubeadm上，您可以传递<code>--pod-network-cidr=192.168.0.0/16</code> 给kubeadm以设置两个Kubernetes控制器标志。</p>
<p>在ConfigMap命名中<code>calico-config</code>，找到<code>typha_service_name</code>，删除<code>none</code>值，并替换为<code>calico-typha</code></p>
<p>我们建议每<code>200</code>个节点至少<code>复制一个副本</code>，不超过20个副本。在生产中，我们建议至少使用<code>三个副本</code>来减少滚动升级和故障的影响。</p>
<p><strong><code>警告：如果您设置typha_service_name而不增加其默认值为0 Felix的副本计数将尝试连接到Typha，找不到要连接的Typha实例，并且无法启动。</code></strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/calico.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="安装heapster-监控插件"><a href="#安装heapster-监控插件" class="headerlink" title="安装heapster 监控插件"></a>安装heapster 监控插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/heapster.yaml</span><br></pre></td></tr></table></figure>
<h3 id="安装dashboard-插件"><a href="#安装dashboard-插件" class="headerlink" title="安装dashboard 插件"></a>安装dashboard 插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply <span class="_">-f</span> www.jiunile.com/k8s/plugin/1.11.5/dashboard.yaml</span><br></pre></td></tr></table></figure>
<h3 id="获取加入master集群命令"><a href="#获取加入master集群命令" class="headerlink" title="获取加入master集群命令"></a>获取加入master集群命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>
<h2 id="初始化所有kubernetes-node"><a href="#初始化所有kubernetes-node" class="headerlink" title="初始化所有kubernetes node"></a>初始化所有kubernetes node</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拉取calico网络对应需要的镜像，拉取比教缓慢</span></span><br><span class="line">docker pull quay.io/calico/typha:v3.2.4</span><br><span class="line">docker pull quay.io/calico/node:v3.2.4</span><br><span class="line">docker pull quay.io/calico/cni:v3.2.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入集群</span></span><br><span class="line">kubeadm join 172.19.170.100:8443 --token mrvxeb.mfr1wx6upq5bbwqt --discovery-token-ca-cert-hash sha256:8ee893d8bf69f1f622f55a983f1401a9f2a236ffa9248894cb614c972de47f48</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以在master机器上查看对应的node状态</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试应用和dns是否正常"><a href="#测试应用和dns是否正常" class="headerlink" title="测试应用和dns是否正常"></a>测试应用和dns是否正常</h3><blockquote>
<p>在随意一台master机器上运行</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一个服务</span></span><br><span class="line"><span class="built_in">cd</span> /root &amp;&amp; mkdir nginx &amp;&amp; <span class="built_in">cd</span> nginx</span><br><span class="line">cat &lt;&lt; EOF &gt; nginx.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    nodePort: 31000</span><br><span class="line">    name: nginx-port</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: nginx</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply <span class="_">-f</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个POD来测试DNS解析</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line">nslookup kubernetes</span><br><span class="line"><span class="comment"># Server:    10.96.0.10</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name:      kubernetes</span></span><br><span class="line"><span class="comment"># Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line">curl nginx</span><br><span class="line"><span class="comment"># 有返回结果表明ok</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
<h3 id="测试master-amp-Haproxy高可用"><a href="#测试master-amp-Haproxy高可用" class="headerlink" title="测试master &amp; Haproxy高可用"></a>测试master &amp; Haproxy高可用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机关掉一台master机器</span></span><br><span class="line">init 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在别的master机器上执行命令</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment">#NAME      STATUS     ROLES     AGE       VERSION</span></span><br><span class="line"><span class="comment">#master-1   NotReady   master    1h        v1.11.5</span></span><br><span class="line"><span class="comment">#master-2   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#master-3   Ready      master    59m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-1     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"><span class="comment">#node-2     Ready      &lt;none&gt;    58m       v1.11.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建一个pod，看看是否能创建成功</span></span><br><span class="line">kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">kubectl delete deployment curl</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[认识Kubernetes Descheduler]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-descheduler.html</url>
      <content type="html"><![CDATA[<p><code>kube-scheduler</code> 是 Kubernetes 中负责调度的组件，它本身的调度功能已经很强大了。但由于 Kubernetes 集群非常活跃，它的状态会随时间而改变，由于各种原因，你可能需要将已经运行的 Pod 移动到其他节点：</p>
<ul>
<li>某些节点负载过高</li>
<li>某些资源对象被添加了 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature" target="_blank" rel="external">node 亲和性</a> 或 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature" target="_blank" rel="external">pod （反）亲和性</a></li>
<li>集群中加入了新节点</li>
</ul>
<p>一旦 Pod 启动之后 <code>kube-scheduler</code> 便不会再尝试重新调度它。根据环境的不同，你可能会有很多需要手动调整 Pod 的分布，例如：如果集群中新加入了一个节点，那么已经运行的 Pod 并不会被分摊到这台节点上，这台节点可能只运行了少量的几个 Pod，这并不理想，对吧？</p>
<h3 id="Descheduler-如何工作？"><a href="#Descheduler-如何工作？" class="headerlink" title="Descheduler 如何工作？"></a>Descheduler 如何工作？</h3><p><a href="https://github.com/kubernetes-incubator/descheduler" target="_blank" rel="external">Descheduler</a> 会检查 Pod 的状态，并根据自定义的策略将不满足要求的 Pod 从该节点上驱逐出去。Descheduler 并不是 <code>kube-scheduler</code> 的替代品，而是要依赖于它。该项目目前放在 Kubernetes 的孵化项目中，还没准备投入生产，但经过我实验发现它的运行效果很好，而且非常稳定。那么该如何安装呢？<br><a id="more"></a></p>
<h3 id="部署方法"><a href="#部署方法" class="headerlink" title="部署方法"></a>部署方法</h3><p>你可以通过 <code>Job</code> 或 <code>CronJob</code> 来运行 descheduler。我已经创建了一个镜像 <code>komljen/descheduler:v0.5.0-4-ga7ceb671</code>（包含在下面的 yaml 文件中），但由于这个项目的更新速度很快，你可以通过以下的命令创建你自己的镜像：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">⚡ git <span class="built_in">clone</span> https://github.com/kubernetes-incubator/descheduler</span><br><span class="line">⚡ <span class="built_in">cd</span> descheduler &amp;&amp; make image</span><br></pre></td></tr></table></figure></p>
<p>然后打好标签 push 到自己的镜像仓库中。</p>
<p>通过我创建的 chart 模板，你可以用 <code>Helm</code> 来部署 descheduler，该模板支持 RBAC 并且已经在 Kubernetes v1.9 上测试通过。</p>
<p>添加我的 helm 私有仓库，然后部署 descheduler：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">⚡ helm repo add akomljen-charts \</span><br><span class="line">    https://raw.githubusercontent.com/komljen/helm-charts/master/charts/</span><br><span class="line"></span><br><span class="line">⚡ helm install --name ds \</span><br><span class="line">    --namespace kube-system \</span><br><span class="line">    akomljen-charts/descheduler</span><br></pre></td></tr></table></figure></p>
<p>你也可以不使用 helm，通过手动部署。首先创建 serviceaccount 和 clusterrolebinding：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a cluster role</span></span><br><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"nodes"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>]</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>, <span class="string">"delete"</span>]</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods/eviction"</span>]</span><br><span class="line">  verbs: [<span class="string">"create"</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a service account</span></span><br><span class="line">⚡ kubectl create sa descheduler -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the cluster role to the service account</span></span><br><span class="line">⚡ kubectl create clusterrolebinding descheduler \</span><br><span class="line">    -n kube-system \</span><br><span class="line">    --clusterrole=descheduler \</span><br><span class="line">    --serviceaccount=kube-system:descheduler</span><br></pre></td></tr></table></figure></p>
<p>然后通过 <code>configmap</code> 创建 descheduler 策略。目前只支持四种策略：</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removeduplicates" target="_blank" rel="external">RemoveDuplicates</a><br>RS、deployment 中的 pod 不能同时出现在一台机器上</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#lownodeutilization" target="_blank" rel="external">LowNodeUtilization</a><br>找到资源使用率比较低的 node，然后驱逐其他资源使用率比较高节点上的 pod，期望调度器能够重新调度让资源更均衡</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removepodsviolatinginterpodantiaffinity" target="_blank" rel="external">RemovePodsViolatingInterPodAntiAffinity</a><br>找到已经违反 Pod Anti Affinity 规则的 pods 进行驱逐，可能是因为反亲和是后面加上去的</li>
<li><a href="https://github.com/kubernetes-incubator/descheduler#removepodsviolatingnodeaffinity" target="_blank" rel="external">RemovePodsViolatingNodeAffinity</a><br>找到违反 Node Affinity 规则的 pods 进行驱逐，可能是因为 node 后面修改了 label</li>
</ul>
<p>默认这四种策略全部开启，你可以根据需要关闭它们。下面在 <code>kube-system</code> 命名空间中创建一个 configmap：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">data:</span><br><span class="line">  policy.yaml: |-  </span><br><span class="line">    apiVersion: descheduler/v1alpha1</span><br><span class="line">    kind: DeschedulerPolicy</span><br><span class="line">    strategies:</span><br><span class="line">      RemoveDuplicates:</span><br><span class="line">         enabled: <span class="literal">false</span></span><br><span class="line">      LowNodeUtilization:</span><br><span class="line">         enabled: <span class="literal">true</span></span><br><span class="line">         params:</span><br><span class="line">           nodeResourceUtilizationThresholds:</span><br><span class="line">             thresholds:</span><br><span class="line">               cpu: 20</span><br><span class="line">               memory: 20</span><br><span class="line">               pods: 20</span><br><span class="line">             targetThresholds:</span><br><span class="line">               cpu: 50</span><br><span class="line">               memory: 50</span><br><span class="line">               pods: 50</span><br><span class="line">      RemovePodsViolatingInterPodAntiAffinity:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">      RemovePodsViolatingNodeAffinity:</span><br><span class="line">        enabled: <span class="literal">true</span></span><br><span class="line">        params:</span><br><span class="line">          nodeAffinityType:</span><br><span class="line">          - requiredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>在 <code>kube-system</code> 命名空间中创建一个 CronJob，该 CroJob 每 30 分钟运行一次：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">⚡ cat &lt;&lt; EOF| kubectl create -n kube-system <span class="_">-f</span> -</span><br><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler</span><br><span class="line">spec:</span><br><span class="line">  schedule: <span class="string">"*/30 * * * *"</span></span><br><span class="line">  jobTemplate:</span><br><span class="line">    metadata:</span><br><span class="line">      name: descheduler</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">"true"</span></span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          serviceAccountName: descheduler</span><br><span class="line">          containers:</span><br><span class="line">          - name: descheduler</span><br><span class="line">            image: komljen/descheduler:v0.6.0</span><br><span class="line">            volumeMounts:</span><br><span class="line">            - mountPath: /policy-dir</span><br><span class="line">              name: policy-volume</span><br><span class="line">            <span class="built_in">command</span>:</span><br><span class="line">            - /bin/descheduler</span><br><span class="line">            - --v=4</span><br><span class="line">            - --max-pods-to-evict-per-node=10</span><br><span class="line">            - --policy-config-file=/policy-dir/policy.yaml</span><br><span class="line">          restartPolicy: <span class="string">"OnFailure"</span></span><br><span class="line">          volumes:</span><br><span class="line">          - name: policy-volume</span><br><span class="line">            configMap:</span><br><span class="line">              name: descheduler</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">⚡ kubectl get cronjobs -n kube-system</span><br><span class="line">NAME             SCHEDULE       SUSPEND   ACTIVE    LAST SCHEDULE   AGE</span><br><span class="line">descheduler      */30 * * * *   False     0         2m              32m</span><br></pre></td></tr></table></figure></p>
<p>当 CronJob 开始工作后，可以通过以下命令查看已经成功结束的 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">⚡ kubectl get pods -n kube-system <span class="_">-a</span> | grep Completed</span><br><span class="line">descheduler-1525520700-297pq          0/1       Completed   0          1h</span><br><span class="line">descheduler-1525521000-tz2ch          0/1       Completed   0          32m</span><br><span class="line">descheduler-1525521300-mrw4t          0/1       Completed   0          2m</span><br></pre></td></tr></table></figure></p>
<p>也可以查看这些 Pod 的日志，然后根据需要调整 descheduler 策略：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">⚡ kubectl logs descheduler-1525521300-mrw4t -n kube-system</span><br><span class="line">I0505 11:55:07.554195       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84</span><br><span class="line">I0505 11:55:07.554255       1 reflector.go:240] Listing and watching *v1.Node from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84</span><br><span class="line">I0505 11:55:07.767903       1 lownodeutilization.go:147] Node <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span> is appropriately utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:41.5, <span class="string">"memory"</span>:1.3635487207675927, <span class="string">"pods"</span>:8.181818181818182&#125;</span><br><span class="line">I0505 11:55:07.767942       1 lownodeutilization.go:149] allPods:9, nonRemovablePods:9, bePods:0, bPods:0, gPods:0</span><br><span class="line">I0505 11:55:07.768141       1 lownodeutilization.go:144] Node <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span> is over utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:48.75, <span class="string">"memory"</span>:61.05259502942694, <span class="string">"pods"</span>:30&#125;</span><br><span class="line">I0505 11:55:07.768156       1 lownodeutilization.go:149] allPods:33, nonRemovablePods:12, bePods:1, bPods:19, gPods:1</span><br><span class="line">I0505 11:55:07.768376       1 lownodeutilization.go:144] Node <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span> is over utilized with usage: api.ResourceThresholds&#123;<span class="string">"cpu"</span>:39.125, <span class="string">"memory"</span>:98.19259268881142, <span class="string">"pods"</span>:33.63636363636363&#125;</span><br><span class="line">I0505 11:55:07.768390       1 lownodeutilization.go:149] allPods:37, nonRemovablePods:8, bePods:0, bPods:29, gPods:0</span><br><span class="line">I0505 11:55:07.768538       1 lownodeutilization.go:147] Node <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span> is appropriately utilized with usage: api.ResourceThresholds&#123;<span class="string">"memory"</span>:43.19826999287199, <span class="string">"pods"</span>:30.90909090909091, <span class="string">"cpu"</span>:35.25&#125;</span><br><span class="line">I0505 11:55:07.768552       1 lownodeutilization.go:149] allPods:34, nonRemovablePods:11, bePods:8, bPods:15, gPods:0</span><br><span class="line">I0505 11:55:07.768556       1 lownodeutilization.go:65] Criteria <span class="keyword">for</span> a node under utilization: CPU: 20, Mem: 20, Pods: 20</span><br><span class="line">I0505 11:55:07.768571       1 lownodeutilization.go:69] No node is underutilized, nothing to <span class="keyword">do</span> here, you might tune your thersholds further</span><br><span class="line">I0505 11:55:07.768576       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.779313       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.796766       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.813303       1 pod_antiaffinity.go:45] Processing node: <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.829109       1 node_affinity.go:40] Executing <span class="keyword">for</span> nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">I0505 11:55:07.829133       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-63-172.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.840416       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-36-223.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.856735       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-41-14.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:07.945566       1 request.go:480] Throttling request took 88.738917ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-41-14.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded</span><br><span class="line">I0505 11:55:07.972702       1 node_affinity.go:45] Processing node: <span class="string">"ip-10-4-34-29.eu-west-1.compute.internal"</span></span><br><span class="line">I0505 11:55:08.145559       1 request.go:480] Throttling request took 172.751657ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-34-29.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded</span><br><span class="line">I0505 11:55:08.160964       1 node_affinity.go:72] Evicted 0 pods</span><br></pre></td></tr></table></figure></p>
<p>哇哦，现在你的集群中已经运行了一个 descheduler！</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Kubernetes 的默认调度器已经做的很好，但由于集群处于不断变化的状态中，某些 Pod 可能运行在错误的节点上，或者你想要均衡集群资源的分配，这时候就需要 descheduler 来帮助你将某些节点上的 Pod 驱逐到正确的节点上去。我很期待正式版的发布！</p>
<p>参考文档:</p>
<ul>
<li>Meet a Kubernetes Descheduler</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Kubernetes正确的处理用户请求]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-handling-client-requests-properly-with-kubernetes.html</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>毫无疑问，我们希望正确处理客户端请求。当pod正在启动或关闭时，我们显然不希望看到断开的连接。Kubernetes本身并不能确保这种情况永远不会发生。您的应用需要遵循一些规则以防止断开连接。本文讨论这些规则。</p>
</blockquote>
<h3 id="确保正确处理所有客户端请求"><a href="#确保正确处理所有客户端请求" class="headerlink" title="确保正确处理所有客户端请求"></a>确保正确处理所有客户端请求</h3><p>我们先从访问Pod的客户端的视角来看看Pod的生命周期（客户端使用pod提供的服务）。我们希望确保客户的请求得到妥善处理，因为如果当pod启动或关闭时连接开始中断，我们就会遇到麻烦。Kubernetes本身并不保证不会发生这种情况，所以让我们看看我们需要做些什么来防止它发生。</p>
<h3 id="当Pod启动时阻止连接中断"><a href="#当Pod启动时阻止连接中断" class="headerlink" title="当Pod启动时阻止连接中断"></a>当Pod启动时阻止连接中断</h3><p>如果你理解services和service endpoints的工作原理，确保在pod启动时正确处理每个连接都非常简单。当pod启动时，它会作为一个endpoints被添加到所有匹配该Pod标签的Services里。Pod也会发出信号告诉Kubernetes它已就绪。只有它变成一个service endpoints时才可以接收来自客户端的请求。</p>
<p>如果你没有在Pod Spec里指定readiness探针，则会始终认为该pod已准备就绪。这意味着它将立即开始接收请求 - 只要第一个Kube-Proxy在其节点上更新iptables规则并且第一个客户端pod尝试连接到该服务。如果那个时候你的应用并没有做好接收请求的准备，那么客户端将会见到“connection refused”类型的错误。</p>
<p>你所需要做的就是保证你的readiness探针当且仅当你的应用可以正确处理收到的请求时才返回成功结果。所以添加一个HTTP GET readiness探针并让它指向你应用的基础URL会是一个很好的开始。在很多情况下，这可以让你省去实现一个特定的readiness端点的工作量。<br><a id="more"></a></p>
<h3 id="在pod关闭期间防止断开连接"><a href="#在pod关闭期间防止断开连接" class="headerlink" title="在pod关闭期间防止断开连接"></a>在pod关闭期间防止断开连接</h3><p>现在让我们看看当一个Pod生命周期结束时发生了什么——当Pod被删除和它里面的容器被停止时。一旦Pod的容器接收到SIGTERM后它就会开始关闭（或者是在那之前先执行prestop钩子），但这是否能保证所有的客户端请求可以被正确地处理？</p>
<p>我们的应用在收到结束信号时应该如何响应？它是否应该继续接收请求？那么那些已经收到的请求但是还未完成的请求呢？那么那些正在发送请求的间隔中且仍然处理打开状态的持久HTTP连接（连接上没有活跃的请求）呢？在回答这些问题之前，我们需要深入了解一下当Pod结束时集群里发生的一系列事件。</p>
<h3 id="了解Pod删除时发生的一系列事件"><a href="#了解Pod删除时发生的一系列事件" class="headerlink" title="了解Pod删除时发生的一系列事件"></a>了解Pod删除时发生的一系列事件</h3><p>您需要始终牢记Kubernetes的各个组件是独立运行在集群的节点上的。它们之间并不是一个巨大的单体应用。这些组件间同步状态会花费一点时间。让我们一起来看看当Pod删除时发生了什么。</p>
<p>当APIserver收到一个停止Pod的请求时，它首先修改了etcd里Pod的状态，并通知关注这个删除事件所有的watcher。这些watcher里包括Kubelet和Endpoint Controller。这两个序列的事件是并行发生的（标记为A和B），如图1所示。<br><img src="/images/k8s/k8s_pod_03.png" alt="pod停止时的事件"></p>
<p>在A系列事件里，你会看到在Kubelet收到该Pod要停止的通知以后会尽快开始停止Pod的一系列操作（执行prestop钩子，发送SIGTERM信号，等待一段时间然后如果这个容器没有自动退出的话就强行杀掉这个容器）。如果应用响应了SIGTERM并迅速停止接收请求，那么任何尝试连接它的客户端都会收到一个Connection Refusd的错误。因为APIserver是直接向Kubelet发送的请求，那么从Pod被删除开始计算，Pod用于执行这段逻辑的时间相当短。</p>
<p>现在，让我们一起看看另外一系列事件都发生了什么——移除这个Pod相关的iptables规则（图中所示事件系列B）。当Endpoints Controller（运行在在Kubernetes控制平面里的Controller Manager里）收到这个Pod被删除的通知，然后它会把这个Pod从所有关联到这个Pod的Service里剔除。它通过向APIserver发送一个REST请求对Endpoint对象进行修改来实现。APIserver然后通知每个监视Endpoint对象的组件。在这些组件里包含了每个工作节点上运行的Kubeproxy。这些代理负责更新它所在节点上的iptables规则，这些规则可以用来阻止外面的请求被转发到要停止的Pod上。这里有个非常重要的细节，移除这些iptables规则对于已有的连接不会有任何影响——连接到这个Pod的客户端仍然可以通过已有连接向它发送请求。</p>
<p>这些请求都是并行发生的。更确切地，关停应用所需要的时间要比iptables更新花费所需的时间稍短一些。这是因为iptables修改的事件链看起来稍微长一些（见图2），因为这些事件需要先到达Endpoints Controller，然后它向APIServer发送一个新请求，接着在Proxy最终修改iptables规则之前，APIserver必须通知到每个KubeProxy。这意味着SIGTERM可能会在所有节点iptables规则更新前发送。<br><img src="/images/k8s/k8s_pod_04.png" alt="删除pod时的事件时间线"></p>
<p>结果是pod在收到终止信号后仍然可以接收客户端请求。如果应用程序立即停止接受连接，则会导致客户端收到“Connection Refused”类型的错误（就像在没有定义Readiness探针时，Pod启动但无法对外提供服务时一样） 。</p>
<h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>谷歌搜索此问题的解决方案，似乎给Pod添加一个Readiness探针就可以解决这个问题。按照推测，你所需要做的就是让你的Readiness探针在应用收到SIGTERM信号后尽快失败。这可以让Pod从Service上移除。只有在Readiness探针连续失败几次后才会从Service上移除（这可以在Readiness探针里进行配置）。并且显然这个移除事件会在iptables规则更新前先到达Kubeproxy。</p>
<p>实际上，Readiness探针在整个过程中并未起到关键作用。一旦Endpoints Controller收到Pod删除事件后（当Pod配置里的deletionTimestamp域不再为空时），它会尽快从Service上移除这些Pod。从那时起，这已经就与Readiness探测结果不相关了。</p>
<p>那么什么是问题的正确解决方案？我们如何保证所有的请求都可以被正确处理？</p>
<p>好吧，很明显，即使收到终止信号后，pod也需要继续接受连接，直到所有Kube代理完成更新iptables规则。那么，不止是Kubeproxy。可能有一些IngressController或者其他直接向Pod转发请求的负载均衡设备等不需要经过Service的。这还包括使用客户端负载平衡的客户端。<br>为了确保没有任何客户端遇到断开的连接，您必须等到所有客户端以某种方式通知您他们将不再转发到该Pod的连接。</p>
<p>但这是不可能的，因为这些组件都分布在不同的计算机上。即使你知道它们每个所在的位置或者等着它们每个都满足停止Pod的需求，如果它们其中之一没有响应你会该怎么办？你要等待多久？记着，在那段时间里，你需要暂停关闭进程。</p>
<p>你唯一可能的做事情是你可以等待足够长的时间直到所有的代理都完成了它们的工作。但是多长时间才算够？在大多数情况中，短暂的几秒就足够了，但是显然这并不能满足全部的情况。当APIserver或者Endpoints Controller过载时，它们可能需要更长时间来通知到每个代理。重要的是要了解您无法完美地解决问题，但是即使是5秒或者10秒都可以显著提升用户体验。你可以设置更长的延迟，但是别太过分，因为这些延迟推迟了容器的停止时间，并且会导致容器在被关停后仍然被显示在列表里，这会对用户带来一定的困扰。</p>
<p>正确关闭应用程序包括以下步骤：</p>
<ul>
<li>等待几秒，然后停止接收新连接</li>
<li>关闭所有未在请求中的keep-alived连接</li>
<li>等到所有活跃的请求关闭，然后</li>
<li>彻底关闭</li>
</ul>
<p>要了解在此过程中连接和请求发生的情况，请仔细检查图3。<br><img src="/images/k8s/k8s_pod_05.png" alt="在收到终止信号后正确处理现有和新连接"></p>
<p>没有像收到终止信号后立即退出过程一样简单，对吧？这一切是值得的吗？这要靠你自己来决定。但是至少你可以添加一个prestop钩子并等待几秒，就像下面所示：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span>  </span><br><span class="line"><span class="attr">    preStop:</span>    </span><br><span class="line"><span class="attr">        exec:</span>      </span><br><span class="line"><span class="attr">            command:</span></span><br><span class="line"><span class="bullet">             -</span> sh</span><br><span class="line"><span class="bullet">             -</span> -c</span><br><span class="line"><span class="bullet">             -</span> <span class="string">"sleep 5"</span></span><br></pre></td></tr></table></figure></p>
<p>这样，您根本不需要修改应用程序的代码。如果您的应用要确保完全处理所有正在进行的请求，这个preStop钩子就可以满足所有你的需要。</p>
<p>参考文档:</p>
<ul>
<li>handling-client-requests-properly-with-kubernetes</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm 证书说明]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ca-desc.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>如果你使用过kubeadm部署过Kubernetes的环境, master主机节点上就一定会在相应的目录创建了一大批证书文件, 本篇文章就来说说kubeadm到底为我们生成了哪些证书</p>
</blockquote>
<p>在Kubernetes的部署中, 创建证书, 配置证书是一道绕不过去坎儿, 好在有kubeadm这样的自动化工具, 帮我们去生成, 配置这些证书. 对于只是想体验Kubernetes或只是想测试的亲来说, 这已经够了, 但是作为Kubernetes的集群维护者来说, kubeadm更像是一个黑盒, 本篇文章就来说说黑盒中关于证书的事儿~</p>
<p>使用kubeadm创建完Kubernetes集群后, 默认会在<code>/etc/kubernetes/pki</code>目录下存放集群中需要用到的证书文件, 整体结构如下图所示:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:/etc/kubernetes/pki<span class="comment"># tree</span></span><br><span class="line">.</span><br><span class="line">|-- apiserver.crt</span><br><span class="line">|-- apiserver-etcd-client.crt</span><br><span class="line">|-- apiserver-etcd-client.key</span><br><span class="line">|-- apiserver.key</span><br><span class="line">|-- apiserver-kubelet-client.crt</span><br><span class="line">|-- apiserver-kubelet-client.key</span><br><span class="line">|-- ca.crt</span><br><span class="line">|-- ca.key</span><br><span class="line">|-- etcd</span><br><span class="line">|   |-- ca.crt</span><br><span class="line">|   |-- ca.key</span><br><span class="line">|   |-- healthcheck-client.crt</span><br><span class="line">|   |-- healthcheck-client.key</span><br><span class="line">|   |-- peer.crt</span><br><span class="line">|   |-- peer.key</span><br><span class="line">|   |-- server.crt</span><br><span class="line">|   `-- server.key</span><br><span class="line">|-- front-proxy-ca.crt</span><br><span class="line">|-- front-proxy-ca.key</span><br><span class="line">|-- front-proxy-client.crt</span><br><span class="line">|-- front-proxy-client.key</span><br><span class="line">|-- sa.key</span><br><span class="line">`-- sa.pub</span><br><span class="line"></span><br><span class="line">1 directory, 22 files</span><br></pre></td></tr></table></figure></p>
<p>以上22个文件就是kubeadm为我们创建的所有证书相关的文件, 下面我们来一一解析</p>
<h3 id="证书分组"><a href="#证书分组" class="headerlink" title="证书分组"></a>证书分组</h3><p>Kubernetes把证书放在了两个文件夹中</p>
<ul>
<li>/etc/kubernetes/pki</li>
<li>/etc/kubernetes/pki/etcd</li>
</ul>
<p>我们再将这22个文件按照更细的粒度去分组<br><a id="more"></a></p>
<h3 id="Kubernetes-集群根证书"><a href="#Kubernetes-集群根证书" class="headerlink" title="Kubernetes 集群根证书"></a>Kubernetes 集群根证书</h3><p>Kubernetes 集群根证书CA(Kubernetes集群组件的证书签发机构)</p>
<ul>
<li>/etc/kubernetes/pki/ca.crt</li>
<li>/etc/kubernetes/pki/ca.key</li>
</ul>
<p>以上这组证书为签发其他Kubernetes组件证书使用的根证书, 可以认为是Kubernetes集群中证书签发机构之一</p>
<p>由此根证书签发的证书有:</p>
<ol>
<li>kube-apiserver 组件持有的服务端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver.crt</li>
<li>/etc/kubernetes/pki/apiserver.key</li>
</ul>
<ol>
<li>kubelet 组件持有的客户端证书, 用作 kube-apiserver 主动向 kubelet 发起请求时的客户端认证</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver-kubelet-client.crt</li>
<li>/etc/kubernetes/pki/apiserver-kubelet-client.key</li>
</ul>
<blockquote>
<p>注意: Kubernetes集群组件之间的交互是双向的, kubelet 既需要主动访问 kube-apiserver, kube-apiserver 也需要主动向 kubelet 发起请求, 所以双方都需要有自己的根证书以及使用该根证书签发的服务端证书和客户端证书. 在 kube-apiserver 中, 一般明确指定用于 https 访问的服务端证书和带有<code>CN 用户名</code>信息的客户端证书. 而在 kubelet 的启动配置中, 一般只指定了 ca 根证书, 而没有明确指定用于 https 访问的服务端证书, 这是因为, 在生成服务端证书时, 一般会指定服务端地址或主机名, kube-apiserver 相对变化不是很频繁, 所以在创建集群之初就可以预先分配好用作 kube-apiserver 的 IP 或主机名/域名, 但是由于部署在 node 节点上的 kubelet 会因为集群规模的变化而频繁变化, 而无法预知 node 的所有 IP 信息, 所以 kubelet 上一般不会明确指定服务端证书, 而是只指定 ca 根证书, 让 kubelet 根据本地主机信息自动生成服务端证书并保存到配置的<code>cert-dir</code>文件夹中.</p>
</blockquote>
<p>好了, 至此, Kubernetes集群根证书所签发的证书都在上面了, 算上根证书一共涉及到6个文件, 22-6=16, 我们还剩下16个文件</p>
<h3 id="汇聚层证书"><a href="#汇聚层证书" class="headerlink" title="汇聚层证书"></a>汇聚层证书</h3><p>kube-apiserver 的另一种访问方式就是使用 <code>kubectl proxy</code> 来代理访问, 而该证书就是用来支持SSL代理访问的. 在该种访问模式下, 我们是以http的方式发起请求到代理服务的, 此时, 代理服务会将该请求发送给 kube-apiserver, 在此之前, 代理会将发送给 kube-apiserver 的请求头里加入证书信息, 以下两个配置</p>
<p>API Aggregation允许在不修改Kubernetes核心代码的同时扩展Kubernetes API. 开启 API Aggregation 需要在 kube-apiserver 中添加如下配置:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-client-ca-file=&lt;path to aggregator CA cert&gt;</span><br><span class="line">--requestheader-allowed-names=front-proxy-client</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class="line">--requestheader-group-headers=X-Remote-Group</span><br><span class="line">--requestheader-username-headers=X-Remote-User</span><br><span class="line">--proxy-client-cert-file=&lt;path to aggregator proxy cert&gt;</span><br><span class="line">--proxy-client-key-file=&lt;path to aggregator proxy key&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>官方警告: 除非你了解保护 CA 使用的风险和机制, 否则不要在不通上下文中重用已经使用过的 CA</strong></p>
<p>如果 kube-proxy 没有和 API server 运行在同一台主机上，那么需要确保启用了如下 apiserver 标记：<code>--enable-aggregator-routing=true</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">客户端 ---发起请求---&gt; 代理 ---Add Header:发起请求---&gt; kube-apiserver</span><br><span class="line">                   (客户端证书)                        (服务端证书)</span><br></pre></td></tr></table></figure>
<p>kube-apiserver 代理根证书(客户端证书)</p>
<p>用在<code>requestheader-client-ca-file</code>配置选项中, kube-apiserver 使用该证书来验证客户端证书是否为自己所签发</p>
<ul>
<li>/etc/kubernetes/pki/front-proxy-ca.crt</li>
<li>/etc/kubernetes/pki/front-proxy-ca.key</li>
</ul>
<p>由此根证书签发的证书只有一组:</p>
<p>代理层(如汇聚层aggregator)使用此套代理证书来向 kube-apiserver 请求认证</p>
<ol>
<li>代理端使用的客户端证书, 用作代用户与 kube-apiserver 认证</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/front-proxy-client.crt</li>
<li>/etc/kubernetes/pki/front-proxy-client.key</li>
</ul>
<p>参考文档:</p>
<ul>
<li>kube-apiserver 配置参数: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver" target="_blank" rel="external">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver</a></li>
<li>使用汇聚层扩展 Kubernetes API: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/" target="_blank" rel="external">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation</a></li>
<li>配置汇聚层: <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/" target="_blank" rel="external">https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer</a></li>
</ul>
<p>至此, 刨除代理专用的证书外, 还剩下 16-4=12 个文件</p>
<h3 id="etcd-集群根证书"><a href="#etcd-集群根证书" class="headerlink" title="etcd 集群根证书"></a>etcd 集群根证书</h3><p>etcd集群所用到的证书都保存在<code>/etc/kubernetes/pki/etcd</code>这路径下, 很明显, 这一套证书是用来专门给etcd集群服务使用的, 设计以下证书文件</p>
<p>etcd 集群根证书CA(etcd 所用到的所有证书的签发机构)</p>
<ul>
<li>/etc/kubernetes/pki/etcd/ca.crt</li>
<li>/etc/kubernetes/pki/etcd/ca.key</li>
</ul>
<p>由此根证书签发机构签发的证书有:</p>
<ol>
<li>etcd server 持有的服务端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/server.crt</li>
<li>/etc/kubernetes/pki/etcd/server.key</li>
</ul>
<ol>
<li>peer 集群中节点互相通信使用的客户端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/peer.crt</li>
<li>/etc/kubernetes/pki/etcd/peer.key</li>
</ul>
<p>注: Peer：对同一个etcd集群中另外一个Member的称呼</p>
<ol>
<li>pod 中定义 Liveness 探针使用的客户端证书<br> kubeadm 部署的 Kubernetes 集群是以 pod 的方式运行 etcd 服务的, 在该 pod 的定义中, 配置了 Liveness 探活探针</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/etcd/healthcheck-client.crt</li>
<li>/etc/kubernetes/pki/etcd/healthcheck-client.key<br>当你 describe etcd 的 pod 时, 会看到如下一行配置:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Liveness:       <span class="built_in">exec</span> [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] delay=15s timeout=15s period=10s <span class="comment">#success=1 #failure=8</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li>配置在 kube-apiserver 中用来与 etcd server 做双向认证的客户端证书</li>
</ol>
<ul>
<li>/etc/kubernetes/pki/apiserver-etcd-client.crt</li>
<li>/etc/kubernetes/pki/apiserver-etcd-client.key</li>
</ul>
<p>至此, 介绍了涉及到 etcd 服务的10个证书文件, 12-10=2, 仅剩两个没有介绍到的文件啦, 胜利在望, 坚持一下~</p>
<h3 id="Serveice-Account秘钥"><a href="#Serveice-Account秘钥" class="headerlink" title="Serveice Account秘钥"></a>Serveice Account秘钥</h3><p>最后介绍的这组”证书”其实不是证书, 而是一组秘钥. 看着后缀名是不是有点眼熟呢, 没错, 这组秘钥对儿其实跟我们在Linux上创建, 用于免密登录的密钥对儿原理是一样的~</p>
<blockquote>
<p>这组的密钥对儿仅提供给 kube-controller-manager 使用. kube-controller-manager 通过 sa.key 对 token 进行签名, master 节点通过公钥 sa.pub 进行签名的验证</p>
</blockquote>
<ul>
<li>/etc/kubernetes/pki/sa.key</li>
<li>/etc/kubernetes/pki/sa.pub</li>
</ul>
<p>至此, kubeadm 工具帮我们创建的所有证书文件都已经介绍完了, 整个 Kubernetes&amp;etcd 集群中所涉及到的绝大部分证书都差不多在这里了. 有的行家可能会看出来, 至少还少了一组证书呀, 就是 kube-proxy 持有的证书怎么没有自动生成呀. 因为 kubeadm 创建的集群, kube-proxy 是以 pod 形式运行的, 在 pod 中, 直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书了. 如果你的 kube-proxy 是以守护进程的方式直接运行在宿主机的, 那么你就需要为它创建一套证书了. 创建的方式也很简单, 直接使用上面第一条提到的 <code>Kubernetes 集群根证书</code> 进行签发就可以了(注意CN和O的设置)</p>
<p>参考文档:</p>
<ul>
<li><a href="https://kubernetes.io/docs/setup/certificates/" target="_blank" rel="external">https://kubernetes.io/docs/setup/certificates/</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/" target="_blank" rel="external">https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/</a></li>
<li>docs.lvrui.io</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubeadm证书过期时间调整]]></title>
      <url>http://team.jiunile.com/blog/2018/12/k8s-kubeadm-ca-upgdate.html</url>
      <content type="html"><![CDATA[<blockquote>
<p> kubeadm 默认证书为一年，一年过期后，会导致api service不可用，使用过程中会出现：x509: certificate has expired or is not yet valid</p>
</blockquote>
<p><strong><code>如何进行调整，下面给了两个方案，供大家选择</code></strong></p>
<h3 id="方案一-通过修改kubeadm-调整证书过期时间"><a href="#方案一-通过修改kubeadm-调整证书过期时间" class="headerlink" title="方案一 通过修改kubeadm 调整证书过期时间"></a>方案一 通过修改kubeadm 调整证书过期时间</h3><a id="more"></a>
<h4 id="修改代码，调整过期时间"><a href="#修改代码，调整过期时间" class="headerlink" title="修改代码，调整过期时间"></a>修改代码，调整过期时间</h4><p> 克隆代码：<code>git clone https://github.com/kubernetes/kubernetes.git</code>, 切换到指定的tag或者版本修改<code>vendor/k8s.io/client-go/util/cert/cert.go</code>文件，<code>git diff</code> 对比如下：<br><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/staging/src/k8s.io/client-go/util/cert/cert.go b/staging/src/k8s.io/client-go/util/cert/cert.go</span><br><span class="line">index fb7f5fa..e800962 100644</span><br><span class="line"><span class="comment">--- a/staging/src/k8s.io/client-go/util/cert/cert.go</span></span><br><span class="line"><span class="comment">+++ b/staging/src/k8s.io/client-go/util/cert/cert.go</span></span><br><span class="line">@@ -104,7 +104,7 @@ func NewSignedCert(cfg Config, key *rsa.PrivateKey, caCert *x509.Certificate, ca</span><br><span class="line">                IPAddresses:  cfg.AltNames.IPs,</span><br><span class="line">                SerialNumber: serial,</span><br><span class="line">                NotBefore:    caCert.NotBefore,</span><br><span class="line"><span class="deletion">-               NotAfter:     time.Now().Add(duration365d).UTC(),</span></span><br><span class="line"><span class="addition">+               NotAfter:     time.Now().Add(duration365d * 10).UTC(),</span></span><br><span class="line">                KeyUsage:     x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature,</span><br><span class="line">                ExtKeyUsage:  cfg.Usages,</span><br><span class="line">        &#125;</span><br><span class="line">@@ -149,7 +149,7 @@ func GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS</span><br><span class="line">                        CommonName: fmt.Sprintf("%s-ca@%d", host, time.Now().Unix()),</span><br><span class="line">                &#125;,</span><br><span class="line">                NotBefore: time.Now(),</span><br><span class="line"><span class="deletion">-               NotAfter:  time.Now().Add(time.Hour * 24 * 365),</span></span><br><span class="line"><span class="addition">+               NotAfter:  time.Now().Add(time.Hour * 24 * 3650),</span></span><br><span class="line"></span><br><span class="line">                KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,</span><br><span class="line">                BasicConstraintsValid: true,</span><br></pre></td></tr></table></figure></p>
<h4 id="编译代码"><a href="#编译代码" class="headerlink" title="编译代码"></a>编译代码</h4><p>编译环境我已经做了对应的1.11.5、1.12.3、1.13.0、1.13.2、1.13.4、1.14.1、1.15.3，已上传到docker hub 上，大家可下载使用，地址如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker pull icyboy/k8s_build:v1.11.5  <span class="comment"># 基于 golang:1.10.3</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.12.3  <span class="comment"># 基于 golang:1.10.4 </span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.0  <span class="comment"># 基于 golang:1.11.2</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.2  <span class="comment"># 基于 golang:1.11.4</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.13.4  <span class="comment"># 基于 golang:1.11.5</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.14.1  <span class="comment"># 基于 golang:1.12.2</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.15.3  <span class="comment"># 基于 golang:1.12.9</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.16.0  <span class="comment"># 基于 golang:1.12.9</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.16.3  <span class="comment"># 基于 golang:1.12.12</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.17.0  <span class="comment"># 基于 golang:1.13.4</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.17.1  <span class="comment"># 基于 golang:1.13.5</span></span><br><span class="line">docker pull icyboy/k8s_build:v1.17.3  <span class="comment"># 基于 golang:1.13.6</span></span><br></pre></td></tr></table></figure></p>
<p>编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -v 你修改后的代码目录:/go/src/k8s.io/kubernetes -it icyboy/k8s_build:v1.11.5 bash</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /go/src/k8s.io/kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubeadm, 这里主要编译kubeadm 即可</span></span><br><span class="line">make all WHAT=cmd/kubeadm GOFLAGS=-v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubelet</span></span><br><span class="line"><span class="comment"># make all WHAT=cmd/kubelet GOFLAGS=-v</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译kubectl</span></span><br><span class="line"><span class="comment"># make all WHAT=cmd/kubectl GOFLAGS=-v</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译完产物在 _output/bin/kubeadm 目录下</span></span><br><span class="line"><span class="comment">#将kubeadm 文件拷贝出来，替换系统中的kubeadm</span></span><br></pre></td></tr></table></figure></p>
<p>对应的kubeadm 文件我也编译好后放到百度云中，大家可放心下载使用，可通过<code>kubeadm version</code> 查看对应的版本信息和官方的进行比对<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译过后的</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"11+"</span>, GitVersion:<span class="string">"v1.11.5-dirty"</span>, GitCommit:<span class="string">"753b2dbc622f5cc417845f0ff8a77f539a4213ea"</span>, GitTreeState:<span class="string">"dirty"</span>, BuildDate:<span class="string">"2018-12-07T05:58:18Z"</span>, GoVersion:<span class="string">"go1.10.3"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#官方的</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"11"</span>, GitVersion:<span class="string">"v1.11.5"</span>, GitCommit:<span class="string">"753b2dbc622f5cc417845f0ff8a77f539a4213ea"</span>, GitTreeState:<span class="string">"clean"</span>, BuildDate:<span class="string">"2018-11-26T14:38:30Z"</span>, GoVersion:<span class="string">"go1.10.3"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>kubeadm 下载地址：<a href="https://pan.baidu.com/s/1PplHyDkYDTusx46j9uHwDA" target="_blank" rel="external">https://pan.baidu.com/s/1PplHyDkYDTusx46j9uHwDA</a><br>提取码：dy6f</p>
<h4 id="替换证书"><a href="#替换证书" class="headerlink" title="替换证书"></a>替换证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用新的kubeadm 替换官方的kubeadm</span></span><br><span class="line">chmod +x kubeadm &amp;&amp; \cp <span class="_">-f</span> kubeadm /usr/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#备份原有的证书</span></span><br><span class="line">mv /etc/kubernetes/pki /etc/kubernetes/pki.old</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成新的证书，kubeadm.yaml 指定你自己服务器上的</span></span><br><span class="line">kubeadm alpha phase certs all --config  ~/kubeadm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#备份原有的conf文件</span></span><br><span class="line">mv /etc/kubernetes/*conf /etc/kubernetes/*conf-old</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据新证书重新生成新的配置文件</span></span><br><span class="line">kubeadm alpha phase kubeconfig all --config ~/kubeadm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#替换老的config文件</span></span><br><span class="line">\cp <span class="_">-f</span> /etc/kubernetes/admin.conf ~/.kube/config</span><br></pre></td></tr></table></figure>
<p><strong>验证</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/pki</span><br><span class="line">openssl x509 -in apiserver-etcd-client.crt -text -noout</span><br><span class="line"><span class="comment">#Certificate:</span></span><br><span class="line"><span class="comment">#    Data:</span></span><br><span class="line"><span class="comment">#        Version: 3 (0x2)</span></span><br><span class="line"><span class="comment">#        Serial Number: 2755977466456048186 (0x263f32e76918023a)</span></span><br><span class="line"><span class="comment">#    Signature Algorithm: sha256WithRSAEncryption</span></span><br><span class="line"><span class="comment">#        Issuer: CN=kubernetes</span></span><br><span class="line"><span class="comment">#        Validity</span></span><br><span class="line"><span class="comment">#            Not Before: Dec  7 09:33:32 2018 GMT</span></span><br><span class="line">             Not After : Dec  4 09:33:32 2028 GMT  <span class="comment">#这里变成10年了</span></span><br><span class="line"><span class="comment">#        Subject: O=system:masters, CN=kube-apiserver-etcd-client</span></span><br><span class="line"><span class="comment">#        Subject Public Key Info:</span></span><br><span class="line"><span class="comment">#        ....</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量验证证书</span></span><br><span class="line"><span class="keyword">for</span> crt <span class="keyword">in</span> $(find /etc/kubernetes/pki/ -name <span class="string">"*.crt"</span>); <span class="keyword">do</span> openssl x509 -in <span class="variable">$crt</span> -noout -dates; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<h3 id="方案二-启用自动轮换kubelet-证书"><a href="#方案二-启用自动轮换kubelet-证书" class="headerlink" title="方案二 启用自动轮换kubelet 证书"></a>方案二 启用自动轮换kubelet 证书</h3><blockquote>
<p>kubelet证书分为server和client两种， <code>k8s 1.9</code>默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启</p>
</blockquote>
<h4 id="增加-kubelet-参数"><a href="#增加-kubelet-参数" class="headerlink" title="增加 kubelet 参数"></a>增加 kubelet 参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数</span></span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true"</span></span><br></pre></td></tr></table></figure>
<h4 id="增加-controller-manager-参数"><a href="#增加-controller-manager-参数" class="headerlink" title="增加 controller-manager 参数"></a>增加 controller-manager 参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数</span></span><br><span class="line">  - <span class="built_in">command</span>:</span><br><span class="line">    - kube-controller-manager</span><br><span class="line">    - --experimental-cluster-signing-duration=87600h0m0s</span><br><span class="line">    - --feature-gates=RotateKubeletServerCertificate=<span class="literal">true</span></span><br><span class="line">    - ....</span><br></pre></td></tr></table></figure>
<h4 id="创建-rbac-对象"><a href="#创建-rbac-对象" class="headerlink" title="创建 rbac 对象"></a>创建 rbac 对象</h4><p>创建rbac对象，允许节点轮换kubelet server证书：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; ca-update.yaml &lt;&lt; EOF</span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRole</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line"><span class="attr">  name:</span> system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> certificates.k8s.io</span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> certificatesigningrequests/selfnodeserver</span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> create</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> rbac.authorization.k8s.io/v1</span><br><span class="line"><span class="attr">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> kubeadm:node-autoapprove-certificate-server</span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> ClusterRole</span><br><span class="line"><span class="attr">  name:</span> system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="attr">  kind:</span> Group</span><br><span class="line"><span class="attr">  name:</span> system:nodes</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl create –f ca-update.yaml</span><br></pre></td></tr></table></figure></p>
<h3 id="如果证书已经过期，如何进行重新签发证书"><a href="#如果证书已经过期，如何进行重新签发证书" class="headerlink" title="如果证书已经过期，如何进行重新签发证书"></a>如果证书已经过期，如何进行重新签发证书</h3><h4 id="针对kubeadm-1-13-x-及以上处理"><a href="#针对kubeadm-1-13-x-及以上处理" class="headerlink" title="针对kubeadm 1.13.x 及以上处理"></a>针对kubeadm 1.13.x 及以上处理</h4><h5 id="准备kubeadm-conf-配置文件一份"><a href="#准备kubeadm-conf-配置文件一份" class="headerlink" title="准备kubeadm.conf 配置文件一份"></a>准备kubeadm.conf 配置文件一份</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> kubeadm.k8s.io/v1beta1</span><br><span class="line"><span class="attr">kind:</span> ClusterConfiguration</span><br><span class="line"><span class="attr">kubernetesVersion:</span> v1<span class="number">.14</span><span class="number">.1</span> <span class="comment">#--&gt;这里改成你集群对应的版本</span></span><br><span class="line"><span class="attr">imageRepository:</span> registry.cn-hangzhou.aliyuncs.com/google_containers </span><br><span class="line"><span class="comment">#这里使用国内的镜像仓库，否则在重新签发的时候会报错：could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt"</span></span><br></pre></td></tr></table></figure>
<h5 id="重新签发命令"><a href="#重新签发命令" class="headerlink" title="重新签发命令"></a>重新签发命令</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha certs renew all --config=/root/kubeadm.conf</span><br><span class="line"></span><br><span class="line">运行如上命令会重新生成以下证书</span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.crt</span></span><br></pre></td></tr></table></figure>
<h5 id="更新-etc-kubernetes-conf文件"><a href="#更新-etc-kubernetes-conf文件" class="headerlink" title="更新/etc/kubernetes/*.conf文件"></a>更新/etc/kubernetes/*.conf文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#备份删除旧的/etc/kubernetes/*.conf文件</span></span><br><span class="line">mkdir /etc/kubernetes/old-conf  </span><br><span class="line">mv /etc/kubernetes/*.conf /etc/kubernetes/old-conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成新的conf文件</span></span><br><span class="line">kubeadm init phase kubeconfig all --config=/root/kubeadm.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#运行如上命令会重新生成以下conf文件</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/admin.conf</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/controller-manager.conf</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/kubelet.conf</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/scheduler.conf</span></span><br></pre></td></tr></table></figure>
<h5 id="完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器-最后覆盖config文件"><a href="#完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器-最后覆盖config文件" class="headerlink" title="完成后重启kube-apiserver,kube-controller,kube-scheduler,etcd这4个容器,最后覆盖config文件"></a>完成后重启<code>kube-apiserver</code>,<code>kube-controller</code>,<code>kube-scheduler</code>,<code>etcd</code>这4个容器,最后覆盖config文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure>
<h4 id="针对kubeadm-1-13-0（不包含1-13-0）-以下处理"><a href="#针对kubeadm-1-13-0（不包含1-13-0）-以下处理" class="headerlink" title="针对kubeadm 1.13.0（不包含1.13.0） 以下处理"></a>针对kubeadm 1.13.0（不包含1.13.0） 以下处理</h4><h5 id="移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】"><a href="#移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】" class="headerlink" title="移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】"></a>移动证书和配置【注意！必须移动，不然会使用现有的证书，不会重新生成】</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes</span><br><span class="line">mkdir ./pki_bak</span><br><span class="line">mkdir ./pki_bak/etcd</span><br><span class="line">mkdir ./conf_bak</span><br><span class="line">mv pki/apiserver* ./pki_bak/</span><br><span class="line">mv pki/front-proxy-client.* ./pki_bak/</span><br><span class="line">mv pki/etcd/healthcheck-client.* ./pki_bak/etcd/</span><br><span class="line">mv pki/etcd/peer.* ./pki_bak/etcd/</span><br><span class="line">mv pki/etcd/server.* ./pki_bak/etcd/</span><br><span class="line">mv ./admin.conf ./conf_bak/</span><br><span class="line">mv ./kubelet.conf ./conf_bak/</span><br><span class="line">mv ./controller-manager.conf ./conf_bak/</span><br><span class="line">mv ./scheduler.conf ./conf_bak/</span><br></pre></td></tr></table></figure>
<h5 id="创建证书"><a href="#创建证书" class="headerlink" title="创建证书"></a>创建证书</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs all --apiserver-advertise-address=<span class="variable">$&#123;MASTER_API_SERVER_IP&#125;</span> --apiserver-cert-extra-sans=主机内网ip,主机公网ip</span><br><span class="line"></span><br><span class="line">运行如上命令会重新生成以下证书</span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-etcd-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/apiserver-kubelet-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/front-proxy-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/healthcheck-client.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/peer.crt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.key</span></span><br><span class="line"><span class="comment">#-- /etc/kubernetes/pki/etcd/server.crt</span></span><br><span class="line"></span><br><span class="line">不移动证书会有如下提示</span><br><span class="line"><span class="comment">#[certificates] Using the existing apiserver certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing apiserver-kubelet-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing front-proxy-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing etcd/server certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing etcd/peer certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing etcd/healthcheck-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing apiserver-etcd-client certificate and key.</span></span><br><span class="line"><span class="comment">#[certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"</span></span><br><span class="line"><span class="comment">#[certificates] Using the existing sa key.</span></span><br></pre></td></tr></table></figure>
<h5 id="生成新配置文件"><a href="#生成新配置文件" class="headerlink" title="生成新配置文件"></a>生成新配置文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --apiserver-advertise-address=<span class="variable">$&#123;MASTER_API_SERVER_IP&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="将新生成的admin配置文件覆盖掉原本的admin文件"><a href="#将新生成的admin配置文件覆盖掉原本的admin文件" class="headerlink" title="将新生成的admin配置文件覆盖掉原本的admin文件"></a>将新生成的admin配置文件覆盖掉原本的admin文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="variable">$HOME</span>/.kube/config <span class="variable">$HOME</span>/.kube/config.old</span><br><span class="line">cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chmod 644 <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure>
<h5 id="完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器"><a href="#完成后重启kube-apiserver-kube-controller-kube-scheduler-etcd这4个容器" class="headerlink" title="完成后重启kube-apiserver,kube-controller,kube-scheduler,etcd这4个容器"></a>完成后重启kube-apiserver,kube-controller,kube-scheduler,etcd这4个容器</h5><h5 id="如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。"><a href="#如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。" class="headerlink" title="如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。"></a>如果有多台master，则将第一台生成的相关证书拷贝到其余master即可。</h5><h3 id="离线一键安装包"><a href="#离线一键安装包" class="headerlink" title="离线一键安装包"></a>离线一键安装包</h3><blockquote>
<p>k8s 离线一键安装包教程&amp;&amp;地址：<a href="http://team.jiunile.com/pro/k8s/">一键安装</a></p>
</blockquote>
<h3 id="kubeadm-1-14-证书调整教程"><a href="#kubeadm-1-14-证书调整教程" class="headerlink" title="kubeadm 1.14 证书调整教程"></a>kubeadm 1.14 证书调整教程</h3><blockquote>
<p><a href="http://team.jiunile.com/blog/2019/05/k8s-kubeadm14-ca-upgrade.html">教程地址</a></p>
</blockquote>
<p><img src="/images/wx_dyh.png" alt="微信订阅号"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes Pod 的生命周期管理]]></title>
      <url>http://team.jiunile.com/blog/2018/11/k8s-k8s-pod-life-cycle.html</url>
      <content type="html"><![CDATA[<h2 id="Pod的生命周期"><a href="#Pod的生命周期" class="headerlink" title="Pod的生命周期"></a>Pod的生命周期</h2><hr>
<h3 id="Pod-phase"><a href="#Pod-phase" class="headerlink" title="Pod phase"></a>Pod phase</h3><p>Pod 的 status 在信息保存在 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471" target="_blank" rel="external">PodStatus</a> 中定义，其中有一个 phase 字段。</p>
<p>Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。</p>
<p>Pod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。</p>
<p>无论你是手动创建 Pod，还是通过 deployment、daemonset 或 statefulset来创建，Pod 的 phase 都有以下几个可能的值：</p>
<ul>
<li><strong>挂起（Pending）</strong>：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。</li>
<li><strong>运行中（Running）</strong>：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。</li>
<li><strong>成功（Successed）</strong>：Pod 中的所有容器都被成功终止，并且不会再重启。</li>
<li><strong>失败（Failed）</strong>：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。</li>
<li><strong>未知（Unkonwn）</strong>：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。</li>
</ul>
<p>下图是 Pod 的生命周期示意图，从图中可以看到 Pod 状态的变化。<br><img src="/images/k8s/k8s_pod_01.jpg" alt="K8s pod 生命周期"></p>
<a id="more"></a>
<h3 id="Pod-状态"><a href="#Pod-状态" class="headerlink" title="Pod 状态"></a>Pod 状态</h3><p>Pod 有一个 PodStatus 对象，其中包含一个 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L1964" target="_blank" rel="external">PodCondition</a> 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 <code>PodScheduled</code>、<code>Ready</code>、<code>Initialized</code> 和 <code>Unschedulable</code>。status 字段是一个字符串，可能的值有 <code>True</code>、<code>False</code> 和 <code>Unknown</code>。</p>
<p>当你通过 <code>kubectl get pod</code> 查看 Pod 时，<code>STATUS</code> 这一列可能会显示与上述5个状态不同的值，例如 <code>Init:0/1</code> 和 <code>CrashLoopBackOff</code>。这是因为 Pod 状态的定义除了包含 phase 之外，还有 <code>InitContainerStatuses</code> 和 <code>containerStatuses</code> 等其他字段，具体代码参考 <a href="https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471" target="_blank" rel="external">overall status of a pod</a> .</p>
<p>如果想知道究竟发生了什么，可以通过命令 <code>kubectl describe pod/$PODNAME</code> 查看输出信息的 <code>Events</code> 条目。通过 Events 条目可以看到一些具体的信息，比如正在拉取容器镜像，Pod 已经被调度，或者某个 container 处于 unhealthy 状态。</p>
<h2 id="Pod-的启动关闭流程"><a href="#Pod-的启动关闭流程" class="headerlink" title="Pod 的启动关闭流程"></a>Pod 的启动关闭流程</h2><hr>
<p>下面通过一个具体的示例来探究一下 Pod 的整个生命周期流程。为了确定事情发生的顺序，通过下面的 manifest 来部署一个 deployment。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span>                   Deployment</span><br><span class="line"><span class="attr">apiVersion:</span>             apps/v1beta1</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span>                 loap</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span>             <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span>            loap</span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span>           init</span><br><span class="line"><span class="attr">        image:</span>          busybox</span><br><span class="line"><span class="attr">        command:</span>       [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): INIT &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span>    /loap</span><br><span class="line"><span class="attr">          name:</span>         timing</span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span>           main</span><br><span class="line"><span class="attr">        image:</span>          busybox</span><br><span class="line"><span class="attr">        command:</span>       [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): START &gt;&gt; /loap/timing;</span><br><span class="line">sleep 10; echo $(date +%s): END &gt;&gt; /loap/timing;'</span>]</span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span>    /loap</span><br><span class="line"><span class="attr">          name:</span>         timing</span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): LIVENESS &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): READINESS &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">        lifecycle:</span></span><br><span class="line"><span class="attr">          postStart:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span>   [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): POST-START &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">          preStop:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span>  [<span class="string">'sh'</span>, <span class="string">'-c'</span>, <span class="string">'echo $(date +%s): PRE-HOOK &gt;&gt; /loap/timing'</span>]</span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span>           timing</span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span>         /tmp/loap</span><br></pre></td></tr></table></figure></p>
<p>等待 Pod 状态变为 <code>Running</code> 之后，通过以下命令来强制停止 Pod：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale deployment loap --replicas=0</span><br></pre></td></tr></table></figure></p>
<p>查看 <code>/tmp/loap/timing</code> 文件的内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /tmp/loap/timing</span><br><span class="line"></span><br><span class="line">1525334577: INIT</span><br><span class="line">1525334581: START</span><br><span class="line">1525334581: POST-START</span><br><span class="line">1525334584: READINESS</span><br><span class="line">1525334584: LIVENESS</span><br><span class="line">1525334588: PRE-HOOK</span><br><span class="line">1525334589: END</span><br></pre></td></tr></table></figure></p>
<p><code>/tmp/loap/timing</code> 文件的内容很好地体现了 Pod 的启动和关闭流程，具体过程如下：<br><img src="/images/k8s/k8s_pod_02.jpg" alt="Pod 的启动和关闭流程"></p>
<ol>
<li>首先启动一个 Infra 容器（又叫 Pause 容器），用来和 Pod 中的其他容器共享 linux 命名空间，并开启 init 进程。（上图中忽略了这一步）</li>
<li>然后启动 Init 容器，它是一种专用的容器，在应用程序容器启动之前运行，用来对 Pod 进行一些初始化操作，并包括一些应用镜像中不存在的实用工具和安装脚本。</li>
<li>4 秒之后，应用程序容器和 <code>post-start hook</code> 同时启动。</li>
<li>7 秒之后开始启动 <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank" rel="external">liveness 和 readiness 探针</a>。</li>
<li>11 秒之后，通过手动杀掉 Pod，<code>pre-stop hook</code> 执行，优雅删除期限过期后（默认是 30 秒），应用程序容器停止。实际的 Pod 终止过程要更复杂，具体参考 <a href="https://jimmysong.io/kubernetes-handbook/concepts/pod.html" target="_blank" rel="external">Pod 的终止</a>。</li>
</ol>
<blockquote>
<p>必须主动杀掉 Pod 才会触发 <code>pre-stop hook</code>，如果是 Pod 自己 Down 掉，则不会执行 <code>pre-stop hook</code>。</p>
</blockquote>
<h2 id="如何快速-DEBUG"><a href="#如何快速-DEBUG" class="headerlink" title="如何快速 DEBUG"></a>如何快速 DEBUG</h2><hr>
<p>当 Pod 出现致命的错误时，如果能够快速 DEBUG，将会帮助我们快速定位问题。为了实现这个目的，可以把把致命事件的信息通过 <code>.spec.terminationMessagePath</code> 配置写入指定位置的文件，就像打印错误、异常和堆栈信息一样。该位置的内容可以很方便的通过 dashboards、监控软件等工具检索和展示，默认路径为 <code>/dev/termination-log</code>。</p>
<p>以下是一个小例子：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># termination-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> v1</span><br><span class="line"><span class="attr">kind:</span> Pod</span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> termination-demo</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> termination-demo-container</span><br><span class="line"><span class="attr">    image:</span> alpine</span><br><span class="line"><span class="attr">    command:</span> [<span class="string">"/bin/sh"</span>]</span><br><span class="line"><span class="attr">    args:</span> [<span class="string">"-c"</span>, <span class="string">"sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log"</span>]</span><br></pre></td></tr></table></figure></p>
<p>这些消息的最后部分会使用其他的规定来单独存储：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create <span class="_">-f</span> termination-demo.yaml</span><br><span class="line"></span><br><span class="line">$ sleep 20</span><br><span class="line"></span><br><span class="line">$ kubectl get pod termination-demo -o go-template=<span class="string">'&#123;&#123;range .status.containerStatuses&#125;&#125;&#123;&#123;.lastState.terminated.message&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line">Sleep expired</span><br><span class="line"></span><br><span class="line">$ kubectl get pod termination-demo -o go-template=<span class="string">'&#123;&#123;range .status.containerStatuses&#125;&#125;&#123;&#123;.lastState.terminated.exitCode&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line">0</span><br></pre></td></tr></table></figure></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><hr>
<ul>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/pod-hook.html" target="_blank" rel="external">Pod hook</a></li>
<li><a href="https://blog.openshift.com/kubernetes-pods-life/" target="_blank" rel="external">Kubernetes: A Pod’s Life</a></li>
<li><a href="https://k8smeetup.github.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/" target="_blank" rel="external">确定 Pod 失败的原因</a></li>
<li>Ryan Yang </li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java和Docker限制的那些事儿]]></title>
      <url>http://team.jiunile.com/blog/2018/07/docker-java%E4%B8%8Edocker%E7%9A%84%E9%82%A3%E7%82%B9%E4%BA%8B.html</url>
      <content type="html"><![CDATA[<h2 id="揭秘"><a href="#揭秘" class="headerlink" title="揭秘"></a>揭秘</h2><p>Java和Docker不是天然的朋友。 Docker可以设置内存和CPU限制，而Java不能自动检测到。使用Java的Xmx标识（繁琐/重复）或新的实验性JVM标识，我们可以解决这个问题。</p>
<h2 id="虚拟化中的不匹配"><a href="#虚拟化中的不匹配" class="headerlink" title="虚拟化中的不匹配"></a>虚拟化中的不匹配</h2><p>Java和Docker的结合并不是完美匹配的，最初的时候离完美匹配有相当大的距离。对于初学者来说，JVM的全部设想就是，虚拟机可以让程序与底层硬件无关。</p>
<p>那么，把我们的Java应用打包到JVM中，然后整个再塞进Docker容器中，能给我们带来什么好处呢？大多数情况下，你只是在复制JVMs和Linux容器，除了浪费更多的内存，没任何好处。感觉这样子挺傻的。</p>
<p>不过，Docker可以把你的程序，设置，特定的JDK，Linux设置和应用服务器，还有其他工具打包在一起，当做一个东西。站在DevOps/Cloud的角度来看，这样一个完整的容器有着更高层次的封装。</p>
<h3 id="问题一：内存"><a href="#问题一：内存" class="headerlink" title="问题一：内存"></a>问题一：内存</h3><p>时至今日，绝大多数产品级应用仍然在使用Java 8（或者更旧的版本），而这可能会带来问题。Java 8（update 131之前的版本）跟Docker无法很好地一起工作。问题是在你的机器上，JVM的可用内存和CPU数量并不是Docker允许你使用的可用内存和CPU数量。</p>
<p>比如，如果你限制了你的Docker容器只能使用100MB内存，但是呢，旧版本的Java并不能识别这个限制。Java看不到这个限制。JVM会要求更多内存，而且远超这个限制。如果使用太多内存，Docker将采取行动并杀死容器内的进程！JAVA进程被干掉了，很明显，这并不是我们想要的。</p>
<p>为了解决这个问题，你需要给Java指定一个最大内存限制。在旧版本的Java（8u131之前），你需要在容器中通过设置-Xmx来限制堆大小。这感觉不太对，你可不想定义这些限制两次，也不太想在你的容器中来定义。</p>
<p>幸运的是我们现在有了更好的方式来解决这个问题。从Java 9之后（8u131+），JVM增加了如下标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap</span><br></pre></td></tr></table></figure></p>
<p>这些标志强制JVM检查Linux的cgroup配置，Docker是通过cgroup来实现最大内存设置的。现在，如果你的应用到达了Docker设置的限制（比如500MB），JVM是可以看到这个限制的。JVM将会尝试GC操作。如果仍然超过内存限制，JVM就会做它该做的事情，抛出OutOfMemoryException。也就是说，JVM能够看到Docker的这些设置。</p>
<p>从Java 10之后（参考下面的测试），这些体验标志位是默认开启的，也可以使用-XX:+UseContainerSupport来使能（你可以通过设置-XX:-UseContainerSupport来禁止这些行为）。</p>
<h3 id="问题二：CPU"><a href="#问题二：CPU" class="headerlink" title="问题二：CPU"></a>问题二：CPU</h3><p>第二个问题是类似的，但它与CPU有关。简而言之，JVM将查看硬件并检测CPU的数量。它会优化你的runtime以使用这些CPUs。但是同样的情况，这里还有另一个不匹配，Docker可能不允许你使用所有这些CPUs。可惜的是，这在Java 8或Java 9中并没有修复，但是在Java 10中得到了解决。</p>
<p>从Java 10开始，可用的CPUs的计算将采用以不同的方式（默认情况下）解决此问题（同样是通过UseContainerSupport）。<br><a id="more"></a></p>
<h2 id="Java和Docker的内存处理测试"><a href="#Java和Docker的内存处理测试" class="headerlink" title="Java和Docker的内存处理测试"></a>Java和Docker的内存处理测试</h2><p>作为一个有趣的练习，让我们验证并测试Docker如何使用几个不同的JVM版本/标志甚至不同的JVM来处理内存不足。</p>
<p>首先，我们创建一个测试应用程序，它只是简单地“吃”内存并且不释放它。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MemEat</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        List l = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">byte</span> b[] = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1048576</span>];</span><br><span class="line">            l.add(b);</span><br><span class="line">            Runtime rt = Runtime.getRuntime();</span><br><span class="line">            System.out.println( <span class="string">"free memory: "</span> + rt.freeMemory() );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们可以启动Docker容器并运行这个应用程序来查看会发生什么。</p>
<h3 id="测试一：Java-8u111"><a href="#测试一：Java-8u111" class="headerlink" title="测试一：Java 8u111"></a>测试一：Java 8u111</h3><p>首先，我们将从具有旧版本Java 8的容器开始（update 111）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it java:openjdk-8u111 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>我们编译并运行MemEat.java文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 67194416</span><br><span class="line">free memory: 66145824</span><br><span class="line">free memory: 65097232</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>正如所料，Docker已经杀死了我们的Java进程。不是我们想要的（！）。你也可以看到输出，Java认为它仍然有大量的内存需要分配。</p>
<p>我们可以通过使用-Xmx标志为Java提供最大内存来解决此问题：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java -Xmx100m MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 1155664</span><br><span class="line">free memory: 1679936</span><br><span class="line">free memory: 2204208</span><br><span class="line">free memory: 1315752</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>在提供了我们自己的内存限制之后，进程正常停止，JVM理解它正在运行的限制。然而，问题在于你现在将这些内存限制设置了两次，Docker一次，JVM一次。</p>
<h3 id="测试二：Java-8u144"><a href="#测试二：Java-8u144" class="headerlink" title="测试二：Java 8u144"></a>测试二：Java 8u144</h3><p>如前所述，随着增加新标志来修复问题，JVM现在可以遵循Docker所提供的设置。我们可以使用版本新一点的JVM来测试它。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk8 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>（在撰写本文时，此OpenJDK Java镜像的版本是Java 8u144）</p>
<p>接下来，我们再次编译并运行MemEat.java文件，不带任何标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 67194416</span><br><span class="line">free memory: 66145824</span><br><span class="line">free memory: 65097232</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>依然存在同样的问题。但是我们现在可以提供上面提到的实验性标志来试试看：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 1679936</span><br><span class="line">free memory: 2204208</span><br><span class="line">free memory: 1155616</span><br><span class="line">free memory: 1155600</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">   at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>这一次我们没有告诉JVM限制的是什么，我们只是告诉JVM去检查正确的限制设置！现在感觉好多了。</p>
<h3 id="测试三：Java-10u23"><a href="#测试三：Java-10u23" class="headerlink" title="测试三：Java 10u23"></a>测试三：Java 10u23</h3><p>有些人在评论和Reddit上提到Java 10通过使实验标志成为新的默认值来解决所有问题。这种行为可以通过禁用此标志来关闭：-XX：-UseContainerSupport。</p>
<p>当我测试它时，它最初不起作用。在撰写本文时，AdoptAJDK OpenJDK10镜像与jdk-10+23一起打包。这个JVM显然还是不理解UseContainerSupport标志，该进程仍然被Docker杀死。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk10 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>测试了代码（甚至手动提供需要的标志）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 96262112</span><br><span class="line">free memory: 94164960</span><br><span class="line">free memory: 92067808</span><br><span class="line">free memory: 89970656</span><br><span class="line">Killed</span><br><span class="line">java -XX:+UseContainerSupport MemEat</span><br><span class="line">Unrecognized VM option <span class="string">'UseContainerSupport'</span></span><br><span class="line">Error: Could not create the Java Virtual Machine.</span><br><span class="line">Error: A fatal exception has occurred. Program will exit.</span><br></pre></td></tr></table></figure></p>
<h3 id="测试四：Java-10u46（Nightly）"><a href="#测试四：Java-10u46（Nightly）" class="headerlink" title="测试四：Java 10u46（Nightly）"></a>测试四：Java 10u46（Nightly）</h3><p>我决定尝试AdoptAJDK OpenJDK 10的最新nightly构建。它包含的版本是Java 10+46，而不是Java 10+23。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk10:nightly /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>然而，在这个ngithly构建中有一个问题，导出的PATH指向旧的Java 10+23目录，而不是10+46，我们需要修复这个问题。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/opt/java/openjdk/jdk-10+46/bin/</span><br><span class="line">javac MemEat.java</span><br><span class="line">java MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 3566824</span><br><span class="line">free memory: 2796008</span><br><span class="line">free memory: 1480320</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">  at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>成功！不提供任何标志，Java 10依然可以正确检测到Dockers内存限制。</p>
<h3 id="测试五：OpenJ9"><a href="#测试五：OpenJ9" class="headerlink" title="测试五：OpenJ9"></a>测试五：OpenJ9</h3><p>我最近也在试用OpenJ9，这个免费的替代JVM已经从IBM J9开源，现在由Eclipse维护。</p>
<p>请在我的下一篇博文<a href="http://royvanrijn.com/blog/2018/05/openj9-jvm-shootout/" target="_blank" rel="external">http://royvanrijn.com/blog/2018/05/openj9-jvm-shootout/</a>阅读关于OpenJ9的更多信息。</p>
<p>它运行速度快，内存管理非常好，性能卓越，经常可以为我们的微服务节省多达30-50％的内存。这几乎可以将Spring Boot应用程序定义为’micro’了，其运行时间只有100-200mb，而不是300mb+。我打算尽快就此写一篇关于这方面的文章。</p>
<p>但令我惊讶的是，OpenJ9还没有类似于Java 8/9/10+中针对cgroup内存限制的标志（backported）的选项。如果我们将以前的测试用例应用到最新的AdoptAJDK OpenJDK 9 + OpenJ9 build：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk9-openj9 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>我们添加OpenJDK标志（OpenJ9会忽略的标志）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 83988984</span><br><span class="line">free memory: 82940400</span><br><span class="line">free memory: 81891816</span><br><span class="line">Killed</span><br></pre></td></tr></table></figure></p>
<p>Oops，JVM再次被Docker杀死。</p>
<p>我真的希望类似的选项将很快添加到OpenJ9中，因为我希望在生产环境中运行这个选项，而不必指定最大内存两次。 Eclipse/IBM正在努力修复这个问题，已经提了issues，甚至已经针对issues提交了PR。</p>
<h3 id="更新：（不推荐Hack）"><a href="#更新：（不推荐Hack）" class="headerlink" title="更新：（不推荐Hack）"></a>更新：（不推荐Hack）</h3><p>一个稍微丑陋/hacky的方式来解决这个问题是使用下面的组合标志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">java -Xmx`cat /sys/fs/cgroup/memory/memory.limit_<span class="keyword">in</span>_bytes` MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 3171536</span><br><span class="line">free memory: 2127048</span><br><span class="line">free memory: 2397632</span><br><span class="line">free memory: 1344952</span><br><span class="line">JVMDUMP039I Processing dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span> at 2018/05/15 14:04:26 - please wait.</span><br><span class="line">JVMDUMP032I JVM requested System dump using <span class="string">'//core.20180515.140426.125.0001.dmp'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I System dump written to //core.20180515.140426.125.0001.dmp</span><br><span class="line">JVMDUMP032I JVM requested Heap dump using <span class="string">'//heapdump.20180515.140426.125.0002.phd'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Heap dump written to //heapdump.20180515.140426.125.0002.phd</span><br><span class="line">JVMDUMP032I JVM requested Java dump using <span class="string">'//javacore.20180515.140426.125.0003.txt'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Java dump written to //javacore.20180515.140426.125.0003.txt</span><br><span class="line">JVMDUMP032I JVM requested Snap dump using <span class="string">'//Snap.20180515.140426.125.0004.trc'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Snap dump written to //Snap.20180515.140426.125.0004.trc</span><br><span class="line">JVMDUMP013I Processed dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span>.</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">  at MemEat.main(MemEat.java:8)</span><br></pre></td></tr></table></figure></p>
<p>在这种情况下，堆大小受限于分配给Docker实例的内存，这适用于较旧的JVM和OpenJ9。这当然是错误的，因为容器本身和堆外的JVM的其他部分也使用内存。但它似乎工作，显然Docker在这种情况下是宽松的。也许某些bash大神会做出更好的版本，从其他进程的字节中减去一部分。</p>
<p>无论如何，不要这样做，它可能无法正常工作。</p>
<h3 id="测试六：OpenJ9（Nightly）"><a href="#测试六：OpenJ9（Nightly）" class="headerlink" title="测试六：OpenJ9（Nightly）"></a>测试六：OpenJ9（Nightly）</h3><p>有人建议使用OpenJ9的最新nightly版本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -m 100m -it adoptopenjdk/openjdk9-openj9:nightly /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>最新的OpenJ9夜间版本，它有两个东西：</p>
<ol>
<li>另一个有问题的PATH参数，需要先解决这个问题</li>
<li>JVM支持新标志UseContainerSupport（就像Java 10一样）</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/opt/java/openjdk/jdk-9.0.4+12/bin/</span><br><span class="line">javac MemEat.java</span><br><span class="line">java -XX:+UseContainerSupport MemEat</span><br><span class="line">...</span><br><span class="line">free memory: 5864464</span><br><span class="line">free memory: 4815880</span><br><span class="line">free memory: 3443712</span><br><span class="line">free memory: 2391032</span><br><span class="line">JVMDUMP039I Processing dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span> at 2018/05/15 21:32:07 - please wait.</span><br><span class="line">JVMDUMP032I JVM requested System dump using <span class="string">'//core.20180515.213207.62.0001.dmp'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I System dump written to //core.20180515.213207.62.0001.dmp</span><br><span class="line">JVMDUMP032I JVM requested Heap dump using <span class="string">'//heapdump.20180515.213207.62.0002.phd'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Heap dump written to //heapdump.20180515.213207.62.0002.phd</span><br><span class="line">JVMDUMP032I JVM requested Java dump using <span class="string">'//javacore.20180515.213207.62.0003.txt'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Java dump written to //javacore.20180515.213207.62.0003.txt</span><br><span class="line">JVMDUMP032I JVM requested Snap dump using <span class="string">'//Snap.20180515.213207.62.0004.trc'</span> <span class="keyword">in</span> response to an event</span><br><span class="line">JVMDUMP010I Snap dump written to //Snap.20180515.213207.62.0004.trc</span><br><span class="line">JVMDUMP013I Processed dump event <span class="string">"systhrow"</span>, detail <span class="string">"java/lang/OutOfMemoryError"</span>.</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.OutOfMemoryError: Java heap space</span><br></pre></td></tr></table></figure>
<p>TADAAA，正在修复中！</p>
<p>奇怪的是，这个标志在OpenJ9中默认没有启用，就像它在Java 10中一样。再说一次：确保你测试了这是你想在一个Docker容器中运行Java。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>简言之：注意资源限制的不匹配。测试你的内存设置和JVM标志，不要假设任何东西。</p>
<p>如果您在Docker容器中运行Java，请确保你设置了Docker内存限制和在JVM中也做了限制，或者你的JVM能够理解这些限制。</p>
<p>如果您无法升级您的Java版本，请使用-Xmx设置您自己的限制。</p>
<p>对于Java 8和Java 9，请更新到最新版本并使用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX：+UnlockExperimentalVMOptions -XX：+UseCGroupMemoryLimitForHeap</span><br></pre></td></tr></table></figure></p>
<p>对于Java 10，确保它支持’UseContainerSupport’（更新到最新版本）。</p>
<p>对于OpenJ9（我强烈建议使用，可以在生产环境中有效减少内存占用量），现在使用-Xmx设置限制，但很快会出现一个支持UseContainerSupport标志的版本。</p>
<p>原文链接：<a href="http://royvanrijn.com/blog/2018/05/java-and-docker-memory-limits/" target="_blank" rel="external">http://royvanrijn.com/blog/2018/05/java-and-docker-memory-limits/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Centos 6.x 上升级go 1.10.3]]></title>
      <url>http://team.jiunile.com/blog/2018/07/go-%E5%8D%87%E7%BA%A7go1-10-x%E9%97%AE%E9%A2%98.html</url>
      <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><blockquote>
<p>目前本地Linux (Centos 6.x) 编译环境还滞留在1.8.x上，为了提升go性能，想将go升级到最新版，但在升级过程中遇到如下问题，故此记录下！忘后续的go友能跳过此坑！</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在Centos 6.x 升级go 1.10.x过程中遇到如下问题：</p>
<ul>
<li>step1.  下载go 1.10.x 源码</li>
<li>step2.  解压，进入go/src</li>
<li>step3.  执行./all.bash</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Building Go cmd/dist using /root/go1.4.</span><br><span class="line">Building Go toolchain1 using /root/go1.4.</span><br><span class="line">Building Go bootstrap cmd/go (go_bootstrap) using Go toolchain1.</span><br><span class="line">Building Go toolchain2 using go_bootstrap and Go toolchain1.</span><br><span class="line">Building Go toolchain3 using go_bootstrap and Go toolchain2.</span><br><span class="line">Building packages and commands <span class="keyword">for</span> linux/amd64.</span><br><span class="line"></span><br><span class="line"><span class="comment">##### Testing packages.</span></span><br><span class="line">.... 过程略长，特此省略</span><br><span class="line">ok      cmd/internal/src    0.001s</span><br><span class="line">ok      cmd/internal/<span class="built_in">test</span>2json    0.097s</span><br><span class="line">ok      cmd/link    1.988s</span><br><span class="line">ok      cmd/link/internal/ld    43.529s</span><br><span class="line">ok      cmd/nm    3.417s</span><br><span class="line">ok      cmd/objdump    1.588s</span><br><span class="line">ok      cmd/pack    1.217s</span><br><span class="line">ok      cmd/trace    0.007s</span><br><span class="line">--- FAIL: TestObjFile (0.01s)</span><br><span class="line">    binutils_test.go:231: SourceLine: unexpected error write |1: broken pipe</span><br><span class="line">FAIL</span><br><span class="line">FAIL    cmd/vendor/github.com/google/pprof/internal/binutils    0.018s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/driver    12.194s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/elfexec    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/graph    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/measurement    0.002s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/report    0.048s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/symbolizer    0.004s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/internal/symbolz    0.004s</span><br><span class="line">ok      cmd/vendor/github.com/google/pprof/profile    0.045s</span><br><span class="line">ok      cmd/vendor/github.com/ianlancetaylor/demangle    0.012s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/arm/armasm    0.007s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/arm64/arm64asm    0.043s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/ppc64/ppc64asm    0.003s</span><br><span class="line">ok      cmd/vendor/golang.org/x/arch/x86/x86asm    0.064s</span><br><span class="line">ok      cmd/vet    1.205s</span><br><span class="line">ok      cmd/vet/internal/cfg    0.002s</span><br><span class="line">2018/07/18 17:59:22 Failed: <span class="built_in">exit</span> status 1</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>为什么在执行binutils_test.go 会Failed，最终查看代码原因是因为如下命令引起：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">120807 3650  execve(<span class="string">"/usr/bin/addr2line"</span>, [<span class="string">"/usr/bin/addr2line"</span>, <span class="string">"-aif"</span>, <span class="string">"-e"</span>, <span class="string">"testdata/exe_linux_64"</span>], [/* 15 vars */] &lt;unfinished ...&gt;</span><br></pre></td></tr></table></figure></p>
<p>有关生成此命令的源代码可查看如下地址：<a href="https://github.com/google/pprof/blob/a74ae6fb3cd7047c79272e3ea0814b08154a2d3c/internal/binutils/addr2liner.go#L92" target="_blank" rel="external">addr2line</a></p>
<p>add2line文件来自于包：binutils</p>
<p>执行命令失败，是因为在CentOS 6.x上，binutils的版本是2.20，<a href="https://sourceware.org/git/gitweb.cgi?p=binutils-gdb.git;a=blob_plain;f=binutils/NEWS;hb=refs/tags/binutils-2_27" target="_blank" rel="external">参考文献</a> ，然后addr2line命令中的-a参数在binutils 2.21版中才添加</p>
<p>因此，为了解决这个问题，我从源码重新进行编译binutils并将其构建的二进制文件添加到PATH中，然后运行测试，并成功通过。</p>
<p>编译安装binutils过程如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://ftp.gnu.org/gnu/binutils/binutils-2.27.tar.gz</span><br><span class="line">tar zxvf binutils-2.27.tar.gz </span><br><span class="line"><span class="built_in">cd</span> binutils-2.27</span><br><span class="line">./configure --prefix=/usr</span><br><span class="line">make </span><br><span class="line">make install</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong><code>特此说明，在Centos 7.x 上能成功避免此坑！！</code></strong></p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Logstash filter插件开发]]></title>
      <url>http://team.jiunile.com/blog/2017/08/log-logstash-filter.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Logstash是一个具有实时管线能力的开源数据收集引擎。在ELK Stack中，通常选择更轻量级的Filebeat收集日志，然后将日志输出到Logstash进行加工处理，再将处理后的日志输出到指定的目标（ElasticSearch，Kafka等）当中。</p>
<p>Logstash事件的处理管线是inputs → filters → outputs，三个阶段都可以自定义插件，本文主要介绍如何开发自定义需求最多的filter插件。</p>
<p>Logstash的安装就不详细介绍了，下载传送门：<a href="https://www.elastic.co/downloads/logstash" target="_blank" rel="external">https://www.elastic.co/downloads/logstash</a>。</p>
<h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><p>我们使用docker来讲解如何给logstash定制一个filter插件，首先，我们下载logstash官方最新版本的镜像。下载方式: <code>docker pull logstash</code>，下载好后我们就来Run这个镜像，命令：<code>docker run -it logstash bash</code>，这样我们就进入到logstash容器中了。接下来我们就安装下两个基本的软件包，一个vim，一个rsyslog，命令如下：<code>apt-get update &amp;&amp; apt-get -y install vim rsyslog &amp;&amp; /etc/init.d/rsysylog start</code>。到此我们的准备工作就完毕了。</p>
<a id="more"></a>
<h2 id="生成filter插件"><a href="#生成filter插件" class="headerlink" title="生成filter插件"></a>生成filter插件</h2><p>cd到Logstash的跟目录，使用<code>bin/logstash-plugin</code>生成filter插件模板，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/share/logstash</span><br><span class="line">mkdir -p vendor/localgems</span><br><span class="line">bin/logstash-plugin generate --type filter --name <span class="built_in">test</span> --path vendor/localgems</span><br><span class="line"><span class="comment">#vendor/localgems 可修改为你自己的路径</span></span><br></pre></td></tr></table></figure></p>
<p>查看filter插件的目录结构，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@c11787c4cef3:/usr/share/logstash/vendor/localgems<span class="comment"># tree .</span></span><br><span class="line">.</span><br><span class="line">└── logstash-filter-test</span><br><span class="line">    ├── CHANGELOG.md</span><br><span class="line">    ├── CONTRIBUTORS</span><br><span class="line">    ├── DEVELOPER.md</span><br><span class="line">    ├── Gemfile</span><br><span class="line">    ├── LICENSE</span><br><span class="line">    ├── README.md</span><br><span class="line">    ├── Rakefile</span><br><span class="line">    ├── lib</span><br><span class="line">    │   └── logstash</span><br><span class="line">    │       └── filters</span><br><span class="line">    │           └── test.rb</span><br><span class="line">    ├── logstash-filter-test.gemspec</span><br><span class="line">    ├── spec</span><br><span class="line">    │   ├── filters</span><br><span class="line">    │   │   └── <span class="built_in">test</span>_spec.rb</span><br><span class="line">    │   └── spec_helper.rb</span><br><span class="line">    └── test.conf</span><br><span class="line"></span><br><span class="line">6 directories, 12 files</span><br></pre></td></tr></table></figure></p>
<h2 id="filter插件初探"><a href="#filter插件初探" class="headerlink" title="filter插件初探"></a>filter插件初探</h2><p>Logstash插件是用ruby写的，查看<code>logstash-filter-test/lib/logstash/filters/test.rb</code>文件，如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># logstash依赖于UTF-8编码，需要在插件代码开始处添加</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#引入了插件必备的包</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">"logstash/filters/base"</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">"logstash/namespace"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插件继承自Base基类，并配置插件的使用名称</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogStash::Filters::Test</span> &lt; LogStash::Filters::<span class="title">Base</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插件名称，在Logstash配置的filter块中使用</span></span><br><span class="line">  <span class="comment">#filter&#123;</span></span><br><span class="line">  <span class="comment">#  test&#123;</span></span><br><span class="line">  <span class="comment">#      source =&gt; "message"</span></span><br><span class="line">  <span class="comment">#  &#125;</span></span><br><span class="line">  <span class="comment">#&#125;</span></span><br><span class="line">  config_name <span class="string">"test"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插件参数配置</span></span><br><span class="line">  <span class="comment"># source是插件test的可选参数，默认值是"Hello World!"。</span></span><br><span class="line">  config <span class="symbol">:source</span>, <span class="symbol">:validate</span> =&gt; <span class="symbol">:string</span>, <span class="symbol">:default</span> =&gt; <span class="string">"Hello World!"</span></span><br><span class="line">  <span class="comment"># 下面是参数的通用配置代码</span></span><br><span class="line">  <span class="comment"># config :variable_name, :validate =&gt; :variable_type, :default =&gt; "Default value", :required =&gt; boolean, :deprecated =&gt; boolean, :obsolete =&gt; string</span></span><br><span class="line">  <span class="comment"># :variable_name：参数名称</span></span><br><span class="line">  <span class="comment"># :validate：验证参数类型，如:string, :password, :boolean, :number, :array, :hash, :path等</span></span><br><span class="line">  <span class="comment"># :required：是否必须配置</span></span><br><span class="line">  <span class="comment"># :default：默认值</span></span><br><span class="line">  <span class="comment"># :deprecated：是否废弃</span></span><br><span class="line">  <span class="comment"># :obsolete：声明该配置不再使用，通常提供升级方案</span></span><br><span class="line"></span><br><span class="line">  public</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">register</span></span></span><br><span class="line">    <span class="comment"># 方法相当于初始化方法，不需要手动调用，可以在这个方法里面调用配置变量，如<span class="doctag">@message</span>，也可以初始化自己的实例变量。</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  public</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span><span class="params">(event)</span></span></span><br><span class="line">    <span class="comment"># 方法是插件的数据处理逻辑，其中event变量封装了数据流，可以通过接口访问event中的内容，具体参见 https://www.elastic.co/guide/en/logstash/5.1/event-api.html。</span></span><br><span class="line">    <span class="keyword">if</span> (source = event.get(@source))      </span><br><span class="line">      datas = source.split(<span class="string">"|"</span>)</span><br><span class="line">      event.set(<span class="string">"data"</span>, datas[<span class="number">0</span>])</span><br><span class="line">      event.set(<span class="string">"data2"</span>, datas[<span class="number">1</span>])</span><br><span class="line">      event.set(<span class="string">"user"</span>, <span class="string">"xp"</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#这个方法用于保证Logstash的配置`add_field`, `remove_field`, `add_tag`和`remove_tag`会被正确执行。</span></span><br><span class="line">    filter_matched(event)</span><br><span class="line">  <span class="keyword">end</span> <span class="comment"># def filter</span></span><br><span class="line"><span class="keyword">end</span> <span class="comment"># class LogStash::Filters::Test</span></span><br></pre></td></tr></table></figure></p>
<p>查看<code>logstash-filter-test/logstash-filter-test.gemspec</code>文件，如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Gem::Specification.new <span class="keyword">do</span> <span class="params">|s|</span></span><br><span class="line">  s.name          = <span class="string">'logstash-filter-test'</span>  </span><br><span class="line">  s.version       = <span class="string">'0.1.0'</span></span><br><span class="line">  s.licenses      = [<span class="string">'Apache License (2.0)'</span>]</span><br><span class="line">  s.summary       = <span class="string">'描述这个插件的概要'</span></span><br><span class="line">  s.description   = <span class="string">'这个插件的详细说明'</span></span><br><span class="line">  s.homepage      = <span class="string">'http://icyxp.github.io'</span></span><br><span class="line">  s.authors       = [<span class="string">'icyboy'</span>]</span><br><span class="line">  s.email         = <span class="string">'icyboy@me.com'</span></span><br><span class="line">  s.require_paths = [<span class="string">'lib'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Files</span></span><br><span class="line">  s.files = Dir[<span class="string">'lib/**/*'</span>,<span class="string">'spec/**/*'</span>,<span class="string">'vendor/**/*'</span>,<span class="string">'*.gemspec'</span>,<span class="string">'*.md'</span>,<span class="string">'CONTRIBUTORS'</span>,<span class="string">'Gemfile'</span>,<span class="string">'LICENSE'</span>,<span class="string">'NOTICE.TXT'</span>]</span><br><span class="line">   <span class="comment"># Tests</span></span><br><span class="line">  s.test_files = s.files.grep(<span class="regexp">%r&#123;^(test|spec|features)/&#125;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Special flag to let us know this is actually a logstash plugin</span></span><br><span class="line">  s.metadata = &#123; <span class="string">"logstash_plugin"</span> =&gt; <span class="string">"true"</span>, <span class="string">"logstash_group"</span> =&gt; <span class="string">"filter"</span> &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Gem dependencies</span></span><br><span class="line">  s.add_runtime_dependency <span class="string">"logstash-core-plugin-api"</span>, <span class="string">"~&gt; 2.0"</span></span><br><span class="line">  s.add_development_dependency <span class="string">'logstash-devutils'</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<h2 id="在Logstash中配置定制的插件"><a href="#在Logstash中配置定制的插件" class="headerlink" title="在Logstash中配置定制的插件"></a>在Logstash中配置定制的插件</h2><p>cd到Logstash根目录下<code>cd /usr/shar/logstash</code>，在<code>Gemfile</code>末尾添加以下配置：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="string">"logstash-filter-test"</span>, <span class="symbol">:path</span> =&gt; <span class="string">"vendor/localgems/logstash-filter-test"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="启动Logstash"><a href="#启动Logstash" class="headerlink" title="启动Logstash"></a>启动Logstash</h2><p>先编写一个配置文件<code>test.conf</code>，这里我使用了rsyslog<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#来源rsyslog</span></span><br><span class="line">input&#123;</span><br><span class="line">    file&#123;</span><br><span class="line">	    path =&gt; <span class="string">"/var/log/messages"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#或者可以设置为终端输入</span></span><br><span class="line"><span class="comment">#input&#123;</span></span><br><span class="line"><span class="comment">#    stdin&#123;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    &#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line">filter&#123;</span><br><span class="line">    <span class="comment"># 定制的filter插件</span></span><br><span class="line">    <span class="built_in">test</span>&#123;</span><br><span class="line">	    <span class="built_in">source</span> =&gt; <span class="string">"message"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#输出到终端</span></span><br><span class="line">output&#123;</span><br><span class="line">    stdout&#123;</span><br><span class="line">        codec =&gt; rubydebug</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行起来<code>logstash -f test.conf</code>，然后往rsyslog里写日志你就可以看下如下情况就说明ok了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#logger "测试一下"</span></span><br><span class="line">&#123;</span><br><span class="line">          <span class="string">"path"</span> =&gt; <span class="string">"/var/log/messages"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2017-08-16T15:27:46.606Z,</span><br><span class="line">          <span class="string">"data"</span> =&gt; <span class="string">"Aug 16 15:27:45 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">         <span class="string">"data2"</span> =&gt; nil,</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"c11787c4cef3"</span>,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"Aug 16 15:27:45 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">          <span class="string">"user"</span> =&gt; <span class="string">"xp"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#logger "测试一下|from by icyboy"</span></span><br><span class="line">&#123;</span><br><span class="line">          <span class="string">"path"</span> =&gt; <span class="string">"/var/log/messages"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2017-08-16T15:28:17.661Z,</span><br><span class="line">          <span class="string">"data"</span> =&gt; <span class="string">"Aug 16 15:28:16 c11787c4cef3 root: 测试一下"</span>,</span><br><span class="line">         <span class="string">"data2"</span> =&gt; <span class="string">"from by icyboy"</span>,</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"c11787c4cef3"</span>,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"Aug 16 15:28:16 c11787c4cef3 root: 测试一下|from by icyboy"</span>,</span><br><span class="line">          <span class="string">"user"</span> =&gt; <span class="string">"xp"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一语点醒技术人：你不是Google]]></title>
      <url>http://team.jiunile.com/blog/2017/07/gossip-%E4%BD%A0%E4%B8%8D%E6%98%AFGoogle.html</url>
      <content type="html"><![CDATA[<p><img src="/images/gossip/01/cover.png" alt="01"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在为问题寻找解决方案时要先充分了解问题本身，而不是一味地盲目崇拜那些巨头公司。Ozan Onay以Amazon、LinkedIn和Google为例，为执迷不悟的人敲响警钟。以下内容已获得作者翻译授权，查看原文：<a href="https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb" target="_blank" rel="external">You Are Not Google</a>。</p>
<p>软件工程师总是着迷于荒唐古怪的事。我们看起来似乎很理性，但在面对技术选型时，总是陷入抓狂——从Hacker News到各种博客，像一只飞蛾一样，来回折腾，最后精疲力尽，无助地飞向一团亮光，跪倒在它的前面——那就是我们一直在寻找的东西。</p>
<p>真正理性的人不是这样做决定的。不过工程师一贯如此，比如决定是否使用MapReduce。</p>
<p>Joe Hellerstein在他的大学数据库教程视频中说道：</p>
<blockquote>
<p>世界上只有差不多5个公司需要运行这么大规模的作业。至于其他公司……他们使用了所有的IO来实现不必要的容错。在2000年代，人们狂热地追随着Google：“我们要做Google做过的每一件事，因为我们也运行着世界上最大的互联网数据服务。”</p>
</blockquote>
<p>超出实际需求的容错没有什么问题，但我们却为此付出了的惨重的代价：不仅增加了IO，还有可能让原先成熟的系统——包含了事务、索引和查询优化器——变得破碎不堪。这是一个多么严重的历史倒退！有多少个Hadoop用户是有意识地做出这种决定的？有多少人知道他们的决定到底是不是一个明智之举？</p>
<p>MapReduce已经成为一个众矢之的，那些盲目崇拜者也意识到事情不对劲。但这种情况却普遍存在：虽然你使用了大公司的技术，但你的情况却与他们大不一样，而且你的决定并没有经过深思熟虑，你只是习以为常地认为，模仿巨头公司就一定也能给你带来同样的财富。</p>
<p>是的，这又是一篇劝大家“不要盲目崇拜”的文章。不过这次我列出了一长串有用的清单，或许能够帮助你们做出更好的决定。</p>
<a id="more"></a>
<h2 id="很酷的技术？UNPHAT"><a href="#很酷的技术？UNPHAT" class="headerlink" title="很酷的技术？UNPHAT"></a>很酷的技术？UNPHAT</h2><p>如果你还在使用Google搜索新技术来重建你的软件架构，那么我建议你不要再这么做了。相反，你可以考虑应用UNPHAT原则。</p>
<ol>
<li>在彻底了解（Understand）你的问题之前，不要急着去寻找解决方案。你的目标应该是在问题领域内“解决”问题，而不是在方案领域内解决问题。</li>
<li>列出（eNumerate）多种方案，不要只把眼睛盯在你最喜欢的方案上。</li>
<li>选择一个候选方案，并阅读相关论文（Paper）。</li>
<li>了解候选方案的产生背景（Historical context）。</li>
<li>比较优点（Advantages）和缺点，扬长避短。</li>
<li>思考（Think）！冷静地思考候选方案是否适合用于解决你的问题。要出现怎样异常的情况才会让你改变注意？例如，数据要少到什么程度才会让你打消使用Hadoop的念头？</li>
</ol>
<h2 id="你不是Amazon"><a href="#你不是Amazon" class="headerlink" title="你不是Amazon"></a>你不是Amazon</h2><p>UNPHAT原则十分直截了当。最近我与一个公司有过一次对话，这个公司打算在一个读密集的系统里使用Cassandra，他们的数据是在夜间加载到系统里的。</p>
<p>他们阅读了Dynamo的相关论文，并且知道Cassandra是最接近Dynamo的一个产品。我们知道，这些分布式数据库优先保证写可用性（Amazon是不会让“添加到购物车”这种操作出现失败的）。为了达到这个目的，他们在一致性以及几乎所有在传统RDBMS中出现过的特性上做出了妥协。但这家公司其实没有必要优先考虑写可用性，因为他们每天只有一次写入操作，只是数据量比较大。</p>
<p>他们之所以考虑使用Cassandra，是因为PostgreSQL查询需要耗费几分钟的时间。他们认为是硬件的问题，经过排查，我们发现数据表里有5000万条数据，每条数据最多80个字节。如果从SSD上整块地读取所有数据大概需要5秒钟，这个不算快，但比起实际的查询，它要快上两个数量级。</p>
<p>我真的很想多问他们几个问题（了解问题！），在问题变得愈加严重时，我为他们准备了5个方案（列出多个候选方案！），不过很显然，Cassandra对于他们来说完全是一个错误的方案。他们只需要耐心地做一些调优，比如对部分数据重新建模，或许可以考虑使用（当然也有可能没有）其他技术……但一定不是这种写高可用的键值存储系统，Amazon当初创建Cassandra是用来解决他们的购物车问题的！</p>
<h2 id="你不是LinkedIn"><a href="#你不是LinkedIn" class="headerlink" title="你不是LinkedIn"></a>你不是LinkedIn</h2><p>我发现一个学生创办的小公司居然在他们的系统里使用Kafka，这让我感到很惊讶。因为据我所知，他们每天只有很少的事务需要处理——最好的情况下，一天最多只有几百个。这样的吞吐量几乎可以直接记在记事本上。</p>
<p>Kafka被设计用于处理LinkedIn内部的吞吐量，那可是一个天文数字。即使是在几年前，这个数字已经达到了每天数万亿，在高峰时段每秒钟需要处理1000万个消息。不过Kafka也可以用于处理低吞吐量的负载，或许再低10个数量级？</p>
<p>或许工程师们在做决定时确实是基于他们的预期需求，并且也很了解Kafka的适用场景。但我猜测他们是抵挡不住社区对Kafka的追捧，并没有仔细想过Kafka是否适合他们。要知道，那可是10个数量级的差距！</p>
<h2 id="再一次，你不是Amazon"><a href="#再一次，你不是Amazon" class="headerlink" title="再一次，你不是Amazon"></a>再一次，你不是Amazon</h2><p>比Amazon的分布式数据库更为著名的是它的可伸缩架构模式，也就是面向服务架构。Werner Vogels在2006年的一次访谈中指出，Amazon在2001年时就意识到他们的前端需要横向伸缩，而面向服务架构有助于他们实现前端伸缩。工程师们面面相觑，最后只有少数几个工程师着手去做这件事情，而几乎没有人愿意将他们的静态网页拆分成小型的服务。</p>
<p>不过Amazon还是决定向SOA转型，他们当时有7800个员工和30亿美元的销售规模。</p>
<p>当然，并不是说你也要等到有7800个员工的时候才能转向SOA……只是你要多想想，它真的能解决你的问题吗？你的问题的根源是什么？可以通过其他的方式解决它们吗？</p>
<p>如果你告诉我说，你那50个人的公司打算转向SOA，那么我不禁感到疑惑：为什么很多大型的公司仍然在乐此不彼地使用具有模块化的大型单体应用？</p>
<h2 id="甚至Google也不是Google"><a href="#甚至Google也不是Google" class="headerlink" title="甚至Google也不是Google"></a>甚至Google也不是Google</h2><p>使用Hadoop和Spark这样的大规模数据流引擎会非常有趣，但在很多情况下，传统的DBMS更适合当前的负载，有时候数据量小到可以直接放进内存。你是否愿意花10,000美金去购买1TB的内存？如果你有十亿个用户，每个用户仅能使用1KB的内存，所以你的投入远远不够。</p>
<p>或许你的负载大到需要把数据写回磁盘。那么你需要多少磁盘？你到底有多少数据量？Google之所以要创建GFS和MapReduce，是要解决整个Web的计算问题，比如重建整个Web的搜索索引。</p>
<p>或许你已经阅读过GFS和MapReduce的论文，Google的部分问题在于吞吐量，而不是容量，他们之所以需要分布式的存储，是因为从磁盘读取字节流要花费太多的时间。那么你在2017年需要使用多少设备吞吐量？你一定不需要像Google那么大的吞吐量，所以你可能会考虑使用更好的设备。如果都用上SSD会给你增加多少成本？</p>
<p>或许你还想要伸缩性。但你有仔细算过吗，你的数据增长速度会快过SSD降价的速度吗？在你的数据撑爆所有的机器之前，你的业务会有多少增长？截止2016年，Stack Exchange每天要处理2亿个请求，但是他们只用了4个SQL Server，一个用于Stack Overflow，一个用于其他用途，另外两个作为备份复本。</p>
<p>或许你在应用UNPHAT原则之后，仍然决定要使用Hadoop或Spark。或许你的决定是对的，但关键的是你要用对工具。Google非常明白这个道理，当他们意识到MapReduce不再适合用于构建索引之后，他们就不再使用它。</p>
<h2 id="先了解你的问题"><a href="#先了解你的问题" class="headerlink" title="先了解你的问题"></a>先了解你的问题</h2><p>我所说的也不是什么新观点，不过或许UNPHAT对于你们来说已经足够了。如果你觉得还不够，可以听听Rich Hickey的演讲“<a href="https://www.youtube.com/watch?v=f84n5oFoZBc" target="_blank" rel="external">吊床驱动开发</a>”，或者看看Polya的书《<a href="https://www.amazon.com/How-Solve-Mathematical-Princeton-Science/dp/069111966X?ie=UTF8&amp;%2aVersion%2a=1&amp;%2aentries%2a=0" target="_blank" rel="external">How to Solve It</a>》， 或者学习一下Hamming的课程“<a href="https://www.youtube.com/playlist?list=PL2FF649D0C4407B30" target="_blank" rel="external">The Art of Doing Science and Engineering</a>”。我恳请你们一定要多思考！在尝试解决问题之前先对它们有充分的了解。最后送上Polya的一个金句名言：</p>
<blockquote>
<p>回答一个你不了解的问题是愚蠢的，到达一个你不期望的终点是悲哀的。</p>
</blockquote>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[高负载微服务系统的诞生过程]]></title>
      <url>http://team.jiunile.com/blog/2017/07/microservice-%E9%AB%98%E8%B4%9F%E8%BD%BD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%9E%E7%94%9F%E8%BF%87%E7%A8%8B.html</url>
      <content type="html"><![CDATA[<p><img src="/images/ms/02/cover.png" alt="封面"></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在2016 <a href="http://highload.co/" target="_blank" rel="external">LighLoad++</a>大会上，“M-Tex”的开发经理Vadim Madison讲述了从一个由数百个微服务组成的系统到包含数千个微服务的高负载项目的发展历程。本文已获得翻译授权，查看英文原文：<a href="https://kukuruku.co/post/microservices-in-a-high-load-project/" target="_blank" rel="external">Microservices in a High-Load Project</a> 。</p>
<p>我将告诉大家我们是如何开始一个高负载微服务项目的。在讲述我们的经历之前，先让我们简单地自我介绍一下。</p>
<p>简单地说，我们从事视频输出方面的工作——我们提供实时的视频。我们负责“NTV-Plus”和“Match TV”频道的视频平台。该平台有30万的并发用户，每小时输出300TB的内容。这是一个很有意思的任务。那么我们是如何做到的呢？</p>
<p>这背后都有哪些故事？这些故事都是关于项目的开发和成长，关于我们对项目的思考。总而言之，是关于如何提升项目的伸缩能力，承受更大的负载，在不宕机和不丢失关键特性的情况下为客户提供更多的功能。我们总是希望能够满足客户的需求。当然，这也涉及到我们是如何实现这一切，以及这一切是如何开始的。</p>
<a id="more"></a>
<p><strong>在最开始，我们有两台运行在Docker集群里的服务器，数据库运行在相同机器的容器里。没有专用的存储，基础设施非常简单。</strong></p>
<p>我们就是这样开始的，只有两台运行在Docker集群里的服务器。那个时候，数据库也运行在同一个集群里。我们的基础设施里没有什么专用的组件，十分简单。<br><img src="/images/ms/02/00.jpg" alt="00"></p>
<p>我们的基础设施最主要的组件就是Docker和TeamCity，我们用它们来交付和构建代码。</p>
<p>在接下来的时期——我称其为我们的发展中期——是我们项目发展的关键时期。我们拥有了80台服务器，并在一组特殊的机器上为数据库搭建了一个单独的专用集群。我们开始使用基于CEPH的分布式存储，并开始思考服务之间的交互问题，同时要更新我们的监控系统。</p>
<p>现在，让我们来看看我们在这一时期都做了哪些事情。Docker集群里已经有数百台服务器，微服务就运行在它们上面。这个时候，我们开始根据数据总线和逻辑分离原则将我们的系统拆分成服务子系统。当微服务越来越多时，我们决定拆分我们的系统，这样维护起来就容易得多（也更容易理解）。<br><img src="/images/ms/02/01.jpg" alt="01"></p>
<p>这张图展示的是我们系统其中的一小部分。这部分系统负责视频剪切。半年前，我在“RIT++”也展示过类似的图片。那个时候只有17个绿色的微服务，而现在有28个绿色的微服务。这些服务只占我们整个系统的二十分之一，所以可以想象我们系统大致的规模有多大。</p>
<h2 id="深入细节"><a href="#深入细节" class="headerlink" title="深入细节"></a>深入细节</h2><p>服务间的通信是一件很有趣的事情。一般来说，我们应该尽可能提升服务间通信效率。我们使用了<a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="external">protobuf</a>，我们认为它就是我们需要的东西。</p>
<p>它看起来是这样的：<br><img src="/images/ms/02/02.jpg" alt="02"></p>
<p>微服务的前面有一个负载均衡器。请求到达前端，或者直接发送给提供了JSON API的服务。protobuf被用于内部服务之间的交互。</p>
<p>protobuf真是一个好东西。它为消息提供了很好的压缩率。现如今有很多框架，只要使用很小的开销就能实现序列化和反序列化。我们可以将其视为有条件的请求类型。</p>
<p>但如果从微服务角度来看，我们会发现，微服务之间也存在某种私有的协议。如果只有一两个或者五个微服务，我们可以为每个微服务打开一个控制台，通过它们来访问微服务，并获得响应结果。如果出现了问题，我们可以对其进行诊断。不过这在一定程度上让微服务的支持工作变得复杂。</p>
<p>在一定时期内，这倒不是什么问题，因为并没有太多的微服务。另外，Google发布了<a href="https://grpc.io/" target="_blank" rel="external">gRPC</a>。在那个时候，gRPC满足了所有我们想做的事情。于是我们逐渐迁移到gRPC。于是我们的技术栈里出现了另一个组件。<br><img src="/images/ms/02/03.jpg" alt="03"></p>
<p>实现的细节也是很有趣的。gRPC默认是基于HTTP/2的。如果你的环境相对稳定，应用程序不怎么发生变更，也不需要在机器间迁移，那么gRCP对于你来说就是个不错的东西。另外，gRPC支持很多客户端和服务器端的编程语言。</p>
<p>现在，我们从微服务角度来看待这个问题。从一方面来看，gRPC是一个好东西，但从另一方面来看，它也有不足之处。当我们开始对日志进行标准化（这样就可以将它们聚合到一个独立的系统里）时，我们发现，从gRPC中抽取日志非常麻烦。</p>
<p>于是，我们决定开发自己的日志系统。它解析消息，并将它们转成我们需要的格式。这样我们才可以获得我们想要的日志。还有一个问题，添加新的微服务会让服务间的依赖变得更加复杂。这是微服务一直存在的问题，这也是除版本问题之外的另一个具有一定复杂性的问题。</p>
<p>于是，我们开始考虑使用JSON。在很长的一段时间里，我们无法相信，在使用了紧凑的二进制协议之后会转回使用JSON。有一天，我们看到一篇文章，来自<a href="http://engineering.dailymotion.com/" target="_blank" rel="external">DailyMotion</a>的一个家伙在文章里提到了同样的事情：“我们知道该如何使用JSON，每个人都可以使用JSON。既然如此，为什么还要自寻烦恼呢？”<br><img src="/images/ms/02/04.jpg" alt="04"></p>
<p>于是，我们逐渐从gRPC转向我们自己实现的JSON。我们保留了HTTP/2，它与JSON组合起来可以带来更快的速度。</p>
<p>现在，我们具备了所有必要的特性。我们可以通过cURL访问我们的服务。我们的QA团队使用<a href="https://www.getpostman.com/" target="_blank" rel="external">Postman</a>，所以他们也感觉很满意。一切都变得简单起来。这是一个有争议性的决定，但却为我们带来了很多好处。</p>
<p>JSON唯一的缺点就是它的紧凑性不足。根据我们的测试结果，它与MessagePack之间有30%的差距。不过对于一个支持系统来说，这不算是个大问题。</p>
<p>况且，我们在转到JSON之后还获得了更多的特性，比如协议版本。有时候，当我们在新版本的协议上使用protobuf时，客户端也必须改用protobuf。如果你有数百个服务，就算只有10%的服务进行了迁移，这也会引起很大的连锁反应。你在一个服务上做了一些变更，就会有十多个服务也需要跟着改动。</p>
<p>因此，我们就会面临这样的一种情况，一个服务的开发人员已经发布了第五个、第六个，甚至第七个版本，但生产环境里仍然在运行第四个版本，就因为其他相关服务的开发人员有他们自己的优先级和截止日期。他们无法持续地更新他们的服务，并使用新版本的协议。所以，新版本的服务虽然发布了，但还派不上用场。然后，我们却要以一种很奇怪的方式来修复旧版本的bug，这让支持工作变得更加复杂。</p>
<p>最后，我们决定停止发布新版本的协议。我们提供协议的基础版本，可以往里面添加少量的属性。服务的消费者开始使用JSON schema。</p>
<p>标准看起来是这样的：<br><img src="/images/ms/02/05.jpg" alt="05"></p>
<p>我们没有使用版本1、2和3，而是只使用版本1和指向它的schema。<br><img src="/images/ms/02/06.jpg" alt="06"></p>
<p>这是从我们服务返回的一个典型的响应结果。它是一个内容管理器，返回有关广播的信息。这里有一个消费者schema的例子。<br><img src="/images/ms/02/07.jpg" alt="07"></p>
<p>最底下的字符串最有意思，也就是”required”那块。我们可以看到，这个服务只需要4个字段——id、content、date和status。如果我们使用了这个schema，那么消费者就只会得到这样的数据。<br><img src="/images/ms/02/08.jpg" alt="08"></p>
<p>它们可以被用在每一个协议版本里，从第一个版本到后来的每一个变更版本。这样，在版本之间迁移就容易很多。在我们发布新版本之后，客户端的迁移就会简单很多。</p>
<p>下一个重要的议题是系统的稳定性问题。这是微服务和其他任何一个系统都需要面临的问题（在微服务架构里，我们可以更强烈地感觉到它的重要性）。系统总会在某个时候变得不稳定。</p>
<p>如果服务间的调用链只包含了一两个服务，那么就没有什么问题。在这种情况下，你看不出单体和分布式系统之间有多大区别。但当调用链里包含了5到7个调用，那么问题就会接踵而至。你根本不知道为什么会这样，也不知道能做些什么。在这种情况下，调试会变得很困难。在单体系统里，你可以通过逐步调试来找出错误。但对于微服务来说，网络不稳定性或高负载下的性能不稳定性也会对微服务造成影响。特别是对于拥有大量节点的分布式系统来说，这些情况就更加显而易见了。<br><img src="/images/ms/02/09.jpg" alt="09"></p>
<p>在一开始，我们采用了传统的办法。我们监控所有的东西，查看问题和问题的发生点，然后尝试尽快修复它们。我们将微服务的度量指标收集到一个独立的数据库里。我们使用<a href="https://github.com/python-diamond/Diamond" target="_blank" rel="external">Diamond</a>来收集系统度量指标。我们使用<a href="https://github.com/google/cadvisor" target="_blank" rel="external">cAdvisor</a>来分析容器的资源使用情况和性能特征。所有的结果都被保存到<a href="https://github.com/influxdata/influxdb" target="_blank" rel="external">InfluxDB</a>，然后我们在Grafana里创建仪表盘。<br><img src="/images/ms/02/10.jpg" alt="10"></p>
<p>于是，我们现在的基础设施里又多了三个组件。</p>
<p>我们比以往更加关注所发生的一切。我们对问题的反应速度更快了。不过，这并没有阻止问题的出现。</p>
<p>奇怪的是，微服务架构的主要问题出在那些不稳定的服务上。它们有的今天运行正常，明天就不行，而且有各种各样的原因。如果服务出现超载，而你继续向它发送负载，它就会宕机一段时间。如果它在一段时间不提供服务，负载就会下降，然后它就又活过来了。这类系统很难维护，也很难知道到底出了什么问题。</p>
<p>最后，我们决定把这些服务停掉，而不是让它们来回折腾。我们因此需要改变服务的实现方式。</p>
<p>我们做了一件很重要的事情。我们对每个服务接收的请求数量设定了一个上限。每个服务知道自己可以处理多少个来自客户端的请求（我们稍后会详细说明）。如果请求数量达到上限，服务将抛出503 Service Unavailable异常。客户端知道这个节点无法提供服务，就会选择另一个节点。</p>
<p>当系统出现问题时，我们就可以通过这种方式来减少请求时间。另外，我们也提升了服务的稳定性。</p>
<p>我们引入了第二种模式——<strong>回路断路器</strong>（Circuit Breaker）。我们在客户端实现了这种模式。</p>
<p>假设有一个服务A，它有4个可以访问的服务B的实例。它向注册中心索要服务B的地址：“给我这些服务的地址”。它得到了服务B的4个地址。服务A向第一个服务B的实例发起了请求。第一个服务B实例正常返回响应。服务A将其标记为可访问：“是的，我可以访问它”。然后，服务A向第二个服务B实例发起请求，不过它没有在期望的时间内得到响应。我们禁用了这个实例，然后向下一个实例发起请求。下一个实例因为某些原因返回了不正确的协议版本。于是我们也将其禁用，然后转向第四个实例。</p>
<p>总得来说，只有一半的服务能够为客户端提供服务。于是服务A将会向能够正常返回响应的两个服务发起请求。而另外两个无法满足要求的实例被禁用了一段时间。</p>
<p>我们通过这种方式来提升性能的稳定性。如果服务出现了问题，我们就将其关闭，并发出告警，然后尝试找出问题所在。</p>
<p>因为引入了回路断路器模式，我们的基础设施里又多了一个组件——<a href="https://github.com/Netflix/Hystrix" target="_blank" rel="external">Hystrix</a>。<br><img src="/images/ms/02/11.jpg" alt="11"></p>
<p>Hystrix不仅实现了回路断路器模式，它也有助于我们了解系统里出现了哪些问题：<br><img src="/images/ms/02/12.jpg" alt="12"></p>
<p>圆环的大小表示服务与其他组件之间的流量大小。颜色表示系统的健康状况。如果圆环是绿色的，那么说明一切正常。如果圆环是红色的，那么就有问题了。</p>
<p>如果一个服务应该被停掉，那么它看起来是这个样子的。圆环是打开的。<br><img src="/images/ms/02/13.jpg" alt="13"></p>
<p>我们的系统变得相对稳定。每个服务至少都有两个可用的实例，这样我们就可以选择停掉其中的一个。不过，尽管是这样，我们仍然不知道我们的系统究竟发生了什么问题。在处理请求期间如果出现了问题，我们应该怎样才能知道问题的根源是什么呢？</p>
<p>这是一个标准的请求：<br><img src="/images/ms/02/14.jpg" alt="14"></p>
<p>这是一个处理链条。用户发送请求到第一个服务，然后是第二个。从第二个服务开始，链条将请求发送到第三个和第四个服务。</p>
<p>然后一个分支不明原因地消失了。在经历了这类场景之后，我们尝试着提升这种场景的可见性，于是我们找到了Appdash。Appdash是一个跟踪服务。<br><img src="/images/ms/02/16.jpg" alt="16"></p>
<p>它看起来是这个样子的：<br><img src="/images/ms/02/17.jpg" alt="17"></p>
<p>可以这么说，我们只是想尝试一下，看看它是否适合我们。将它用在我们的系统里是一件很容易的事情，因为我们那个时候使用的是Go语言。Appdash提供了一个开箱即用的包。我们认为Appdash是一个好东西，只是它的实现并不是很适合我们。<br><img src="/images/ms/02/18.jpg" alt="18"></p>
<p>于是，我们决定使用<a href="http://zipkin.io/" target="_blank" rel="external">Zipkin</a>来代替Appdash。Zipkin是由Twitter开源的。它看起来是这个样子的：<br><img src="/images/ms/02/19.jpg" alt="19"></p>
<p>我认为这样会更清楚一些。我们可以从中看到一些服务，也可以看到我们的请求是如何通过请求链的，还可以看到请求在每个服务里都做了哪些事情。一方面，我们可以看到服务的总时长和每个分段的时长，另一方面，我们完全可以添加描述服务内容的信息。</p>
<p>我们可以在这里添加一些与数据库的调用、文件系统的读取、缓存的访问有关的信息，这样就可以知道请求里哪一部分使用了最多的时间。TraceID可以帮助我们做到这一点。稍后我会介绍更多细节。<br><img src="/images/ms/02/20.jpg" alt="20"></p>
<p>我们就是通过这种方式知道请求在处理过程中发生了什么问题，以及为什么有时候无法被正常处理。刚开始一切都正常，然后突然间，其中的一个出现了问题。我们稍作排查，就知道出问题的服务发生了什么。<br><img src="/images/ms/02/21.jpg" alt="21"></p>
<p>不久前，一些厂商推出了一个跟踪系统的标准。为了简化系统的实现，主要的几个跟踪系统厂商在如何设计客户端API和客户端类库上达成了一致。现在已经有了<a href="http://opentracing.io/" target="_blank" rel="external">OpenTracing</a>的实现，支持几乎所有的主流开发语言。现在就可以使用它了。</p>
<p>我们已经有办法知道那些突然间崩溃的服务。我们可以看到其中的某部分在垂死挣扎，但是不知道为什么。光有环境信息是不够的，</p>
<p>我们还需要日志。是的，这应该成为标准的一部分，它就是Elasticsearch、Logstash和Kibana（ELK）。不过我们对它们做了一些改动。<br><img src="/images/ms/02/22.jpg" alt="22"></p>
<p>我们并没有将大量的日志直接通过forward传给Logstash，而是先传给syslog，让它把日志聚合到构建机器上，然后再通过forward导入到<strong>Elasticsearch</strong>和<strong>Kibana</strong>。这是一个很标准的流程，那么巧妙的地方在哪里呢？<br><img src="/images/ms/02/23.jpg" alt="23"></p>
<p>巧妙的是，我们可以在任何可能的地方往日志里加入Zipkin的TraceID。</p>
<p>这样一来，我们就可以在Kibana仪表盘上看到完整的用户请求执行情况。也就是说，一旦服务进入生产环境，就为运营做好了准备。它已经通过了自动化测试，如果有必要，QA可以再进行手动检查。它应该没有什么问题。如果它出现了问题，那说明有一些先决条件没有得到满足。日志里详细地记录了这些先决条件，通过过滤，我们可以看到某个请求的跟踪信息。我们因此可以快速地查出问题的根源，为我们节省了很多时间。</p>
<p>我们后来引入了动态调试模式。现在的日志数量还不是很大，大概只有100 GB到150 GB，我记不太清楚具体数字了。不过，这些日志是在正常的日志模式下生成的。如果我们添加更多的细节，那么日志就可能变成TB级别的，处理起来就很耗费资源。</p>
<p>当我们发现某些服务出现问题，就打开调试模式（通过一个API），看看发生了什么事情。有时候，我们找到出现问题的服务，在不将它关闭的情况下打开调试模式，尝试找出问题所在。</p>
<p>最后，我们在ELK端查找问题。我们还对关键服务的错误进行聚合。服务知道哪些错误是关键性的，哪些不是关键性的，然后将它们传给<a href="https://sentry.io/" target="_blank" rel="external">Sentry</a>。<br><img src="/images/ms/02/24.jpg" alt="24"></p>
<p>Sentry能够智能地收集错误日志，并形成度量指标，还会进行一些基本的过滤。我们在很多服务上使用了Sentry。我们从单体应用时期就开始使用它了。</p>
<p>那么最有趣的问题是，我们是如何进行伸缩的？这里需要先介绍一些概念。我们把每个机器看成一个黑盒。<br><img src="/images/ms/02/25.jpg" alt="25"></p>
<p>我们有一个编排系统，最开始使用<a href="https://www.nomadproject.io/" target="_blank" rel="external">Nomad</a>。确切地说，应该是<a href="https://www.ansible.com/" target="_blank" rel="external">Ansible</a>。我们自己编写脚本，但光是这些还不能满足要求。那个时候，Nomad的某些版本可以简化我们的工作，于是我们决定迁移到Nomad。<br><img src="/images/ms/02/26.jpg" alt="26"></p>
<p>同时还使用了<a href="https://www.consul.io/" target="_blank" rel="external">Consul</a>，将它作为服务发现的注册中心。还有Vault，用于存储敏感数据，比如密码、秘钥和其他所有不能保存在Git上的东西。</p>
<p>这样，所有的机器几乎都变得一模一样。每个机器上都安装了<strong>Docker</strong>，还有<strong>Consul</strong>和<strong>Nomad</strong>代理。总的来说，每一个机器都处于备用状态，可以在任何时候投入使用。如果不用了，我们就让它们下线。如果你构建了云平台，你就可以先准备好机器，在高峰期时将它们打开，在负载下降时将它们关闭。这会节省大量的成本。<br><img src="/images/ms/02/27.jpg" alt="27"></p>
<p>后来，我们决定从<strong>Nomad</strong>迁移到<a href="https://github.com/kubernetes/kubernetes" target="_blank" rel="external">Kubernetes</a>，<strong>Consul</strong>也因此成为了集中式的配置系统。</p>
<p>这样一来，部分栈可以进行自动伸缩。那么我们是怎么做的呢？</p>
<p>第一步，我们对内存、CPU和网络进行限制。<br><img src="/images/ms/02/28.jpg" alt="28"></p>
<p>我们分别将这三个元素分成三个等级，砍掉其中的一部分。例如，<br><img src="/images/ms/02/29.jpg" alt="29"></p>
<p>R3-C2-N1，我们已经限定只给某个服务一小部分网络流量、多一点点的CPU和更多的内存。这个服务真的很耗费资源。</p>
<p>我们在这里使用了助记符，我们的决策服务可以设置很多的组合值，这些值看起来是这样的：<br><img src="/images/ms/02/30.jpg" alt="30"></p>
<p>事实上，我们还有C4和R4，不过它们已经超出了这些标准的限制。标准看起来是这样的：<br><img src="/images/ms/02/31.jpg" alt="31"></p>
<p>下一步开始做一些预备工作。我们先确定服务的伸缩类型。</p>
<p>独立的服务最容易伸缩，它可以进行线性地伸缩。如果用户增长了两倍，我们就运行两倍的服务实例。这就万事大吉了。</p>
<p>第二种伸缩类型：服务依赖了外部的资源，比如那些使用了数据库的服务。数据库有它自己的容量上限，这个一定要注意。你还要知道，如果系统性能出现衰退，就不应该再增加更多的实例，而且你要知道这种情况会在什么时候发生。</p>
<p>第三种情况是，服务受到外部系统的牵制。例如，外部的账单系统。就算运行了100个服务实例，它也没办法处理超过500个请求。我们要考虑到这些限制。在确定了服务类型并设置了相应的标记之后，是时候看看它们是如何通过我们的构建管道的。<br><img src="/images/ms/02/32.jpg" alt="32"></p>
<p>我们在CI服务器上运行了一些单元测试，然后在测试环境运行集成测试，我们的QA团队会对它们做一些检查。在这之后，我们就进入了预生产环境的负载测试。<br><img src="/images/ms/02/33.jpg" alt="33"></p>
<p>如果是第一种类型的服务，我们使用一个实例，并在这个环境里运行它，给它最大的负载。在运行了几轮之后，我们取其中的最小值，将它存入<strong>InfluxDB</strong>，将它作为该服务的负载上限。</p>
<p>如果是第二种类型的服务，我们逐渐加大负载，直到出现了性能衰退。我们对这个过程进行评估，如果我们知道该系统的负载，那么就比较当前负载是否已经足够，否则，我们就会设置告警，不会把这个服务发布到生产环境。我们会告诉开发人员：“你们需要分离出一些东西，或者加进去另一个工具，让这个服务可以更好地伸缩。”<br><img src="/images/ms/02/34.jpg" alt="34"></p>
<p>因为我们知道第三种类型服务的上限，所以我们只运行一个实例。我们也会给它一些负载，看看它可以服务多少个用户。如果我们知道账单系统的上限是1000个请求，并且每个服务实例可以处理200个请求，那么就需要5个实例。</p>
<p>我们把这些信息都保存到了InfluxDB。我们的决策服务开始派上用场了。它会检查两个边界：上限和下限。如果超出了上限，那么就应该增加服务实例。如果超出下限，那么就减少实例。如果负载下降（比如晚上的时候），我们就不需要这么多机器，可以减少它们的数量，并关掉一部分机器，省下一些费用。</p>
<p>整体看起来是这样的：<br><img src="/images/ms/02/35.jpg" alt="35"></p>
<p>每个服务的度量指标表明了它们当前的负载。负载信息被保存到InfluxDB，如果决策服务发现服务实例达到了上限，它会向Nomad和Kubernetes发送命令，要求增加服务实例。有可能在云端已经有可用的实例，或者开始做一些准备工作。不管怎样，发出要求增加新服务实例的告警才是关键所在。</p>
<p>一些受限的服务如果达到上限，也会发出相关的告警。对于这类情况，我们除了加大等待队列，也做不了其他什么事情。不过最起码我们知道我们很快就会面临这样的问题，并开始做好应对措施。</p>
<p>这就是我想告诉大家有关伸缩性方面的事情。除了这些，还有另外一个东西——<a href="https://about.gitlab.com/gitlab-ci/" target="_blank" rel="external">Gitlab CI</a>。<br><img src="/images/ms/02/36.jpg" alt="36"></p>
<p>我们一般是通过TeamCity来开发服务的。后来，我们意识到，所有的服务都有一个共性，这些服务都是不一样的，并且知道自己该如何部署到容器里。要生成这么的项目真的很困难，不过如果使用yml文件来描述它们，并把这个文件与服务放在一起，就会方便很多。虽然我们只做了一些小的改变，不过却为我们带来了非常多的可能性。</p>
<p>现在，我想说一些一直想对自己说的话。</p>
<p>关于微服务开发，我建议在一开始就使用<strong>编排系统</strong>。可以使用最简单的编排系统，比如Nomad，通过nomad agent -dev命令启动一个编排系统，包括Consul和其他东西。</p>
<p>我们仿佛是在一个黑盒子工作。你试图避免被绑定到某台特定的机器上，或者被附加到某台特定机器的文件系统上。这些事情会让你开始重新思考。</p>
<p>在开发阶段，<strong>每个服务至少需要两个实例</strong>，如果其中一个出现问题，就可以关掉它，由另一个接管继续服务。</p>
<p>还有一些有关架构的问题。在微服务架构里，<strong>消息总线</strong>是一个非常重要的组件。</p>
<p>假设你有一个用户注册系统，那么如何以最简单的方式实现它呢？对于注册系统来说，需要创建账户，然后在账单系统里创建一个用户，并为他创建头像和其他东西。你有一组服务，其中的超级服务收到了一个请求，它将请求分发给其他服务。经过几次之后，它就知道该触发哪些服务来完成注册。</p>
<p>不过，我们可以使用一种更简单、更可靠、更高效的方式来实现。我们使用一个服务来处理注册，它注册了一个用户，然后发送一个事件到消息总线，比如“我已经注册了一个yoghurt，ID是……”。相关的服务会收到这个事件，其中的一个服务会在账单系统里创建一个账户，另一个服务会发送一封欢迎邮件。</p>
<p>不过，系统会因此失去强一致性。这个时候你没有超级服务，也不知道每个服务的状态。不过，这样的系统很容易维护。</p>
<p>现在，我再说一些之前提到过的问题。<strong>不要试图修复</strong>出问题的服务。如果某些服务实例出现了问题，将它找出来，然后把流量定向到其他服务实例（可能是新增的实例）上，然后再诊断问题。这样可以显著提升系统的可用性。</p>
<p>通过<strong>收集度量指标</strong>来了解系统的状态自然不在话下。</p>
<p>不过要注意，如果你对某个度量指标不了解，不知道怎么使用它，或者它对你来说没有什么意义，就不要收集它。因为有时候，这样的度量指标会有数百万个。你在这些无用的度量指标上面浪费了很多资源和时间。这些是无效的负载。</p>
<p>如果你认为你需要某些度量指标，那么就收集它们。如果不需要，就不要收集。</p>
<p>如果你发现了一个问题，不要急着去修复。在很多情况下，<strong>系统会对此作出反应</strong>。当系统需要你采取行动的时候，它会给你发出告警。如果它不要求你在半夜跑去修复问题，那么它就不算是一个告警。它只不过是一种警告，你可以在把它当成一般的问题来处理。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[微服务，够了]]></title>
      <url>http://team.jiunile.com/blog/2017/07/microservice-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%A4%9F%E4%BA%86.html</url>
      <content type="html"><![CDATA[<p><img src="/images/ms/01/cover.png" alt="01"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  资深架构师<a href="https://twitter.com/aadrake" target="_blank" rel="external">Adam Drake</a>在他的博客上分享了他对微服务的看法,他 从自己的经验出发,结合Martin Fowler对微服务的见解,帮助想要采用 微服务的公司重新审视微服务。以下内容已获得作者翻译授权,查看英文 原文 <a href="https://aadrake.com/posts/2017-05-20-enough-with-the-microservices.html" target="_blank" rel="external">Enough with the microservices</a>。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  关于微服务的优势和劣势已经有过太多的讨论,不过我仍然看到很多 成长型初创公司对它进行着盲目崇拜。冒着“重复发明轮子”的风险(Martin Fowler已经写过“Microservice Premium”的文章),我想把我的一些 想法写下来,在必要的时候可以发给客户,也希望能够帮助人们避免犯下我之前见过的那些错误。在进行架构或技术选型时,将网络上找到的一些所谓的最佳实践文章作为指南,一旦做出了错误的决定,就要付出惨重的代价。如果能够帮助哪怕一个公司避免犯下这种错误,那么写这篇文章都是值得的。</p>
<p>  如今微服务是个热门技术,微服务架构一直以来都存在(面向服务架构也算是吧?),但对于我所见过的大部分公司来说,微服务不仅浪费了他们的时间,分散了他们的注意力,而且让事情变得更糟糕。</p>
<p>  这听起来似乎很奇怪,因为大部分关于微服务的文章都会肯定微服务 的各种好处,比如解耦系统、更好的伸缩性、消除开发团队之间的依赖, 等等。如果你的公司有 Uber、Airbnb、Facebook 或 Twitter 那样的规模, 那么就没有什么问题。我曾经帮助一些大型组织转型到微服务架构,包括 搭建消息系统和采用一些能够提升伸缩性的技术。不过,对于成长型初创 公司来说,很少需要这些技术和服务。</p>
<p>  Russ Miles在他的《<a href="http://www.russmiles.com/essais/8-ways-to-lose-at-microservices-adoption" target="_blank" rel="external">让微服务失效的八种方式</a>》这篇文章中表达了 他的首要观点,而在我看来,这些场景却到处可见。成长型初创公司总是 想模仿那些大公司的最佳实践,用它们来弥补自身的不足。但是,最佳实 践是要视情况而定的。有些东西对于 Facebook 来说是最佳实践,但对于 只有不到百人的初创公司来说,它们就不一定也是最佳实践。</p>
<p>  如果你的公司比那些大公司小一些,在一定程度上你仍然能够从微服务架构中获益。但是,对于成长型初创公司来说,大规模地迁移到微服务是一种过错,而且对技术人来说是不公平的。</p>
<a id="more"></a>
<h2 id="为什么选择微服务"><a href="#为什么选择微服务" class="headerlink" title="为什么选择微服务?"></a>为什么选择微服务?</h2><p>  一般来说,成长型初创公司采用微服务架构最主要的目的为了减少或消除开发团队之间的依赖,或者提升系统处理大流量负载的能力(比如伸缩性)。开发人员经常抱怨的问题和常见的症状包括合并冲突、由未完整实现的功能引起的部署错误以及伸缩性问题。接下来让我们逐个说明这些问题。</p>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>  在初创公司的早期阶段,开发团队规模不大,使用的技术也很简单。人们在一起工作,不会出现混乱,要实现一些功能也比较快。一切看起来都很美好。</p>
<p>  随着公司的不断发展,开发团队也在壮大,代码库也在增长,然后就出现了多个团队在同一个代码库上工作的情况。这些团队的大部分成员都是公司早期的员工。因为初创公司的早期员工一般都是初级开发人员,他们并没有意识到一个问题,那就是在团队规模增长和代码库增长的同时,沟通效率也需要随之提升。对于缺乏经验的技术人员来说,他们倾向于通过技术问题来解决人的问题,并希望通过微服务来减少开发团队之间的依赖和耦合。</p>
<p>  实际上,他们真正需要做的是通过有效的沟通来解决人的问题。当一个初创公司有多个开发团队时,团队之间需要协调,团队成员需要知道每个人都在做什么,他们需要协作。在这样规模的企业里,软件开发其实具备了社交的性质。如果团队之间缺乏沟通或者缺乏信息分享,不管用不用微服务,一样存在依赖问题,而就算使用了微服务,也仍然存在负面的技术问题。</p>
<p>  将代码模块化作为解决这个问题的技术方案,确实能够缓解软件开发固有的团队依赖问题,但团队间的沟通仍然要随着团队规模的增长而不断改进。</p>
<p>  不要混淆了解耦和分布式二者的含义。由模块和接口组成的单体可以帮助你达到解耦的目的,而且你也应该这么做。你没有必要把应用程序拆分成分布式的多个独立服务,在模块间定义清晰的接口同样能达到解耦的目的。</p>
<h2 id="部分功能实现"><a href="#部分功能实现" class="headerlink" title="部分功能实现"></a>部分功能实现</h2><p>  微服务里需要用到<a href="https://en.wikipedia.org/wiki/Feature_toggle" target="_blank" rel="external">功能标志</a> (feature flag),微服务开发人员需要熟悉这种技术。特别是在进行快速开发(下面会深入讨论)的时候,你可能需要部署一些功能,这些功能在某些平台上还没有实现,或者前端已经完全实现,但后端还没有。随着公司的发展,部署和运维系统变得越来越自动化和复杂,功能标志也变得越来越重要。</p>
<h2 id="水平伸缩"><a href="#水平伸缩" class="headerlink" title="水平伸缩"></a>水平伸缩</h2><p>  通过部署同一个微服务的多个实例来获得伸缩性,这是微服务的优点 之一。不过,大多数过早采用微服务的公司在这些微服务背后使用了同一 个存储系统。也就是说,这些服务具备了伸缩性,但整个应用并不具备伸 缩性。如果你正打算使用这样的伸缩方式,那为什么不直接在负载均衡器 后面部署多个单体实例呢?你可以用更简单的方式达到相同的目的。再者, 水平伸缩应该被作为杀手锏来使用。你首先要关注的应该是如何提升应用 程序的性能。一些简单的优化常常能带来数百倍的性能提升,这里也包括 如何正确地使用其他服务。例如,我在一篇博文里提到的<a href="https://aadrake.com/posts/2017-05-15-redis-performance-triage-handbook.html" target="_blank" rel="external">Redis性能诊断</a>。</p>
<h2 id="我们为微服务做好准备了吗"><a href="#我们为微服务做好准备了吗" class="headerlink" title="我们为微服务做好准备了吗?"></a>我们为微服务做好准备了吗?</h2><p>  在讨论架构选型时,人们经常会忽略这个问题,但其实却是最重要的。 高级技术人员在了解了开发人员或业务人员的抱怨或痛点之后,在网上找 寻找解决方案,他们总是宣称能解决这些问题。但在这些信誓旦旦的观点 背后,有很多需要注意的地方。微服务有利也有弊。如果你的企业足够成 熟,并且具有一定的技术积累,那么采用微服务所面临的挑战会小很多, 并且能够带来更多正面好处。那么怎样才算已经为微服务做好准备了呢? Martin Fowler在多年前表达了他对<a href="https://martinfowler.com/bliki/MicroservicePrerequisites.html" target="_blank" rel="external">微服务先决条件</a>的看法,但是从我的 经验来看,大多数成长型初创公司完全忽略了他的观点。Martin 的观点 是一个很好的切入点,让我们来逐个说明。</p>
<p>  我敢说,大部分成长型初创公司几乎连一个先决条件都无法满足,更不用说满足所有的条件了。如果你的技术团队不具备快速配置、部署和监控能力,那么在迁移到微服务前必须先获得这些能力。接下来让我们更详细地讨论这些先决条件。</p>
<h2 id="快速配置"><a href="#快速配置" class="headerlink" title="快速配置"></a>快速配置</h2><p>  如果你的开发团队里只有少数几个人可以配置新服务、虚拟环境或其 他配套设施,那说明你们还没有为微服务做好准备。你的每个团队里都应 该要有几个这样的人,他们具备了配置基础设施和部署服务的能力,而且 不需要求助于外部。要注意,光是有一个 DevOps 团队并不意味着你在实 施 DevOps。开发人员应该参与管理与应用程序相关的组件,包括基础设施。</p>
<p>  类似的,如果你没有灵活的基础设施(易于伸缩并且可以由团队里的不同人员来管理)来支撑当前的架构,那么在迁移到微服务前必须先解决这个问题。你当然可以在裸机上运行微服务,以更低的成本获得出众的性能,但在服务的运维和部署方面也必须具备灵活性。</p>
<h2 id="基本的监控"><a href="#基本的监控" class="headerlink" title="基本的监控"></a>基本的监控</h2><p>  如果你不曾对你的单体应用进行过性能监控,那么在迁移到微服务时, 你的日子会很难过。你需要熟悉系统级别的度量指标(比如 CPU 和内存)、 应用级别的度量指标(比如端点的请求延迟或端点的错误)和业务级别的 度量指标(比如每秒事务数或每秒收益),这样才可以更好地理解系统的 性能。在性能方面,微服务生态系统比单体系统要复杂得多,就更不用提 诊断问题的复杂性了。你可以搭建一个监控系统(如 Prometheus),在 将单体应用拆分成微服务之前对应用做一些增强,以便进行监控。</p>
<h2 id="快速部署"><a href="#快速部署" class="headerlink" title="快速部署"></a>快速部署</h2><p>  如果你的单体系统没有一个很好的持续集成流程和部署系统,那么要 集成和部署好你的微服务几乎是件不可能的事。想象一下这样的场景:10 个团队和 100 个服务,它们都需要进行手动测试和部署,然后再将这些工 作与测试和部署一个单体所需要的工作进行对比。100 个服务会出现多少 种问题?而单体系统呢?这些先决条件很好地说明了微服务的复杂性。</p>
<p>  Phil Calcado在Fowler的先决条件清单里添加了一些东西,不过我 认为它们更像是重要的扩展,而不是真正的先决条件。</p>
<h2 id="如果我们具备了这些先决条件呢"><a href="#如果我们具备了这些先决条件呢" class="headerlink" title="如果我们具备了这些先决条件呢?"></a>如果我们具备了这些先决条件呢?</h2><p>  就算具备了这些条件,仍然需要注意微服务的负面因素,确保微服务能够为你的业务带来真正的价值。事实上,很多技术人员对微服务中存在的<a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="external">分布式计算谬论</a>视而不见,但为了确保能够成功,这些问题是必须要考虑到的。对于大部分成长型初创公司来说,基于各种原因,他们应该避免使用微服务。</p>
<h2 id="运营成本的增加"><a href="#运营成本的增加" class="headerlink" title="运营成本的增加"></a>运营成本的增加</h2><p>  快速部署这一先决条件已经涵盖了一部分成本,除此之外,对 微服务进行容器化(可能使用 Docker)和使用容器编排系统(比如 Kubernetes)也需要耗费很多成本。Docker 和 Kubernetes 都是很优秀的 技术,但是对于大部分成长型初创公司来说,它们都是一种负担。我见过 初创公司使用 rsync 作为部署和编排工具,我也见过很多的初创公司陷入 运维工具的复杂性泥潭里,他们因此浪费了很多时间,而这些时间本来可 以用于为用户开发更多的功能。</p>
<h2 id="你的应用会被拖慢"><a href="#你的应用会被拖慢" class="headerlink" title="你的应用会被拖慢"></a>你的应用会被拖慢</h2><p>  如果你的单体系统里包含了多个模块,并且在模块间定义了良好的 API,那么 API 之间的交互就几乎没有什么额外开销。但对于微服务来说 就不是这么一回事了,因为它们一般运行在不同的机器上,它们之间需要 通过网络进行交互。这样会在一定程度上拖慢整个系统。如果一个请求需 要多个服务进行同步交互,那么情况会变得更加糟糕。我曾经工作过的一 个公司,他们需要调用将近 10 个服务才能处理完某些请求。处理请求的 每一个步骤都需要额外的网络开销和延迟,但实际上,他们可以把这些服 务放在单个软件包里,按照不同的模块来区分,或者把它们设计成异步的。这样可以为他们节省大量的基础设施成本。</p>
<h2 id="本地开发变得更加困难"><a href="#本地开发变得更加困难" class="headerlink" title="本地开发变得更加困难"></a>本地开发变得更加困难</h2><p>  如果你有一个单体应用,后端只有一个数据库,那么在开发过程中, 在本地运行这个应用是很容易的。如果你有 100 个服务,并使用了多个数 据存储系统,而且它们之间互相依赖,那么本地开发就会变成一个噩梦。即使是 Docker 也无法把你从这种复杂性泥潭中拯救出来。虽然事情原本 可以简单一些,不过仍然需要处理依赖问题。理论上说,微服务不存在这 些问题,因为微服务被认为是相互独立的。不过,对于成长型初创公司来说,就不是这么一回事了。技术人员一般需要在本地运行所有(或者几乎 所有)的服务才能进行新功能的开发和测试。这种复杂性是对资源的巨大浪费。</p>
<h2 id="难以伸缩"><a href="#难以伸缩" class="headerlink" title="难以伸缩"></a>难以伸缩</h2><p>  对单体系统进行伸缩的最简单方式是在负载均衡器后面部署单体系 统的多个实例。在流量增长的情况下,这是一种非常简单的伸缩方式, 而且从运维角度来讲,它的复杂性是最低的。你的系统在编排平台(如 <a href="https://aws.amazon.com/cn/elasticbeanstalk/" target="_blank" rel="external">Elastic Beanstalk</a>)上运行的时间越长越好,你和你的团队就可以集中 精力开发客户需要的东西,而不是忙于解决部署管道问题。使用合适的 CI/CD 系统可以缓解这个问题,但在微服务生态系统里,事情要复杂得多, 而且这些复杂性所造成的麻烦已经超过了它们所能带来的好处。</p>
<h2 id="然后呢"><a href="#然后呢" class="headerlink" title="然后呢?"></a>然后呢?</h2><p>  如果你刚好身处一个成长型初创公司里,需要对架构做一些调整,而微服务似乎不能解决你的问题,这个时候应该怎么办?</p>
<p>  Fowler 提出的先决条件可以说是技术领域的<a href="https://en.wikipedia.org/wiki/Capability_Maturity_Model" target="_blank" rel="external">能力成熟度模型</a>, Fowler 在他的文章里对成熟度模型进行过介绍。如果这种成熟度模型对于公司来说是说得通的,那么我们可以按照 Fowler 提出的先决条件,并使用其他的一些中间步骤为向微服务迁移做好准备。下面的内容引用自 Fowler 的文章。</p>
<p>  关键是你要认识到,成熟度模型的评估结果并不代表你的当前水平,它们只是在告诉你需要做哪些工作才能朝着改进的目标前进。你当前的水平只是一种中间工作,用于确定下一步该获得什么样的技能。</p>
<p>  那么,我们该做出怎样的改进,以及如何达成这些目标?我们需要经过一些简单的步骤,其中前面两步就可以解决很多在向微服务迁移过程中会出现的问题,而且不会带来相关的复杂性。</p>
<ul>
<li>清理应用程序。确保应用程序具有良好的自动化测试套件,并使 用了最新版本的软件包、框架和编程语言。</li>
<li>重构应用程序,把它拆分成多个模块,为模块定义清晰的API。不 要让外部代码直接触及模块内部,所有的交互都应该通过模块提 供的API来进行。</li>
<li>从应用程序中选择一个模块,并把它拆分成独立的应用程序,部 署在相同的主机上。你可以从中获得一些好处,而不会带来太多 的运维麻烦。不过,你仍然需要解决这两个应用之间的交互问 题,虽然它们都部署在同一个主机上。不过你可以无视微服务架 构里固有的网络分区问题和分布式系统的可用性问题。</li>
<li>把独立出来的模块移动到不同的主机上。现在,你需要处理跨网 络交互问题,不过这样可以让这两个系统之间的耦合降得更低。</li>
<li>如果有可能,可以重构数据存储系统,让另一个主机上的模块负 责自己的数据存储。</li>
</ul>
<p>在我所见过的公司里,如果他们能够完成前面两个步骤就算万事大吉了。如果他们能够完成前面两个步骤,那么剩下的步骤一般不会像他们最初想象的那么重要了。如果你决定在这个过程的某个点上停下来,而系统仍然具有可维护性和比刚开始时更好的状态,那么就再好不过了。</p>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>  我不能说这些想法都是独一无二的,也不能说是我所独有的。我只是 从其他遭遇了相同问题的人那里收集想法,并连同观察到的现象在这里作 了一次总结。还有其他很多比我更有经验的人也写过这方面的文章,他们 剖析地更加深入,比如Sander Mak写的有关模块和<a href="https://www.oreilly.com/ideas/modules-vs-microservices" target="_blank" rel="external">微服务的文章</a>。不管 怎样,对于正在考虑对他们的未来架构做出调整的公司来说,这些经验都 是非常重要的。认真地思考每一个问题,确保微服务对你们的组织来说是 一个正确的选择。</p>
<p>  最起码在完成了上述的前面两个步骤之后,再慎重考虑一下微服务对于你的组织来说是否是正确的方向。你之前的很多问题可能会迎刃而解。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL性能监控慢日志分析利器]]></title>
      <url>http://team.jiunile.com/blog/2017/07/mysql-mysql-performance-monitoring.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>入题之前先讲讲为什么写这篇文章，这就不得不提起MySQL与percona，阿里基于mysql开发了AliSQL，写这篇文章的时候阿里已经将其开源，percona是一家领先的MySQL咨询公司，该公司基于mysql开发了Percona Server，Percona Server是一款独立的数据库产品，为用户提供了换出其MySQL安装并换入Percona Server产品的能力。percona除了开发了多款数据库产品，还开发了数据库监控程序：pmm（Percona Monitoring and Management）服务器，我们都知道mysql自身缺乏实时的监控功能，而此时pmm-server就恰好解决了我们这一难题，好了废话不多说，先看一张pmm server的监控图。<br><img src="/images/mysql_pmm_1.png" alt="PMM"></p>
<p>常规的监测项目都有了，最吸引我的一点在于它的慢日志分析功能，如下图所示：<br><img src="/images/mysql_pmm_2.png" alt="PMM"></p>
<a id="more"></a>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="1-在vmware或者virtualbox上安装ubuntu14-04-Server镜像，可以选择清华大学的镜像，下载速度快"><a href="#1-在vmware或者virtualbox上安装ubuntu14-04-Server镜像，可以选择清华大学的镜像，下载速度快" class="headerlink" title="1. 在vmware或者virtualbox上安装ubuntu14.04 Server镜像，可以选择清华大学的镜像，下载速度快"></a>1. 在vmware或者virtualbox上安装ubuntu14.04 Server镜像，可以选择清华大学的镜像，下载速度快</h3><h3 id="2-系统装完后接下来就要在ubuntu上安装docker了，执行命令："><a href="#2-系统装完后接下来就要在ubuntu上安装docker了，执行命令：" class="headerlink" title="2. 系统装完后接下来就要在ubuntu上安装docker了，执行命令："></a>2. 系统装完后接下来就要在ubuntu上安装docker了，执行命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="_">-s</span>SL https://get.daocloud.io/docker | sh</span><br></pre></td></tr></table></figure>
<p>等待完成即可，这是一种安装docker比较快的方式，而且安装的docker版本也比较高，安装完成后输入docker -v看到下面信息说明安装完成：<br><code>Docker version 17.04.0-ce, build 4845c56</code></p>
<h3 id="3-安装完docker，接下来就需要下载pmm-server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像："><a href="#3-安装完docker，接下来就需要下载pmm-server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像：" class="headerlink" title="3. 安装完docker，接下来就需要下载pmm server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像："></a>3. 安装完docker，接下来就需要下载pmm server的镜像，由于下载国外镜像速度慢而且网络不稳定，这里推荐一个中科大的开源docker镜像：</h3><p>在 Docker 的启动参数中加入:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--registry-mirror=https://docker.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure></p>
<p>Ubuntu 用户（包括使用 systemd 的 Ubuntu 15.04）可以修改 /etc/default/docker 文件，加入如下参数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_OPTS=<span class="string">"--registry-mirror=https://docker.mirrors.ustc.edu.cn"</span></span><br></pre></td></tr></table></figure></p>
<p>其他 systemd 用户可以通过执行 sudo systemctl edit docker.service 来修改设置, 覆盖默认的启动参数:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/docker <span class="_">-d</span> -H fd:// --registry-mirror=https://docker.mirrors.ustc.edu.cn</span><br></pre></td></tr></table></figure></p>
<h3 id="4-接下来下载pmm镜像的速度就会大大提升，执行下面命令："><a href="#4-接下来下载pmm镜像的速度就会大大提升，执行下面命令：" class="headerlink" title="4. 接下来下载pmm镜像的速度就会大大提升，执行下面命令："></a>4. 接下来下载pmm镜像的速度就会大大提升，执行下面命令：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull percona/pmm-server:1.2.0</span><br></pre></td></tr></table></figure>
<p>然后等待完成即可。</p>
<h3 id="5-创建PMM-数据容器："><a href="#5-创建PMM-数据容器：" class="headerlink" title="5. 创建PMM 数据容器："></a>5. 创建PMM 数据容器：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker create \</span><br><span class="line">   -v /opt/prometheus/data \</span><br><span class="line">   -v /opt/consul-data \</span><br><span class="line">   -v /var/lib/mysql \</span><br><span class="line">   -v /var/lib/grafana \</span><br><span class="line">   --name pmm-data \</span><br><span class="line">   percona/pmm-server:1.2.0 /bin/<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="6-运行PMM-server容器"><a href="#6-运行PMM-server容器" class="headerlink" title="6. 运行PMM server容器:"></a>6. 运行PMM server容器:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run <span class="_">-d</span> \</span><br><span class="line">   -p 80:80 \</span><br><span class="line">   --volumes-from pmm-data \</span><br><span class="line">   --name pmm-server \</span><br><span class="line">   --restart always \</span><br><span class="line">   percona/pmm-server:1.2.0</span><br></pre></td></tr></table></figure>
<h3 id="7-安装PMM客户端："><a href="#7-安装PMM客户端：" class="headerlink" title="7. 安装PMM客户端："></a>7. 安装PMM客户端：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh https://www.percona.com/downloads/pmm-client/pmm-client-1.2.0/binary/redhat/7/x86_64/pmm-client-1.2.0-1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<h3 id="8-连接PMM服务器："><a href="#8-连接PMM服务器：" class="headerlink" title="8. 连接PMM服务器："></a>8. 连接PMM服务器：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmm-admin config --server pmm服务器主机地址</span><br></pre></td></tr></table></figure>
<h3 id="9-配置mysql监控："><a href="#9-配置mysql监控：" class="headerlink" title="9. 配置mysql监控："></a>9. 配置mysql监控：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmm-admin add mysql --user root --password 123456 --host mysql ip地址 --port 3306 instace3306</span><br></pre></td></tr></table></figure>
<p><code>注：pmm-client收的监控数据来源有这么几方面</code></p>
<ul>
<li>MySQL所在机器的系统指标</li>
<li>MySQL的performance_schema库</li>
<li>slow-log(慢查询日志–mysql要开启慢日志功能)</li>
</ul>
<p><code>如果我们想收集a和c中的指标的话，最好还是将pmm-client部署在MySQL所在机器</code></p>
<h3 id="10-至此访问pmm服务器ip地址即可查看接口"><a href="#10-至此访问pmm服务器ip地址即可查看接口" class="headerlink" title="10. 至此访问pmm服务器ip地址即可查看接口"></a>10. 至此访问pmm服务器ip地址即可查看接口</h3>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL too many connection 问题分析]]></title>
      <url>http://team.jiunile.com/blog/2016/09/mysql-mysql-sleep.html</url>
      <content type="html"><![CDATA[<h2 id="问题缘由"><a href="#问题缘由" class="headerlink" title="问题缘由"></a>问题缘由</h2><p>线上一个网站在运行一段时间后，页面打开速度变慢随之出现<code>502 bad gateway</code>的错误。</p>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>由<code>502 bad gateway</code>的错误，我们知道就是nginx链接后端的php，而php程序没有及时返回，造成了超时。<br>那么造成php超时的原因也有多种，比如：</p>
<ol>
<li>php-fpm资源消耗光</li>
<li>调用外部资源超时，比如外部的web service、数据库等等</li>
<li>….</li>
</ol>
<p>出现问题，我们先登录服务器查看相关日志。</p>
<p>结合我们的业务，首先想到了mysql数据库，先查看了一下mysql数据库的状态，通过show full processlist命令发现有大量的链接处于sleep状态。</p>
<p><code>sleep</code>状态的意思就是说，某个客户端一直占着这个链接，但是什么事也不干，或者是客户端压根儿就已经断开了，而服务端却不知道。</p>
<p>我们知道，mysql的连接数是有限制的，比如默认是151个，那么当大量的链接处于sleep状态时，php程序就无法同mysql建立链接，就会发生超时现象。</p>
<a id="more"></a>
<p>那么造成sleep的原因，有三个，下面是mysql手册给出的解释：</p>
<ol>
<li>客户端程序在退出之前没有调用<code>mysql_close()</code>。[写程序的疏忽，或者数据库的db类库没有自动关闭每次的连接。。。]</li>
<li>客户端sleep的时间在<code>wait_timeout</code>或<code>interactive_timeout</code>规定的秒内没有发出任何请求到服务器. [类似常连，类似于不完整的tcp ip协议构造，服务端一直认为客户端仍然存在（有可能客户端已经断掉了）]</li>
<li>客户端程序在结束之前向服务器发送了请求还没得到返回结果就结束掉了。 [参看：tcp ip协议的三次握手]</li>
</ol>
<p>那么知道了问题所在，就要找到是什么原因导致的sleep线程的存在，</p>
<p>通过上面的信息，我们知道是 192.168.1.2这个IP的20318端口和mysql建立的链接，而192.168.1.2正是我们的web服务器，</p>
<p>于是ssh登录服务器，通过<code>netstat -tunp</code>找到端口20318所对应的进程和pid，一看就是php-fpm引起的。</p>
<p>下面就是要看一下这个php-fpm是调用的哪一个php文件，找到了具体的php文件就好办了。</p>
<p>具体可以通过<code>lsof</code>列出这个<code>pid</code>打开的文件，也可以通过<code>strace</code>跟踪进程的系统调用。</p>
<p>下面是lsof的部分输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lsof -p 23018</span></span><br><span class="line">COMMAND   PID   USER   FD   TYPE     DEVICE     SIZE       NODE NAME</span><br><span class="line">php-fpm 15687 daemon  cwd    DIR      104,2     4096   69437193 /xxx/daemon.php</span><br><span class="line">php-fpm 15687 daemon  rtd    DIR      104,6     4096          2 /</span><br><span class="line">php-fpm 15687 daemon  txt    REG      104,5 27714205    3466635 /app/php/sbin/php-fpm</span><br></pre></td></tr></table></figure></p>
<p>从中可以看到，是我们的<code>daemon.php</code>引起的，这个程序是我们向ios设备推送通知的程序，其中要跟苹果（Apple）的服务器建立链接，可能是苹果服务器不稳定,超时引起的。</p>
<p>程序大致流程：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql_connect();  <span class="comment">//建立链接</span></span><br><span class="line"></span><br><span class="line">$queryBid = mysql_query(<span class="string">"select bundleId from apps"</span>);</span><br><span class="line"><span class="keyword">while</span> ($row = mysql_fetch_assoc($queryBid)) &#123;</span><br><span class="line">	<span class="comment">//取出通知内容，连接苹果服务器，进行推送</span></span><br><span class="line">	<span class="comment">//这里有时候会花一二十分钟，此时mysql服务器那里的连接一直是sleep状态</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mysql_close();</span><br></pre></td></tr></table></figure></p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>方案一：修改的思路也很简单，我们先通过mysql把数据取出来，之后马上关掉mysql连接，释放mysql资源，剩下的就慢慢干好了。 修改后的程序是这样的：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql_connect();  <span class="comment">//建立链接</span></span><br><span class="line"></span><br><span class="line">$arr_bid=<span class="keyword">array</span>();</span><br><span class="line">$queryBid = mysql_query(<span class="string">"select bundleId from apps"</span>);</span><br><span class="line"><span class="keyword">while</span>($row = mysql_fetch_assoc($queryBid)) &#123;</span><br><span class="line">	$arr_bid[] = $row;</span><br><span class="line">&#125;</span><br><span class="line">mysql_close(); <span class="comment">//从mysql中取完数据就马上关闭连接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">foreach</span>($arr_bid <span class="keyword">as</span> $row)&#123;</span><br><span class="line">	<span class="comment">//取出通知内容，连接苹果服务器，进行推送</span></span><br><span class="line">	<span class="comment">//这里有时候会花一二十分钟</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>方案二：[不治本，有弊端]<br>写一个定时脚本，每分钟检查下mysql连接数，超过sleep时间的自动kill掉<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span>  </span><br><span class="line">define(<span class="string">'MAX_SLEEP_TIME'</span>， <span class="number">120</span>);  </span><br><span class="line">  </span><br><span class="line">$hostname = <span class="string">"localhost"</span>;  </span><br><span class="line">$username = <span class="string">"root"</span>;  </span><br><span class="line">$password = <span class="string">"password"</span>;  </span><br><span class="line">  </span><br><span class="line">$connect = mysql_connect($hostname， $username， $password);  </span><br><span class="line">$result = mysql_query(<span class="string">"SHOW PROCESSLIST"</span>， $connect);  </span><br><span class="line"><span class="keyword">while</span> ($proc = mysql_fetch_assoc($result)) &#123;  </span><br><span class="line">	<span class="keyword">if</span> ($proc[<span class="string">"Command"</span>] == <span class="string">"Sleep"</span> &amp;&amp; $proc[<span class="string">"Time"</span>] &gt; MAX_SLEEP_TIME) &#123;  </span><br><span class="line">	@mysql_query(<span class="string">"KILL "</span> . $proc[<span class="string">"Id"</span>]， $connect);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br><span class="line">mysql_close($connect);</span><br></pre></td></tr></table></figure></p>
<p>加入到crontab定时计划里<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * php /usr/<span class="built_in">local</span>/sbin/<span class="built_in">kill</span>-mysql-sleep-proc.php</span><br></pre></td></tr></table></figure></p>
<p>方案三：[不推荐]<br>修改mysql配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="built_in">wait</span>_timeout=10</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">mysql&gt; <span class="built_in">set</span> global <span class="built_in">wait</span>_timeout=10;</span><br></pre></td></tr></table></figure></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MaxScale 在 slave 有故障后如何处理？]]></title>
      <url>http://team.jiunile.com/blog/2016/08/mysql-maxscale-02.html</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前介绍了 <a href="/blog/2016/08/maxscale-01.html">MaxScale</a> 可以实现 Mysql 的读写分离和读负载均衡，那么当 slave 出现故障后，MaxScale 会如何处理呢？</p>
<p>例如有 3 台数据库服务器，一主二从的结构，数据库名称分别为 master, slave1, slave2</p>
<p>现在我们实验以下两种情况：</p>
<ol>
<li>当一台从服务器（ slave1 或者 slave2 ）出现故障后，查看 MaxScale 如何应对，及故障服务器重新上线后的情况</li>
<li>当两台从服务器（ slave1 和 slave2 ）都出现故障后，查看 MaxScale 如何应对，及故障服务器重新上线后的情况</li>
</ol>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>为了更深入的查看 MaxScale 的状态，需要把 MaxScale 的日志打开</p>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [maxscale] 部分，这里用来进行全局设置，在其中添加日志配置<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log_info</span>=<span class="number">1</span></span><br><span class="line"><span class="attr">logdir</span>=/tmp/</span><br></pre></td></tr></table></figure></p>
<p>通过开启 log_info 级别，可以看到 MaxScale 的路由日志</p>
<p>修改配置后，重启 MaxScale </p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><a id="more"></a>
<h3 id="单个-slave-故障的情况"><a href="#单个-slave-故障的情况" class="headerlink" title="单个 slave 故障的情况"></a>单个 slave 故障的情况</h3><p>初始状态是一切正常<br><img src="/images/maxscale_08.png" alt="Maxscale"></p>
<p>停掉 slave2 的复制，登录 slave2 的 mysql 执行<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; stop slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_09.png" alt="Maxscale"></p>
<p>slave2 已经失效了</p>
<p>查看日志信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/maxscale1.log</span><br><span class="line"><span class="comment">#尾部显示：</span></span><br><span class="line">2016-08-15 12:26:02   notice : Server changed state: slave2[172.17.0.4:3306]: lost_slave</span><br></pre></td></tr></table></figure></p>
<p>提示 slave2 已经丢失</p>
<p>查看客户端查询结果<br><img src="/images/maxscale_10.png" alt="Maxscale"></p>
<p>查询操作全都转到了 <code>slave1</code></p>
<p>可以看到， 在有 slave 故障后，MaxScale 会自动进行排除，不再向其转发请求</p>
<p>下面看下 slave2 再次<strong>上线后的情况</strong></p>
<p>登录 slave2 的 mysql 执行<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_11.png" alt="Maxscale"></p>
<p>恢复了正常状态，重新识别到了 slave2</p>
<p>查看日志信息，显示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-08-15 12:32:36   notice : Server changed state: slave2[172.17.0.4:3306]: new_slave</span><br></pre></td></tr></table></figure></p>
<p>查看客户端查询结果<br><img src="/images/maxscale_12.png" alt="Maxscale"></p>
<p>slave2 又可以正常接受查询请求</p>
<p>通过实验可以看到，在部分 slave 发生故障时，MaxScale 可以自动识别出来，并移除路由列表，当故障恢复重新上线后，MaxScale 也能自动将其加入路由，过程透明</p>
<h3 id="全部-slave-故障的情况"><a href="#全部-slave-故障的情况" class="headerlink" title="全部 slave 故障的情况"></a>全部 slave 故障的情况</h3><p>分别登陆 slave1 和 slave2 的 mysql，执行停止复制的命令<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; stop slave;</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 服务器状态<br><img src="/images/maxscale_13.png" alt="Maxscale"></p>
<p>发现各个服务器的角色都识别不出来了</p>
<p>查看日志<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2016-08-15 12:44:11   notice : Server changed state: master[172.17.0.2:3306]: lost_master</span><br><span class="line">2016-08-15 12:44:11   notice : Server changed state: slave1[172.17.0.3:3306]: lost_slave</span><br><span class="line">2016-08-15 12:44:11   notice : Server changed state: slave2[172.17.0.4:3306]: lost_slave</span><br><span class="line">2016-08-15 12:44:11   error  : No Master can be determined. Last known was 172.17.0.2:3306</span><br></pre></td></tr></table></figure></p>
<p>从日志中看到，MaxScale 发现2个slave 和 master 都丢了，然后报错：没有 master 了</p>
<p>客户端连接 MaxScale 时也失败了<br><img src="/images/maxscale_14.png" alt="Maxscale"></p>
<p>说明从服务器全部失效后，会导致 master 也无法识别，使整个数据库服务都失效了</p>
<p>对于 slave 全部失效的情况，能否让 master 还可用？这样至少可以正常提供数据库服务</p>
<p>这需要修改 MaxScale 的配置，告诉 MaxScale 我们需要一个稳定的 master</p>
<h4 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h4><p>先恢复两个 slave，让集群回到正常状态，登陆两个 slave 的mysql<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure></p>
<p>修改 MaxScale 配置文件，添加新的配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [MySQL Monitor] 部分，添加：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">detect_stale_master</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>保存退出，然后重启 MaxScale</p>
<h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>停掉两台 slave ，查看 MaxScale 服务器状态<br><img src="/images/maxscale_15.png" alt="Maxscale"></p>
<p>可以看到，虽然 slave 都无法识别了，但 master 还在，并提示处于稳定状态</p>
<p>客户端执行请求<br><img src="/images/maxscale_16.png" alt="Maxscale"></p>
<p>客户端可以连接 MaxScale，而且请求都转到了 master 上，说明 slave 全部失效时，由 master 支撑了全部请求</p>
<p>当恢复两个 slave 后，整体状态自动恢复正常，从客户端执行请求时，又可以转到 slave 上<br><img src="/images/maxscale_17.png" alt="Maxscale"></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>通过测试发现，在部分 slave 故障情况下，对于客户端是完全透明的，当全部 slave 故障时，经过简单的配置，MaxScale 也可以很好的处理</p>
<hr>
<p>来源：杜亦舒</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Mysql 读写分离中间件 MaxScale]]></title>
      <url>http://team.jiunile.com/blog/2016/08/mysql-maxscale-01.html</url>
      <content type="html"><![CDATA[<h2 id="MaxScale-是干什么的？"><a href="#MaxScale-是干什么的？" class="headerlink" title="MaxScale 是干什么的？"></a>MaxScale 是干什么的？</h2><p>配置好了 Mysql 的主从复制结构后，我们希望实现读写分离，把读操作分散到从服务器中，并且对多个从服务器能实现负载均衡</p>
<p><strong>读写分离</strong>和<strong>负载均衡</strong>是 Mysql 集群的基础需求，<strong>MaxScale</strong> 就可以帮着我们方便的实现这些功能<br><img src="/images/maxscale_01.png" alt="Maxscale"></p>
<h2 id="MaxScale-的基础构成"><a href="#MaxScale-的基础构成" class="headerlink" title="MaxScale 的基础构成"></a>MaxScale 的基础构成</h2><ul>
<li>MaxScale 是 Mysql 的兄弟公司 MariaDB 开发的，现在已经发展得非常成熟</li>
<li>MaxScale 是插件式结构，允许用户开发适合自己的插件</li>
<li>MaxScale 目前提供的插件功能分为<strong>5类</strong></li>
</ul>
<h3 id="认证插件"><a href="#认证插件" class="headerlink" title="认证插件"></a>认证插件</h3><p>提供了登录认证功能，MaxScale 会读取并缓存数据库中 user 表中的信息，当有连接进来时，先从缓存信息中进行验证，如果没有此用户，会从后端数据库中更新信息，再次进行验证</p>
<h3 id="协议插件"><a href="#协议插件" class="headerlink" title="协议插件"></a>协议插件</h3><p>包括客户端连接协议，和连接数据库的协议</p>
<h3 id="路由插件"><a href="#路由插件" class="headerlink" title="路由插件"></a>路由插件</h3><p>决定如何把客户端的请求转发给后端数据库服务器，读写分离和负载均衡的功能就是由这个模块实现的</p>
<h3 id="监控插件"><a href="#监控插件" class="headerlink" title="监控插件"></a>监控插件</h3><p>对各个数据库服务器进行监控，例如发现某个数据库服务器响应很慢，那么就不向其转发请求了</p>
<h3 id="日志和过滤插件"><a href="#日志和过滤插件" class="headerlink" title="日志和过滤插件"></a>日志和过滤插件</h3><p>提供简单的数据库防火墙功能，可以对SQL进行过滤和容错</p>
<a id="more"></a>
<h2 id="MaxScale-的安装使用"><a href="#MaxScale-的安装使用" class="headerlink" title="MaxScale 的安装使用"></a>MaxScale 的安装使用</h2><p>例如有 3 台数据库服务器，是一主二从的结构</p>
<h3 id="过程概述"><a href="#过程概述" class="headerlink" title="过程概述"></a>过程概述</h3><ol>
<li>配置好集群环境</li>
<li>下载安装 MaxScale</li>
<li>配置 MaxScale，添加各数据库信息</li>
<li>启动 MaxScale，查看是否正确连接数据库</li>
<li>客户端连接 MaxScale，进行测试</li>
</ol>
<h3 id="详细过程"><a href="#详细过程" class="headerlink" title="详细过程"></a>详细过程</h3><h4 id="配置一主二从的集群环境"><a href="#配置一主二从的集群环境" class="headerlink" title="配置一主二从的集群环境"></a>配置一主二从的集群环境</h4><p>准备3台服务器，安装 Mysql，配置一主二从的复制结构<br>主从复制的配置过程略过</p>
<h4 id="安装-MaxScale"><a href="#安装-MaxScale" class="headerlink" title="安装 MaxScale"></a>安装 MaxScale</h4><p>最好在另一台服务器上安装，如果资源不足，可以和某个 Mysql 放在一起</p>
<p>MaxScale 的下载地址<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://downloads.mariadb.com/files/MaxScale</span><br></pre></td></tr></table></figure></p>
<p>根据自己的服务器选择合适的安装包</p>
<p>以 centos 7 为例 安装步骤如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install libaio.x86_64 libaio-devel.x86_64 novacom-server.x86_64 libedit -y</span><br><span class="line">rpm -ivh maxscale-1.4.3-1.centos.7.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<h4 id="配置-MaxScale"><a href="#配置-MaxScale" class="headerlink" title="配置 MaxScale"></a>配置 MaxScale</h4><p>在开始配置之前，需要在 master 中为 MaxScale 创建两个用户，用于监控模块和路由模块</p>
<p>创建监控用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user scalemon@'%' identified by "111111";</span><br><span class="line">mysql&gt; grant replication slave, replication client on *.* to scalemon@'%';</span><br></pre></td></tr></table></figure></p>
<p>创建路由用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user maxscale@'%' identified by "111111";</span><br><span class="line">mysql&gt; grant select on mysql.* to maxscale@'%';</span><br></pre></td></tr></table></figure></p>
<p>用户创建完成后，开始配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>找到 [server1] 部分，修改其中的 address 和 port，指向 master 的 IP 和端口</p>
<p>复制2次 [server1] 的整块儿内容，改为 [server2] 与 [server3]，同样修改其中的 address 和 port，分别指向 slave1 和 slave2<br><img src="/images/maxscale_02.png" alt="Maxscale"></p>
<p>找到 <code>[MySQL Monitor]</code> 部分，修改 servers 为 server1,server2,server3，修改 user 和 passwd 为之前创建的监控用户的信息（scalemon,111111）<br><img src="/images/maxscale_03.png" alt="Maxscale"></p>
<p>找到 <code>[Read-Write Service]</code> 部分，修改 servers 为 server1,server2,server3，修改 user 和 passwd 为之前创建的路由用户的信息（maxscale,111111）<br><img src="/images/maxscale_04.png" alt="Maxscale"></p>
<p>由于我们使用了 <code>[Read-Write Service]</code>，需要删除另一个服务 <code>[Read-Only Service]</code>，删除其整块儿内容即可</p>
<p>配置完成，保存并退出编辑器</p>
<h4 id="启动-MaxScale"><a href="#启动-MaxScale" class="headerlink" title="启动 MaxScale"></a>启动 MaxScale</h4><p>执行启动命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maxscale --config=/etc/maxscale.cnf</span><br></pre></td></tr></table></figure></p>
<p>查看 MaxScale 的响应端口是否已经就绪<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ntelp</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/maxscale_05.png" alt="Maxscale"></p>
<ul>
<li>4006 是连接 MaxScale 时使用的端口</li>
<li>6603 是 MaxScale 管理器的端口</li>
</ul>
<p>登录 MaxScale 管理器，查看一下数据库连接状态，默认的用户名和密码是 admin/mariadb<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maxadmin --user=admin --password=mariadb</span><br><span class="line">MaxScale&gt; list servers</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/maxscale_06.png" alt="Maxscale"></p>
<p>可以看到，MaxScale 已经连接到了 master 和 slave</p>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>先在 master 上创建一个测试用户<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant ALL PRIVILEGES on *.* to rtest@"%" Identified by "111111";</span><br></pre></td></tr></table></figure></p>
<p>使用 Mysql 客户端到连接 MaxScale<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h MaxScale所在的IP -P 4006 -u rtest -p111111</span><br></pre></td></tr></table></figure></p>
<p>执行查看数据库服务器名的操作来知道当前实际所在的数据库<br><img src="/images/maxscale_07.png" alt="Maxscale"></p>
<p>开启事务后，就自动路由到了 master，普通的查询操作，是在 slave上</p>
<p>MaxScale 的配置完成了</p>
<hr>
<p>来源：杜亦舒</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[双机高可用-负载均衡-MySQL(读写分离、主从自动切换)架构设计]]></title>
      <url>http://team.jiunile.com/blog/2016/08/ha-load-balance-mysql-master-slave-architecture.html</url>
      <content type="html"><![CDATA[<h2 id="架构简介"><a href="#架构简介" class="headerlink" title="架构简介"></a>架构简介</h2><p>只有两台机器，需要实现其中一台死机之后另一台能接管这台机器的服务，并且在两台机器正常服务时，两台机器都能用上。如何设计这样的架构场景。<br><img src="/images/high_availability.png" alt="High Availability"></p>
<p>此架构主要是由keepalived实现双机高可用，维护了一个外网VIP，一个内网VIP。正常情况时，外网VIP和内网VIP都绑定在server1服务器，web请求发送到server1的nginx，nginx对于静态资源请求就直接在本机检索并返回，对于php的动态请求，则负载均衡到server1和server2。对于SQL请求，会将此类请求发送到Atlas MySQL中间件，Atlas接收到请求之后，把涉及写操作的请求发送到内网VIP，读请求操作发送到mysql从，这样就实现了读写分离。</p>
<p>当主服务器server1宕机时，keepalived检测到后，立即把外网VIP和内网VIP绑定到server2，并把server2的mysql切换成主库。此时由于外网VIP已经转移到了server2，web请求将发送给server2的nginx。nginx检测到server1宕机，不再把请求转发到server1的php-fpm。之后的sql请求照常发送给本地的atlas，atlas把写操作发送给内网VIP，读操作发送给mysql从，由于内网VIP已经绑定到server2了，server2的mysql同时接受写操作和读操作。</p>
<p>当主服务器server1恢复后，server1的mysql自动设置为从，与server2的mysql主同步。keepalived不抢占server2的VIP，继续正常服务。</p>
<h2 id="架构要求"><a href="#架构要求" class="headerlink" title="架构要求"></a>架构要求</h2><p>要实现此架构，需要三个条件：</p>
<ol>
<li>服务器可以设置内网IP，并且设置的内网IP互通。</li>
<li>服务器可以随意绑定IDC分配给我们使用的外网IP，即外网IP没有绑定MAC地址。</li>
<li>MySQL服务器支持GTID，即MySQL-5.6.5以上版本。</li>
</ol>
<h2 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h2><blockquote>
<p><strong>对外VIP</strong>：10.96.153.239<br><strong>对内VIP</strong>：192.168.1.150</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">描述</th>
<th style="text-align:left">网卡1：eth0(公网IP)</th>
<th style="text-align:left">网卡2：eth1(内网IP)</th>
<th style="text-align:left">操作系统</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">server1</td>
<td style="text-align:left">10.96.153.110</td>
<td style="text-align:left">192.168.1.100</td>
<td style="text-align:left">Centos 6.X</td>
</tr>
<tr>
<td style="text-align:left">server2</td>
<td style="text-align:left">10.96.153.114</td>
<td style="text-align:left">192.168.1.101</td>
<td style="text-align:left">Centos 6.X</td>
</tr>
</tbody>
</table>
<h2 id="架构配置"><a href="#架构配置" class="headerlink" title="架构配置"></a>架构配置</h2><a id="more"></a>
<h3 id="hosts设置"><a href="#hosts设置" class="headerlink" title="hosts设置"></a>hosts设置</h3><p>Server1与Server2 hosts设置如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/hosts</span></span><br><span class="line">192.168.1.100 server1</span><br><span class="line">192.168.1.101 server2</span><br></pre></td></tr></table></figure></p>
<h3 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h3><p>Nginx、PHP、MySQL、Memcached安装略</p>
<h3 id="解决session共享问题"><a href="#解决session共享问题" class="headerlink" title="解决session共享问题"></a>解决session共享问题</h3><p>php默认的session存储是在/tmp目录下，现在我们是用两台服务器作php请求的负载，这样会造成session分布在两台服务器的/tmp目录下，导致依赖于session的功能不正常。我们可以使用memcached来解决此问题。</p>
<p>上一步我们已经安装好了memcached，现在只需要配置php.ini来使用memcached，配置如下，打开php.ini配置文件，修改为如下两行的值：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session.save_handler = memcache</span><br><span class="line">session.save_path = "tcp://192.168.1.100:11211,tcp://192.168.1.101:11211"</span><br></pre></td></tr></table></figure></p>
<p>之后重启php-fpm生效。</p>
<h3 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h3><h4 id="Server1配置"><a href="#Server1配置" class="headerlink" title="Server1配置"></a>Server1配置</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    [...]</span><br><span class="line">    upstream php-server &#123;</span><br><span class="line">        server 192.168.1.101:9000;</span><br><span class="line">        server 127.0.0.1:9000;</span><br><span class="line">        keepalive 100;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">    server &#123;</span><br><span class="line">        [...]</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            fastcgi_pass   php-server;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line">        [...]</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Server2配置"><a href="#Server2配置" class="headerlink" title="Server2配置"></a>Server2配置</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    [...]</span><br><span class="line">    upstream php-server &#123;</span><br><span class="line">        server 192.168.1.100:9000;</span><br><span class="line">        server 127.0.0.1:9000;</span><br><span class="line">        keepalive 100;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">    server &#123;</span><br><span class="line">        [...]</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            fastcgi_pass   php-server;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line">        [...]</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个配置主要的作用是设置php请求的负载均衡。</p>
<h3 id="MySQL配置"><a href="#MySQL配置" class="headerlink" title="MySQL配置"></a>MySQL配置</h3><h4 id="mysql-util安装"><a href="#mysql-util安装" class="headerlink" title="mysql util安装"></a>mysql util安装</h4><p>我们需要安装mysql util里的主从配置工具来实现主从切换。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-utilities-1.5.3.tar.gz</span><br><span class="line">tar xzf mysql-utilities-1.5.3.tar.gz</span><br><span class="line"><span class="built_in">cd</span> mysql-utilities-1.5.3</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p>
<h4 id="mysql-my-cnf配置"><a href="#mysql-my-cnf配置" class="headerlink" title="mysql my.cnf配置"></a>mysql my.cnf配置</h4><p><strong>server1：</strong><br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mysql]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="attr">protocol</span> = tcp</span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="comment"># BINARY LOGGING</span></span><br><span class="line"><span class="attr">log-bin</span> = /usr/local/mysql/data/mysql-bin</span><br><span class="line"><span class="attr">expire-logs-days</span> = <span class="number">14</span></span><br><span class="line"><span class="attr">binlog-format</span> = row</span><br><span class="line"><span class="attr">log-slave-updates</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">gtid-mode</span> = <span class="literal">on</span></span><br><span class="line"><span class="attr">enforce-gtid-consistency</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">master-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">relay-log-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">server-id</span> = <span class="number">1</span></span><br><span class="line"><span class="attr">report-host</span> = server1</span><br><span class="line"><span class="attr">report-port</span> = <span class="number">3306</span></span><br><span class="line"><span class="section">[...]</span></span><br></pre></td></tr></table></figure></p>
<p><strong>server2：</strong><br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mysql]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="attr">protocol</span> = tcp</span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="section">[...]</span></span><br><span class="line"><span class="comment"># BINARY LOGGING</span></span><br><span class="line"><span class="attr">log-bin</span> = /usr/local/mysql/data/mysql-bin</span><br><span class="line"><span class="attr">expire-logs-days</span> = <span class="number">14</span></span><br><span class="line"><span class="attr">binlog-format</span> = row</span><br><span class="line"><span class="attr">log-slave-updates</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">gtid-mode</span> = <span class="literal">on</span></span><br><span class="line"><span class="attr">enforce-gtid-consistency</span> = <span class="literal">true</span></span><br><span class="line"><span class="attr">master-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">relay-log-info-repository</span> = TABLE</span><br><span class="line"><span class="attr">server-id</span> = <span class="number">2</span></span><br><span class="line"><span class="attr">report-host</span> = server2</span><br><span class="line"><span class="attr">report-port</span> = <span class="number">3306</span></span><br><span class="line"><span class="section">[...]</span></span><br></pre></td></tr></table></figure></p>
<p>这两个配置主要是设置了binlog和启用gtid-mode，并且需要设置不同的server-id和report-host。</p>
<h4 id="开放root帐号远程权限"><a href="#开放root帐号远程权限" class="headerlink" title="开放root帐号远程权限"></a>开放root帐号远程权限</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant all on *.* to 'root'@'192.168.1.%' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; grant all on *.* to 'root'@'server1' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; grant all on *.* to 'root'@'server2' identified by 'Xp29at5F37' with grant option;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<h4 id="设置mysql主从"><a href="#设置mysql主从" class="headerlink" title="设置mysql主从"></a>设置mysql主从</h4><p>在任意一台执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysqlreplicate --master=root:Xp29at5F37@server1:3306 --slave=root:Xp29at5F37@server2:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># slave on server2: … connected.</span></span><br><span class="line"><span class="comment"># Checking for binary logging on master…</span></span><br><span class="line"><span class="comment"># Setting up replication…</span></span><br><span class="line"><span class="comment"># …done.</span></span><br></pre></td></tr></table></figure></p>
<h4 id="显示主从关系"><a href="#显示主从关系" class="headerlink" title="显示主从关系"></a>显示主从关系</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysqlrplshow --master=root:Xp29at5F37@server1 --discover-slaves-login=root:Xp29at5F37</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># Finding slaves for master: server1:3306</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Replication Topology Graph</span></span><br><span class="line">server1:3306 (MASTER)</span><br><span class="line">|</span><br><span class="line">+— server2:3306 – (SLAVE)</span><br></pre></td></tr></table></figure>
<h4 id="检查主从状态"><a href="#检查主从状态" class="headerlink" title="检查主从状态"></a>检查主从状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mysqlrplcheck --master=root:Xp29at5F37@server1 --slave=root:Xp29at5F37@server2</span><br><span class="line"></span><br><span class="line"><span class="comment"># master on server1: … connected.</span></span><br><span class="line"><span class="comment"># slave on server2: … connected.</span></span><br><span class="line">Test Description Status</span><br><span class="line">—————————————————————————</span><br><span class="line">Checking <span class="keyword">for</span> binary logging on master [pass]</span><br><span class="line">Are there binlog exceptions? [pass]</span><br><span class="line">Replication user exists? [pass]</span><br><span class="line">Checking server_id values [pass]</span><br><span class="line">Checking server_uuid values [pass]</span><br><span class="line">Is slave connected to master? [pass]</span><br><span class="line">Check master information file [pass]</span><br><span class="line">Checking InnoDB compatibility [pass]</span><br><span class="line">Checking storage engines compatibility [pass]</span><br><span class="line">Checking lower_<span class="keyword">case</span>_table_names settings [pass]</span><br><span class="line">Checking slave delay (seconds behind master) [pass]</span><br><span class="line"><span class="comment"># …done.</span></span><br></pre></td></tr></table></figure>
<h3 id="Keepalived配置"><a href="#Keepalived配置" class="headerlink" title="Keepalived配置"></a>Keepalived配置</h3><h4 id="安装-两台都装"><a href="#安装-两台都装" class="headerlink" title="安装(两台都装)"></a>安装(两台都装)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install keepalived</span><br><span class="line">chkconfig keepalived on</span><br></pre></td></tr></table></figure>
<h4 id="keepalived配置-server1"><a href="#keepalived配置-server1" class="headerlink" title="keepalived配置(server1)"></a>keepalived配置(server1)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">vrrp_sync_group VG_1 &#123;</span><br><span class="line">	group &#123;</span><br><span class="line">		inside_network</span><br><span class="line">		outside_network</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance inside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth1</span><br><span class="line">	virtual_router_id 51</span><br><span class="line">	priority 101</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		192.168.1.150/24</span><br><span class="line">	&#125;</span><br><span class="line">	nopreempt</span><br><span class="line">	notify /data/sh/mysqlfailover-server1.sh</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance outside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth0</span><br><span class="line">	virtual_router_id 50</span><br><span class="line">	priority 101</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">	auth_<span class="built_in">type</span> PASS</span><br><span class="line">	auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		10.96.153.239/24</span><br><span class="line">	&#125;</span><br><span class="line">	nopreempt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="keepalived配置-server2"><a href="#keepalived配置-server2" class="headerlink" title="keepalived配置(server2)"></a>keepalived配置(server2)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">vrrp_sync_group VG_1 &#123;</span><br><span class="line">	group &#123;</span><br><span class="line">		inside_network</span><br><span class="line">		outside_network</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance inside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth1</span><br><span class="line">	virtual_router_id 51</span><br><span class="line">	priority 100</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		192.168.1.150</span><br><span class="line">	&#125;</span><br><span class="line">	notify /data/sh/mysqlfailover-server2.sh</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">vrrp_instance outside_network &#123;</span><br><span class="line">	state BACKUP</span><br><span class="line">	interface eth0</span><br><span class="line">	virtual_router_id 50</span><br><span class="line">	priority 100</span><br><span class="line">	advert_int 1</span><br><span class="line">	authentication &#123;</span><br><span class="line">		auth_<span class="built_in">type</span> PASS</span><br><span class="line">		auth_pass 3489</span><br><span class="line">	&#125;</span><br><span class="line">	virtual_ipaddress &#123;</span><br><span class="line">		10.96.153.239/24</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此keepalived配置需要注意的是：</p>
<ol>
<li>两台server的state都设置为backup，server1增加nopreempt配置，并且server1 priority比server2高，这样用来实现当server1从宕机恢复时，不抢占VIP;</li>
<li>server1设置notify /data/sh/mysqlfailover-server1.sh,server2设置notify /data/sh/mysqlfailover-server2.sh,作用是自动切换主从</li>
</ol>
<p>/data/sh/mysqlfailover-server1.sh脚本内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"> </span></span><br><span class="line">sleep 10</span><br><span class="line">state=<span class="variable">$3</span></span><br><span class="line">result=`mysql -h127.0.0.1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status;'</span>`</span><br><span class="line">[[ <span class="string">"<span class="variable">$result</span>"</span> == <span class="string">""</span> ]] &amp;&amp; mysqlState=<span class="string">"master"</span> || mysqlState=<span class="string">"slave"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"MASTER"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"slave"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlrpladmin --slave=root:Xp29at5F37@server1:3306 failover</span><br><span class="line">	<span class="keyword">fi</span> </span><br><span class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"BACKUP"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"master"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlreplicate --master=root:Xp29at5F37@server2:3306 --slave=root:Xp29at5F37@server1:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/proxy-read-only-backend-addresses.*/proxy-read-only-backend-addresses = 192.168.1.150:3306/'</span> /usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br><span class="line">mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"REMOVE BACKEND 2;"</span></span><br></pre></td></tr></table></figure></p>
<p>/data/sh/mysqlfailover-server2.sh脚本内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line">sleep 10</span><br><span class="line">state=<span class="variable">$3</span></span><br><span class="line">result=`mysql -h127.0.0.1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status;'</span>`</span><br><span class="line">[[ <span class="string">"<span class="variable">$result</span>"</span> == <span class="string">""</span> ]] &amp;&amp; mysqlState=<span class="string">"master"</span> || mysqlState=<span class="string">"slave"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"MASTER"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"slave"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlrpladmin --slave=root:Xp29at5F37@server2:3306 failover</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$state</span>"</span> == <span class="string">"BACKUP"</span> ]];<span class="keyword">then</span></span><br><span class="line">	<span class="keyword">if</span> [[ <span class="string">"<span class="variable">$mysqlState</span>"</span> == <span class="string">"master"</span> ]];<span class="keyword">then</span></span><br><span class="line">		mysqlreplicate --master=root:Xp29at5F37@server1:3306 --slave=root:Xp29at5F37@server2:3306 --rpl-user=rpl:o67DhtaW</span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/proxy-read-only-backend-addresses.*/proxy-read-only-backend-addresses = 192.168.1.150:3306/'</span> /usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br><span class="line">mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"REMOVE BACKEND 2;"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Atlas设置"><a href="#Atlas设置" class="headerlink" title="Atlas设置"></a>Atlas设置</h3><h4 id="atlas安装（两台服务器）"><a href="#atlas安装（两台服务器）" class="headerlink" title="atlas安装（两台服务器）"></a>atlas安装（两台服务器）</h4><p>到这里下载最新版本，<a href="https://github.com/Qihoo360/Atlas/releases" target="_blank" rel="external">https://github.com/Qihoo360/Atlas/releases</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/Qihoo360/Atlas/releases/download/2.2.1/Atlas-2.2.1.el6.x86_64.rpm</span><br><span class="line">rpm -i Atlas-2.2.1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<h4 id="atlas配置（两台服务器）"><a href="#atlas配置（两台服务器）" class="headerlink" title="atlas配置（两台服务器）"></a>atlas配置（两台服务器）</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/mysql-proxy/conf</span><br><span class="line">cp test.cnf my.cnf</span><br><span class="line">vi my.cnf</span><br><span class="line">#调整如下参数</span><br><span class="line">proxy-backend-addresses = 192.168.1.150:3306</span><br><span class="line">proxy-read-only-backend-addresses = 192.168.1.101:3306</span><br><span class="line">pwds = root:qtyU1btXOo074Itvx0UR9Q==</span><br><span class="line">event-threads = 8</span><br></pre></td></tr></table></figure>
<p><strong><code>注意</code></strong>：</p>
<ul>
<li>proxy-backend-addresse设置为内网VIP</li>
<li>proxy-read-only-backend-addresses设置为server2的IP</li>
<li>root:qtyU1btXOo074Itvx0UR9Q==设置数据库的用户和密码，密码是通过/usr/local/mysql-proxy/bin/encrypt Xp29at5F37生成。</li>
<li>更详细参数解释请查看，<a href="https://github.com/Qihoo360/Atlas/wiki/Atlas%E9%83%A8%E5%88%86%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E5%8F%8A%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3" target="_blank" rel="external">Atlas配置详解</a>。</li>
</ul>
<h4 id="启动atlas（两台服务器）"><a href="#启动atlas（两台服务器）" class="headerlink" title="启动atlas（两台服务器）"></a>启动atlas（两台服务器）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/mysql-proxy/bin/mysql-proxy --defaults-file=/usr/<span class="built_in">local</span>/mysql-proxy/conf/my.cnf</span><br></pre></td></tr></table></figure>
<p>之后程序里配置mysql就配置127.0.0.1:1234就好。</p>
<h4 id="部署atlas自动维护脚本（两台服务器）"><a href="#部署atlas自动维护脚本（两台服务器）" class="headerlink" title="部署atlas自动维护脚本（两台服务器）"></a>部署atlas自动维护脚本（两台服务器）</h4><p>添加定时任务（如每2分钟运行一次）我们把脚本放在/data/sh/auto_maintain_atlas.sh,脚本内容为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"> </span></span><br><span class="line">count=`mysql -N -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"select * from backends;"</span> | wc <span class="_">-l</span>`</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$count</span>"</span> == <span class="string">"1"</span> ]];<span class="keyword">then</span></span><br><span class="line">    result=`mysql -hserver1 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status\G'</span>`</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | grep Slave_IO_State;<span class="keyword">then</span></span><br><span class="line">        slaveIP=192.168.1.100</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        result=`mysql -hserver2 -P3306 -uroot -pXp29at5F37 <span class="_">-e</span> <span class="string">'show slave status\G'</span>`</span><br><span class="line">        slaveIP=192.168.1.101</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    slaveIORunning=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_IO_Running:/&#123;print $2&#125;'</span>`</span><br><span class="line">    slaveSQLRunning=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_SQL_Running:/&#123;print $2&#125;'</span>`</span><br><span class="line">    SlaveSQLRunning_State=`<span class="built_in">echo</span> <span class="string">"<span class="variable">$result</span>"</span> | awk -F<span class="string">':'</span> <span class="string">'/Slave_SQL_Running_State:/&#123;print $2&#125;'</span>`</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$slaveIORunning</span>"</span> =~ <span class="string">"Yes"</span> &amp;&amp; <span class="string">"<span class="variable">$slaveSQLRunning</span>"</span> =~ <span class="string">"Yes"</span> &amp;&amp; <span class="string">"<span class="variable">$SlaveSQLRunning_State</span>"</span> =~ <span class="string">"Slave has read all relay log"</span> ]];<span class="keyword">then</span></span><br><span class="line">        mysql -h127.0.0.1 -P2345 -uuser -ppwd <span class="_">-e</span> <span class="string">"add slave <span class="variable">$&#123;slaveIP&#125;</span>:3306;"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></p>
<p>为什么需要这个脚本呢？假设目前mysql主服务器在s1，s1宕机后，s2接管VIP，接着删除atlas中设置的slave backend，将mysql提升为主。过一段时间后，s1从宕机中恢复，这时候s1的mysql自动切换为从，接着删除atlas中设置的slave backend，开始连接s2的mysql主同步数据。到这个时候我们发现，已经不存在读写分离了，所有的sql都发送给了s2的mysql。auto_maintain_atlas.sh脚本就派上用场了，此脚本会定时的检查主从是否已经同步完成，如果完成就自动增加slave backend，这样读写分离又恢复了，完全不需要人工干预。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="测试keepalived是否工作正常"><a href="#测试keepalived是否工作正常" class="headerlink" title="测试keepalived是否工作正常"></a>测试keepalived是否工作正常</h4><p>我们来模拟server1宕机。<br>在server1上执行shutdown关机命令。<br>此时我们登录server2，执行ip addr命令，输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1: lo: mtu 16436 qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet6 ::1/128 scope host</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:81:9d:42 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 10.96.153.114/24 brd 10.96.153.255 scope global eth0</span><br><span class="line">inet 10.96.153.239/24 scope global secondary eth0</span><br><span class="line">inet6 fe80::20c:29ff:fe81:9d42/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">3: eth1: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:81:9d:4c brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 192.168.1.101/24 brd 192.168.1.255 scope global eth1</span><br><span class="line">inet 192.168.1.150/32 scope global eth1</span><br><span class="line">inet6 fe80::20c:29ff:fe81:9d4c/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>我们看到对外VIP 10.96.153.239和对内IP 192.168.1.150已经转移到server2了，证明keepalived运行正常。</p>
<h4 id="测试是否自动切换了主从"><a href="#测试是否自动切换了主从" class="headerlink" title="测试是否自动切换了主从"></a>测试是否自动切换了主从</h4><p>登录server2的mysql服务器，执行show slave status;命令，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show slave status\G</span><br><span class="line">Empty <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>我们发现从状态已经为空，证明已经切换为主了。</p>
<h4 id="测试server1是否抢占VIP"><a href="#测试server1是否抢占VIP" class="headerlink" title="测试server1是否抢占VIP"></a>测试server1是否抢占VIP</h4><p>为什么要测试这个呢？如果server1恢复之后抢占了VIP，而我们的Atlas里后端设置的是VIP，这样server1启动之后，sql的写操作就会向server1的mysql发送，而server1的mysql数据是旧于server2的，所以这样会造成数据不一致，这个是非常重要的测试。<br>我们先来启动server1，之后执行ip addr，输出如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1: lo: mtu 16436 qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet6 ::1/128 scope host</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:f1:4f:4e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 10.96.153.110/24 brd 10.96.153.255 scope global eth0</span><br><span class="line">inet6 fe80::20c:29ff:fef1:4f4e/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br><span class="line">3: eth1: mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">link/ether 00:0c:29:f1:4f:58 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet 192.168.1.100/24 brd 192.168.1.255 scope global eth1</span><br><span class="line">inet6 fe80::20c:29ff:fef1:4f58/64 scope link</span><br><span class="line">valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>我们看到，server1并没有抢占VIP，测试正常。不过另人郁闷的是，在虚拟机的环境并没有测试成功，不知道为什么。</p>
<h4 id="测试server2的atlas是否已经删除slave-backend"><a href="#测试server2的atlas是否已经删除slave-backend" class="headerlink" title="测试server2的atlas是否已经删除slave backend"></a>测试server2的atlas是否已经删除slave backend</h4><p>我们测试这个是为了保证atlas已经没有slave backend，也就是没有从库的设置了，否则当server1恢复时，有可能会把读请求发送给server1的mysql，造成读取了旧数据的问题。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]<span class="comment"># mysql -h127.0.0.1 -P2345 -uuser -ppwd</span></span><br><span class="line">mysql&gt; select * from backends;</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| backend_ndx | address | state | <span class="built_in">type</span> |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| 1 | 192.168.1.150:3306 | up | rw |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">1 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>如果看到只有一个后端，证明运作正常。</p>
<h4 id="测试server1-mysql是否设置为从"><a href="#测试server1-mysql是否设置为从" class="headerlink" title="测试server1 mysql是否设置为从"></a>测试server1 mysql是否设置为从</h4><p>serve1恢复后，登录server1的mysql服务器，执行show slave status;命令，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">Slave_IO_State: Opening tables</span><br><span class="line">Master_Host: server1</span><br><span class="line">Master_User: rpl</span><br><span class="line">Master_Port: 3306</span><br><span class="line">Connect_Retry: 60</span><br><span class="line">Master_Log_File: mysql-bin.000015</span><br><span class="line">Read_Master_Log_Pos: 48405991</span><br><span class="line">Relay_Log_File: mysql-relay-bin.000002</span><br><span class="line">Relay_Log_Pos: 361</span><br><span class="line">Relay_Master_Log_File: mysql-bin.000015</span><br><span class="line">Slave_IO_Running: Yes</span><br><span class="line">Slave_SQL_Running: yes</span><br></pre></td></tr></table></figure></p>
<h4 id="测试是否自动恢复读写分离"><a href="#测试是否自动恢复读写分离" class="headerlink" title="测试是否自动恢复读写分离"></a>测试是否自动恢复读写分离</h4><p>server1恢复后一段时间，我们可以看是读写分离是否已经恢复。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@server1 ~]<span class="comment"># mysql -h127.0.0.1 -P2345 -uuser -ppwd</span></span><br><span class="line">Warning: Using a password on the <span class="built_in">command</span> line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor. Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1</span><br><span class="line">Server version: 5.0.99-agent-admin</span><br><span class="line">Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line">Type ‘<span class="built_in">help</span>;’ or ‘\h’ <span class="keyword">for</span> help. Type ‘\c’ to clear the current input statement.</span><br><span class="line">mysql&gt; select * from backends;</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| backend_ndx | address | state | <span class="built_in">type</span> |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">| 1 | 192.168.1.150:3306 | up | rw |</span><br><span class="line">| 2 | 192.168.1.100:3306 | up | ro |</span><br><span class="line">+————-+——————–+——-+——+</span><br><span class="line">2 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure></p>
<p>我们看到server1已经被添加为slave backend了。这表示已经成功恢复读写分离。</p>
<hr>
<p>来自：<a href="https://www.centos.bz" target="_blank" rel="external">https://www.centos.bz</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用lsyncd实时同步文件]]></title>
      <url>http://team.jiunile.com/blog/2016/07/lsyncd.html</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Lysncd 实际上是lua语言封装了 inotify 和 rsync 工具，采用了 Linux 内核（2.6.13 及以后）里的 inotify 触发机制，然后通过rsync去差异同步，达到实时的效果。我认为它最令人称道的特性是，完美解决了 inotify + rsync海量文件同步带来的文件频繁发送文件列表的问题 —— 通过时间延迟或累计触发事件次数实现。另外，它的配置方式很简单，lua本身就是一种配置语言，可读性非常强。lsyncd也有多种工作模式可以选择，本地目录cp，本地目录rsync，远程目录rsyncssh。</p>
<p>实现简单高效的本地目录同步备份（网络存储挂载也当作本地目录），一个命令搞定。</p>
<p>github地址：<a href="https://github.com/axkibe/lsyncd" target="_blank" rel="external">https://github.com/axkibe/lsyncd</a> </p>
<h2 id="使用-lsyncd-本地目录实时备份"><a href="#使用-lsyncd-本地目录实时备份" class="headerlink" title="使用 lsyncd 本地目录实时备份"></a>使用 lsyncd 本地目录实时备份</h2><p>这一节实现的功能是，本地目录source实时同步到另一个目录target，而在source下有大量的文件，并且有部分目录和临时文件不需要同步。</p>
<h3 id="安装lsyncd"><a href="#安装lsyncd" class="headerlink" title="安装lsyncd"></a>安装lsyncd</h3><p>安装<code>lsyncd</code>极为简单，已经收录在ubuntu的官方镜像源里，直接通过<code>apt-get install lsyncd</code>就可以。<br>在Redhat系（我的环境是CentOS 6.2 x86_64 ），可以手动去下载 <a href="ftp://195.220.108.108/linux/fedora/linux/updates/21/x86_64/l/lsyncd-2.1.5-6.fc21.x86_64.rpm" target="_blank" rel="external">lsyncd-2.1.5-6.fc21.x86_64.rpm</a>，但首先你得安装两个依赖<code>yum install lua lua-devel</code>。也可以通过在线安装，需要<code>epel-release</code>扩展包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">yum install lsyncd</span><br></pre></td></tr></table></figure></p>
<p><strong>源码编译安装</strong><br>从源码编译安装可以使用最新版的lsyncd程序，但必须要相应的依赖库文件和编译工具：<code>yum install lua lua-devel asciidoc cmake</code>。</p>
<p>从 <a href="http://code.google.com/p/lsyncd/downloads/list" target="_blank" rel="external">googlecode lsyncd</a> 上下载的<code>lsyncd-2.1.5.tar.gz</code>，直接<code>./configure、make &amp;&amp; make install</code>就可以了。</p>
<p>从github上下载<a href="https://github.com/axkibe/lsyncd/archive/master.zip" target="_blank" rel="external">lsyncd-master.zip</a> 的2.1.5版本使用的是 cmake 编译工具，无法<code>./configure</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uzip lsyncd-master.zip</span><br><span class="line"><span class="built_in">cd</span> lsyncd-master</span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span>/lsyncd-2.1.5</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></p>
<p>我这个版本编译时有个小bug，如果按照<code>INSTALL</code>在    <code>build</code>目录中make，会提示：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[100%] Generating doc/lsyncd.1</span><br><span class="line">Updating the manpage</span><br><span class="line">a2x: failed: <span class="built_in">source</span> file not found: doc/lsyncd.1.txt</span><br><span class="line">make[2]: *** [doc/lsyncd.1] Error 1</span><br><span class="line">make[1]: *** [CMakeFiles/manpage.dir/all] Error 2</span><br><span class="line">make: *** [all] Error 2</span><br></pre></td></tr></table></figure></p>
<p>解决办法是要么直接在解压目录下cmake，不要<code>mkdir build</code>，要么在<code>CMakeList.txt</code>中搜索doc字符串，在前面加上<code>${PROJECT_SOURCE_DIR}</code>。</p>
<a id="more"></a>
<h3 id="lsyncd-conf"><a href="#lsyncd-conf" class="headerlink" title="lsyncd.conf"></a>lsyncd.conf</h3><p>下面都是在编译安装的情况下操作。</p>
<h4 id="lsyncd同步配置"><a href="#lsyncd同步配置" class="headerlink" title="lsyncd同步配置"></a>lsyncd同步配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /usr/local/lsyncd-2.1.5</span></span><br><span class="line"><span class="comment"># mkdir etc var</span></span><br><span class="line"><span class="comment"># vi etc/lsyncd.conf</span></span><br><span class="line">settings &#123;</span><br><span class="line">    logfile      =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.log"</span>,</span><br><span class="line">    statusFile   =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.status"</span>,</span><br><span class="line">    inotifyMode  = <span class="string">"CloseWrite"</span>,</span><br><span class="line">    maxProcesses = 7,</span><br><span class="line">    -- nodaemon =<span class="literal">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest"</span>,</span><br><span class="line">    -- excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary    = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive   = <span class="literal">true</span>,</span><br><span class="line">        compress  = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>到这启动 lsycnd 就可以完成实时同步了，默认的许多参数可以满足绝大部分需求，非常简单。</p>
<h4 id="lsyncd-conf配置选项说明"><a href="#lsyncd-conf配置选项说明" class="headerlink" title="lsyncd.conf配置选项说明"></a>lsyncd.conf配置选项说明</h4><p><strong>settings</strong><br>里面是全局设置，<code>--</code>开头表示注释，下面是几个常用选项说明：</p>
<ul>
<li><code>logfile</code> 定义日志文件</li>
<li><code>stausFile</code> 定义状态文件</li>
<li><code>nodaemon=true</code> 表示不启用守护模式，默认</li>
<li><code>statusInterval</code> 将lsyncd的状态写入上面的statusFile的间隔，默认10秒</li>
<li><code>inotifyMode</code> 指定inotify监控的事件，默认是<code>CloseWrite</code>，还可以是<code>Modify</code>或<code>CloseWrite or Modify</code></li>
<li><code>maxProcesses</code> 同步进程的最大个数。假如同时有20个文件需要同步，而<code>maxProcesses = 8</code>，则最大能看到有8个rysnc进程</li>
<li><code>maxDelays</code> 累计到多少所监控的事件激活一次同步，即使后面的<code>delay</code>延迟时间还未到</li>
</ul>
<p><strong>sync</strong><br>里面是定义同步参数，可以继续使用<code>maxDelays</code>来重写settings的全局变量。一般第一个参数指定lsyncd以什么模式运行：<code>rsync</code>、<code>rsyncssh</code>、<code>direct</code>三种模式：</p>
<ul>
<li><p><code>default.rsync</code>：本地目录间同步，使用rsync，也可以达到使用ssh形式的远程rsync效果，或daemon方式连接远程rsyncd进程；<br><code>default.direct</code> ：本地目录间同步，使用<code>cp</code>、<code>rm</code>等命令完成差异文件备份；<br><code>default.rsyncssh</code> ：同步到远程主机目录，rsync的ssh模式，需要使用key来认证</p>
</li>
<li><p><code>source</code> 同步的源目录，使用绝对路径。</p>
</li>
<li><p><code>target</code> 定义目的地址.对应不同的模式有几种写法：<br><code>/tmp/dest</code> ：本地目录同步，可用于<code>direct</code>和<code>rsync</code>模式<br><code>172.29.88.223:/tmp/dest</code> ：同步到远程服务器目录，可用于<code>rsync</code>和<code>rsyncssh</code>模式，拼接的命令类似于<code>/usr/bin/rsync -ltsd --delete --include-from=- --exclude=* SOURCE TARGET</code>，剩下的就是rsync的内容了，比如指定username，免密码同步<br><code>172.29.88.223::module</code> ：同步到远程服务器目录，用于<code>rsync</code>模式<br>三种模式的示例会在后面给出。</p>
</li>
<li><p><code>init</code> 这是一个优化选项，当<code>init = false</code>，只同步进程启动以后发生改动事件的文件，原有的目录即使有差异也不会同步。默认是<code>true</code></p>
</li>
<li><p><code>delay</code> 累计事件，等待rsync同步延时时间，默认15秒（最大累计到1000个不可合并的事件）。也就是15s内监控目录下发生的改动，会累积到一次rsync同步，避免过于频繁的同步。（可合并的意思是，15s内两次修改了同一文件，最后只同步最新的文件）</p>
</li>
<li><p><code>excludeFrom</code> 排除选项，后面指定排除的列表文件，如<code>excludeFrom = &quot;/etc/lsyncd.exclude&quot;</code>，如果是简单的排除，可以使用<code>exclude = LIST</code>。<br>这里的排除规则写法与原生rsync有点不同，更为简单：</p>
<ul>
<li>监控路径里的任何部分匹配到一个文本，都会被排除，例如<code>/bin/foo/bar</code>可以匹配规则<code>foo</code></li>
<li>如果规则以斜线<code>/</code>开头，则从头开始要匹配全部</li>
<li>如果规则以<code>/</code>结尾，则要匹配监控路径的末尾</li>
<li><code>?</code>匹配任何字符，但不包括<code>/</code></li>
<li><code>*</code>匹配0或多个字符，但不包括<code>/</code></li>
<li><code>**</code>匹配0或多个字符，可以是<code>/</code></li>
</ul>
</li>
<li><p><code>delete</code> 为了保持target与souce完全同步，Lsyncd默认会<code>delete = true</code>来允许同步删除。它除了<code>false</code>，还有<code>startup</code>、<code>running</code>值，请参考 <a href="https://github.com/axkibe/lsyncd/wiki/Lsyncd%202.1.x%20%E2%80%96%20Layer%204%20Config%20%E2%80%96%20Default%20Behavior" target="_blank" rel="external">Lsyncd 2.1.x ‖ Layer 4 Config ‖ Default Behavior</a>。</p>
</li>
</ul>
<p><strong>rsync</strong><br>（提示一下，<code>delete</code>和<code>exclude</code>本来都是<strong>rsync</strong>的选项，上面是配置在sync中的，我想这样做的原因是为了减少rsync的开销）</p>
<ul>
<li><code>bwlimit</code> 限速，单位kb/s，与rsync相同（这么重要的选项在文档里竟然没有标出）</li>
<li><code>compress</code> 压缩传输默认为true。在带宽与cpu负载之间权衡，本地目录同步可以考虑把它设为<code>false</code></li>
<li><code>perms</code> 默认保留文件权限。</li>
<li>其它rsync的选项</li>
</ul>
<p>其它还有rsyncssh模式独有的配置项，如<code>host</code>、<code>targetdir</code>、<code>rsync_path</code>、<code>password_file</code>，见后文示例。<code>rsyncOps={&quot;-avz&quot;,&quot;--delete&quot;}</code>这样的写法在2.1.*版本已经不支持。</p>
<p><code>lsyncd.conf</code>可以有多个<code>sync</code>，各自的source，各自的target，各自的模式，互不影响。</p>
<h3 id="启动lsyncd"><a href="#启动lsyncd" class="headerlink" title="启动lsyncd"></a>启动lsyncd</h3><p>使用命令加载配置文件，启动守护进程，自动同步目录操作。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsyncd -log Exec /usr/<span class="built_in">local</span>/lsyncd-2.1.5/etc/lsyncd.conf</span><br></pre></td></tr></table></figure></p>
<h3 id="lsyncd-conf其它模式示例"><a href="#lsyncd-conf其它模式示例" class="headerlink" title="lsyncd.conf其它模式示例"></a>lsyncd.conf其它模式示例</h3><p>以下配置本人都已经过验证可行，必须根据实际需要裁剪配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">settings &#123;</span><br><span class="line">    logfile =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.log"</span>,</span><br><span class="line">    statusFile =<span class="string">"/usr/local/lsyncd-2.1.5/var/lsyncd.status"</span>,</span><br><span class="line">    inotifyMode = <span class="string">"CloseWrite"</span>,</span><br><span class="line">    maxProcesses = 8,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- I. 本地目录同步，direct：cp/rm/mv。 适用：500+万文件，变动不大</span><br><span class="line">sync &#123;</span><br><span class="line">    default.direct,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest"</span>,</span><br><span class="line">    delay = 1</span><br><span class="line">    maxProcesses = 1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- II. 本地目录同步，rsync模式：rsync</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"/tmp/dest1"</span>,</span><br><span class="line">    excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        bwlimit   = 2000</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- III. 远程目录同步，rsync模式 + rsyncd daemon</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"syncuser@172.29.88.223::module1"</span>,</span><br><span class="line">    delete=<span class="string">"running"</span>,</span><br><span class="line">    exclude = &#123; <span class="string">".*"</span>, <span class="string">".tmp"</span> &#125;,</span><br><span class="line">    delay = 30,</span><br><span class="line">    init = <span class="literal">false</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span>,</span><br><span class="line">        password_file = <span class="string">"/etc/rsyncd.d/rsync.pwd"</span>,</span><br><span class="line">        _extra    = &#123;<span class="string">"--bwlimit=200"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- IV. 远程目录同步，rsync模式 + ssh shell</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsync,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src"</span>,</span><br><span class="line">    target    = <span class="string">"172.29.88.223:/tmp/dest"</span>,</span><br><span class="line">    -- target    = <span class="string">"root@172.29.88.223:/remote/dest"</span>,</span><br><span class="line">    -- 上面target，注意如果是普通用户，必须拥有写权限</span><br><span class="line">    maxDelays = 5,</span><br><span class="line">    delay = 30,</span><br><span class="line">    -- init = <span class="literal">true</span>,</span><br><span class="line">    rsync     = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        bwlimit   = 2000</span><br><span class="line">        -- rsh = <span class="string">"/usr/bin/ssh -p 22 -o StrictHostKeyChecking=no"</span></span><br><span class="line">        -- 如果要指定其它端口，请用上面的rsh</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">-- V. 远程目录同步，rsync模式 + rsyncssh，效果与上面相同</span><br><span class="line">sync &#123;</span><br><span class="line">    default.rsyncssh,</span><br><span class="line">    <span class="built_in">source</span>    = <span class="string">"/tmp/src2"</span>,</span><br><span class="line">    host      = <span class="string">"172.29.88.223"</span>,</span><br><span class="line">    targetdir = <span class="string">"/remote/dir"</span>,</span><br><span class="line">    excludeFrom = <span class="string">"/etc/rsyncd.d/rsync_exclude.lst"</span>,</span><br><span class="line">    -- maxDelays = 5,</span><br><span class="line">    delay = 0,</span><br><span class="line">    -- init = <span class="literal">false</span>,</span><br><span class="line">    rsync    = &#123;</span><br><span class="line">        binary = <span class="string">"/usr/bin/rsync"</span>,</span><br><span class="line">        archive = <span class="literal">true</span>,</span><br><span class="line">        compress = <span class="literal">true</span>,</span><br><span class="line">        verbose   = <span class="literal">true</span>,</span><br><span class="line">        _extra = &#123;<span class="string">"--bwlimit=2000"</span>&#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">    ssh      = &#123;</span><br><span class="line">        port  =  1234</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的内容几乎涵盖了所有同步的模式，其中第III个要求像rsync一样配置rsyncd服务端，见本文开头。第IV、V配置ssh方式同步，达到的效果相同，但实际同步时你会发现每次同步都会提示输入ssh的密码，可以通过以下方法解决：</p>
<p>在远端被同步的服务器上开启ssh无密码登录，请注意用户身份：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user$ ssh-keygen -t rsa</span><br><span class="line">一路回车...</span><br><span class="line">user$ <span class="built_in">cd</span> ~/.ssh</span><br><span class="line">user$ cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>把<code>id_rsa</code>私钥拷贝到执行lsyncd的机器上<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user$ chmod 600 ~/.ssh/id_rsa</span><br><span class="line">测试能否无密码登录</span><br><span class="line">user$ ssh user@172.29.88.223</span><br></pre></td></tr></table></figure></p>
<h2 id="lsyncd的其它功能"><a href="#lsyncd的其它功能" class="headerlink" title="lsyncd的其它功能"></a>lsyncd的其它功能</h2><p><code>lsyncd</code>的功能不仅仅是同步，官方手册 <a href="https://github.com/axkibe/lsyncd/wiki/Lsyncd%202.1.x%20%E2%80%96%20Layer%202%20Config%20%E2%80%96%20Advanced%20onAction" target="_blank" rel="external">Lsyncd 2.1.x ‖ Layer 2 Config ‖ Advanced onAction</a> 高级功能提到，还可以监控某个目录下的文件，根据触发的事件自己定义要执行的命令，example是监控某个某个目录，只要是有jpg、gif、png格式的文件参数，就把它们转成pdf，然后同步到另一个目录。正好在我运维的一个项目中有这个需求，现在都是在java代码里转换，还容易出现异常，通过lsyncd可以代替这样的功能。但，门槛在于要会一点点lua语言（根据官方example还是可以写出来）。</p>
<p>另外偶然想到个问题，同时设置了<code>maxDelays</code>和<code>delay</code>，当监控目录一直没有文件变化了，也会发生同步操作，虽然没有可rsync的文件。</p>
<hr>
<p>来源：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL数据库开发规范]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-develop-standard.html</url>
      <content type="html"><![CDATA[<h2 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h2><h3 id="库名、表名、字段名必须使用小写字母，并采用下划线分割"><a href="#库名、表名、字段名必须使用小写字母，并采用下划线分割" class="headerlink" title="库名、表名、字段名必须使用小写字母，并采用下划线分割"></a>库名、表名、字段名必须使用小写字母，并采用下划线分割</h3><ul>
<li>MySQL有配置参数lower_case_table_names=1，即库表名以小写存储，大小写不敏感。如果是0，则库表名以实际情况存储，大小写敏感；如果是2，以实际情况存储，但以小写比较。</li>
<li>如果大小写混合使用，可能存在abc，Abc，ABC等多个表共存，容易导致混乱。</li>
<li>字段名显示区分大小写，但实际使⽤时不区分，即不可以建立两个名字一样但大小写不一样的字段。</li>
<li>为了统一规范， 库名、表名、字段名使用小写字母。</li>
</ul>
<h3 id="库名以-d-开头，表名以-t-开头，字段名以-f-开头"><a href="#库名以-d-开头，表名以-t-开头，字段名以-f-开头" class="headerlink" title="库名以 d 开头，表名以 t 开头，字段名以 f_ 开头"></a>库名以 d 开头，表名以 t 开头，字段名以 f_ 开头</h3><ul>
<li>比如表 <code>t_crm_relation</code>，中间的 crm 代表业务模块名</li>
<li>视图以<code>view_</code>开头，事件以<code>event_</code>开头，触发器以<code>trig_</code>开头，存储过程以<code>proc_</code>开头，函数以<code>func_</code>开头</li>
<li>普通索引以<code>idx_col1_col2</code>命名，唯一索引以<code>uk_col1_col2</code>命名（可去掉f_公共部分）。如 <code>idx_companyid_corpid_contacttime</code>(f_company_id, f_corp_id, f_contact_time)</li>
</ul>
<h3 id="库名、表名、字段名禁止超过32个字符，需见名知意"><a href="#库名、表名、字段名禁止超过32个字符，需见名知意" class="headerlink" title="库名、表名、字段名禁止超过32个字符，需见名知意"></a>库名、表名、字段名禁止超过32个字符，需见名知意</h3><p>库名、表名、字段名支持最多64个字符，但为了统一规范、易于辨识以及减少传输量，禁止超过32个字符</p>
<h3 id="临时库、表名须以tmp加日期为后缀"><a href="#临时库、表名须以tmp加日期为后缀" class="headerlink" title="临时库、表名须以tmp加日期为后缀"></a>临时库、表名须以tmp加日期为后缀</h3><p>如 t_crm_relation_tmp0425。备份表也类似，形如 <code>_bak20160425</code> 。</p>
<h3 id="按日期时间分表须符合-YYYY-MM-DD-格式"><a href="#按日期时间分表须符合-YYYY-MM-DD-格式" class="headerlink" title="按日期时间分表须符合_YYYY[MM][DD]格式"></a>按日期时间分表须符合_YYYY[MM][DD]格式</h3><p>这也是为将来有可能分表做准备的，比如<code>t_crm_ec_record_201403</code>，但像 t_crm_contact_at201506就打破了这种规范。<br>不具有时间特性的，直接以 <code>t_tbname_001</code> 这样的方式命名。</p>
<h2 id="库表基础规范"><a href="#库表基础规范" class="headerlink" title="库表基础规范"></a>库表基础规范</h2><h3 id="使用Innodb存储引擎"><a href="#使用Innodb存储引擎" class="headerlink" title="使用Innodb存储引擎"></a>使用Innodb存储引擎</h3><p>5.5版本开始mysql默认存储引擎就是InnoDB，5.7版本开始，系统表都放弃MyISAM了。</p>
<a id="more"></a>
<h3 id="表字符集统一使用UTF8"><a href="#表字符集统一使用UTF8" class="headerlink" title="表字符集统一使用UTF8"></a>表字符集统一使用UTF8</h3><ul>
<li>UTF8字符集存储汉字占用3个字节，存储英文字符占用一个字节</li>
<li>校对字符集使用默认的 utf8_general_ci</li>
<li>连接的客户端也使用utf8，建立连接时指定charset或SET NAMES UTF8;。（对于已经在项目中长期使用latin1的，救不了了）</li>
<li>如果遇到EMOJ等表情符号的存储需求，可申请使用UTF8MB4字符集</li>
</ul>
<h3 id="所有表都要添加注释"><a href="#所有表都要添加注释" class="headerlink" title="所有表都要添加注释"></a>所有表都要添加注释</h3><ul>
<li>尽量给字段也添加注释</li>
<li>类status型需指明主要值的含义，如”0-离线，1-在线”</li>
</ul>
<h3 id="控制单表字段数量"><a href="#控制单表字段数量" class="headerlink" title="控制单表字段数量"></a>控制单表字段数量</h3><ul>
<li>单表字段数上限30左右，再多的话考虑垂直分表，一是冷热数据分离，二是大字段分离，三是常在一起做条件和返回列的不分离。</li>
<li>表字段控制少而精，可以提高IO效率，内存缓存更多有效数据，从而提高响应速度和并发能力，后续 alter table 也更快。</li>
</ul>
<h3 id="所有表都必须要显式指定主键"><a href="#所有表都必须要显式指定主键" class="headerlink" title="所有表都必须要显式指定主键"></a>所有表都必须要显式指定主键</h3><ul>
<li>主键尽量采用自增方式，InnoDB表实际是一棵索引组织表，顺序存储可以提高存取效率，充分利用磁盘空间。还有对一些复杂查询可能需要自连接来优化时需要用到。</li>
<li>需要全局唯一主键时，使用外部发号器ticket server（建设中）</li>
<li>如果没有主键或唯一索引，update/delete是通过所有字段来定位操作的行，相当于每行就是一次全表扫描</li>
<li>少数情况可以使用联合唯一主键，需与DBA协商</li>
</ul>
<h3 id="不强制使用外键参考"><a href="#不强制使用外键参考" class="headerlink" title="不强制使用外键参考"></a>不强制使用外键参考</h3><p>即使2个表的字段有明确的外键参考关系，也不使用 FOREIGN KEY ，因为新纪录会去主键表做校验，影响性能。</p>
<h3 id="适度使用存储过程、视图，禁止使用触发器、事件"><a href="#适度使用存储过程、视图，禁止使用触发器、事件" class="headerlink" title="适度使用存储过程、视图，禁止使用触发器、事件"></a>适度使用存储过程、视图，禁止使用触发器、事件</h3><ul>
<li>存储过程（procedure）虽然可以简化业务端代码，在传统企业写复杂逻辑时可能会用到，而在互联网企业变更是很频繁的，在分库分表的情况下要升级一个存储过程相当麻烦。又因为它是不记录log的，所以也不方便debug性能问题。如果使用过程，一定考虑如果执行失败的情况。</li>
<li>使用视图一定程度上也是为了降低代码里SQL的复杂度，但有时候为了视图的通用性会损失性能（比如返回不必要的字段）。</li>
<li>触发器（trigger）也是同样，但也不应该通过它去约束数据的强一致性，mysql只支持“基于行的触发”，也就是说，触发器始终是针对一条记录的，而不是针对整个sql语句的，如果变更的数据集非常大的话，效率会很低。掩盖一条sql背后的工作，一旦出现问题将是灾难性的，但又很难快速分析和定位。再者需要ddl时无法使用pt-osc工具。放在transaction执行。</li>
<li>事件（event）也是一种偷懒的表现，目前已经遇到数次由于定时任务执行失败影响业务的情况，而且mysql无法对它做失败预警。建立专门的 job scheduler 平台。</li>
</ul>
<h3 id="单表数据量控制在5000w以内"><a href="#单表数据量控制在5000w以内" class="headerlink" title="单表数据量控制在5000w以内"></a>单表数据量控制在5000w以内</h3><h3 id="数据库中不允许存储明文密码"><a href="#数据库中不允许存储明文密码" class="headerlink" title="数据库中不允许存储明文密码"></a>数据库中不允许存储明文密码</h3><h2 id="字段规范"><a href="#字段规范" class="headerlink" title="字段规范"></a>字段规范</h2><h3 id="char、varchar、text等字符串类型定义"><a href="#char、varchar、text等字符串类型定义" class="headerlink" title="char、varchar、text等字符串类型定义"></a>char、varchar、text等字符串类型定义</h3><ul>
<li>对于长度基本固定的列，如果该列恰好更新又特别频繁，适合char</li>
<li>varchar虽然存储变长字符串，但不可太小也不可太大。UTF8最多能存21844个汉字，或65532个英文</li>
<li>varbinary(M)保存的是二进制字符串，它保存的是字节而不是字符，所以没有字符集的概念，M长度0-255（字节）。只用于排序或比较时大小写敏感的类型，不包括密码存储</li>
<li>TEXT类型与VARCHAR都类似，存储可变长度，最大限制也是2^16，但是它20bytes以后的内容是在数据页以外的空间存储（row_format=dynamic），对它的使用需要多一次寻址，没有默认值。<br>一般用于存放容量平均都很大、操作没有其它字段那样频繁的值。<br>网上部分文章说要避免使用text和blob，要知道如果纯用varchar可能会导致行溢出，效果差不多，但因为每行占用字节数过多，会导致buffer_pool能缓存的数据行、页下降。另外text和blob上面一般不会去建索引，而是利用sphinx之类的第三方全文搜索引擎，如果确实要创建（前缀）索引，那就会影响性能。凡事看具体场景。<br>另外尽可能把text/blob拆到另一个表中</li>
<li>BLOB可以看出varbinary的扩展版本，内容以二进制字符串存储，无字符集，区分大小写，有一种经常提但不用的场景：不要在数据库里存储图片。</li>
</ul>
<h3 id="int、tinyint、decimal等数字类型定义"><a href="#int、tinyint、decimal等数字类型定义" class="headerlink" title="int、tinyint、decimal等数字类型定义"></a>int、tinyint、decimal等数字类型定义</h3><ul>
<li>使用tinyint来代替 enum和boolean<br>ENUM类型在需要修改或增加枚举值时，需要在线DDL，成本较高；ENUM列值如果含有数字类型，可能会引起默认值混淆<br>tinyint使用1个字节，一般用于status,type,flag的列</li>
<li>建议使用 UNSIGNED 存储非负数值<br>相比不使用 unsigned，可以扩大一倍使用数值范围</li>
<li>int使用固定4个字节存储，int(11)与int(4)只是显示宽度的区别</li>
<li>使用Decimal 代替float/double存储精确浮点数<br>对于货币、金额这样的类型，使用decimal，如 decimal(9,2)。float默认只能能精确到6位有效数字</li>
</ul>
<h3 id="timestamp与datetime选择"><a href="#timestamp与datetime选择" class="headerlink" title="timestamp与datetime选择"></a>timestamp与datetime选择</h3><ul>
<li>datetime 和 timestamp类型所占的存储空间不同，前者8个字节，后者4个字节，这样造成的后果是两者能表示的时间范围不同。前者范围为1000-01-01 00:00:00 ~ 9999-12-31 23:59:59，后者范围为 1970-01-01 08:00:01 到 2038-01-19 11:14:07 。所以 TIMESTAMP 支持的范围比 DATATIME 要小。</li>
<li>timestamp可以在insert/update行时，自动更新时间字段（如 f_set_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP），但一个表只能有一个这样的定义。</li>
<li>timestamp显示与时区有关，内部总是以 UTC 毫秒 来存的。还受到严格模式的限制</li>
<li>优先使用timestamp，datetime也没问题</li>
<li>where条件里不要对时间列上使用时间函数</li>
</ul>
<h3 id="建议字段都定义为NOT-NULL"><a href="#建议字段都定义为NOT-NULL" class="headerlink" title="建议字段都定义为NOT NULL"></a>建议字段都定义为NOT NULL</h3><ul>
<li>如果是索引字段，一定要定义为not null 。因为null值会影响cordinate统计，影响优化器对索引的选择</li>
<li>如果不能保证insert时一定有值过来，定义时使用default ‘’ ，或 0</li>
</ul>
<h3 id="同一意义的字段定义必须相同"><a href="#同一意义的字段定义必须相同" class="headerlink" title="同一意义的字段定义必须相同"></a>同一意义的字段定义必须相同</h3><p>比如不同表中都有 f_user_id 字段，那么它的类型、字段长度要设计成一样</p>
<h2 id="索引规范"><a href="#索引规范" class="headerlink" title="索引规范"></a>索引规范</h2><h3 id="任何新的select-update-delete上线，都要先explain，看索引使用情况"><a href="#任何新的select-update-delete上线，都要先explain，看索引使用情况" class="headerlink" title="任何新的select,update,delete上线，都要先explain，看索引使用情况"></a>任何新的select,update,delete上线，都要先explain，看索引使用情况</h3><p>尽量避免extra列出现：Using File Sort，Using Temporary，rows超过1000的要谨慎上线。<br><strong><code>explain解读</code></strong></p>
<ul>
<li><code>type</code>：ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）</li>
<li><code>possible_keys</code>：指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用</li>
<li><code>key</code>：表示MySQL实际决定使用的键（索引）<br>如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX</li>
<li><code>ref</code>：表示选择 key 列上的索引，哪些列或常量被用于查找索引列上的值</li>
<li><code>rows</code>：根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数</li>
<li><code>Extra</code><ul>
<li><code>Using temporary</code>：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询</li>
<li><code>Using filesort</code>：MySQL中无法利用索引完成的排序操作称为“文件排序”</li>
</ul>
</li>
</ul>
<h3 id="索引个数限制"><a href="#索引个数限制" class="headerlink" title="索引个数限制"></a>索引个数限制</h3><ul>
<li>索引是双刃剑，会增加维护负担，增大IO压力，索引占用空间是成倍增加的</li>
<li>单张表的索引数量控制在5个以内，或不超过表字段个数的20%。若单张表多个字段在查询需求上都要单独用到索引，需要经过DBA评估。</li>
</ul>
<h3 id="避免冗余索引"><a href="#避免冗余索引" class="headerlink" title="避免冗余索引"></a>避免冗余索引</h3><ul>
<li>InnoDB表是一棵索引组织表，主键是和数据放在一起的聚集索引，普通索引最终指向的是主键地址，所以把主键做最后一列是多余的。如f_crm_id作为主键，联合索引(f_user_id,f_crm_id)上的f_crm_id就完全多余</li>
<li>(a,b,c)、(a,b)，后者为冗余索引。可以利用前缀索引来达到加速目的，减轻维护负担</li>
</ul>
<h3 id="没有特殊要求，使用自增id作为主键"><a href="#没有特殊要求，使用自增id作为主键" class="headerlink" title="没有特殊要求，使用自增id作为主键"></a>没有特殊要求，使用自增id作为主键</h3><ul>
<li>主键是一种聚集索引，顺序写入。组合唯一索引作为主键的话，是随机写入，适合写少读多的表</li>
<li>主键不允许更新</li>
</ul>
<h3 id="索引尽量建在选择性高的列上"><a href="#索引尽量建在选择性高的列上" class="headerlink" title="索引尽量建在选择性高的列上"></a>索引尽量建在选择性高的列上</h3><ul>
<li>不在低基数列上建立索引，例如性别、类型。但有一种情况，idx_feedbackid_type (f_feedback_id,f_type)，如果经常用 f_type=1 比较，而且能过滤掉90%行，那这个组合索引就值得创建。有时候同样的查询语句，由于条件取值不同导致使用不同的索引，也是这个道理。</li>
<li>索引选择性计算方法（基数 ÷ 数据行数）<br>Selectivity = Cardinality / Total Rows = select count(distinct col1)/count(*) from tbname，越接近1说明col1上使用索引的过滤效果越好</li>
<li>走索引扫描行数超过30%时，改全表扫描</li>
</ul>
<h3 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h3><ul>
<li>mysql使用联合索引时，从左向右匹配，遇到断开或者范围查询时，无法用到后续的索引列<br>比如索引idx_c1_c2_c3 (c1,c2,c3)，相当于创建了(c1)、(c1,c2)、(c1,c2,c3)三个索引，where条件包含上面三种情况的字段比较则可以用到索引，但像 where c1=a and c3=c 只能用到c1列的索引，像 c2=b and c3=c等情况就完全用不到这个索引</li>
<li>遇到范围查询(&gt;、&lt;、between、like)也会停止索引匹配，比如 c1=a and c2 &gt; 2 and c3=c，只有c1,c2列上的比较能用到索引，(c1,c2,c3)排列的索引才可能会都用上</li>
<li>where条件里面字段的顺序与索引顺序无关，mysql优化器会自动调整顺序</li>
</ul>
<h3 id="前缀索引"><a href="#前缀索引" class="headerlink" title="前缀索引"></a>前缀索引</h3><ul>
<li>对超过30个字符长度的列创建索引时，考虑使用前缀索引，如 idx_cs_guid2 (f_cs_guid(26))表示截取前26个字符做索引，既可以提高查找效率，也可以节省空间</li>
<li>前缀索引也有它的缺点是，如果在该列上 ORDER BY 或 GROUP BY 时无法使用索引，也不能把它们用作覆盖索引(Covering Index)</li>
<li>如果在varbinary或blob这种以二进制存储的列上建立前缀索引，要考虑字符集，括号里表示的是字节数</li>
</ul>
<h3 id="合理使用覆盖索引减少IO"><a href="#合理使用覆盖索引减少IO" class="headerlink" title="合理使用覆盖索引减少IO"></a>合理使用覆盖索引减少IO</h3><p>INNODB存储引擎中，secondary index(非主键索引，又称为辅助索引、二级索引)没有直接存储行地址，而是存储主键值。<br>如果用户需要查询secondary index中所不包含的数据列，则需要先通过secondary index查找到主键值，然后再通过主键查询到其他数据列，因此需要查询两次。覆盖索引则可以在一个索引中获取所有需要的数据列，从而避免回表进行二次查找，节省IO因此效率较高。<br>例如SELECT email，uid FROM user_email WHERE uid=xx，如果uid不是主键，适当时候可以将索引添加为index(uid，email)，以获得性能提升。</p>
<h3 id="尽量不要在频繁更新的列上创建索引"><a href="#尽量不要在频繁更新的列上创建索引" class="headerlink" title="尽量不要在频繁更新的列上创建索引"></a>尽量不要在频繁更新的列上创建索引</h3><p>如不在定义了 ON UPDATE CURRENT_STAMP 的列上创建索引，维护成本太高（好在mysql有insert buffer，会合并索引的插入）</p>
<h2 id="SQL设计"><a href="#SQL设计" class="headerlink" title="SQL设计"></a>SQL设计</h2><h3 id="杜绝直接-SELECT-读取全部字段"><a href="#杜绝直接-SELECT-读取全部字段" class="headerlink" title="杜绝直接 SELECT * 读取全部字段"></a>杜绝直接 SELECT * 读取全部字段</h3><p>即使需要所有字段，减少网络带宽消耗，能有效利用覆盖索引，表结构变更对程序基本无影响</p>
<h3 id="能确定返回结果只有一条时，使用-limit-1"><a href="#能确定返回结果只有一条时，使用-limit-1" class="headerlink" title="能确定返回结果只有一条时，使用 limit 1"></a>能确定返回结果只有一条时，使用 limit 1</h3><p><strong>在保证数据不会有误的前提下</strong>，能确定结果集数量时，多使用limit，尽快的返回结果。</p>
<h3 id="小心隐式类型转换"><a href="#小心隐式类型转换" class="headerlink" title="小心隐式类型转换"></a>小心隐式类型转换</h3><ul>
<li><p>转换规则</p>
<blockquote>
<p>a. 两个参数至少有一个是 NULL 时，比较的结果也是 NULL，例外是使用 &lt;=&gt; 对两个 NULL 做比较时会返回 1，这两种情况都不需要做类型转换<br>b. 两个参数都是字符串，会按照字符串来比较，不做类型转换<br>c. 两个参数都是整数，按照整数来比较，不做类型转换<br>d. 十六进制的值和非数字做比较时，会被当做二进制串<br>e. 有一个参数是 TIMESTAMP 或 DATETIME，并且另外一个参数是常量，常量会被转换为 timestamp<br>f. 有一个参数是 decimal 类型，如果另外一个参数是 decimal 或者整数，会将整数转换为 decimal 后进行比较，如果另外一个参数是浮点数，则会把 decimal 转换为浮点数进行比较<br>g. 所有其他情况下，两个参数都会被转换为浮点数再进行比较。</p>
</blockquote>
</li>
<li><p>如果一个索引建立在string类型上，如果这个字段和一个int类型的值比较，符合第 g 条。如f_phone定义的类型是varchar，但where使用f_phone in (098890)，两个参数都会被当成成浮点型。发生这个隐式转换并不是最糟的，最糟的是string转换后的float，mysql无法使用索引，这才导致了性能问题。如果是 f_user_id = ‘1234567’ 的情况，符合第 b 条,直接把数字当字符串比较。</p>
</li>
</ul>
<h3 id="禁止在where条件列上使用函数"><a href="#禁止在where条件列上使用函数" class="headerlink" title="禁止在where条件列上使用函数"></a>禁止在where条件列上使用函数</h3><ul>
<li>会导致索引失效，如lower(email)，f_qq % 4。可放到右边的常量上计算</li>
<li>返回小结果集不是很大的情况下，可以对返回列使用函数，简化程序开发</li>
</ul>
<h3 id="使用like模糊匹配，-不要放首位"><a href="#使用like模糊匹配，-不要放首位" class="headerlink" title="使用like模糊匹配，%不要放首位"></a>使用like模糊匹配，%不要放首位</h3><p>会导致索引失效，有这种搜索需求是，考虑其它方案，如sphinx全文搜索</p>
<h3 id="涉及到复杂sql时，务必先参考已有索引设计，先explain"><a href="#涉及到复杂sql时，务必先参考已有索引设计，先explain" class="headerlink" title="涉及到复杂sql时，务必先参考已有索引设计，先explain"></a>涉及到复杂sql时，务必先参考已有索引设计，先explain</h3><ul>
<li>简单SQL拆分，不以代码处理复杂为由。</li>
<li>比如 OR 条件： f_phone=’10000’ or f_mobile=’10000’，两个字段各自有索引，但只能用到其中一个。可以拆分成2个sql，或者union all。</li>
<li>先explain的好处是可以为了利用索引，增加更多查询限制条件</li>
</ul>
<h3 id="使用join时，where条件尽量使用充分利用同一表上的索引"><a href="#使用join时，where条件尽量使用充分利用同一表上的索引" class="headerlink" title="使用join时，where条件尽量使用充分利用同一表上的索引"></a>使用join时，where条件尽量使用充分利用同一表上的索引</h3><ul>
<li>如 select t1.a,t2.b * from t1,t2 and t1.a=t2.a and t1.b=123 and t2.c= 4 ，如果t1.c与t2.c字段相同，那么t1上的索引(b,c)就只用到b了。此时如果把where条件中的t2.c=4改成t1.c=4，那么可以用到完整的索引</li>
<li>这种情况可能会在字段冗余设计（反范式）时出现</li>
<li>正确选取inner join和left join</li>
</ul>
<h3 id="少用子查询，改用join"><a href="#少用子查询，改用join" class="headerlink" title="少用子查询，改用join"></a>少用子查询，改用join</h3><p>小于5.6版本时，子查询效率很低，不像Oracle那样先计算子查询后外层查询。5.6版本开始得到优化</p>
<h3 id="考虑使用union-all，少使用union，注意考虑去重"><a href="#考虑使用union-all，少使用union，注意考虑去重" class="headerlink" title="考虑使用union all，少使用union，注意考虑去重"></a>考虑使用union all，少使用union，注意考虑去重</h3><ul>
<li>union all不去重，而少了排序操作，速度相对比union要快，如果没有去重的需求，优先使用union all</li>
<li>如果UNION结果中有使用limit，在2个子SQL可能有许多返回值的情况下，各自加上limit。如果还有order by，请找DBA。</li>
</ul>
<h3 id="IN的内容尽量不超过200个"><a href="#IN的内容尽量不超过200个" class="headerlink" title="IN的内容尽量不超过200个"></a>IN的内容尽量不超过200个</h3><p>超过500个值使用批量的方式，否则一次执行会影响数据库的并发能力，因为单SQL只能且一直占用单CPU，而且可能导致主从复制延迟</p>
<h3 id="拒绝大事务"><a href="#拒绝大事务" class="headerlink" title="拒绝大事务"></a>拒绝大事务</h3><p>比如在一个事务里进行多个select，多个update，如果是高频事务，会严重影响MySQL并发能力，因为事务持有的锁等资源只在事务rollback/commit时才能释放。但同时也要权衡数据写入的一致性。</p>
<h3 id="避免使用is-null-is-not-null这样的比较"><a href="#避免使用is-null-is-not-null这样的比较" class="headerlink" title="避免使用is null, is not null这样的比较"></a>避免使用is null, is not null这样的比较</h3><h3 id="order-by-limit"><a href="#order-by-limit" class="headerlink" title="order by .. limit"></a>order by .. limit</h3><p>这种查询更多的是通过索引去优化，但order by的字段有讲究，比如主键id与f_time都是顺序递增，那就可以考虑order by id而非 f_time 。</p>
<h3 id="c1-lt-a-order-by-c2"><a href="#c1-lt-a-order-by-c2" class="headerlink" title="c1 &lt; a order by c2"></a>c1 &lt; a order by c2</h3><p>与上面不同的是，order by之前有个范围查询，由前面的内容可知，用不到类似(c1,c2)的索引，但是可以利用(c2,c1)索引。另外还可以改写成join的方式实现。</p>
<h3 id="分页优化"><a href="#分页优化" class="headerlink" title="分页优化"></a>分页优化</h3><p>建议使用合理的分页方式以提高分页效率，大页情况下不使用跳跃式分页<br>假如有类似下面分页语句:<br>SELECT FROM table1 ORDER BY ftime DESC LIMIT 10000,10;<br>这种分页方式会导致大量的io，因为MySQL使用的是提前读取策略。<br>推荐分页方式：<br><code>SELECT FROM table1 WHERE ftime &lt; last_time ORDER BY ftime DESC LIMIT 10</code><br>即传入上一次分页的界值</p>
<p>SELECT * FROM table as t1 inner JOIN (SELECT id FROM table ORDER BY time LIMIT 10000，10) as t2 ON t1.id=t2.id</p>
<h3 id="count计数"><a href="#count计数" class="headerlink" title="count计数"></a>count计数</h3><ul>
<li>首先count()、count(1)、count(col1)是有区别的，count()表示整个结果集有多少条记录，count(1)表示结果集里以primary key统计数量，绝大多数情况下count()与count(1)效果一样的，但count(col1)表示的是结果集里 col1 列 NOT null 的记录数。优先采用count()</li>
<li>大数据量count是消耗资源的操作，甚至会拖慢整个库，查询性能问题无法解决的，应从产品设计上进行重构。例如当频繁需要count的查询，考虑使用汇总表</li>
<li>遇到distinct的情况，group by方式可能效率更高。</li>
</ul>
<h3 id="delete-update语句改成select再explain"><a href="#delete-update语句改成select再explain" class="headerlink" title="delete,update语句改成select再explain"></a>delete,update语句改成select再explain</h3><p>select最多导致数据库慢，写操作才是锁表的罪魁祸首</p>
<h3 id="减少与数据库交互的次数，尽量采用批量SQL语句"><a href="#减少与数据库交互的次数，尽量采用批量SQL语句" class="headerlink" title="减少与数据库交互的次数，尽量采用批量SQL语句"></a>减少与数据库交互的次数，尽量采用批量SQL语句</h3><ul>
<li><code>INSERT ... ON DUPLICATE KEY UPDATE ...</code>，插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行旧行UPDATE，如果不重复则直接插入，影响1行。</li>
<li><code>REPLACE INTO</code>类似，但它是冲突时删除旧行。<code>INSERT IGNORE</code>相反，保留旧行，丢弃要插入的新行。</li>
<li>INSERT INTO VALUES(),(),()，合并插入。</li>
</ul>
<h3 id="杜绝危险SQL"><a href="#杜绝危险SQL" class="headerlink" title="杜绝危险SQL"></a>杜绝危险SQL</h3><ul>
<li>去掉where 1=1 这样无意义或恒真的条件，如果遇到update/delete或遭到sql注入就恐怖了</li>
<li>SQL中不允许出现DDL语句。一般也不给予create/alter这类权限，但阿里云RDS只区分读写用户</li>
</ul>
<h2 id="行为规范"><a href="#行为规范" class="headerlink" title="行为规范"></a>行为规范</h2><ul>
<li>不允许在DBA不知情的情况下导现网数据</li>
<li>大批量更新，如修复数据，避开高峰期，并通知DBA。直接执行sql的由运维或DBA同事操作</li>
<li>及时处理已下线业务的SQL</li>
<li>复杂sql上线审核</li>
<li>重要项目的数据库方案选型和设计必须提前通知DBA参与</li>
</ul>
<hr>
<p>原文地址：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Advanced MySQL Query Tuning]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-query.html</url>
      <content type="html"><![CDATA[<iframe src="//www.slideshare.net/slideshow/embed_code/key/3HLJJcJmM9KLGT" width="100%" height="550" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" allowfullscreen> </iframe>

<p>Youtube: <a href="https://www.youtube.com/watch?v=TPFibi2G_oo" target="_blank" rel="external">https://www.youtube.com/watch?v=TPFibi2G_oo</a></p>
<p>Percona webinars上有许多类似的分享，传送门： <a href="https://www.percona.com/resources/webinars" target="_blank" rel="external">https://www.percona.com/resources/webinars</a> 。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[nginx配置location与rewrite规则教程]]></title>
      <url>http://team.jiunile.com/blog/2016/07/nginx-localtion-rewrite.html</url>
      <content type="html"><![CDATA[<h2 id="location教程"><a href="#location教程" class="headerlink" title="location教程"></a>location教程</h2><p><strong>示例：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">location  = / &#123;</span><br><span class="line">    # 精确匹配 / ，主机名后面不能带任何字符串</span><br><span class="line">    [ configuration A ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location  / &#123;</span><br><span class="line">    # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求</span><br><span class="line">    # 但是正则和最长字符串会优先匹配</span><br><span class="line">    [ configuration B ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /documents/ &#123;</span><br><span class="line">    # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">    # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">    [ configuration C ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ /documents/Abc &#123;</span><br><span class="line">    # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">    # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">    [ configuration CC ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ^~ /images/ &#123;</span><br><span class="line">    # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。</span><br><span class="line">    [ configuration D ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~* \.(gif|jpg|jpeg)$ &#123;</span><br><span class="line">    # 匹配所有以 gif,jpg或jpeg 结尾的请求</span><br><span class="line">    # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则</span><br><span class="line">    [ configuration E ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /images/ &#123;</span><br><span class="line">    # 字符匹配到 /images/，继续往下，会发现 ^~ 存在</span><br><span class="line">    [ configuration F ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location /images/abc &#123;</span><br><span class="line">    # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在</span><br><span class="line">    # F与G的放置顺序是没有关系的</span><br><span class="line">    [ configuration G ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ /images/abc/ &#123;</span><br><span class="line">    # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用</span><br><span class="line">    [ configuration H ] </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~* /js/.*/\.js</span><br></pre></td></tr></table></figure></p>
<ul>
<li>已=开头表示精确匹配<br>如 A 中只匹配根目录结尾的请求，后面不能带任何字符串。</li>
<li>^~ 开头表示uri以某个常规字符串开头，不是正则匹配</li>
<li>~ 开头表示区分大小写的正则匹配</li>
<li>~* 开头表示不区分大小写的正则匹配</li>
<li>/ 通用匹配, 如果没有其它匹配,任何请求都会匹配到</li>
</ul>
<a id="more"></a>
<p><strong><code>顺序&amp;&amp;优先级</code></strong></p>
<blockquote>
<p>(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/)</p>
</blockquote>
<p>按照上面的location写法，以下的匹配示例成立：</p>
<ul>
<li><p>/ —&gt; config A</p>
<blockquote>
<p>精确完全匹配，即使/index.html也匹配不了</p>
</blockquote>
</li>
<li><p>/downloads/download.html —&gt; config B</p>
<blockquote>
<p>匹配B以后，往下没有任何匹配，采用B</p>
</blockquote>
</li>
<li><p>/images/1.gif —&gt; configuration D</p>
<blockquote>
<p>匹配到F，往下匹配到D，停止往下</p>
</blockquote>
</li>
<li><p>/images/abc/def —&gt; config D</p>
<blockquote>
<p>最长匹配到G，往下匹配D，停止往下<br>你可以看到 任何以/images/开头的都会匹配到D并停止，FG写在这里是没有任何意义的，H是永远轮不到的，这里只是为了说明匹配顺序</p>
</blockquote>
</li>
<li><p>/documents/document.html —&gt; config C</p>
<blockquote>
<p>匹配到C，往下没有任何匹配，采用C</p>
</blockquote>
</li>
<li><p>/documents/1.jpg —&gt; configuration E</p>
<blockquote>
<p>匹配到C，往下正则匹配到E</p>
</blockquote>
</li>
<li><p>/documents/Abc.jpg —&gt; config CC</p>
<blockquote>
<p>最长匹配到C，往下正则顺序匹配到CC，不会往下到E</p>
</blockquote>
</li>
</ul>
<h3 id="实际使用建议"><a href="#实际使用建议" class="headerlink" title="实际使用建议"></a>实际使用建议</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">所以实际使用中，个人觉得至少有三个匹配规则定义，如下：</span><br><span class="line">#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。</span><br><span class="line">#这里是直接转发给后端应用服务器了，也可以是一个静态首页</span><br><span class="line"># 第一个必选规则</span><br><span class="line">location = / &#123;</span><br><span class="line">    proxy_pass http://tomcat:8080/index</span><br><span class="line">&#125;</span><br><span class="line"># 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项</span><br><span class="line"># 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用</span><br><span class="line">location ^~ /static/ &#123;</span><br><span class="line">    root /webroot/static/;</span><br><span class="line">&#125;</span><br><span class="line">location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ &#123;</span><br><span class="line">    root /webroot/res/;</span><br><span class="line">&#125;</span><br><span class="line">#第三个规则就是通用规则，用来转发动态请求到后端应用服务器</span><br><span class="line">#非静态文件请求就默认是动态请求，自己根据实际把握</span><br><span class="line">#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了</span><br><span class="line">location / &#123;</span><br><span class="line">    proxy_pass http://tomcat:8080/</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Rewrite教程"><a href="#Rewrite教程" class="headerlink" title="Rewrite教程"></a>Rewrite教程</h2><p>rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在<code>server{},location{},if{}</code>中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 <code>http://seanlook.com/a/we/index.php?id=1&amp;u=str</code> 只对<code>/a/we/index.php</code>重写。语法<code>rewrite regex replacement [flag];</code></p>
<p>如果相对域名或参数字符串起作用，可以使用全局变量匹配，也可以使用proxy_pass反向代理。</p>
<p>表明看rewrite和location功能有点像，都能实现跳转，主要区别在于rewrite是在同一域名内更改获取资源的路径，而location是对一类路径做控制访问或反向代理，可以proxy_pass到其他机器。很多情况下rewrite也会写在location里，它们的执行顺序是：</p>
<ol>
<li>执行server块的rewrite指令</li>
<li>执行location匹配</li>
<li>执行选定的location中的rewrite指令</li>
</ol>
<p>如果其中某步URI被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 Internal Server Error错误。</p>
<h3 id="flag标志位"><a href="#flag标志位" class="headerlink" title="flag标志位"></a>flag标志位</h3><ul>
<li><code>last</code> : 相当于Apache的[L]标记，表示完成rewrite</li>
<li><code>break</code>: 停止执行当前虚拟主机的后续rewrite指令集</li>
<li><code>redirect</code> : 返回302临时重定向，地址栏会显示跳转后的地址</li>
<li><code>permanent</code> : 返回301永久重定向，地址栏会显示跳转后的地址</li>
</ul>
<p>因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解：</p>
<ol>
<li>last一般写在server和if中，而break一般使用在location中</li>
<li>last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配</li>
<li>break和last都能组织继续执行后面的rewrite指令</li>
</ol>
<h3 id="if指令与全局变量"><a href="#if指令与全局变量" class="headerlink" title="if指令与全局变量"></a>if指令与全局变量</h3><p><strong>if判断指令</strong><br>语法为<code>if(condition){...}</code>，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：</p>
<ul>
<li>当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false</li>
<li>直接比较变量和内容时，使用<code>=</code>或<code>!=</code></li>
<li><code>~</code>正则表达式匹配，<code>~*</code>不区分大小写的匹配，<code>!~</code>区分大小写的不匹配</li>
<li><code>-f</code>和<code>!-f</code>用来判断是否存在文件</li>
<li><code>-d</code>和<code>!-d</code>用来判断是否存在目录</li>
<li><code>-e</code>和<code>!-e</code>用来判断是否存在文件或目录</li>
<li><code>-x</code>和<code>!-x</code>用来判断文件是否可执行</li>
</ul>
<p><strong>例如：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">if ($http_user_agent ~ MSIE) &#123;</span><br><span class="line">    rewrite ^(.*)$ /msie/$1 break;</span><br><span class="line">&#125; #如果UA包含"MSIE"，rewrite请求到/msid/目录下</span><br><span class="line"></span><br><span class="line">if ($http_cookie ~* "id=([^;]+)(?:;|$)") &#123;</span><br><span class="line">    set $id $1;</span><br><span class="line"> &#125; #如果cookie匹配正则，设置变量$id等于正则引用部分</span><br><span class="line"></span><br><span class="line">if ($request_method = POST) &#123;</span><br><span class="line">    return 405;</span><br><span class="line">&#125; #如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302</span><br><span class="line"></span><br><span class="line">if ($slow) &#123;</span><br><span class="line">    limit_rate 10k;</span><br><span class="line">&#125; #限速，$slow可以通过 set 指令设置</span><br><span class="line"></span><br><span class="line">if (!-f $request_filename)&#123;</span><br><span class="line">    break;</span><br><span class="line">    proxy_pass  http://127.0.0.1; </span><br><span class="line">&#125; #如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查</span><br><span class="line"></span><br><span class="line">if ($args ~ post=140)&#123;</span><br><span class="line">    rewrite ^ http://example.com/ permanent;</span><br><span class="line">&#125; #如果query string中包含"post=140"，永久重定向到example.com</span><br><span class="line"></span><br><span class="line">location ~* \.(gif|jpg|png|swf|flv)$ &#123;</span><br><span class="line">    valid_referers none blocked www.jefflei.com www.leizhenfang.com;</span><br><span class="line">    if ($invalid_referer) &#123;</span><br><span class="line">        return 404;</span><br><span class="line">    &#125; #防盗链</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>全局变量</strong><br>下面是可以用作if判断的全局变量</p>
<ul>
<li><code>$args</code>： #这个变量等于请求行中的参数，同<code>$query_string</code></li>
<li><code>$content_length</code> ： 请求头中的Content-length字段。</li>
<li><code>$content_type</code> ： 请求头中的Content-Type字段。</li>
<li><code>$document_root</code> ： 当前请求在root指令中指定的值。</li>
<li><code>$host</code> ： 请求主机头字段，否则为服务器名称。</li>
<li><code>$http_user_agent</code> ： 客户端agent信息</li>
<li><code>$http_cookie</code> ： 客户端cookie信息</li>
<li><code>$limit_rate</code> ： 这个变量可以限制连接速率。</li>
<li><code>$request_method</code> ： 客户端请求的动作，通常为GET或POST。</li>
<li><code>$remote_addr</code> ： 客户端的IP地址。</li>
<li><code>$remote_port</code> ： 客户端的端口。</li>
<li><code>$remote_user</code> ： 已经经过Auth Basic Module验证的用户名。</li>
<li><code>$request_filename</code> ： 当前请求的文件路径，由root或alias指令与URI请求生成。</li>
<li><code>$scheme</code> ： HTTP方法（如http，https）。</li>
<li><code>$server_protocol</code> ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。</li>
<li><code>$server_addr</code> ： 服务器地址，在完成一次系统调用后可以确定这个值。</li>
<li><code>$server_name</code> ： 服务器名称。</li>
<li><code>$server_port</code> ： 请求到达服务器的端口号。</li>
<li><code>$request_uri</code> ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。</li>
<li><code>$uri</code> ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。</li>
<li><code>$document_uri</code> ： 与$uri相同。</li>
</ul>
<p>示例：<code>http://localhost:88/test1/test2/test.php</code></p>
<ul>
<li><code>$host</code>：localhost</li>
<li><code>$server_port</code>：88</li>
<li><code>$request_uri</code>：<a href="http://localhost:88/test1/test2/test.php" target="_blank" rel="external">http://localhost:88/test1/test2/test.php</a></li>
<li><code>$document_uri</code>：/test1/test2/test.php</li>
<li><code>$document_root</code>：/var/www/html</li>
<li><code>$request_filename</code>：/var/www/html/test1/test2/test.php</li>
</ul>
<h3 id="常用正则"><a href="#常用正则" class="headerlink" title="常用正则"></a>常用正则</h3><ul>
<li><code>.</code> ： 匹配除换行符以外的任意字符</li>
<li><code>?</code> ： 重复0次或1次</li>
<li><code>+</code> ： 重复1次或更多次</li>
<li><code>*</code> ： 重复0次或更多次</li>
<li><code>\d</code> ：匹配数字</li>
<li><code>^</code> ： 匹配字符串的开始</li>
<li><code>$</code> ： 匹配字符串的介绍</li>
<li><code>{n}</code> ： 重复n次</li>
<li><code>{n,}</code> ： 重复n次或更多次</li>
<li><code>[c]</code> ： 匹配单个字符c</li>
<li><code>[a-z]</code> ： 匹配a-z小写字母的任意一个</li>
</ul>
<p>小括号<code>()</code>之间匹配的内容，可以在后面通过<code>$1</code>来引用，<code>$2</code>表示的是前面第二个<code>()</code>里的内容。正则里面容易让人困惑的是<code>\</code>转义特殊字符。</p>
<h3 id="rewrite实例"><a href="#rewrite实例" class="headerlink" title="rewrite实例"></a>rewrite实例</h3><p><strong>例1：</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="comment"># 定义image日志格式</span></span><br><span class="line">    <span class="attribute">log_format</span> imagelog <span class="string">'[<span class="variable">$time_local</span>] '</span> <span class="variable">$image_file</span> <span class="string">' '</span> <span class="variable">$image_type</span> <span class="string">' '</span> <span class="variable">$body_bytes_sent</span> <span class="string">' '</span> <span class="variable">$status</span>;</span><br><span class="line">    <span class="comment"># 开启重写日志</span></span><br><span class="line">    <span class="attribute">rewrite_log</span> <span class="literal">on</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">root</span> /home/www;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">location</span> / &#123;</span><br><span class="line">                <span class="comment"># 重写规则信息</span></span><br><span class="line">                <span class="attribute">error_log</span> logs/rewrite.log <span class="literal">notice</span>; </span><br><span class="line">                <span class="comment"># 注意这里要用‘’单引号引起来，避免&#123;&#125;</span></span><br><span class="line">                <span class="attribute">rewrite</span> <span class="string">'^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$'</span> /data?file=<span class="variable">$3</span>.<span class="variable">$4</span>;</span><br><span class="line">                <span class="comment"># 注意不能在上面这条规则后面加上“last”参数，否则下面的set指令不会执行</span></span><br><span class="line">                <span class="attribute">set</span> <span class="variable">$image_file</span> <span class="variable">$3</span>;</span><br><span class="line">                <span class="attribute">set</span> <span class="variable">$image_type</span> <span class="variable">$4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">location</span> /data &#123;</span><br><span class="line">                <span class="comment"># 指定针对图片的日志格式，来分析图片类型和大小</span></span><br><span class="line">                <span class="attribute">access_log</span> logs/images.log mian;</span><br><span class="line">                <span class="attribute">root</span> /data/images;</span><br><span class="line">                <span class="comment"># 应用前面定义的变量。判断首先文件在不在，不在再判断目录在不在，如果还不在就跳转到最后一个url里</span></span><br><span class="line">                <span class="attribute">try_files</span> /<span class="variable">$arg_file</span> /image404.html;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="attribute">location</span> = /image404.html &#123;</span><br><span class="line">                <span class="comment"># 图片不存在返回特定的信息</span></span><br><span class="line">                <span class="attribute">return</span> <span class="number">404</span> <span class="string">"image not found\n"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对形如<code>/images/ef/uh7b3/test.png</code>的请求，重写到<code>/data?file=test.png</code>，于是匹配到<code>location /data</code>，先看<code>/data/images/test.png</code>文件存不存在，如果存在则正常响应，如果不存在则重写<code>tryfiles</code>到新的<code>image404 location</code>，直接返回404状态码。</p>
<p><strong>例2：</strong><br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rewrite</span><span class="regexp"> ^/images/(.*)_(\d+)x(\d+)\.(png|jpg|gif)$</span> /resizer/<span class="variable">$1</span>.<span class="variable">$4</span>?width=<span class="variable">$2</span>&amp;height=<span class="variable">$3</span>? <span class="literal">last</span>;</span><br></pre></td></tr></table></figure></p>
<p>对形如<code>/images/bla_500x400.jpg</code>的文件请求，重写到<code>/resizer/bla.jpg?width=500&amp;height=400</code>地址，并会继续尝试匹配location。</p>
<hr>
<p>来源：<a href="http://seanlook.com/" target="_blank" rel="external">http://seanlook.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL事务隔离级别]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-transaction-level.html</url>
      <content type="html"><![CDATA[<h2 id="四类隔离级别"><a href="#四类隔离级别" class="headerlink" title="四类隔离级别"></a>四类隔离级别</h2><p>SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。</p>
<ul>
<li>Read Uncommitted（读取未提交内容）</li>
</ul>
<p>在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 </p>
<ul>
<li>Read Committed（读取提交内容）</li>
</ul>
<p>这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。</p>
<ul>
<li>Repeatable Read（可重读）</li>
</ul>
<p>这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。</p>
<ul>
<li>Serializable（可串行化）</li>
</ul>
<p>这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。<br><a id="more"></a></p>
<h2 id="隔离级别与一致性"><a href="#隔离级别与一致性" class="headerlink" title="隔离级别与一致性"></a>隔离级别与一致性</h2><p>这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：</p>
<ul>
<li>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</li>
<li>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</li>
<li>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</li>
</ul>
<p>在MySQL中，实现了这四种隔离级别，分别有可能产生问题如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:left">隔离级别</th>
<th style="text-align:left">脏读</th>
<th style="text-align:left">不可重复读</th>
<th style="text-align:left">幻读</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读未提交(Read Uncommitted)</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">读已提交(Read Committed)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">yes</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">可重复读(Repeatable Read)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
<td style="text-align:left">yes</td>
</tr>
<tr>
<td style="text-align:left">可串行化(Serializable)</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
<td style="text-align:left">no</td>
</tr>
</tbody>
</table>
<h2 id="设置当前隔离级别"><a href="#设置当前隔离级别" class="headerlink" title="设置当前隔离级别"></a>设置当前隔离级别</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 取消autocommit</span></span><br><span class="line"><span class="keyword">set</span> autocommit=<span class="number">0</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">"%autocommit%"</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 查看隔离级别</span></span><br><span class="line"><span class="keyword">SELECT</span> @@global.tx_isolation;</span><br><span class="line"><span class="keyword">SELECT</span> @@session.tx_isolation;</span><br><span class="line"><span class="keyword">SELECT</span> @@tx_isolation;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%iso%'</span>;</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| Variable_name | Value           |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| tx_isolation  | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> <span class="keyword">global</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%iso%'</span>;</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| Variable_name | Value           |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line">| tx_isolation  | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">---------------+-----------------+</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 设置隔离级别</span></span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">read</span> uncommitted;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">read</span> committed;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> repeatable <span class="keyword">read</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> <span class="keyword">TRANSACTION</span> <span class="keyword">ISOLATION</span> <span class="keyword">LEVEL</span> <span class="keyword">serializable</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 事务操作</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> text.tx;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> text.tx;</span><br><span class="line"><span class="keyword">update</span> text.tx <span class="keyword">set</span> <span class="keyword">num</span> =<span class="number">10</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> text.tx(<span class="keyword">id</span>,<span class="keyword">num</span>) <span class="keyword">values</span>(<span class="number">9</span>,<span class="number">9</span>);</span><br><span class="line"><span class="keyword">rollback</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<h2 id="my-cnf设置"><a href="#my-cnf设置" class="headerlink" title="my.cnf设置"></a>my.cnf设置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MySQL支持4种事务隔离级别，他们分别是：</span></span><br><span class="line"><span class="comment"># READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE.</span></span><br><span class="line"><span class="comment"># 如没有指定，MySQL默认采用的是REPEATABLE-READ，ORACLE默认的是READ-COMMITTED</span></span><br><span class="line">transaction_isolation = REPEATABLE-READ</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MySQL为什么需要一个自增主键]]></title>
      <url>http://team.jiunile.com/blog/2016/07/mysql-mysql-auto-increment-primary-key.html</url>
      <content type="html"><![CDATA[<h2 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h2><p>表中每一行都应该有可以唯一标识自己的一列（或一组列）。</p>
<p>一个顾客可以使用顾客编号列，而订单可以使用订单ID，雇员可以使用雇员ID 或 雇员社会保险号。</p>
<p>主键（primary key） 一列（或一组列），其值能够唯一区分表中的每个行。<br>唯一标识表中每行的这个列（或这组列）称为主键。<strong><code>没有主键，更新或删除表中特定行很困难，因为没有安全的方法保证只涉及相关的行。</code></strong></p>
<p>虽然并不总是都需要主键，但大多数数据库设计人员都应保证他们创建的每个表有一个主键，以便于以后数据操纵和管理</p>
<p>表中的任何列都可以作为主键，只要它满足一下条件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">任何两行都不具有相同的主键值</span><br><span class="line">每个行都必须具有一个主键值（主键列不允许NULL值）</span><br></pre></td></tr></table></figure></p>
<p>主键值规范：这里列出的规则是MySQL本身强制实施的。</p>
<p>主键的最好习惯：<br>除MySQL强制实施的规则外，应该坚持的几个普遍认为的最好习惯为:</p>
<pre><code class="plain">1、不更新主键列的值
2、不重用主键列的值
3、不在主键列中使用可能会更改的值（例如，如果使用一个名字作为主键以标识某个供应商，应该供应商合并和更改其名字时，必须更改这个主键）
</code></pre>
<p>总之：不应该使用一个具有意义的column（id 本身并不保存表 有意义信息） 作为主键，并且一个表必须要有一个主键，为方便扩展、松耦合，高可用的系统做铺垫。<br><a id="more"></a></p>
<h3 id="无特殊需求下Innodb建议使用与业务无关的自增ID作为主键"><a href="#无特殊需求下Innodb建议使用与业务无关的自增ID作为主键" class="headerlink" title="无特殊需求下Innodb建议使用与业务无关的自增ID作为主键"></a>无特殊需求下Innodb建议使用与业务无关的自增ID作为主键</h3><p>InnoDB引擎使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）</p>
<p>1、如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示：<br><img src="/images/mysql_aipk_1.jpg" alt="mysql_primary_key"><br>这样就会形成一个紧凑的索引结构，近似顺序填满。<strong><code>由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。</code></strong></p>
<p>2、 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置：<br><img src="/images/mysql_aipk_2.jpg" alt="mysql_primary_key"><br><strong><code>此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片。</code></strong>得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p>
<p>在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。</p>
<p><strong><code>mysql 在频繁的更新、删除操作，会产生碎片。而含碎片比较大的表，查询效率会降低。此时需对表进行优化，这样才会使查询变得更有效率。</code></strong></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zephir安装和初体验]]></title>
      <url>http://team.jiunile.com/blog/2016/06/zephir-zephir-02.html</url>
      <content type="html"><![CDATA[<h2 id="Zephir安装"><a href="#Zephir安装" class="headerlink" title="Zephir安装"></a>Zephir安装</h2><h3 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h3><p>Zephir主要依赖于下面环境</p>
<ul>
<li>gcc &gt;= 4.x/clang &gt;= 3.x</li>
<li>re2c 0.13或更高版本</li>
<li>gnu 3.81或更高版本</li>
<li>autoconf 2.31或更高版本</li>
<li>automake 1.14或更高版本</li>
<li>libpcre3</li>
<li>php开发工具-phpize</li>
</ul>
<p>如果你使用Ubuntu，你可以安装所需要的包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install git gcc make re2c php5 php5-json php5-dev libpcre3-dev</span><br></pre></td></tr></table></figure></p>
<p>由于Zephir是用PHP编写的，所以你需要安装php<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ php -v</span><br><span class="line">PHP 5.6.5 (cli) (built: Jan 24 2015 20:04:31)</span><br><span class="line">Copyright (c) 1997-2014 The PHP Group</span><br><span class="line">Zend Engine v2.6.0, Copyright (c) 1998-2014 Zend Technologies</span><br><span class="line">with Zend OPcache v7.0.4-dev, Copyright (c) 1999-2014, by Zend Technologies</span><br></pre></td></tr></table></figure></p>
<p>同时也必须确保安装了PHP开发库<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ phpize -v</span><br><span class="line">Configuring <span class="keyword">for</span>:</span><br><span class="line">PHP Api Version:         20131106</span><br><span class="line">Zend Module Api No:      20131226</span><br><span class="line">Zend Extension Api No:   220131226</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="安装Zephir"><a href="#安装Zephir" class="headerlink" title="安装Zephir"></a>安装Zephir</h3><ol>
<li><p>下载最新稳定版</p>
</li>
<li><p>运行Zephir安装程序(编译/创建解析器)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> zephir</span><br><span class="line">$ ./install-json</span><br><span class="line">$ ./install -c</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zephir <span class="built_in">help</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>会得到如下返回</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> _____              __    _</span><br><span class="line">/__  /  ___  ____  / /_  (_)____</span><br><span class="line">  / /  / _ \/ __ \/ __ \/ / ___/</span><br><span class="line"> / /__/  __/ /_/ / / / / / /</span><br><span class="line">/____/\___/ .___/_/ /_/_/_/</span><br><span class="line">         /_/</span><br><span class="line"></span><br><span class="line">Zephir version 0.9.2a-dev</span><br><span class="line"></span><br><span class="line">Usage: </span><br><span class="line">    <span class="built_in">command</span> [options]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">    install             Installs the extension (requires root password)</span><br><span class="line">    builddev            Generate/Compile/Install a Zephir extension <span class="keyword">in</span> development mode</span><br><span class="line">    <span class="built_in">help</span>                Displays this <span class="built_in">help</span></span><br><span class="line">    build               Generate/Compile/Install a Zephir extension</span><br><span class="line">    compile             Compile a Zephir extension</span><br><span class="line">    stubs               Generates extension PHP stubs</span><br><span class="line">    version             Shows the Zephir version</span><br><span class="line">    init [namespace]    Initializes a Zephir extension</span><br><span class="line">    fullclean           Cleans the generated object files <span class="keyword">in</span> compilation</span><br><span class="line">    api [--theme-path=/path][--output-directory=/path][--theme-options=&#123;json&#125;|/path]Generates a HTML API</span><br><span class="line">    generate            Generates C code from the Zephir code</span><br><span class="line">    clean               Cleans the generated object files <span class="keyword">in</span> compilation</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">    <span class="_">-f</span>([a-z0-9\-]+)     Enables compiler optimizations</span><br><span class="line">    -fno-([a-z0-9\-]+)  Disables compiler optimizations</span><br><span class="line">    -w([a-z0-9\-]+)     Turns a warning on</span><br><span class="line">    -W([a-z0-9\-]+)     Turns a warning off</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Zephir初体验"><a href="#Zephir初体验" class="headerlink" title="Zephir初体验"></a>Zephir初体验</h2><p>还记得在开篇那个Helloword例子吗？我们先来简单介绍一下Zephir编译机制，在用例子介绍一下Zephir的语法。</p>
<h3 id="编译-解释"><a href="#编译-解释" class="headerlink" title="编译/解释"></a>编译/解释</h3><p>每一种语言都会有它们的”Hello World!”例子，对于Zehpir来说也不例外，下面的这个引导例子列举了许多它重要的特性。</p>
<p>Zephir的代码必须放置在类中。Zephir是基于面向对象类/框架打造的。所以代码放置在类的外面是不允许的。另外，一个命名空间也是必须的。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Test</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">say</span><span class="params">()</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"Hello World!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>一但这个类被编译完成，它会产生下面的一段C代码（gcc/clang/vc++编译）<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">EPHIR_INIT_CLASS(Test_Hello) &#123;</span><br><span class="line">    ZEPHIR_REGISTER_CLASS(Test, Hello, hello, test_hello_method_entry, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> SUCCESS;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PHP_METHOD(Test_Hello, say) &#123;</span><br><span class="line">    php_printf(<span class="string">"%s"</span>, <span class="string">"Hello World!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>事实上，使用Zephir的开发者无需懂得C语言，如果你有使用编译器，或者php内部的构造，或者C语言本身的经验， 在使用Zephir的时候你将会感到更加的清晰。</p>
<h3 id="Zephir初试"><a href="#Zephir初试" class="headerlink" title="Zephir初试"></a>Zephir初试</h3><p>在接下来的例子中，我们将会尽详细的描述，以便你知道是怎么回事。 我们的目标是让你感觉一下到底Zephir是怎么样的一个东西。 随便我们将会详细的探索Zephir的新特性。    </p>
<p>下面的例子很简单，它提供一个类和一个函数，检测一个数组的类型</p>
<p>让我们认真的检查下面的代码，开始认真的的学习Zephir. 这几行代码包括了很多详细的东西，我们将会慢慢的解释。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * MyTest (test/mytest.zep)</span><br><span class="line"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTest</span> </span>&#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">someMethod</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">/* 变量必须声明 */</span></span><br><span class="line">		 <span class="keyword">var</span> myArray;</span><br><span class="line">		 int i = <span class="number">0</span>, length;</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/*创建一个数组 */</span></span><br><span class="line">		 let myArray = [<span class="string">"hello"</span>, <span class="number">0</span>, <span class="number">100.25</span>, <span class="keyword">false</span>, <span class="keyword">null</span>];</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/* 数组有多少个元素*/</span></span><br><span class="line">		 let length = count(myArray);</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">/* 打印值类型 */</span></span><br><span class="line">		 <span class="keyword">while</span> i &lt; length &#123;</span><br><span class="line">		     <span class="keyword">echo</span> typeof myArray[i], <span class="string">"\n"</span>;</span><br><span class="line">		     let i++;</span><br><span class="line">		 &#125;</span><br><span class="line">		 </span><br><span class="line">		 <span class="keyword">return</span> myArray;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在函数中，第一行使用了’var’ 和 ‘int’ 关键词来声明一个函数内的私有变量。 在函数中的每一个变量必须事先声明它们自己的类型。这些声明并不是随意的，它帮助编译器来报告给你关于 错误的变量，或者变量的使用是否超出的它的范围，通常它会在最后抛出错误。</p>
<p>动态的变量必须以关键词’var’来声明。这些变量可以被指定或再指定成不同的变量类型。另一方面，’i’ and ‘length’使用了整数的静态变量，在执行程序的过程中，它只能改变值，而不能改变变量的类型。</p>
<p>与PHP不同的是，你不用在变量的前面加上($)符号。</p>
<p>Zephir的注释和Java, C#, C++等等一些语言的一样。</p>
<p>默认的，变量是不变的，意思是说Zephir期望大部分的变量保持不变。变量保持它们原始的值不变可以优化成静态常量。 如果需要改变变量的值，请使用关键词’let’<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 创建一个数组 */</span></span><br><span class="line">let myArray = [<span class="string">"hello"</span>, <span class="number">0</span>, <span class="number">100.25</span>, <span class="keyword">false</span>, <span class="keyword">null</span>];</span><br></pre></td></tr></table></figure></p>
<p>默认的，数组是一种象PHP一样的动态变量，它包含了许多不同类型的值。令人吃惊的是，PHP内部的函数可以在Zephir中使用，在下面的例子中，’count’ 函数被使用了，编辑器可以以最佳的状态来执行，因为它已经知道了数组的长度了。<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*数组有多少个元素 */</span></span><br><span class="line">let length = count(myArray);</span><br></pre></td></tr></table></figure></p>
<p>同样的，我们可以使用花括号来控制程序的流程.<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> i &lt; length &#123;</span><br><span class="line">    <span class="keyword">echo</span> typeof myArray[i], <span class="string">"\n"</span>;</span><br><span class="line">    let i++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>PHP的变量总是动态的，函数总是返回的是可变的动态变量，这就意味着如果一个静态变量在Zphir中被返回了，在PHP的调用中 你得到的却是一个动态变量。</p>
<p><strong>请注意！内存是在编译器中自动管理的，所以你没有必要像C语言一样去分配和释放内存。</strong> 这和PHP是很相似的。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Zephir介绍]]></title>
      <url>http://team.jiunile.com/blog/2016/06/zephir-zephir-01.html</url>
      <content type="html"><![CDATA[<h2 id="zephir介绍"><a href="#zephir介绍" class="headerlink" title="zephir介绍"></a>zephir介绍</h2><p>Zephir是一种可以让PHP开发者尝试编写和编译可以被PHP执行代码的一种语言。它是动态/静态类型，它的一些特性对于PHP 开发者来说是非常的相似的。</p>
<p>Zephir的名字是取自Zend Engine/PHP/Intermediate的缩写。建议发音为zephyr相同。事实上Zephir的创造者发音为zaefire_.</p>
<h3 id="简单易于开发"><a href="#简单易于开发" class="headerlink" title="简单易于开发"></a>简单易于开发</h3><p>相信大家和我有一样的经历，看到了yaf和phalcon在想为什么C语言的拓展框架可以这么的快，我自己能不能写一个出来呢？然后屁颠屁颠的跑去找资料找大神了解，大神说你去看一下 “PHP扩展开发及内核应用”，结果大家都知道醉了。</p>
<p>主要原因是需要对C相对的熟悉并且对PHP内核API也要很熟悉，我觉得这已经不是门槛的问题了是太平洋的距离，就草草结束了研究。</p>
<p>当遇到zephir首先了解的就是复杂程度，结果花了10分钟就跟着流程做了一个小DEMO，就这点看来就开发效率这点看来无可厚非的的高效快速，大家感受一下。<br><a id="more"></a><br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">Icyboy</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">function</span> <span class="title">hi</span><span class="params">()</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        <span class="keyword">echo</span>  <span class="string">"hello world"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译之后引入到php.ini里面，使用方式如下<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="title">Icyboy</span>\<span class="title">Hello</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">echo</span> Hello::hi() . PHP_EOL;</span><br></pre></td></tr></table></figure></p>
<p>zephir是一个解释器语言和PHP非常近似，通过zephir的机制编译成C语言，然后通过C编译出PHP拓展提供使用，把中间过程高度封装，很大程度让PHP拓展开发简单了很多。</p>
<p><strong>PHP扩展开发及内核应用</strong> <a href="http://www.walu.cc/phpbook" target="_blank" rel="external">http://www.walu.cc/phpbook</a></p>
<h3 id="zephir特性"><a href="#zephir特性" class="headerlink" title="zephir特性"></a>zephir特性</h3><ul>
<li>zephir是静态动态结合语言，在zephir内可以使用传统静态变量，也可以使用动态变量，灵活度高。</li>
<li>内存安全，熟悉C程序的童鞋都知道C可以控制内存指针，其实用的不好是一件很危险的事情，zephir它不允许你使用指针，它提供了一个<strong>task-local垃圾收集器</strong>，以避免内存泄漏。</li>
<li>编译模式，zephir能够编译主流系统Liunx/OSX/Windows能够识别的拓展程序。</li>
<li>开发源代码的高级语言，以面向对象为基础，编写拓展都需要基于面向对象。</li>
</ul>
<h3 id="感受一下"><a href="#感受一下" class="headerlink" title="感受一下"></a>感受一下</h3><p>下面是官方提供的一个让大家感受一下的小例子作用是过滤变量返回字母字符<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="title">MyLibrary</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span></span><br><span class="line"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">function</span> <span class="title">alpha</span><span class="params">(string str)</span></span><br><span class="line">    </span>&#123;</span><br><span class="line">        char ch; string filtered = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span> ch in str &#123;</span><br><span class="line">           <span class="keyword">if</span> (ch &gt;= <span class="string">'a'</span> &amp;&amp; ch &lt;= <span class="string">'z'</span>) || (ch &gt;= <span class="string">'A'</span> &amp;&amp; ch &lt;= <span class="string">'Z'</span>) &#123;</span><br><span class="line">              let filtered .= ch;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> filtered;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>从PHP类可以使用如下<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"></span><br><span class="line">$filter = <span class="keyword">new</span> MyLibrary\Filter();</span><br><span class="line"><span class="keyword">echo</span> $filter-&gt;alpha(<span class="string">"01he#l.lo?/1"</span>); <span class="comment">// 结果输出 hello</span></span><br></pre></td></tr></table></figure></p>
<h2 id="为什么是Zephir"><a href="#为什么是Zephir" class="headerlink" title="为什么是Zephir"></a>为什么是Zephir</h2><p>今天的PHP应用程序必须平衡一系列问题包括稳定性、性能和功能。</p>
<p>每一个PHP应用程序是基于一组常见的组件或者说框架，这些公共组件是库/框架或它们的组合。一旦安装后很少改变，作为应用程序的基础，他们必须是有非常快的,</p>
<p>快速和强大的库会很复杂，由于高水平的抽象，一般的做法是约定基础库或框架很少改变，才有机会来改善性能和资源消耗。</p>
<p>Zephir，您可以实现面向对象库/框架/应用程序，使您的应用程序速度提高，改善用户体验。</p>
<h3 id="如果你是一个PHP程序员……"><a href="#如果你是一个PHP程序员……" class="headerlink" title="如果你是一个PHP程序员……"></a>如果你是一个PHP程序员……</h3><p>PHP是在使用的Web应用程序开发中最流行的语言之一。像PHP动态类型和解释语言，由于其灵活性，提供非常高的效率。</p>
<p>PHP是基于Zend引擎的实现。这是执行从字节码表示的PHP代码的虚拟机。Zend引擎是世界上每一个PHP的安装几乎目前，随着Zephir，您可以创建在Zend引擎运行PHP扩展。</p>
<p>PHP托管Zephir，所以他们显然有很多相似的地方，但是，他们有给Zephir自己的个性的重要差异。例如，Zephir更加严格，它可以让你减少编译步骤。</p>
<h3 id="如果你是一个C程序员……"><a href="#如果你是一个C程序员……" class="headerlink" title="如果你是一个C程序员……"></a>如果你是一个C程序员……</h3><p>C是有史以来最强大的和流行的语言之一。 事实上，PHP是用C编写的。</p>
<p>然而，用C开发大型应用程序可以把PHP或Zephir相比比预期的要长很多，一些错误是很难找到。如果你不是一个有经验的开发人员。</p>
<p>Zephir设计是安全的，所以它没有实现指针或手动内存管理，如果你是一个C程序员，你会觉得Zephir强大，比C更加的友好。</p>
<h3 id="编译VS解读"><a href="#编译VS解读" class="headerlink" title="编译VS解读"></a>编译VS解读</h3><p>编译通常会减慢下来的发展；你需要多一点耐心，使你的代码编译运行它之前。此外，该解释趋于降低有利于生产率的性能。</p>
<p>为了更高的效率，Zephir需要编译你的代码，但是他不会影响高生产效率，开发人员可以决定哪些应用程序部分应当在Zephir，哪些不是。</p>
<h3 id="静态类型和动态类型语言"><a href="#静态类型和动态类型语言" class="headerlink" title="静态类型和动态类型语言"></a>静态类型和动态类型语言</h3><p>一般来说，在静态类型语言中，变量是绑定到一个特定类型的一生。 其类型不能改变，只能参考实例和兼容操作。 像C / c++语言实现的方案<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int a = <span class="number">0</span>;</span><br><span class="line">a = <span class="string">"hello"</span>; <span class="comment">// not allowed</span></span><br></pre></td></tr></table></figure></p>
<p>在动态类型，绑定到类型的值，而不是变量。 所以，一个变量可能引用值的类型，然后重新分配后的值类型无关。 Javascript / PHP的例子 动态类型语言<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">0</span>;</span><br><span class="line">a = <span class="string">"hello"</span>; <span class="comment">// allowed</span></span><br></pre></td></tr></table></figure></p>
<p>尽管动态类型有着生产力的优势，但是动态语言并不能成为所有应用的选择，特别是对于非常大型代码库和高性能的应用程序。</p>
<p>优化性能的动态语言像PHP比静态语言(如C)是更具挑战性的。 在静态语言中，优化器可以利用类型信息做出决策。 在动态语言中，只有很有限的信息是可用的，这使得优化器的选择更加困难。</p>
<p>如果你需要非常高的性能,，静态语言可能是一个更安全的选择。</p>
<p>静态语言的另一个好处是编译器执行额外的检查。 编译器无法发现逻辑错误，这更重要但是编译器可以提前发现错误，动态语言只能在运行提示报错信息。</p>
<p>Zephir是静态和动态类型都允许使用的。</p>
<h3 id="代码保护"><a href="#代码保护" class="headerlink" title="代码保护"></a>代码保护</h3><p>在某些情况下，编译不显著提高性能，这可能是因为瓶颈所在。 在应用程序的I / O(很有可能)，而不是计算/内存限制。 然而，编译代码也可能带来某种程度的intelectual保护您的应用程序。 Zephir产生本地二进制文件，你也有能力“隐藏”用户或客户的原始代码。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Zephir不是用来取代PHP或C，相反我们认为这是一个补充，允许开发者进入代码编译和静态类型。Zephir正是试图加入从C和PHP的世界，美好的事物寻找机会使他们的应用程序更快！如果你喜欢PHP，如果你渴望执行效率，那就别犹豫赶快尝试一下Zephir吧！</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Cache 应用中的服务过载案例研究]]></title>
      <url>http://team.jiunile.com/blog/2016/06/cache-server.html</url>
      <content type="html"><![CDATA[<p>简单地说，过载是外部请求对系统的访问量突然激增，造成请求堆积，服务不可用，最终导致系统崩溃。本文主要分析引入Cache可能造成的服务过载，并讨论相关的预防、恢复策略。</p>
<p>Cache在现代系统中使用广泛，由此引入的服务过载隐患无处不在，但却非常隐蔽，容易被忽视。本文希望能为开发者在设计和编写相关类型应用，以及服务过载发生处理时能够有章可循。</p>
<h2 id="一个服务过载案例"><a href="#一个服务过载案例" class="headerlink" title="一个服务过载案例"></a>一个服务过载案例</h2><p>本文讨论的案例是指存在正常调用关系的两个系统（假设调用方为A系统，服务方为B系统），A系统对B系统的访问突然超出B系统的承受能力，造成B系统崩溃。造成服务过载的原因很多，这里分析的是严重依赖Cache的系统服务过载。首先来看一种包含Cache的体系结构（如下图所示）。<br><img src="/images/2_cache.png" alt="Cache应用体系结构"></p>
<p>A系统依赖B系统的读服务，A系统是60台机器组成的集群，B系统是6台机器组成的集群，之所以6台机器能够扛住60台机器的访问，是因为A系统并不是每次都访问B，而是首先请求Cache，只有Cache的相应数据失效时才会请求B。<br><a id="more"></a><br>这正是Cache存在的意义，它让B系统节省了大量机器；如果没有Cache，B系统不得不组成60台机器的集群，如果A也同时依赖除B系统外的另一个系统（假设为C系统）呢？那么C系统也要60台机器，放大的流量将很快耗尽公司的资源。</p>
<p>然而Cache的引入也不是十全十美的，这个结构中如果Cache发生问题，全部的流量将流向依赖方，造成流量激增，从而引发依赖系统的过载。</p>
<p>回到A和B的架构，造成服务过载的原因至少有下面三种：</p>
<ul>
<li>B系统的前置代理发生故障或者其他原因造成B系统暂时不可用，等B系统系统服务恢复时，其流量将远远超过正常值。</li>
<li>Cache系统故障，A系统的流量将全部流到B系统，造成B系统过载。</li>
<li>Cache故障恢复，但这时Cache为空，Cache瞬间命中率为0，相当于Cache被击穿，造成B系统过载。</li>
</ul>
<p>第一个原因不太好理解，为什么B系统恢复后流量会猛增呢？主要原因就是缓存的超时时间。当有数据超时的时候，A系统会访问B系统，但是这时候B系统偏偏故障不可用，那么这个数据只好超时，等发现B系统恢复时，发现缓存里的B系统数据已经都超时了，都成了旧数据，这时当然所有的请求就打到了B。</p>
<p>下文主要介绍服务过载的预防和发生后的一些补救方法，以预防为主，从调用方和服务方的视角阐述一些可行方案。</p>
<h2 id="服务过载的预防"><a href="#服务过载的预防" class="headerlink" title="服务过载的预防"></a>服务过载的预防</h2><p>所谓Client端指的就是上文结构中的A系统，相对于B系统，A系统就是B系统的Client，B系统相当于Server。</p>
<h3 id="Client端的方案"><a href="#Client端的方案" class="headerlink" title="Client端的方案"></a>Client端的方案</h3><p>针对上文阐述的造成服务过载的三个原因：B系统故障恢复、Cache故障、Cache故障恢复，我们看看A系统有哪些方案可以应对。</p>
<blockquote>
<p>合理使用Cache应对B系统宕机</p>
</blockquote>
<p>一般情况下，Cache的每个Key除了对应Value，还对应一个过期时间T，在T内，get操作直接在Cache中拿到Key对应Value并返回。但是在T到达时，get操作主要有五种模式：</p>
<h4 id="基于超时的简单（stupid）模式"><a href="#基于超时的简单（stupid）模式" class="headerlink" title="基于超时的简单（stupid）模式"></a>基于超时的简单（stupid）模式</h4><p>在T到达后，任何线程get操作发现Cache中的Key和对应Value将被清除或标记为不可用，get操作将发起调用远程服务获取Key对应的Value，并更新写回Cache，然后get操作返回新值；如果远程获取Key-Value失败，则get抛出异常。</p>
<p>为了便于理解，举一个码头工人取货的例子：5个工人（线程）去港口取同样Key的货（get），发现货已经过期被扔掉了，这时5个工人各自分别去对岸取新货，然后返回。</p>
<h4 id="基于超时的常规模式"><a href="#基于超时的常规模式" class="headerlink" title="基于超时的常规模式"></a>基于超时的常规模式</h4><p>在T到达后，Cache中的Key和对应Value将被清除或标记为不可用，get操作将调用远程服务获取Key对应的Value，并更新写回Cache；此时，如果另一个线程发现Key和Value已经不可用，get操作还需要判断有没有其他线程发起了远程调用，如果有，那么自己就等待，直到那个线程远程获取操作成功，Cache中得Key变得可用，get操作返回新的Value。如果远程获取操作失败，则get操作抛出异常，不会返回任何Value。</p>
<p>还是码头工人的例子：5个工人（线程）去港口取同样Key的货（get），发现货已经过期被扔掉了，那么只需派出一个人去对岸取货，其他四个人在港口等待即可，而不用5个人全去。</p>
<p>基于超时的简单模式和常规模式区别在于对于同一个超时的Key，前者每个get线程一旦发现Key不存在，则发起远程调用获取值；而后者每个get线程发现Key不存在，则还要判断当前是否有其他线程已经发起了远程调用操作获取新值，如果有，自己就简单的等待即可。</p>
<p>显然基于超时的常规模式比基于超时的简单模式更加优化，减少了超时时并发访问后端的调用量。</p>
<p>实现基于超时的常规模式就需要用到经典的Double-checked locking惯用法了。</p>
<h4 id="基于刷新的简单（stupid）模式"><a href="#基于刷新的简单（stupid）模式" class="headerlink" title="基于刷新的简单（stupid）模式"></a>基于刷新的简单（stupid）模式</h4><p>在T到达后，Cache中的Key和相应Value不动，但是如果有线程调用get操作，将触发refresh操作，根据get和refresh的同步关系，又分为两种模式：</p>
<ul>
<li>同步模式：任何线程发现Key过期，都触发一次refresh操作，get操作等待refresh操作结束，refresh结束后，get操作返回当前Cache中Key对应的Value。注意refresh操作结束并不意味着refresh成功，还可能抛了异常，没有更新Cache，但是get操作不管，get操作返回的值可能是旧值。</li>
<li>异步模式：任何线程发现Key过期，都触发一次refresh操作，get操作触发refresh操作，不等refresh完成，直接返回Cache中的旧值。</li>
</ul>
<p>举上面码头工人的例子说明基于刷新的常规模式：这次还是5工人去港口取货，这时货都在，但是已经旧了，这时5个工人有两种选择：</p>
<ul>
<li>5个人各自去远程取新货，如果取货失败，则拿着旧货返回（同步模式）</li>
<li>5个人各自通知5个雇佣工去取新货，5个工人拿着旧货先回（异步模式）</li>
</ul>
<h4 id="基于刷新的常规模式"><a href="#基于刷新的常规模式" class="headerlink" title="基于刷新的常规模式"></a>基于刷新的常规模式</h4><p>在T到达后，Cache中的Key和相应Value都不会被清除，而是被标记为旧数据，如果有线程调用get操作，将触发refresh更新操作，根据get和refresh的同步关系，又分为两种模式：</p>
<ul>
<li>同步模式：get操作等待refresh操作结束，refresh结束后，get操作返回当前Cache中Key对应的Value，注意：refresh操作结束并不意味着refresh成功，还可能抛了异常，没有更新Cache，但是get操作不管，get操作返回的值可能是旧值。如果其他线程进行get操作，Key已经过期，并且发现有线程触发了refresh操作，则自己不等refresh完成直接返回旧值。</li>
<li>异步模式：get操作触发refresh操作，不等refresh完成，直接返回Cache中的旧值。如果其他线程进行get操作，发现Key已经过期，并且发现有线程触发了refresh操作，则自己不等refresh完成直接返回旧值。</li>
</ul>
<p>再举上面码头工人的例子说明基于刷新的常规模式：这次还是5工人去港口取货，这时货都在，但是已经旧了，这时5个工人有两种选择：</p>
<ul>
<li>派一个人去远方港口取新货，其余4个人拿着旧货先回（同步模式）。</li>
<li>5个人通知一个雇佣工去远方取新货，5个人都拿着旧货先回（异步模式）。</li>
</ul>
<p>基于刷新的简单模式和基于刷新的常规模式区别就在于取数线程之间能否感知当前数据是否正处在刷新状态，因为基于刷新的简单模式中取数线程无法感知当前过期数据是否正处在刷新状态，所以每个取数线程都会触发一个刷新操作，造成一定的线程资源浪费。</p>
<p>而基于超时的常规模式和基于刷新的常规模式区别在于前者过期数据将不能对外访问，所以一旦数据过期，各线程要么拿到数据，要么抛出异常；后者过期数据可以对外访问，所以一旦数据过期，各线程要么拿到新数据，要么拿到旧数据。</p>
<h4 id="基于刷新的续费模式"><a href="#基于刷新的续费模式" class="headerlink" title="基于刷新的续费模式"></a>基于刷新的续费模式</h4><p>该模式和基于刷新的常规模式唯一的区别在于refresh操作超时或失败的处理上。在基于刷新的常规模式中，refresh操作超时或失败时抛出异常，Cache中的相应Key-Value还是旧值，这样下一个get操作到来时又会触发一次refresh操作。</p>
<p>在基于刷新的续费模式中，如果refresh操作失败，那么refresh将把旧值当成新值返回，这样就相当于旧值又被续费了T时间，后续T时间内get操作将取到这个续费的旧值而不会触发refresh操作。</p>
<p>基于刷新的续费模式也像常规模式那样分为同步模式和异步模式，不再赘述。</p>
<p>下面讨论这5种Cache get模式在服务过载发生时的表现，首先假设如下：</p>
<ul>
<li>假设A系统的访问量为每分钟M次。</li>
<li>假设Cache能存Key为C个，并且Key空间有N个。</li>
<li>假设正常状态下，B系统访问量为每分钟W次，显然W&lt;N&lt;M。</li>
</ul>
<p>这时因为某种原因，比如B长时间故障，造成Cache中得Key全部过期，B系统这时从故障中恢复，五种get模式分析表现分析如下：</p>
<ul>
<li>在基于超时和刷新的简单模式中，B系统的瞬间流量将达到和A的瞬时流量M大体等同，相当于Cache被击穿。这就发生了服务过载，这时刚刚恢复的B系统将肯定会被大流量压垮。</li>
<li>在基于超时和刷新的常规模式中，B系统的瞬间流量将和Cache中Key空间N大体等同。这时是否发生服务过载，就要看Key空间N是否超过B系统的流量上限了。</li>
<li>在基于刷新的续费模式中，B系统的瞬间流量为W，和正常情况相同而不会发生服务过载。实际上，在基于刷新的续费模式中，不存在Cache Key全部过期的情况，就算把B系统永久性地干掉，A系统的Cache也会基于旧值长久的平稳运行。</li>
</ul>
<p>第3点，B系统不会发生服务过载的主要原因是基于刷新的续费模式下不会出现chache中的Key全部长时间过期的情况，即使B系统长时间不可用，基于刷新的续费模式也会在一个过期周期内把旧值当成新值继续使用。所以当B系统恢复时，A系统的Cache都处在正常工作状态。</p>
<p>从B系统的角度看，能够抵抗服务过载的基于刷新的续费模式最优。</p>
<p>从A系统的角度看，由于一般情况下A系统是一个高访问量的在线web应用，这种应用最讨厌的一个词就是“线程等待”，因此基于刷新的各种异步模式较优。</p>
<p>综合考虑，基于刷新的异步续费模式是首选。然而凡事有利就有弊，有两点需要注意的地方：</p>
<ul>
<li>基于刷新模式最大的缺点是Key-Value一旦放入Cache就不会被清除，每次更新也是新值覆盖旧值，JVM GC永远无法对其进行垃圾收集，而基于超时的模式中，Key-Value超时后如果新的访问没有到来，内存是可以被GC垃圾回收的。所以如果你使用的是寸土寸金的本地内存做Cache就要小心了。</li>
<li>基于刷新的续费模式需要做好监控，不然有可能Cache中的值已经和真实的值相差很远了，应用还以为是新值而使用。</li>
</ul>
<p>关于具体的Cache，来自Google的Guava本地缓存库支持上文的第二种、第四种和第五种get操作模式。</p>
<p>但是对于Redis等分布式缓存，只提供原始的get、set方法，而提供的get仅仅是获取，与上文提到的五种get操作模式不是一个概念。开发者想用这五种get操作模式的话不得不自己封装和实现。</p>
<p>五种get操作模式中，基于超时和刷新的简单模式是实现起来最简单的模式，但遗憾的是这两种模式对服务过载完全无免疫力，这可能也是服务过载在大量依赖缓存的系统中频繁发生的一个重要原因吧。</p>
<p>本文之所以把第1、3种模式称为stupid模式，是想强调这种模式应该尽量避免，Guava里面根本没有这种模式，而Redis只提供简单的读写操作，很容易就把系统实现成了这种方式。</p>
<blockquote>
<p>应对分布式Cache宕机</p>
</blockquote>
<p>如果是Cache直接挂了，那么就算是基于刷新的异步续费模式也无能为力了。这时A系统铁定无法对Cache进行存取操作，只能将流量完全打到B系统，B系统面对服务过载在劫难逃……</p>
<p>本节讨论的预防Cache宕机仅限于分布式Cache，因为本地Cache一般和A系统应用共享内存和进程，本地Cache挂了A系统也挂了，不会出现本地Cache挂了而A系统应用正常的情况。</p>
<p>首先，A系统请求线程检查分布式Cache状态，如果无应答则说明分布式Cache挂了，则转向请求B系统，这样一来大流量将压垮B系统。这时可选的方案如下：</p>
<ul>
<li>A系统的当前线程不请求B系统，而是打个日志并设置一个默认值。</li>
<li>A系统的当前线程按照一定概率决定是否请求B系统。</li>
<li>A系统的当前线程检查B系统运行情况，如果良好则请求B系统。</li>
</ul>
<p><strong>方案1</strong> 最简单，A系统知道如果没有Cache，B系统可能扛不住自己的全部流量，索性不请求B系统，等待Cache恢复。但这时B系统利用率为0，显然不是最优方案，而且当请求的Value不容易设置默认值时，这个方案就不行了。</p>
<p><strong>方案2</strong> 可以让一部分线程请求B系统，这部分请求肯定能被B系统hold住。可以保守的设置这个概率 u =（B系统的平均流量）/（A系统的峰值流量）。</p>
<p><strong>方案3</strong> 是一种更为智能的方案，如果B系统运行良好，当前线程请求；如果B系统过载，则不请求，这样A系统将让B系统处于一种宕机与不宕机的临界状态，最大限度挖掘B系统性能。这种方案要求B系统提供一个性能评估接口返回Yes和No，Yes表示B系统良好，可以请求；No表示B系统情况不妙，不要请求。这个接口将被频繁调用，必须高效。</p>
<p>方案3的关键在于如何评估一个系统的运行状况。一个系统中当前主机的性能参数有CPU负载、内存使用率、Swap使用率、GC频率和GC时间、各个接口平均响应时间等，性能评估接口需要根据这些参数返回Yes或者No，是不是机器学习里的二分类问题？??关于这个问题已经可以单独写篇文章讨论了，在这里就不展开了，你可以想一个比较简单傻瓜的保守策略，缺点是A系统的请求无法很好的逼近B系统的性能极限。</p>
<p>综合以上分析，方案2比较靠谱。如果选择方案3，建议由专门团队负责研究并提供统一的系统性能实时评估方案和工具。</p>
<blockquote>
<p>应对分布式Cache宕机后的恢复</p>
</blockquote>
<p>不要以为成功hold住分布式Cache宕机就万事大吉了，真正的考验是分布式Cache从宕机过程恢复之后，这时分布式Cache中什么都没有。</p>
<p>即使是上文中提到了基于刷新的异步续费策略这时也没用，因为分布式Cache为空，无论如何都要请求B系统。这时B系统的最大流量是Key的空间取值数量。</p>
<p>如果Key的取值空间数量很少，则相安无事；如果Key的取值空间数量大于B系统的流量上限，服务过载依然在所难免。</p>
<p>这种情况A系统很难处理，关键原因是A系统请求Cache返回Key对应Value为空，A系统无法知道是因为当前Cache是刚刚初始化，所有内容都为空；还是因为仅仅是自己请求的那个Key没在Cache里。</p>
<p>如果是前者，那么当前线程就要像处理Cache宕机那样进行某种策略的回避；如果是后者，直接请求B系统即可，因为这是正常的Cache使用流程。</p>
<p>对于Cache宕机的恢复，A系统真的无能为力，只能寄希望于B系统的方案了。</p>
<h3 id="Server端的方案"><a href="#Server端的方案" class="headerlink" title="Server端的方案"></a>Server端的方案</h3><p>相对于Client端需要应对各种复杂问题，Server端需要应对的问题非常简单，就是如何从容应对过载的问题。无论是缓存击穿也好，还是拒绝服务攻击也罢，对于Server端来说都是过载保护的问题。对于过载保护，主要给出两种可行方案，以及一种比较复杂的方案思路。</p>
<blockquote>
<p>流量控制</p>
</blockquote>
<p>流量控制就是B系统实时监控当前流量，如果超过预设的值或者系统承受能力，则直接拒绝掉一部分请求，以实现对系统的保护。</p>
<p>流量控制根据基于的数据不同，可分为两种：</p>
<ul>
<li>基于流量阈值的流控：流量阈值是每个主机的流量上限，流量超过该阈值主机将进入不稳定状态。阈值提前进行设定，如果主机当前流量超过阈值，则拒绝掉一部分流量，使得实际被处理流量始终低于阈值。</li>
<li>基于主机状态的流控：每个接受每个请求之前先判断当前主机状态，如果主机状况不佳，则拒绝当前请求。</li>
</ul>
<p>基于阈值的流控实现简单，但是最大的问题是需要提前设置阈值，而且随着业务逻辑越来越复杂，接口越来越多，主机的服务能力实际应该是下降的，这样就需要不断下调阈值，增加了维护成本，而且万一忘记调整的话，呵呵……</p>
<p>主机的阈值可以通过压力测试确定，选择的时候可以保守些。</p>
<p>基于主机状态的流控免去了人为控制，但是其最大的确定上文已经提到：如何根据当前主机各个参数判断主机状态呢？想要完美的回答这个问题目测并不容易，因此在没有太好答案之前，我推荐基于阈值的流控。</p>
<p>流量控制基于实现位置的不同，又可以分为两种：</p>
<ul>
<li>反向代理实现流控：在反向代理如Nginx上基于各种策略进行流量控制。这种一般针对HTTP服务。</li>
<li>借助服务治理系统：如果Server端是RMI、RPC等服务，可以构建专门的服务治理系统进行负载均衡、流控等服务。</li>
<li>服务容器实现流控：在应用代码里，业务逻辑之前实现流量控制。</li>
</ul>
<p>第3种在服务器的容器（如Java容器）中实现流控并不推荐，因为流控和业务代码混在一起容易混乱；其次实际上流量已经全量进入到了业务代码里，这时的流控只是阻止其进入真正的业务逻辑，所以流控效果将打折；还有，如果流量策略经常变动，系统将不得不为此经常更改。</p>
<p>因此，推荐前两种方式。</p>
<p>最后提一个注意点：当因为流控而拒绝请求时，务必在返回的数据中带上相关信息（比如“当前请求因为超出流量而被禁止访问”），如果返回值什么都没有将是一个大坑。因为造成调用方请求没有被响应的原因很多，可能是调用方Bug，也可能是服务方Bug，还可能是网络不稳定，这样一来很可能在排查一整天后发现是流控搞的鬼……</p>
<blockquote>
<p>服务降级</p>
</blockquote>
<p>服务降级一般由人为触发，属于服务过载造成崩溃恢复时的策略，但为了和流控对比，将其放到这里。</p>
<p>流量控制本质上是减小访问量，而服务处理能力不变；而服务降级本质上是降低了部分服务的处理能力，增强另一部分服务处理能力，而访问量不变。</p>
<p>服务降级是指在服务过载时关闭不重要的接口（直接拒绝处理请求），而保留重要的接口。比如服务由10个接口，服务降级时关闭了其中五个，保留五个，这时这个主机的服务处理能力将增强到二倍左右。</p>
<p>然而，服务过载发生时动辄就超出系统处理能力10倍，而服务降级能使主机服务处理能力提高10倍么？显然很困难，因此服务过载的应对不能只依靠服务降级策略。</p>
<blockquote>
<p>动态扩展</p>
</blockquote>
<p>动态扩展指的是在流量超过系统服务能力时，自动触发集群扩容，自动部署并上线运行；当流量过去后又自动回收多余机器，完全弹性。</p>
<p>这个方案是不是感觉很不错。但是目前互联网公司的在线应用跑在云上的本身就不多，要完全实现在线应用的自动化弹性运维，要走的路就更多了。</p>
<h2 id="崩溃恢复"><a href="#崩溃恢复" class="headerlink" title="崩溃恢复"></a>崩溃恢复</h2><p>如果服务过载造成系统崩溃还是不幸发生了，这时需要运维控制流量，等后台系统启动完毕后循序渐进的放开流量，主要目的是让Cache慢慢预热。流量控制刚开始可以为10%，然后20%，然后50%，然后80%，最后全量，当然具体的比例，尤其是初始比例，还要看后端承受能力和前端流量的比例，各个系统并不相同。</p>
<p>如果后端系统有专门的工具进行Cache预热，则省去了运维的工作，等Cache热起来再发布后台系统即可。但是如果Cache中的Key空间很大，开发预热工具将比较困难。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>“防患于未然”放在服务过载的应对上也是适合的，预防为主，补救为辅。综合上文分析，具体的预防要点如下：</p>
<ul>
<li>调用方（A系统）采用基于刷新的异步续费模式使用Cache，或者至少不能使用基于超时或刷新的简单（stupid）模式。</li>
<li>调用方（A系统）每次请求Cache时检查Cache是否可用（available），如果不可用则按照一个保守的概率访问后端，而不是无所顾忌的直接访问后端。</li>
<li>服务方（B系统）在反向代理处设置流量控制进行过载保护，阈值需要通过压测获得。</li>
</ul>
<p>崩溃的补救主要还是靠运维和研发在发生时的通力合作：观察流量变化准确定位崩溃原因，运维控流量研发持续关注性能变化。</p>
<p>未来如果有条件的话可以研究下主机应用健康判断问题和动态弹性运维问题，毕竟自动化比人为操作要靠谱。</p>
<hr>
<p>来源：美团点评技术团队-张杨<br>链接：<a href="http://tech.meituan.com/avalanche-study.html" target="_blank" rel="external">http://tech.meituan.com/avalanche-study.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[通过iptables实现端口转发与内网共享上网]]></title>
      <url>http://team.jiunile.com/blog/2016/06/iptables-forward-internet-share.html</url>
      <content type="html"><![CDATA[<p>iptables是一个Linux下优秀的nat+防火墙工具，我使用该工具以较低配置的传统pc配置了一个灵活强劲的防火墙+nat系统,小有心得，看了网上也有很多这方面的文章，但是似乎要么说的比较少，要么就是比较偏，内容不全，容易误导，我研究了一段时间的iptables同时也用了很久，有点滴经验，写来供大家参考，同时也备日后自己翻阅。</p>
<p>首先要说明的是，iptables操作的是2.4以上内核的netfilter。所以需要linux的内核在2.4以上。其功能与安全性远远比其前辈ipfwadm,ipchains强大，iptables大致是工作在OSI七层的二、三、四层，其前辈ipchains不能单独实现对tcp/udp port以及对mac地址的的定义与操作，所以我想ipchains应该是仅仅工作在三层上的。</p>
<h2 id="netfilter工作流程"><a href="#netfilter工作流程" class="headerlink" title="netfilter工作流程"></a>netfilter工作流程</h2><p>我们先简单介绍一下netfilter的大致工作流程，也就是一个数据包（或者叫分组、packet,我个人习惯叫包）在到达linux的网络接口的时候 （网卡）如何处理这个包，然后再介绍一下如何用iptables改变或者说控制对这个数据包进行操作。</p>
<ul>
<li>netfilter内部分为三个表，分别是 filter,nat,mangle，每个表又有不同的操作链（Chains）。</li>
<li>在filter（过滤）表中，也就是他的 防火墙功能 的这个表，定义了三个 Chain。分别是INPUT,FORWARD,OUTPUT。也就是对包的入、转发、出进行定义的三个过滤链。对于这个filter表的操作和控制也是我们实现防火墙功能的一个重要手段</li>
<li>在nat(Network Address Translation、网络地址翻译)表中，也就是我们用以实现地址转换和端口转发功能的这个表，定义了PREROUTING, POSTROUTING,OUTPUT三个链,下面我们会对这三个链作详细的说明</li>
<li>而netfilter的mangle表则是一个自定义表，里面包括上面 的filter以及nat表中的各种chains，它可以让我们进行一些自定义的操作，同时这个mangle表中的chains在netfilter对包 的处理流程中处在一个比较优先的位置。<a id="more"></a>
下面有一张图清晰的描绘了netfilter对包的处理流程（该图摘自网上，不知作者是谁，在此深表敬意！），一般情况下，我们用不到这个mangle表，在这里我们就不做介绍了。<br><img src="/images/iptables_netfilter_chains.png" alt="iptables包处理流程"></li>
<li>ebtables基本使用: <a href="http://www.cnblogs.com/peteryj/archive/2011/07/24/2115602.html" target="_blank" rel="external">http://www.cnblogs.com/peteryj/archive/2011/07/24/2115602.html</a><br><img src="/images/iptables_entables.png" alt="iptables_entables处理流程图"></li>
</ul>
<h3 id="PREROUTING-DNAT"><a href="#PREROUTING-DNAT" class="headerlink" title="PREROUTING(DNAT)"></a>PREROUTING(DNAT)</h3><p>PREROUTING这个chain在最前面，当一个包来到linux的网络接口的时候先过mangle的PREROUTING；然后是nat的PREROUTING,从这个chain的名字我们可以看出，这个chain是在路由之前(pre-routing)要过的。</p>
<p>为什么要在路由之前过呢？大家可以看到这个图上，上面有一个菱形的部分叫ROUTING,这个ROUTING部分就是Linux的route box,也就是路由系统，它同样有很高深的功能，可以实现策略路由等等一些高级特性，此处我们不做详细解释。单说这个PREROUTING链，因为在这个链里面我们对包的操作是DNAT,也就是改变目的地址和（或端口），通常用在端口转发，或者nat到内网的DMZ区，也就是说当一个包过来的时候我们要改变它的目的地址，大家可以想想,如果一个包在改变目的地址之前就被扔进了route box,让系统选好路之后再改变目的地址，那么选路就可能是错的，或者说毫无意义了，所以，PREROUTING这个Chain一定要在进Routing 之前做。</p>
<p>比如说，我们的公网ip是60.1.1.1/24，位于linux中的eth0内网ip是10.1.1.1/24，位于linux中的eth1, 我们的内网有一台web服务器，地址是10.1.1.2/24,我们怎么样能让internet用户通过这个公网ip访问我们内部的这个web服务器呢？ 我们就可以在这个PREROUTING链上面定义一个规则，把访问60.1.1.1:80的用户的目的地址改变一下，改变为10.1.1.2:80,这样 就实现了internet用户对内网服务器的访问了，当然了，这个端口是比较灵活的，我们可以定义任何一个端口的转发，不一定是80–&gt;80，具体的命令我们在下面的例子中介绍，这里我们只谈流程与概念上的实现方法。</p>
<h3 id="FORWARD"><a href="#FORWARD" class="headerlink" title="FORWARD"></a>FORWARD</h3><p>好了，我们接着往下走，这个包已经过了两个PREROUTING链了，这个时候，出现了一个分支转折的地方，也就是图中下方的那个菱形（FORWARD）,转发！这里有一个对目的地址的判断（这里同样说明了PREROUTING一定要在最先，不仅要在route box之前，甚至是这个对目的地址的判断之前，因为我们可能做一个去某某某ip的地方转到自己的ip的规则，所以PREROUTING是最先处理这个包的Chain）！</p>
<p>如果包的目的地是本机ip,那么包向上走，走入INPUT链处理，然后进入LOCAL PROCESS,如果非本地，那么就进入FORWARD链进行过滤，我们在这里就不介绍INPUT,OUTPUT的处理了，因为那主要是对于本机安全的一种处理，我们这里主要说对转发的过滤和nat的实现。</p>
<p>这里的FORWARD我简单说一下，当linux收到了一个 目的ip地址不是本地的包 ，Linux会把这个包丢弃，因为默认情况下，Linux的三层包转发功能是关闭的，如果要让我们的linux实现转发，则需要打开这个转发功能，可以 改变它的一个系统参数，使用sysctl net.ipv4.ip_forward=1或者echo “1” &gt; /proc/sys/net/ipv4/ip_forward命令打开转发功能。</p>
<p>好了，在这里我们让linux允许转发，这个包的目的地址也不是本机，那么它将接着走入FORWARD链，在FORWARD链里面，我们就可以定义详细的规则，也就是是否允许他通过，或者对这个包的方向流程进行一些改变，这也是我们实现访问控制的地方，这里同样也是Mangle_FORWARD然后filter_FORWARD,我们操作任何一个链都会影响到这个包的命运，在下面的介绍中，我们就忽略掉mangle表，我们基本用不到操作它，所以我们假设它是透明的。</p>
<h3 id="POSTROUTING-SNAT"><a href="#POSTROUTING-SNAT" class="headerlink" title="POSTROUTING(SNAT)"></a>POSTROUTING(SNAT)</h3><p>假设这个包被我们的规则放过去了，也就是ACCEPT了，它将进入POSTROUTING部分， 注意！这里我注意到一个细节问题，也就是上面的图中数据包过了FORWARD链之后直接进入了POSTROUITNG链，我觉得这中间缺少一个环节，也就是route box,对于转发的包来说，linux同样需要在选路（路由）之后才能将它送出，这个图却没有标明这一点，我认为它是在过了route box之后才进入的POSTROUITNG，当然了，这对于我们讨论iptables的过滤转发来说不是很重要，只是我觉得流程上有这个问题，还是要说明 一下。</p>
<p>同样的，我们在这里从名字就可以看出，这个POSTROUTING链应该是路由之后的一个链，也就是这个包要送出这台Linux的 最后一个环节了，这也是极其重要的一个环节！！这个时候linux已经完成(has done.._)了对这个包的路由（选路工作），已经找到了合适的接口送出这个包了，在这个链里面我们要进行重要的操作，就是被Linux称为 SNAT 的一个动作，修改源ip地址！为什么修改源ip地址？很多情况需要修改源地址阿，最常见的就是我们内网多台机器需要共享一个或几个公网ip访问internet,因为我们的内网地址是私有的，假如就让linux给路由出去，源地址也不变，这个包应该能访问到目的地，但是却回不来，因为 internet上的N多个路由节点不会转发私有地址的数据包，也就是说，不用合法ip,我们的数据包有去无回。有人会说：“既然是这样，我就不用私有 ip了，我自己分配自己合法的地址不行吗？那样包就会回来了吧？”答案是否定的，ip地址是ICANN来分配的，你的数据包或许能发到目的地，但是回来的 时候人家可不会转到你那里，internet上的路由器中的路由信息会把这个返回包送到那个合法的获得ip的地方去，你同样收不到,而你这种行为有可能被定义为一种ip欺骗，很多设备会把这样的包在接入端就给滤掉了，可能都到不了你要访问的那个服务器，呵呵。</p>
<p>那么Linux如何做SNAT呢？比如一个内网的10.1.1.11的pc访问202.2.2.2的一个web服务器，linux的内网接口10.1.1.1在收到这个包之后把原来的 PC的 ip10.1.1.11改变为60.1.1.1的合法地址然后送出，同时在自己的ip_conntrack表里面做一个记录,记住是内网的哪一个ip的哪 个端口访问的这个web服务器，自己把它的源地址改成多少了，端口改成多少了，以便这个web服务器返回数据包的时候linux将它准确的送回给发送请求 的这个pc.</p>
<p>大体的数据转发流程我们说完了,我们看看iptables使用什么样的参数来完成这些操作。</p>
<h2 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解"></a>概念理解</h2><p>在描述这些具体的操作之前，我还要说几个我对iptables的概念的理解（未必完全正确），这将有助于大家理解这些规则，以实现更精确的控制。</p>
<p>上文中我们提到过，对包的控制是由我们在不同的Chain(链)上面添加不同的规则来实现的，比如我们对过滤表（filter table）添加规则来执行对包的操控。那么既然叫链，一定就是一条或者多条规则组成的了，这时就有一个问题了，如果多个规则对同一种包进行了定义，会发生什么事情呢？ 在Chain中，所有的规则都是从上向下来执行的 ，也就是说，如果匹配了第一行，那么就按照第一行的规则执行，一行一行的往下找，直到找到 符合这个类型的包的规则为止。如果找了一遍没有找到符合这个包的规则怎么办呢？itpables里面有一个概念，就是 Policy ，也就是策略。一说这个东西大家可能就会觉得比较麻烦，什么策略阿，我对于它的理解就是所谓这个策略就是chain中的最后一条规则，也就是说如果找了一遍找不到符合处理这个包的规则，就按照policy来办。这样理解起来就容易多了。iptables 使用-P来设置Chain的策略。</p>
<p>好了，我们言归正传，来说说iptables到底怎样实现对包的控制。</p>
<h4 id="链操作"><a href="#链操作" class="headerlink" title="链操作"></a>链操作</h4><p>先介绍一下iptables如何操作链</p>
<p>对链的操作就那么几种：</p>
<ul>
<li>-I(插入)</li>
<li>-A(追加)</li>
<li>-R(替换)</li>
<li>-D（删除）</li>
<li>-L（列表显示）</li>
</ul>
<p>这里要说明的就是-I将会把规则放在第一行，-A将会放在最后一行。</p>
<p>比如我们要添加一个规则到filter表的FORWARD链：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#意思为：追加一个规则至filter表中的FORWARD链尾，允许（-j ACCEPT）源地址为10.1.1.11目的地址为202.1.1.1的数据包通过。其中-t后面跟的是表名，在-A后面跟Chain名，后面的小写的 -s为源地址，-d为目的地址，-j为处理方向。</span></span><br><span class="line">iptables -t filter -A FORWARD <span class="_">-s</span> 10.1.1.11 <span class="_">-d</span> 202.1.1.1 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#在iptables中，默认的表名就是filter，所以这里可以省略-t filter直接写成: </span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.11 <span class="_">-d</span> 202.1.1.1 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<h4 id="匹配参数"><a href="#匹配参数" class="headerlink" title="匹配参数"></a>匹配参数</h4><p>iptables中的匹配参数： 我们在这里就介绍几种常用的参数，详细地用法可以man iptables看它的联机文档，你会有意外的收获。</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-s</td>
<td style="text-align:left">匹配源地址</td>
</tr>
<tr>
<td style="text-align:left">-d</td>
<td style="text-align:left">匹配目的地址</td>
</tr>
<tr>
<td style="text-align:left">-p</td>
<td style="text-align:left">协议匹配</td>
</tr>
<tr>
<td style="text-align:left">-i</td>
<td style="text-align:left">入接口匹配</td>
</tr>
<tr>
<td style="text-align:left">-o</td>
<td style="text-align:left">出接口匹配</td>
</tr>
<tr>
<td style="text-align:left">–sport，–dport</td>
<td style="text-align:left">源和目的端口匹配</td>
</tr>
<tr>
<td style="text-align:left">-j</td>
<td style="text-align:left">跳转,也就是包的方向</td>
</tr>
<tr>
<td style="text-align:left">!</td>
<td style="text-align:left">取反</td>
</tr>
</tbody>
</table>
<p>其中还有一个!参数，使用!就是取反的意思。下面我们简单举几个例子介绍一下。</p>
<ul>
<li>-s 这个参数呢就是指定源地址的，如果使用这个参数也就是告诉netfilter，对于符合这样一个源地址的包怎么去处理，可以指定某一个单播ip地址，也可以指定一个网络，如果单个的ip地址其实隐含了一个32位的子网掩码，比如-s 10.1.1.11 其实就是-s 10.1.1.11/32，同样我们可以指定不同的掩码用以实现源网络地址的规则，比如一个C类地址我们可以用-s 10.1.1.0/24来指定。</li>
<li>-d参数与-s格式一样。</li>
<li>-i参数是指定入接口的网络接口，比如我仅仅允许从eth3接口过来的包通过FORWARD链，就可以这样指定iptables -A FORWARD -i eth3 -j ACCEPT</li>
<li>-o是出接口,与上同。</li>
</ul>
<p>我们下面用一些简单的实例来step by step看看iptables的具体配置方法。</p>
<h4 id="实例一：简单的nat路由器"><a href="#实例一：简单的nat路由器" class="headerlink" title="实例一：简单的nat路由器"></a>实例一：简单的nat路由器</h4><blockquote>
<p><strong>环境介绍</strong></p>
<ul>
<li>linux 2.4 +</li>
<li>2个网络接口</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>目的：实现内网中的节点（10.1.1.0/24）可控的访问internet。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先将Lan的节点pc的网关指向10.1.1.254。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#确定你的linux的ip配置无误，可以正确的ping通内外的地址。同时用route命令查看linux的本地路由表，确认指定了可用的ISP提供的默认网关。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打开linux的转发功能：</span></span><br><span class="line">sysctl net.ipv4.ip_forward=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#将FORWARD链的策略设置为DROP，这样做的目的是做到对内网ip的控制，你允许哪一个访问internet就可以增加一个规则，不在规则中的ip将无法访问internet.</span></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line"></span><br><span class="line"><span class="comment">#这条规则规定允许任何地址到任何地址的确认包和关联包通过。一定要加这一条，否则你只允许lan IP访问没有用，至于为什么，下面我们再详细说。</span></span><br><span class="line">iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#这条规则做了一个SNAT，也就是源地址转换，将来自10.1.1.0/24的地址转换为60.1.1.1</span></span><br><span class="line"><span class="comment">#(Deven：因为是让内网上网，因此对于代理服务器而言POSTROUTING（经过路由之后的包应该要把源地址改变为60.1.1.1，否则包无法返回）)</span></span><br><span class="line">iptables -t nat -A POSTROUTING <span class="_">-s</span> 10.1.1.0/24 -j SNAT --to 60.1.1.1</span><br><span class="line"><span class="comment">#有这几条规则，一个简单的nat路由器就实现了。这时你可以将允许访问的ip添加至FORWARD链，他们就能访问internet了。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#比如我想让10.1.1.9这个地址访问internet,那么你就加如下的命令就可以了。</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.9 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以精确控制他的访问地址,比如我就允许10.1.1.99访问3.3.3.3这个ip</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.99 <span class="_">-d</span> 3.3.3.3 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者只允许他们访问80端口。</span></span><br><span class="line">iptables -A FORWARD <span class="_">-s</span> 10.1.1.0/24 -p tcp --dport http -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#更多的控制可以自己灵活去做,或者查阅iptables的联机文档。</span></span><br></pre></td></tr></table></figure>
<h4 id="实例二：端口转发"><a href="#实例二：端口转发" class="headerlink" title="实例二：端口转发"></a>实例二：端口转发</h4><blockquote>
<p><strong>环境介绍</strong></p>
<ul>
<li>linux 2.4 +</li>
<li>2个网络接口</li>
<li>Lan口:10.1.1.254/24 eth0</li>
<li>Lan内web server: 10.1.1.1:80</li>
<li>Lan内ftp server: 10.1.1.2:21</li>
<li>Wan口:60.1.1.1/24 eth1</li>
<li>目的：对内部server进行端口转发实现internet用户访问内网服务器。</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#同样确认你的linux的各项配置正常，能够访问内外网。</span></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line">iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment">#也需要加入确认包和关联包的允许通过</span></span><br><span class="line"><span class="comment">#如果你要把访问60.1.1.1:80的数据包转发到Lan内web server,用下面的命令</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 60.1.1.1 -p tcp --dport 80 -j DNAT --to 10.1.1.1:80</span><br><span class="line"></span><br><span class="line"><span class="comment">#ftp服务也同样，命令如下：</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 60.1.1.1 -p tcp --dport 21 -j DNAT --to 10.1.1.2:21</span><br></pre></td></tr></table></figure>
<p>好了，命令完成了，端口转发也做完了，本例能不能转发呢？不能，为什么呢？我下面详细分析一下。</p>
<p>对于iptables好像往外访问的配置比较容易，而对内的转发似乎就有一些问题了，在一开始的时候我就先说了一些关于netfilter的流程问题，那么我就简单说说做了这些配置之后为什么有可能还不行呢？</p>
<p>能引起这个配置失败的原因有很多，我们一个个的来说：</p>
<p><strong>第一</strong> 本例中，我们的FORWARD策略是DROP,那么也就是说，没有符合规则的包将被丢弃，不管内到外还是外到内，我们在这里依然不讨论那个确认包和关联包的问题，我们不用考虑他的问题，下面我会详细说一下这个东西，那么如何让本例可以成功呢？加入下面的规则。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -A FORWARD <span class="_">-d</span> 10.1.1.1 -p tcp --dport 80 -j ACCEPT</span><br><span class="line">iptables -A FORWARD <span class="_">-d</span> 10.1.1.2 -p tcp --dport 21 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<p>有没有觉得有一些晕？为什么目的地址是10.xxx而不是60.xxx人家internet用户不是访问的60.xxx吗？呵呵，回到上面看看那个图吧，FORWARD链在什么位置上，它是在PREROUTING之后，也就是说当这个包到达FORWARD链的时候，目的地址已经变成10.xxx了，假如internet用户的请求是这样202.1.1.1:1333–&gt;60.1.1.1:80，在经过了我们的PREROUTING链之后将变成 202.1.1.1:1333–&gt;10.1.1.1:80,这个时候如果你设置一个目的地址为60.xxx的规则有用吗？呵呵，这是问题一。这个时候应该可以完成端口转发的访问了，但是有一些时候还是不行？为什么？看问题二。</p>
<p><strong>第二</strong> 内网server的ip配置问题，这里我们以web server为例说明一下（ftp情况有一些特殊，下面我们再详细讨论，说确认包和关联包的时候讨论这个问题），上面说到，有的时候可以访问了，有的时候却不行，就是这个web server的ip设置问题了，如果web server没有指定默认的网关，那么在作了上面的配置之后，web server会收到internet的请求，但是，他不知道往哪里回啊，人家的本地路由表不知道你那个internet的ip,202.1.1.1该怎么走。如果你使用截包工具在web server上面察看，你会发现server收到了来自202.1.1.1:1333–&gt;10.1.1.1:80的请求，由于你没有给web server配置默认网关，它不知道怎么回去，所以就出现了不通的情况。怎么办呢？两个解决方法：</p>
<p>一就是给这个server配置一个默认网关，当然要指向这个配置端口转发的linux,本例是10.1.1.254,配置好了，就一定能访问了。有一个疑问？难道不需要在FORWARD链上面设置一个允许web server的ip地址访问外网的规则吗？它的包能出去？答案是肯定的，能出去。因为我们那一条允许确认包与关联包的规则，否则它是出不去的。</p>
<p><strong>第二种方法</strong>，比较麻烦一些，但是对服务器来说这样似乎更安全一些。方法就是对这个包再作一次SNAT，也就是在POSTROUTING链上添加规则。命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING <span class="_">-d</span> 10.1.1.1 -p tcp --dport 80 -j SNAT --to 10.1.1.254</span><br></pre></td></tr></table></figure></p>
<p>ftp 的方法相同。这条命令不太好懂？？其实很简单，如果使用这条命令，那么你的web server不需要再设置默认网关，就能收到这个请求，只要他和linux的lan ip地址是能互访的（也就是说web server和Linux的Lan ip在一个广播域），我们在根据上面的netfilter流程图来分析这个包到底被我们怎么样了：</p>
<ul>
<li>首先一个请求202.1.1.1:1333–&gt; 60.1.1.1:80被linux收到了，进入PREROUTING；</li>
<li>发现一个规则iptables -t nat -A PREROUTING -d 60.1.1.1 -p tcp –dport 80 -j DNAT –to 10.1.1.1:80符合，好了，改你的目的地址，于是这个包变成了202.1.1.1:1333–&gt;10.1.1.1:80，继续往前走；</li>
<li>进入FORWARD链，okay,也有一条规则允许通过iptables -A FORWARD -d 10.1.1.1 -p tcp –dport 80 -j ACCEPT；</li>
<li>进入route box选路，找到合适的路径了，继续进入POSTROUTING链；</li>
<li>耶？又发现一个符合的规则iptables -t nat -A POSTROUTING -d 10.1.1.1 -p tcp –dport 80 -j SNAT –to 10.1.1.254,原来是一个SNAT,改你的源地址，于是这个包变成了10.1.1.254:xxxx–&gt;10.1.1.1:80。为什么用xxxx了，这里的端口是随机的，我也不知道会是什么。</li>
<li>而整个的两次变化的过程都会记录在linux的ip_conntrack中；</li>
<li>当web server收到这个包的时候，发现，原来是一个内网自己兄弟来的请求阿，又在一个广播域，不用找网关，把返回包直接扔给交换机了；</li>
<li>linux在收到返回包之后，会根据他的ip_conntrack中的条目进行两次变换，返回真正的internet用户，于是完成这一次的访问。</li>
</ul>
<p>看了上面的两个例子，不知道大家是否清楚了iptables的转发流程，希望对大家有所帮助。</p>
<h4 id="状态机制"><a href="#状态机制" class="headerlink" title="状态机制"></a>状态机制</h4><p>下面我们就说说我一直在上面提到的关于那个ESTABLISHED,RELATED的规则是怎么回事，到底有什么用处。</p>
<p>说这个东西就要简单说一下网络的数据通讯的方式，我们知道，网络的访问是双向的，也就是说一个Client与Server之间完成数据交换需要双方的发包与收包。在netfilter中，有几种状态，也就是new, established,related,invalid。</p>
<p>当一个客户端，在本文例一中，内网的一台机器访问外网，我们设置了规则允许他出去，但是没有设置允许回来的规则阿，怎么完成访问呢？这就是netfilter的 状态机制 ，当一个lan用户通过这个linux访问外网的时候，它发送了一个请求包，这个包的状态是new,当外网回包的时候他的状态就是established,所以，linux知道，哦，这个包是我的内网的一台机器发出去的应答包，他就放行了。</p>
<p>而外网试图对内发起一个新的连接的时候，他的状态是new,所以linux压根不去理会它。这就是我们为什么要加这一句的原因。</p>
<p>还有那个related,他是一个关联状态，什么会用到呢？tftp,ftp都会用到，因为他们的传输机制决定了，它不像http访问那样，Client_IP: port–&gt;server:80然后server:80–&gt;Client_IP:port，ftp使用tcp21建立连接，使用20端口发送数据，其中又有两种方式，一种主动active mode，一种被动passive mode。主动模式下，client使用port命令告诉server我用哪一个端口接受数据，然后server主动发起对这个端口的请求。被动模式下，server使用port命令告诉客户端，它用那个端口监听，然后客户端发起对他的数据传输，所以这对于一个防火墙来说就是比较麻烦的事情，因为有可能会有new状态的数据包，但是它又是合理的请求，这个时候就用到这个related状态了，他就是一种关联，在linux中，有个叫 ftp_conntrack的模块，它能识别port命令，然后对相应的端口进行放行。</p>
<p>一口气写了这么多东西，不知道质量如何，大家凑和着看吧，希望多多交流共同进步，我还是一个linux的初学者，难免很多谬误，希望高手赐教指正，以期不断进步。</p>
<h4 id="实用命令"><a href="#实用命令" class="headerlink" title="实用命令"></a>实用命令</h4><p>对了，还有几个在实际中比较实用（也比较受用:-)）的命令参数，写出来供大家参考<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">iptables -L -n</span><br><span class="line"><span class="comment">#这样的列表会跳过linux的domain lookup,有的时候使用iptables -L会比较慢，因为linux会尝试解析ip的域名，真是罗嗦，如果你的dns server比较不爽的话，iptables -L就会让你很不爽，加一个-n参数就好了。列表刷的就出来。当然了，如果你的linux就是做防火墙，建议把nameserver去掉，在 /etc/resolve.conf里面，因为有时候使用route命令也会比较慢列出来，很是不爽。</span></span><br><span class="line"></span><br><span class="line">iptables -L -v</span><br><span class="line"><span class="comment">#这个命令会显示链中规则的包和流量计数，嘿嘿，看看哪些小子用的流量那么多，用tc限了他。</span></span><br><span class="line"></span><br><span class="line">iptables -t nat -L -vn</span><br><span class="line"><span class="comment">#查看nat表中的规则。</span></span><br><span class="line"></span><br><span class="line">cat /proc/net/ip_conntrack</span><br><span class="line"><span class="comment">#查看目前的conntrack，可能会比较多哦，最好加一个|grep "关键字"，看看你感兴趣的链接跟踪</span></span><br><span class="line"></span><br><span class="line">wc <span class="_">-l</span> /proc/net/ip_conntrack</span><br><span class="line"><span class="comment">#看看总链接有多少条。</span></span><br><span class="line"></span><br><span class="line">iptables-save &gt;/etc/iptables</span><br><span class="line"><span class="comment">#把当前的所有链备份一下，之所以放到/etc下面叫iptables，因为这样重起机器的时候会自动加载所有的链，经常地备份一下吧，否则如果链多，万一掉电重启，你还是会比较痛苦。</span></span><br></pre></td></tr></table></figure></p>
<p><strong>转发</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#之前因为一个网段被封了，因此通过iptables做转发：</span></span><br><span class="line"><span class="comment">#代理服务器WAN IP：111.**.**.219，LAN IP：192.168.0.219</span></span><br><span class="line"><span class="comment">#内网服务器IP：192.168.0.41</span></span><br><span class="line"><span class="comment">#1.在代理服务器打开转发功能（sysctl.conf）</span></span><br><span class="line"><span class="comment">#2.添加以下规则</span></span><br><span class="line">iptables -t nat -A PREROUTING <span class="_">-d</span> 111.**.**.219 -p tcp --dport 9999 -j DNAT --to-destination 192.168.0.41:9999</span><br><span class="line">iptables -t nat -A POSTROUTING <span class="_">-d</span> 192.168.0.41 -p tcp --dport 9999 -j SNAT --to-source 192.168.0.219</span><br></pre></td></tr></table></figure></p>
<p> 原文：<a href="http://wwdhks.blog.51cto.com/839773/1154032" target="_blank" rel="external">http://wwdhks.blog.51cto.com/839773/1154032</a></p>
]]></content>
    </entry>
    
  
  
</search>
